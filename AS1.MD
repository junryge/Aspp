# AMHS 예측 모델 개발 보고서

**작성일:** 2025년 12월

---

## 1. 예측 모델에 사용한 데이터

### 데이터 소스

| 구분 | 대상 | 주요 데이터 |
|------|------|-------------|
| **FAB** | M14_Q | CURRENT_M16A_3F_JOB_2 (예측대상), Storage 사용률, CMD 처리량, Inflow/Outflow Job 수 |
| **HUBROOM** | HUB_M16 | HUBROOMTOTAL (가용 공간), Queue 대기량, OHT 가동률, 평균 처리시간, 지연 건수 |

### 데이터 규모

- **수집 기간:** 2025년 1월 ~ 12월 (12개월)
- **데이터 주기:** 1분 단위 수집
- **입력 컬럼:** 32개 (V7 기존 19개 + V8 신규 13개)
- **생성 Feature:** 222개 (통계량 기반 파생)

---

## 2. 어떤 모델을 사용해서 어떤 과정으로 만들었나

### 2-1. 왜 XGBoost를 사용했는가?

#### 딥러닝 모델 (LSTM, GRU, Spike) 사용하지 않은 이유

> ⚠️ **문제점: 급증 상황 사전감지 실패**
> - M14: **1700 이상** 급증 → 사전감지 못함
> - HUB: **300 이상** 급증 → 사전감지 못함

**원인:**
- 급증 데이터가 전체의 5% 미만 (클래스 불균형)
- 딥러닝은 다수 클래스(정상)에 과적합
- 가중치 조절이 어렵고 학습 시간이 오래 걸림

#### 여러 모델 비교 실험 결과

| 모델 | 전체예측 | 사전감지 | 급증감지 | 문제점 |
|------|----------|----------|----------|--------|
| LSTM | 60% | 18% | M14 1700 ✗ | 급증 미감지 |
| GRU | 58% | 15% | HUB 300 ✗ | 급증 미감지 |
| Spike (극단감지) | 55% | 20% | 일부만 감지 | 불안정 |
| 앙상블 (RF+GB) | 80% | 15% | 일부만 감지 | 사전감지↓ |
| **XGBoost** | **90%** | **80%+** | **✓ 감지 성공** | **최종 선정** |

---

### XGBoost 핵심 원리 및 수식

#### ■ XGBoost = eXtreme Gradient Boosting

> **핵심 개념:** 여러 개의 약한 학습기(Decision Tree)를 순차적으로 학습시켜 이전 모델의 오차를 보완하는 앙상블 기법

#### ■ 목적 함수 (Objective Function)

$$Obj = \sum_{i} L(y_i, \hat{y}_i) + \sum_{k} \Omega(f_k)$$

- $L(y_i, \hat{y}_i)$ : 예측값과 실제값의 오차 (손실)
- $\Omega(f_k)$ : 모델 복잡도 제어 (과적합 방지)

#### ■ 정규화 항 (Regularization)

$$\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j} w_j^2$$

- $\gamma$ (gamma): 리프 노드 수 페널티 → 트리 단순화
- $\lambda$ (lambda): 가중치 크기 페널티 → 과적합 방지
- $T$ : 리프 노드 개수
- $w_j$ : 리프 노드의 가중치

#### ■ Gradient Boosting 업데이트

$$\hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + \eta \cdot f_t(x_i)$$

- $\eta$ (learning_rate): 학습률 (0.03 사용)
- $f_t$ : t번째 트리의 예측값
- 매 단계마다 이전 오차를 줄이는 방향으로 학습

#### ■ 본 프로젝트 적용 (급증 감지 핵심)

$$L = \sum_{i} w_i \cdot (y_i - \hat{y}_i)^2$$

| 샘플 유형 | 가중치 (wᵢ) |
|-----------|-------------|
| 일반 샘플 | 1.0 |
| 급증 샘플 (M14 1700+, HUB 300+) | **3 ~ 5** |

> 🔥 **급증 샘플의 오차에 3~5배 페널티 부여 → 모델이 급증 패턴을 더 중요하게 학습**

#### 참고 레퍼런스

- Chen & Guestrin (2016) *"XGBoost: A Scalable Tree Boosting System"*, KDD
- Friedman (2001) *"Greedy Function Approximation: A Gradient Boosting Machine"*

---

### 2-2. 어떤 과정으로 모델을 만들었나?

| 단계 | 내용 |
|------|------|
| **STEP 1** 데이터 수집 | 2025년 1월 ~ 12월 (12개월), FAB 운영 시스템 + HUBROOM 모니터링 시스템 |
| **STEP 2** 전처리 | 30분 시퀀스 → 222개 Feature 생성 (mean, std, slope 등) |
| **STEP 3** 모델 학습 | V100 GPU x 2, 약 3시간, n_estimators=250, max_depth=8, learning_rate=0.03 |
| **STEP 4** 핵심 적용 | 급증 샘플(M14 1700+, HUB 300+)에 **3~5배 가중치** 적용 |

---

## 3. 운영 데이터 적용 결과

### 3-1. 현재 성능 (V8 기준)

| 적용 대상 | 전체 예측 정확도 | 사전 감지율 | 예측 시점 |
|-----------|------------------|-------------|-----------|
| M14_Q (FAB) | 90% | 80% | 10분 전 |
| HUB_M16 | 90% | 89% | 10분 전 |

> ✅ **모델 적용 시 약 70~80% 정도로 장애를 사전 예측 가능**
> 
> ✅ **M14 1700 이상, HUB 300 이상 급증 상황 사전감지 성공**

---

### 3-2. V1부터 V8까지 점점 좋아지는 과정

#### M14_Q 모델 발전 과정

| 버전 | 날짜 | 전체 | 사전감지 | 변경 내용 |
|------|------|------|----------|-----------|
| V1 | 2025-08-07 | 60% | 0% | 초기 모델 (트렌드 잡기) |
| V2 | 2025-08-11 | 70% | 10% | 예측값 관계성 개선 |
| V4 | 2025-08-14 | 80% | 8% | 앙상블 추가 |
| V5 | 2025-08-18 | 80% | 15% | 앙상블 확장 (운영 반영) |
| V6 | 2025-09-18 | 80% | 18% | 딥러닝 시도 (1700 감지 실패) |
| V7 | 2025-09-24 | 80% | 45% | **XGBoost 전환 테스트** |
| V8 | 2025-10-01 | 80% | 70% | XGBoost 본격 적용 (운영 반영) |
| **V8.2** | 2025-11-07 | **90%** | **80%** | 사전감지 컬럼 추가 **(현재 운영)** |

#### HUB_M16 모델 발전 과정

| 버전 | 날짜 | 전체 | 사전감지 | 변경 내용 |
|------|------|------|----------|-----------|
| V1 | 2025-08-07 | 40% | 20% | 초기 모델 |
| V2 | 2025-08-11 | 50% | 15% | 딥러닝 수식화 (300 감지 실패) |
| V4 | 2025-08-14 | 50% | 15% | 앙상블 추가 |
| V5 | 2025-09-24 | 60% | 25% | 극단 감지 기법 적용 |
| V6 | 2025-10-18 | 60% | 50% | **XGBoost 전환** |
| V6.2 | 2025-10-23 | 75% | 68% | 10/15/25분 다중 모델 |
| **V8.0** | 2025-11-17 | **90%** | **89%** | 수치형+범주형 완성 **(현재 운영)** |

---

### 3-3. 핵심 개선 포인트

1. **V1~V6:** LSTM/GRU/앙상블 시도
   - 전체 예측은 향상되었으나, 급증(M14 1700+, HUB 300+) **사전감지 실패**

2. **V7:** XGBoost 전환
   - 가중치 적용으로 사전감지율 급상승 **(15% → 45~50%)**

3. **V8:** 가중치 강화 + Feature 추가
   - 최종 **80~89% 사전감지 달성**

---

## 4. 결론

| 항목 | 내용 |
|------|------|
| 딥러닝 한계 | LSTM, GRU, Spike는 급증 상황(M14 1700+, HUB 300+) 사전감지 실패 |
| XGBoost 해결 | 가중 손실함수(wᵢ=3~5)로 급증 패턴 학습 성공 |
| 개선 성과 | V1 → V8 사전감지율 **0% → 80~89%** 달성 |
| 현재 성능 | 운영 환경에서 약 **70~80%** 정확도로 장애 사전 예측 가능 |

---

*끝*