#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Step 2: TEST.CSVìš© RAG ì‹œìŠ¤í…œ ì‹¤í–‰
"""

import os
os.environ['USE_TF'] = '0'
os.environ['USE_TORCH'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TRANSFORMERS_NO_TF'] = '1'

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from datetime import datetime
import re
import pickle
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TimeSeriesRAG:
    """TEST.CSVìš© RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, 
                 csv_path,
                 model_path,
                 db_persist_path="./chroma_db",
                 embedding_model_path="./embeddings/all-MiniLM-L6-v2"):
        
        self.csv_path = csv_path
        self.model_path = model_path
        self.db_persist_path = db_persist_path
        self.embedding_model_path = embedding_model_path
        
        # ì´ˆê¸°í™”
        self._initialize()
    
    def _initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        logger.info("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # 1. CSV ë°ì´í„° ë¡œë“œ
        self._load_csv_data()
        
        # 2. ë²¡í„°DB ë¡œë“œ
        self._load_vectorstore()
        
        # 3. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self._load_embedding_model()
        
        # 4. LLM ì„¤ì •
        self._setup_llm()
        
        logger.info("ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _load_csv_data(self):
        """CSV ë°ì´í„° ë¡œë“œ"""
        logger.info(f"CSV ë¡œë“œ: {self.csv_path}")
        
        df = pd.read_csv(self.csv_path, encoding='utf-8')
        self.df = df[['í˜„ì¬ì‹œê°„', 'ì‹¤ì œì‹œì ', 'ì‹¤ì œê°’', 
                     'ë³´ì •ì˜ˆì¸¡', 'ì˜¤ì°¨', 'M14AM14B', 'M14BM14A', 
                     'queue_gap', 'TRANSPORT']].copy()
        
        self.df['í˜„ì¬ì‹œê°„'] = pd.to_datetime(self.df['í˜„ì¬ì‹œê°„'])
        self.df['ì‹¤ì œì‹œì '] = pd.to_datetime(self.df['ì‹¤ì œì‹œì '])
        self.df.set_index('í˜„ì¬ì‹œê°„', inplace=True)
        
        logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê°œ")
    
    def _load_vectorstore(self):
        """ë²¡í„°DB ë¡œë“œ"""
        db_file = os.path.join(self.db_persist_path, 'vectordb.pkl')
        
        if not os.path.exists(db_file):
            raise FileNotFoundError(
                f"ë²¡í„°DBê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë²¡í„°DB_TEST.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\n"
                f"ê²½ë¡œ: {db_file}"
            )
        
        logger.info(f"ë²¡í„°DB ë¡œë“œ ì¤‘: {db_file}")
        
        with open(db_file, 'rb') as f:
            db_data = pickle.load(f)
        
        self.documents = db_data['documents']
        self.embeddings = db_data['embeddings']
        
        logger.info(f"ë²¡í„°DB ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
    
    def _load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {self.embedding_model_path}")
        
        try:
            import torch
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        except:
            device = 'cpu'
        
        logger.info(f"ë””ë°”ì´ìŠ¤: {device}")
        
        from sentence_transformers import SentenceTransformer
        self.embedding_model = SentenceTransformer(self.embedding_model_path)
        if device == 'cuda':
            self.embedding_model = self.embedding_model.to('cuda')
        
        logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def _setup_llm(self):
        """LLM ì„¤ì •"""
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {self.model_path}")
        
        logger.info(f"LLM ë¡œë“œ: {self.model_path}")
        
        from llama_cpp import Llama
        
        self.llm = Llama(
            model_path=self.model_path,
            n_ctx=4096,  # ì»¨í…ìŠ¤íŠ¸ ëŠ˜ë¦¼
            n_batch=512,  # ë°°ì¹˜ ëŠ˜ë¦¼
            n_gpu_layers=-1,  # ì „ì²´ GPU
            n_threads=4,
            temperature=0.7,
            max_tokens=512,  # í¬ê²Œ ëŠ˜ë¦¼
            verbose=True
        )
        
        logger.info("LLM ë¡œë“œ ì™„ë£Œ")
    
    def _cosine_similarity(self, a, b):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    def _search_similar(self, query, k=5):
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        query_embedding = self.embedding_model.encode([query])[0]
        
        similarities = []
        for i, doc_embedding in enumerate(self.embeddings):
            sim = self._cosine_similarity(query_embedding, doc_embedding)
            similarities.append((i, sim))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_k = similarities[:k]
        
        results = []
        for idx, score in top_k:
            results.append(self.documents[idx]['content'])
        
        return results
    
    def _call_llm(self, prompt, max_tokens=300):
        """LLM í˜¸ì¶œ"""
        print(f"ğŸ’¬ í† í° ìƒì„± ì‹œì‘... (ìµœëŒ€ {max_tokens}ê°œ)")
        response = self.llm(
            prompt, 
            max_tokens=max_tokens, 
            temperature=0.7,  # ë” ì°½ì˜ì ìœ¼ë¡œ
            top_p=0.95,
            repeat_penalty=1.15,
            stop=["ì§ˆë¬¸:", "###"]  # stop í† í° ìµœì†Œí™”
        )
        result = response['choices'][0]['text'].strip()
        print(f"âœ… ìƒì„± ì™„ë£Œ ({len(result)}ì)")
        return result
    
    def get_data_at_time(self, datetime_str):
        """íŠ¹ì • ì‹œê°„ ë°ì´í„° ì¡°íšŒ"""
        try:
            dt = pd.to_datetime(datetime_str)
            
            if dt in self.df.index:
                data = self.df.loc[dt]
                return f"""
ğŸ“Š {dt} ë°ì´í„°:
- ì‹¤ì œê°’: {data['ì‹¤ì œê°’']:.1f}
- ì˜ˆì¸¡ê°’: {data['ë³´ì •ì˜ˆì¸¡']:.1f}  
- ì˜¤ì°¨: {data['ì˜¤ì°¨']:.1f}
- M14AM14B: {data['M14AM14B']}
- M14BM14A: {data['M14BM14A']}
- queue_gap: {data['queue_gap']}
- TRANSPORT: {data['TRANSPORT']}
                """
            else:
                nearest_idx = self.df.index.get_indexer([dt], method='nearest')[0]
                nearest_time = self.df.index[nearest_idx]
                data = self.df.iloc[nearest_idx]
                
                return f"""
ğŸ“Š ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„: {nearest_time}
- ì‹¤ì œê°’: {data['ì‹¤ì œê°’']:.1f}
- ì˜ˆì¸¡ê°’: {data['ë³´ì •ì˜ˆì¸¡']:.1f}
- ì˜¤ì°¨: {data['ì˜¤ì°¨']:.1f}
- M14AM14B: {data['M14AM14B']}
- M14BM14A: {data['M14BM14A']}
                """
        except Exception as e:
            return f"ì˜¤ë¥˜: {e}"
    
    def analyze_query(self, query):
        """ì¿¼ë¦¬ ë¶„ì„ ë° ì²˜ë¦¬"""
        # ì‹œê°„ íŒ¨í„´ ì¶”ì¶œ
        time_pattern = r'(\d{4}-\d{2}-\d{2}\s+\d{1,2}:\d{2})'
        time_match = re.search(time_pattern, query)
        
        # ì‹¤ì œê°’/ì˜ˆì¸¡ê°’ ì¡°íšŒ
        if time_match and ("ì‹¤ì œê°’" in query or "ì˜ˆì¸¡ê°’" in query or "ë°ì´í„°" in query):
            return self.get_data_at_time(time_match.group(1))
        
        # ì¼ë°˜ RAG ì¿¼ë¦¬
        else:
            print("ğŸ” ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
            similar_docs = self._search_similar(query, k=5)  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
            context = "\n---\n".join(similar_docs)
            
            print("ğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘...")
            # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¤ìŒì€ ë°˜ë„ì²´ ì œì¡° ë°ì´í„°ì…ë‹ˆë‹¤.

ë°ì´í„°:
{context}

ì§ˆë¬¸: {query}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì‹œê°„, ê°’, íŠ¹ì§•ì„ í¬í•¨í•´ì„œ 2-3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.

ë‹µë³€:"""
            
            result = self._call_llm(prompt, max_tokens=300)
            
            return result

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    CSV_PATH = "./TEST.CSV"
    MODEL_PATH = "./models/Qwen3-Coder-30B-A3B-Instruct-Q3_K_M.gguf"
    DB_PATH = "./chroma_db"
    
    print("="*60)
    print("TEST.CSV RAG ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*60)
    
    rag = TimeSeriesRAG(
        csv_path=CSV_PATH,
        model_path=MODEL_PATH,
        db_persist_path=DB_PATH
    )
    
    # ëŒ€í™” ë£¨í”„
    while True:
        query = input("\nì§ˆë¬¸ (ì¢…ë£Œ: quit): ").strip()
        
        if query.lower() in ['quit', 'exit']:
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if query:
            print("\nì²˜ë¦¬ ì¤‘...")
            answer = rag.analyze_query(query)
            print(f"\n{answer}")

if __name__ == "__main__":
    main()