#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Step 2: RAG ì‹œìŠ¤í…œ ì‹¤í–‰ (ë²¡í„°DB ë¡œë“œí•˜ì—¬ ì‚¬ìš©)
"""

import pandas as pd
import numpy as np
from datetime import datetime
import os
import re
import json
import logging
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import LlamaCpp
from langchain.chains import RetrievalQA
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TimeSeriesRAG:
    """ë²¡í„°DBë¥¼ ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•˜ëŠ” RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, 
                 csv_path,
                 model_path,
                 db_persist_path="./chroma_db",
                 embedding_model_path="./embeddings/all-MiniLM-L6-v2"):
        
        self.csv_path = csv_path
        self.model_path = model_path
        self.db_persist_path = db_persist_path
        self.embedding_model_path = embedding_model_path
        
        # ì´ˆê¸°í™”
        self._initialize()
    
    def _initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        logger.info("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # 1. CSV ë°ì´í„° ë¡œë“œ (ì¿¼ë¦¬ìš©)
        self._load_csv_data()
        
        # 2. ê¸°ì¡´ ë²¡í„°DB ë¡œë“œ
        self._load_vectorstore()
        
        # 3. LLM ì„¤ì •
        self._setup_llm()
        
        # 4. RAG ì²´ì¸ ì„¤ì •
        self._setup_rag_chain()
        
        logger.info("ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _load_csv_data(self):
        """CSV ë°ì´í„° ë¡œë“œ"""
        logger.info(f"CSV ë¡œë“œ: {self.csv_path}")
        
        df = pd.read_csv(self.csv_path)
        self.df = df[['ë‚ ì§œ', 'íƒ€ì¼“ë‚ ì§œ', 'ì‹¤ì œê°’', 
                     'ExtraTrees_ì˜ˆì¸¡', 'ExtraTrees_ì˜¤ì°¨']].copy()
        
        self.df['ë‚ ì§œ'] = pd.to_datetime(self.df['ë‚ ì§œ'])
        self.df['íƒ€ì¼“ë‚ ì§œ'] = pd.to_datetime(self.df['íƒ€ì¼“ë‚ ì§œ'])
        self.df.set_index('ë‚ ì§œ', inplace=True)
        
        logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê°œ")
    
    def _load_vectorstore(self):
        """ê¸°ì¡´ ë²¡í„°DB ë¡œë“œ"""
        if not os.path.exists(self.db_persist_path):
            raise FileNotFoundError(
                f"ë²¡í„°DBê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € step1_create_vectordb.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\n"
                f"ê²½ë¡œ: {self.db_persist_path}"
            )
        
        logger.info(f"ë²¡í„°DB ë¡œë“œ ì¤‘: {self.db_persist_path}")
        
        # ì„ë² ë”© ëª¨ë¸
        embeddings = HuggingFaceEmbeddings(
            model_name=self.embedding_model_path,
            model_kwargs={'device': 'cuda'}
        )
        
        # ChromaDB ë¡œë“œ
        self.vector_store = Chroma(
            persist_directory=self.db_persist_path,
            embedding_function=embeddings,
            collection_name="timeseries_data"
        )
        
        logger.info("ë²¡í„°DB ë¡œë“œ ì™„ë£Œ")
    
    def _setup_llm(self):
        """LLM ì„¤ì •"""
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {self.model_path}")
        
        logger.info(f"LLM ë¡œë“œ: {self.model_path}")
        
        # ì½œë°± ì„¤ì • (ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥)
        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
        
        self.llm = LlamaCpp(
            model_path=self.model_path,
            n_ctx=4096,
            n_batch=512,
            n_gpu_layers=35,  # GPU ì‚¬ìš©
            temperature=0.3,
            max_tokens=1000,
            callback_manager=callback_manager,
            verbose=False
        )
        
        logger.info("LLM ë¡œë“œ ì™„ë£Œ")
    
    def _setup_rag_chain(self):
        """RAG ì²´ì¸ ì„¤ì •"""
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vector_store.as_retriever(
                search_kwargs={"k": 5}
            ),
            return_source_documents=True
        )
    
    def get_data_at_time(self, datetime_str):
        """íŠ¹ì • ì‹œê°„ ë°ì´í„° ì¡°íšŒ"""
        try:
            dt = pd.to_datetime(datetime_str)
            
            if dt in self.df.index:
                data = self.df.loc[dt]
                return f"""
ğŸ“Š {dt} ë°ì´í„°:
- ì‹¤ì œê°’: {data['ì‹¤ì œê°’']:.1f}
- ì˜ˆì¸¡ê°’: {data['ExtraTrees_ì˜ˆì¸¡']:.1f}  
- ì˜¤ì°¨: {data['ExtraTrees_ì˜¤ì°¨']:.1f}
                """
            else:
                nearest_idx = self.df.index.get_indexer([dt], method='nearest')[0]
                nearest_time = self.df.index[nearest_idx]
                data = self.df.iloc[nearest_idx]
                
                return f"""
ğŸ“Š ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„: {nearest_time}
- ì‹¤ì œê°’: {data['ì‹¤ì œê°’']:.1f}
- ì˜ˆì¸¡ê°’: {data['ExtraTrees_ì˜ˆì¸¡']:.1f}
- ì˜¤ì°¨: {data['ExtraTrees_ì˜¤ì°¨']:.1f}
                """
        except Exception as e:
            return f"ì˜¤ë¥˜: {e}"
    
    def get_sequence_before(self, datetime_str, length=280):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ì¡°íšŒ"""
        try:
            dt = pd.to_datetime(datetime_str)
            end_idx = self.df.index.get_indexer([dt], method='nearest')[0]
            start_idx = max(0, end_idx - length + 1)
            
            sequence = self.df.iloc[start_idx:end_idx + 1]
            
            return f"""
ğŸ“ˆ ì‹œí€€ìŠ¤ ë¶„ì„ ({len(sequence)}ê°œ):
ê¸°ê°„: {sequence.index[0]} ~ {sequence.index[-1]}

ì‹¤ì œê°’:
- í‰ê· : {sequence['ì‹¤ì œê°’'].mean():.2f}
- ìµœëŒ€: {sequence['ì‹¤ì œê°’'].max():.1f}  
- ìµœì†Œ: {sequence['ì‹¤ì œê°’'].min():.1f}
- í‘œì¤€í¸ì°¨: {sequence['ì‹¤ì œê°’'].std():.2f}

ì˜ˆì¸¡ ì„±ëŠ¥:
- í‰ê·  ì˜¤ì°¨: {sequence['ExtraTrees_ì˜¤ì°¨'].mean():.2f}
- RMSE: {np.sqrt((sequence['ExtraTrees_ì˜¤ì°¨']**2).mean()):.2f}

ìµœê·¼ 5ê°œ:
{sequence.tail(5)[['ì‹¤ì œê°’', 'ExtraTrees_ì˜ˆì¸¡']].to_string()}
            """
        except Exception as e:
            return f"ì˜¤ë¥˜: {e}"
    
    def analyze_query(self, query):
        """ì¿¼ë¦¬ ë¶„ì„ ë° ì²˜ë¦¬"""
        # ì‹œê°„ íŒ¨í„´ ì¶”ì¶œ
        time_pattern = r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})'
        time_match = re.search(time_pattern, query)
        
        # ì‹¤ì œê°’/ì˜ˆì¸¡ê°’ ì¡°íšŒ
        if time_match and ("ì‹¤ì œê°’" in query or "ì˜ˆì¸¡ê°’" in query):
            return self.get_data_at_time(time_match.group(1))
        
        # ì‹œí€€ìŠ¤ ë¶„ì„
        elif time_match and ("ì‹œí€€ìŠ¤" in query or "280" in query or "ì„¤ëª…" in query):
            sequence_info = self.get_sequence_before(time_match.group(1))
            
            # LLM ì„¤ëª… ì¶”ê°€
            prompt = f"{sequence_info}\n\nìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…:"
            result = self.qa_chain({"query": prompt})
            
            return f"{sequence_info}\n\nğŸ’¡ ì„¤ëª…:\n{result['result']}"
        
        # ì¼ë°˜ RAG ì¿¼ë¦¬
        else:
            result = self.qa_chain({"query": query})
            return result['result']

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    # ì„¤ì •
    CSV_PATH = "./CSVíŒŒì¼.csv"
    MODEL_PATH = "./models/Qwen3-Coder-30B-A3B-Instruct-Q3_K_M.gguf"
    DB_PATH = "./chroma_db"
    
    # RAG ì‹œìŠ¤í…œ ì‹œì‘
    print("="*60)
    print("TimeSeriesRAG ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*60)
    
    rag = TimeSeriesRAG(
        csv_path=CSV_PATH,
        model_path=MODEL_PATH,
        db_persist_path=DB_PATH
    )
    
    # ëŒ€í™” ë£¨í”„
    while True:
        query = input("\nì§ˆë¬¸ (ì¢…ë£Œ: quit): ").strip()
        
        if query.lower() in ['quit', 'exit']:
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if query:
            print("\nì²˜ë¦¬ ì¤‘...")
            answer = rag.analyze_query(query)
            print(f"\n{answer}")

if __name__ == "__main__":
    main()