#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¡œê·¸í”„ë ˆì†Œ ë§¤ë‰´ì–¼ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ
"""

import os
os.environ['USE_TF'] = '0'
os.environ['USE_TORCH'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TRANSFORMERS_NO_TF'] = '1'

import warnings
warnings.filterwarnings('ignore')

import pickle
import numpy as np
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LogpressoRAG:
    """ë¡œê·¸í”„ë ˆì†Œ ë§¤ë‰´ì–¼ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self,
                 model_path,
                 db_path="./logpresso_db",
                 embedding_model_path="./embeddings/all-MiniLM-L6-v2"):
        
        self.model_path = model_path
        self.db_path = db_path
        self.embedding_model_path = embedding_model_path
        
        self._initialize()
    
    def _initialize(self):
        """ì´ˆê¸°í™”"""
        logger.info("ë¡œê·¸í”„ë ˆì†Œ RAG ì´ˆê¸°í™” ì¤‘...")
        
        # 1. ë²¡í„°DB ë¡œë“œ
        self._load_vectorstore()
        
        # 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self._load_embedding_model()
        
        # 3. LLM ì„¤ì •
        self._setup_llm()
        
        logger.info("ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _load_vectorstore(self):
        """ë²¡í„°DB ë¡œë“œ"""
        db_file = os.path.join(self.db_path, 'vectordb.pkl')
        
        if not os.path.exists(db_file):
            raise FileNotFoundError(
                f"ë²¡í„°DB ì—†ìŒ. ë¨¼ì € ë¡œê·¸í”„ë ˆì†Œ_ë²¡í„°DBìƒì„±.py ì‹¤í–‰\n"
                f"ê²½ë¡œ: {db_file}"
            )
        
        logger.info(f"ë²¡í„°DB ë¡œë“œ: {db_file}")
        
        with open(db_file, 'rb') as f:
            db_data = pickle.load(f)
        
        self.documents = db_data['documents']
        self.embeddings = db_data['embeddings']
        
        logger.info(f"ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ")
    
    def _load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {self.embedding_model_path}")
        
        try:
            import torch
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        except:
            device = 'cpu'
        
        from sentence_transformers import SentenceTransformer
        self.embedding_model = SentenceTransformer(self.embedding_model_path)
        if device == 'cuda':
            self.embedding_model = self.embedding_model.to('cuda')
        
        logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def _setup_llm(self):
        """LLM ì„¤ì •"""
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {self.model_path}")
        
        logger.info(f"LLM ë¡œë“œ: {self.model_path}")
        
        from llama_cpp import Llama
        
        self.llm = Llama(
            model_path=self.model_path,
            n_ctx=2048,       # MD ë¬¸ì„œëŠ” ê¸¸ ìˆ˜ ìˆìŒ
            n_batch=256,
            n_gpu_layers=30,
            n_threads=2,
            verbose=False
        )
        
        logger.info("LLM ë¡œë“œ ì™„ë£Œ")
    
    def _cosine_similarity(self, a, b):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    def _search_similar(self, query, k=3):
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        query_embedding = self.embedding_model.encode([query])[0]
        
        similarities = []
        for i, doc_embedding in enumerate(self.embeddings):
            sim = self._cosine_similarity(query_embedding, doc_embedding)
            similarities.append((i, sim, self.documents[i]['content']))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ kê°œ ë°˜í™˜
        results = []
        for idx, score, content in similarities[:k]:
            # ë„ˆë¬´ ê¸´ ë¬¸ì„œëŠ” ì•ë¶€ë¶„ë§Œ
            truncated = content[:1000] if len(content) > 1000 else content
            results.append(truncated)
        
        return results
    
    def _call_llm(self, prompt, max_tokens=300):
        """LLM í˜¸ì¶œ"""
        print(f"ğŸ’¬ AI ë‹µë³€ ìƒì„± ì¤‘... (ìµœëŒ€ {max_tokens} í† í°)")
        response = self.llm(
            prompt,
            max_tokens=max_tokens,
            temperature=0.3,  # ë§¤ë‰´ì–¼ì€ ì •í™•í•´ì•¼ í•˜ë¯€ë¡œ ë‚®ê²Œ
            top_p=0.9,
            repeat_penalty=1.2,
            stop=["ì§ˆë¬¸:", "###", "\n\nì§ˆë¬¸"]
        )
        result = response['choices'][0]['text'].strip()
        print(f"âœ… ìƒì„± ì™„ë£Œ ({len(result)}ì)")
        return result
    
    def ask(self, question):
        """ì§ˆë¬¸í•˜ê¸°"""
        print(f"\nğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
        
        # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        similar_docs = self._search_similar(question, k=3)
        context = "\n\n---\n\n".join(similar_docs)
        
        print(f"âœ… {len(similar_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
        
        # LLM í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¹ì‹ ì€ ë¡œê·¸í”„ë ˆì†Œ(Logpresso) ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}

ìœ„ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. 
ëª…ë ¹ì–´ ì‚¬ìš©ë²•ì´ë‚˜ ì˜ˆì œê°€ ìˆìœ¼ë©´ í¬í•¨í•´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

ë‹µë³€:"""
        
        # LLM ë‹µë³€ ìƒì„±
        answer = self._call_llm(prompt, max_tokens=400)
        
        return answer

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    MODEL_PATH = "Qwen3-Coder-30B-A3B-Instruct-Q3_K_M.gguf"
    
    print("="*60)
    print("ë¡œê·¸í”„ë ˆì†Œ ë§¤ë‰´ì–¼ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")
    print("="*60)
    
    rag = LogpressoRAG(model_path=MODEL_PATH)
    
    print("\nì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤! ë¡œê·¸í”„ë ˆì†Œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”.")
    print("ì˜ˆ: set ëª…ë ¹ì–´ ì‚¬ìš©ë²• ì•Œë ¤ì¤˜")
    print("ì˜ˆ: í”„ë¡œì‹œì €ë€ ë¬´ì—‡ì¸ê°€ìš”?")
    print("ì˜ˆ: ì¿¼ë¦¬ ë§¤ê°œë³€ìˆ˜ ì„¤ëª…í•´ì¤˜\n")
    
    # ëŒ€í™” ë£¨í”„
    while True:
        question = input("\nì§ˆë¬¸ (ì¢…ë£Œ: quit): ").strip()
        
        if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if question:
            answer = rag.ask(question)
            print(f"\nğŸ“ ë‹µë³€:\n{answer}")

if __name__ == "__main__":
    main()