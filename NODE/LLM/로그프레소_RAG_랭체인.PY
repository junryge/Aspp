#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¡œê·¸í”„ë ˆì†Œ RAG - ë­ì²´ì¸(LangChain) ë²„ì „
"""

import os
os.environ['USE_TF'] = '0'
os.environ['USE_TORCH'] = '1'

from langchain.text_splitter import MarkdownHeaderTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

print("="*60)
print("ë¡œê·¸í”„ë ˆì†Œ RAG - ë­ì²´ì¸ ë²„ì „")
print("="*60)

# 1. MD íŒŒì¼ ë¡œë“œ
print("\nğŸ“‚ MD íŒŒì¼ ë¡œë“œ ì¤‘...")
md_file = "ë¡œê·¸í”„ë ˆì†Œ_ë§¤ë‰´ì–¼.md"

with open(md_file, 'r', encoding='utf-8') as f:
    markdown_text = f.read()

print(f"âœ… íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(markdown_text):,}ì")

# 2. ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ì¤€ ë¶„í• 
print("\nâœ‚ï¸ ë§ˆí¬ë‹¤ìš´ ì²­í¬ ë¶„í•  ì¤‘...")

headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on
)

md_header_splits = markdown_splitter.split_text(markdown_text)

print(f"âœ… ì²­í¬ ìƒì„± ì™„ë£Œ: {len(md_header_splits)}ê°œ")

# 3. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
print("\nğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")

embeddings = HuggingFaceEmbeddings(
    model_name="./embeddings/all-MiniLM-L6-v2",
    model_kwargs={'device': 'cuda'},
    encode_kwargs={'normalize_embeddings': True}
)

print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# 4. ë²¡í„° ì €ì¥ì†Œ ìƒì„±
print("\nğŸ’¾ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")

vectorstore = Chroma.from_documents(
    documents=md_header_splits,
    embedding=embeddings,
    persist_directory="./logpresso_chroma_db"
)

vectorstore.persist()

print("âœ… ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ")

# 5. LLM ì„¤ì •
print("\nğŸ¤– LLM ë¡œë“œ ì¤‘...")

llm = LlamaCpp(
    model_path="Qwen3-Coder-30B-A3B-Instruct-Q3_K_M.gguf",
    n_ctx=2048,
    n_batch=256,
    n_gpu_layers=30,
    temperature=0.3,
    max_tokens=400,
    verbose=False
)

print("âœ… LLM ë¡œë“œ ì™„ë£Œ")

# 6. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt_template = """ë‹¹ì‹ ì€ ë¡œê·¸í”„ë ˆì†Œ(Logpresso) ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}

ìœ„ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ëª…ë ¹ì–´ ì‚¬ìš©ë²•ì´ë‚˜ ì˜ˆì œê°€ ìˆìœ¼ë©´ í¬í•¨í•´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

ë‹µë³€:"""

PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

# 7. QA ì²´ì¸ ìƒì„±
print("\nğŸ”— QA ì²´ì¸ ìƒì„± ì¤‘...")

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    chain_type_kwargs={"prompt": PROMPT},
    return_source_documents=True
)

print("âœ… QA ì²´ì¸ ìƒì„± ì™„ë£Œ")

# 8. ì§ˆì˜ì‘ë‹µ ë£¨í”„
print("\n" + "="*60)
print("ë¡œê·¸í”„ë ˆì†Œ ë§¤ë‰´ì–¼ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ (ë­ì²´ì¸)")
print("="*60)
print("\nì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤! ë¡œê·¸í”„ë ˆì†Œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”.")
print("ì˜ˆ: set ëª…ë ¹ì–´ ì‚¬ìš©ë²• ì•Œë ¤ì¤˜")
print("ì˜ˆ: í”„ë¡œì‹œì €ë€ ë¬´ì—‡ì¸ê°€ìš”?\n")

while True:
    question = input("\nì§ˆë¬¸ (ì¢…ë£Œ: quit): ").strip()
    
    if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
        print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break
    
    if question:
        print("\nğŸ” ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘...")
        
        result = qa_chain({"query": question})
        
        print(f"\nğŸ“ ë‹µë³€:\n{result['result']}")
        
        # ì°¸ê³ í•œ ë¬¸ì„œ ì¶œë ¥ (ì„ íƒ)
        print(f"\nğŸ“š ì°¸ê³  ë¬¸ì„œ: {len(result['source_documents'])}ê°œ")