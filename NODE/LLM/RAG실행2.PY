#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Step 2: RAG ì‹œìŠ¤í…œ ì‹¤í–‰ (pickle ë²¡í„°DB ì‚¬ìš©)
"""

import os
os.environ['USE_TF'] = '0'
os.environ['USE_TORCH'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TRANSFORMERS_NO_TF'] = '1'

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from datetime import datetime
import re
import pickle
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TimeSeriesRAG:
    """pickle ë²¡í„°DBë¥¼ ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•˜ëŠ” RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, 
                 csv_path,
                 model_path,
                 db_persist_path="./chroma_db",
                 embedding_model_path="./embeddings/all-MiniLM-L6-v2"):
        
        self.csv_path = csv_path
        self.model_path = model_path
        self.db_persist_path = db_persist_path
        self.embedding_model_path = embedding_model_path
        
        # ì´ˆê¸°í™”
        self._initialize()
    
    def _initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        logger.info("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # 1. CSV ë°ì´í„° ë¡œë“œ (ì¿¼ë¦¬ìš©)
        self._load_csv_data()
        
        # 2. ê¸°ì¡´ ë²¡í„°DB ë¡œë“œ
        self._load_vectorstore()
        
        # 3. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self._load_embedding_model()
        
        # 4. LLM ì„¤ì •
        self._setup_llm()
        
        logger.info("ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _load_csv_data(self):
        """CSV ë°ì´í„° ë¡œë“œ"""
        logger.info(f"CSV ë¡œë“œ: {self.csv_path}")
        
        df = pd.read_csv(self.csv_path, encoding='utf-8')
        self.df = df[['í˜„ì¬ì‹œê°„', 'ì‹¤ì œì‹œì ', 'ì‹¤ì œê°’', 
                     'ë³´ì •ì˜ˆì¸¡', 'ì˜¤ì°¨']].copy()
        
        self.df['í˜„ì¬ì‹œê°„'] = pd.to_datetime(self.df['í˜„ì¬ì‹œê°„'])
        self.df['ì‹¤ì œì‹œì '] = pd.to_datetime(self.df['ì‹¤ì œì‹œì '])
        self.df.set_index('í˜„ì¬ì‹œê°„', inplace=True)
        
        logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê°œ")
    
    def _load_vectorstore(self):
        """ê¸°ì¡´ ë²¡í„°DB ë¡œë“œ"""
        db_file = os.path.join(self.db_persist_path, 'vectordb.pkl')
        
        if not os.path.exists(db_file):
            raise FileNotFoundError(
                f"ë²¡í„°DBê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë²¡í„°DB.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\n"
                f"ê²½ë¡œ: {db_file}"
            )
        
        logger.info(f"ë²¡í„°DB ë¡œë“œ ì¤‘: {db_file}")
        
        with open(db_file, 'rb') as f:
            db_data = pickle.load(f)
        
        self.documents = db_data['documents']
        self.embeddings = db_data['embeddings']
        
        logger.info(f"ë²¡í„°DB ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
    
    def _load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {self.embedding_model_path}")
        
        try:
            import torch
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        except:
            device = 'cpu'
        
        logger.info(f"ë””ë°”ì´ìŠ¤: {device}")
        
        from sentence_transformers import SentenceTransformer
        self.embedding_model = SentenceTransformer(self.embedding_model_path)
        if device == 'cuda':
            self.embedding_model = self.embedding_model.to('cuda')
        
        logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def _setup_llm(self):
        """LLM ì„¤ì •"""
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {self.model_path}")
        
        logger.info(f"LLM ë¡œë“œ: {self.model_path}")
        
        from llama_cpp import Llama
        
        self.llm = Llama(
            model_path=self.model_path,
            n_ctx=4096,
            n_batch=512,
            n_gpu_layers=35,  # GPU ì‚¬ìš©
            temperature=0.3,
            max_tokens=1000,
            verbose=True  # ì§„í–‰ìƒí™© í‘œì‹œ
        )
        
        logger.info("LLM ë¡œë“œ ì™„ë£Œ")
    
    def _cosine_similarity(self, a, b):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    def _search_similar(self, query, k=5):
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedding_model.encode([query])[0]
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i, doc_embedding in enumerate(self.embeddings):
            sim = self._cosine_similarity(query_embedding, doc_embedding)
            similarities.append((i, sim))
        
        # ìƒìœ„ kê°œ ì„ íƒ
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_k = similarities[:k]
        
        results = []
        for idx, score in top_k:
            results.append(self.documents[idx]['content'])
        
        return results
    
    def _call_llm(self, prompt):
        """LLM í˜¸ì¶œ"""
        print("\nğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘... (30B ëª¨ë¸ì´ë¼ ì‹œê°„ì´ ì¢€ ê±¸ë¦½ë‹ˆë‹¤)")
        response = self.llm(prompt, max_tokens=1000, temperature=0.3)
        return response['choices'][0]['text']
    
    def get_data_at_time(self, datetime_str):
        """íŠ¹ì • ì‹œê°„ ë°ì´í„° ì¡°íšŒ"""
        try:
            dt = pd.to_datetime(datetime_str)
            
            if dt in self.df.index:
                data = self.df.loc[dt]
                return f"""
ğŸ“Š {dt} ë°ì´í„°:
- ì‹¤ì œê°’: {data['ì‹¤ì œê°’']:.1f}
- ì˜ˆì¸¡ê°’: {data['ë³´ì •ì˜ˆì¸¡']:.1f}  
- ì˜¤ì°¨: {data['ì˜¤ì°¨']:.1f}
                """
            else:
                nearest_idx = self.df.index.get_indexer([dt], method='nearest')[0]
                nearest_time = self.df.index[nearest_idx]
                data = self.df.iloc[nearest_idx]
                
                return f"""
ğŸ“Š ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„: {nearest_time}
- ì‹¤ì œê°’: {data['ì‹¤ì œê°’']:.1f}
- ì˜ˆì¸¡ê°’: {data['ë³´ì •ì˜ˆì¸¡']:.1f}
- ì˜¤ì°¨: {data['ì˜¤ì°¨']:.1f}
                """
        except Exception as e:
            return f"ì˜¤ë¥˜: {e}"
    
    def get_sequence_before(self, datetime_str, length=280):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ì¡°íšŒ"""
        try:
            dt = pd.to_datetime(datetime_str)
            end_idx = self.df.index.get_indexer([dt], method='nearest')[0]
            start_idx = max(0, end_idx - length + 1)
            
            sequence = self.df.iloc[start_idx:end_idx + 1]
            
            return f"""
ğŸ“ˆ ì‹œí€€ìŠ¤ ë¶„ì„ ({len(sequence)}ê°œ):
ê¸°ê°„: {sequence.index[0]} ~ {sequence.index[-1]}

ì‹¤ì œê°’:
- í‰ê· : {sequence['ì‹¤ì œê°’'].mean():.2f}
- ìµœëŒ€: {sequence['ì‹¤ì œê°’'].max():.1f}  
- ìµœì†Œ: {sequence['ì‹¤ì œê°’'].min():.1f}
- í‘œì¤€í¸ì°¨: {sequence['ì‹¤ì œê°’'].std():.2f}

ì˜ˆì¸¡ ì„±ëŠ¥:
- í‰ê·  ì˜¤ì°¨: {sequence['ì˜¤ì°¨'].mean():.2f}
- RMSE: {np.sqrt((sequence['ì˜¤ì°¨']**2).mean()):.2f}

ìµœê·¼ 5ê°œ:
{sequence.tail(5)[['ì‹¤ì œê°’', 'ë³´ì •ì˜ˆì¸¡']].to_string()}
            """
        except Exception as e:
            return f"ì˜¤ë¥˜: {e}"
    
    def analyze_query(self, query):
        """ì¿¼ë¦¬ ë¶„ì„ ë° ì²˜ë¦¬"""
        # ì‹œê°„ íŒ¨í„´ ì¶”ì¶œ
        time_pattern = r'(\d{4}-\d{2}-\d{2}\s+\d{1,2}:\d{2})'
        time_match = re.search(time_pattern, query)
        
        # ì‹¤ì œê°’/ì˜ˆì¸¡ê°’ ì¡°íšŒ
        if time_match and ("ì‹¤ì œê°’" in query or "ì˜ˆì¸¡ê°’" in query):
            return self.get_data_at_time(time_match.group(1))
        
        # ì‹œí€€ìŠ¤ ë¶„ì„
        elif time_match and ("ì‹œí€€ìŠ¤" in query or "280" in query or "ì„¤ëª…" in query):
            sequence_info = self.get_sequence_before(time_match.group(1))
            
            # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
            similar_docs = self._search_similar(query, k=5)
            context = "\n\n".join(similar_docs)
            
            # LLM ì„¤ëª… ì¶”ê°€
            prompt = f"{sequence_info}\n\nê´€ë ¨ ë°ì´í„°:\n{context}\n\nìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…:"
            result = self._call_llm(prompt)
            
            return f"{sequence_info}\n\nğŸ’¡ ì„¤ëª…:\n{result}"
        
        # ì¼ë°˜ RAG ì¿¼ë¦¬
        else:
            # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
            similar_docs = self._search_similar(query, k=5)
            context = "\n\n".join(similar_docs)
            
            # LLM ë‹µë³€ ìƒì„±
            prompt = f"ì§ˆë¬¸: {query}\n\nê´€ë ¨ ì •ë³´:\n{context}\n\në‹µë³€:"
            result = self._call_llm(prompt)
            
            return result

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    # ì„¤ì •
    CSV_PATH = "./WITH.CSV"
    MODEL_PATH = "./models/Qwen3-Coder-30B-A3B-Instruct-Q3_K_M.gguf"
    DB_PATH = "./chroma_db"
    
    # RAG ì‹œìŠ¤í…œ ì‹œì‘
    print("="*60)
    print("TimeSeriesRAG ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*60)
    
    rag = TimeSeriesRAG(
        csv_path=CSV_PATH,
        model_path=MODEL_PATH,
        db_persist_path=DB_PATH
    )
    
    # ëŒ€í™” ë£¨í”„
    while True:
        query = input("\nì§ˆë¬¸ (ì¢…ë£Œ: quit): ").strip()
        
        if query.lower() in ['quit', 'exit']:
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if query:
            print("\nì²˜ë¦¬ ì¤‘...")
            answer = rag.analyze_query(query)
            print(f"\n{answer}")

if __name__ == "__main__":
    main()