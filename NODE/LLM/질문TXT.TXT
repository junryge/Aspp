# 예측 + 설명 플로우 Q&A
prediction_qa = """
================================================================================
예측 + 설명 플로우 - 예상 질문 및 답변
================================================================================

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[기본 동작 원리]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Q1. 예측 플로우는 어떻게 동작하나요?
────────────────────────────────────────────────────────────────────────────
A1. 
30분간의 시계열 데이터를 입력받아 다음 단계로 처리합니다:

1단계: 30분 시퀀스 데이터 수집
   - CURRENT_M16A_3F_JOB_2 값 (타겟)
   - hub, storage, cmd 관련 지표
   - 분 단위로 30개 데이터 포인트

2단계: Feature 생성 (총 80+ Features)
   - Momentum Features: acceleration, is_rising, rapid_rise 등
   - hub 플래그: hub_high, hub_critical 등
   - storage 플래그: storage_util_high, storage_util_critical 등
   - surge_risk_score: 급증 위험도 점수

3단계: XGBoost V7 모델 예측
   - 10분 후 최대값 예측
   - 신뢰도(Confidence) 함께 출력

4단계: SHAP 분석
   - 각 Feature의 예측 기여도 계산
   - 상위 10개 중요 Feature 추출

5단계: Qwen 80B LLM 해석
   - SHAP 값을 자연어로 변환
   - "hub_high가 1로 병목 발생, 이로 인해 예측값 상승" 형태로 설명

6단계: 통합 응답 생성
   - 예측값: 305.2
   - 신뢰도: 87%
   - 주요 원인 3가지
   - 자연어 설명

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[예측 성능]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Q2. 예측 정확도는 어느 정도인가요?
────────────────────────────────────────────────────────────────────────────
A2.
V7 모델 성능 지표:

- 급증 케이스 감지율: 60-70% (목표 달성)
  - 급증 정의: seq_max < 300 & future ≥ 300
  - 임계값 280 사용 시 최적 성능

- 극단값(≥300) 감지율: 94-98%
  - 위험 상황 거의 100% 포착

- 전체 예측 정확도:
  - MAE (평균 절대 오차): < 20
  - R² (결정 계수): > 0.85
  - RMSE: < 30

- 가중치 전략:
  - 급증 케이스 10배 가중치
  - 일반 케이스도 정확도 유지


Q3. 예측 시간은 얼마나 걸리나요?
────────────────────────────────────────────────────────────────────────────
A3.
- Feature 생성: ~50ms
- XGBoost 예측: ~30ms
- SHAP 분석: ~100ms
- LLM 해석: ~2초 (Qwen 80B, H200 GPU)
───────────────────────
총 응답 시간: 약 2.2초

※ GPU 최적화 시 1.5초까지 단축 가능


Q4. 왜 XGBoost를 선택했나요? 다른 모델은 안 되나요?
────────────────────────────────────────────────────────────────────────────
A4.
XGBoost 선택 이유:

1. 시계열 Feature에 강점
   - Gradient Boosting이 시퀀스 패턴 학습에 유리
   - Tree 기반 모델이라 비선형 관계 잘 포착

2. 불균형 데이터 처리
   - 급증 케이스가 전체의 5% 미만
   - 가중치 조정 기능 우수

3. SHAP 해석 용이
   - TreeExplainer로 빠른 분석
   - Feature 중요도 직관적

4. 실시간 추론 성능
   - 30ms 이내 예측 완료
   - GPU/CPU 모두 지원

대안 모델 비교:
- LSTM: 학습 시간 10배 이상, 정확도 유사
- Transformer: 과적합 위험, 데이터 부족
- Random Forest: 정확도 5% 낮음


Q5. SHAP 설명은 얼마나 신뢰할 수 있나요?
────────────────────────────────────────────────────────────────────────────
A5.
SHAP (SHapley Additive exPlanations)는 게임 이론 기반으로 수학적으로 
보장된 설명 방법입니다.

신뢰성 근거:
1. 일관성 보장
   - 같은 입력 → 항상 같은 설명

2. 공정한 기여도 분배
   - Shapley Value로 각 Feature의 기여도 계산
   - 노벨상 수상 이론 기반

3. 검증 방법
   - 상위 Feature만으로 예측 재현 시 정확도 90% 이상
   - 도메인 전문가와 교차 검증

4. 실제 사례
   - hub_high=1 → surge_risk_score 상승 → 예측값 증가
   - 인과 관계가 논리적으로 일치


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[Feature 엔지니어링]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Q6. Momentum Feature는 무엇인가요?
────────────────────────────────────────────────────────────────────────────
A6.
Momentum Feature는 데이터의 변화 속도와 방향을 포착하는 Feature입니다.

1. target_acceleration (가속도)
   공식: (최근 5분 평균 - 이전 5분 평균) / 5
   의미: 값이 얼마나 빠르게 증가/감소하는가
   예시: 0.8 → 분당 0.8씩 가속 (급증 신호)

2. target_is_rising (상승 여부)
   공식: 현재값 > 5분 전 값
   의미: 상승 추세인가
   예시: 1 (True) → 상승 중

3. target_rapid_rise (급상승 여부)
   공식: (현재 - 5분전) > 10
   의미: 급격한 상승인가
   예시: 1 (True) → 5분간 10 이상 상승

4. target_last_10_mean (최근 평균)
   공식: 최근 10분 평균
   의미: 단기 추세
   예시: 250 → 최근 평균이 높음

중요도: Momentum Feature가 예측에 30-40% 기여


Q7. surge_risk_score는 어떻게 계산하나요?
────────────────────────────────────────────────────────────────────────────
A7.
surge_risk_score = hub_high × 3 + storage_util_critical × 2 + 
                   total_cmd_low × 1 + storage_util_high × 1

점수 해석:
- ≥5: CRITICAL (즉시 조치 필요)
- ≥3: HIGH (주의 필요)
- ≥1: MEDIUM (모니터링)
- <1: LOW (정상)

예시:
hub_high=1, storage_util_critical=1, total_cmd_low=1, storage_util_high=0
→ score = 3 + 2 + 1 + 0 = 6 (CRITICAL)

이 점수는 과거 데이터 분석으로 검증된 가중치를 사용합니다.


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[모델 업데이트]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Q8. 모델은 얼마나 자주 업데이트하나요?
────────────────────────────────────────────────────────────────────────────
A8.
업데이트 전략:

1. 정기 업데이트: 월 1회
   - 최근 1개월 데이터로 재학습
   - 성능 저하 없으면 배포

2. 긴급 업데이트: 성능 저하 시
   - 급증 감지율 50% 이하 시
   - MAE > 30 시
   - 즉시 재학습 및 배포

3. 버전 관리:
   - V7, V6, V5... 모두 보관
   - 롤백 가능
   - A/B 테스트 지원

4. 자동화:
   - 성능 모니터링 자동화
   - 알람 시스템 구축
   - 재학습 파이프라인 자동화


Q9. 새로운 Feature를 추가하면 어떻게 되나요?
────────────────────────────────────────────────────────────────────────────
A9.
Feature 추가 프로세스:

1단계: Feature 정의
   - 비즈니스 로직 확인
   - 데이터 가용성 검증

2단계: Feature 생성 코드 작성
   - Feature 생성 함수 추가
   - 학습/평가 코드 모두 수정

3단계: 모델 재학습
   - 전체 데이터로 재학습
   - 교차 검증

4단계: 성능 비교
   - 기존 모델 vs 신규 모델
   - Feature 중요도 분석

5단계: 배포
   - 성능 향상 시 배포
   - 버전 번호 증가 (V7 → V8)

소요 시간: 약 1-2일


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[LLM 해석]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Q10. LLM이 SHAP 값을 어떻게 자연어로 바꾸나요?
────────────────────────────────────────────────────────────────────────────
A10.
LLM 프롬프트 예시:

입력:
"""
예측값: 305.2
신뢰도: 87%

주요 Feature 영향도:
1. hub_high: 1 (SHAP: +25.3) - 증가
2. target_acceleration: 0.8 (SHAP: +18.7) - 증가
3. storage_util: 210 (SHAP: +12.4) - 증가
4. target_rapid_rise: 1 (SHAP: +9.2) - 증가
5. surge_risk_score: 6 (SHAP: +8.1) - 증가

다음을 설명하시오:
1. 왜 이런 예측이 나왔는지
2. 어떤 조치가 필요한지
3. 위험도 평가
"""

출력:
"""
10분 후 CURRENT_M16A_3F_JOB_2 값이 305.2로 예측됩니다 (신뢰도 87%).

주요 원인:
1. 허브 병목 현상 (hub_high=1)
   - 허브 용량이 610 이하로 떨어져 데이터 처리 지연 발생
   - 예측값에 +25.3 영향

2. 급격한 상승 추세 (target_acceleration=0.8)
   - 분당 0.8씩 가속 중
   - 급증 패턴 감지

3. Storage 임계치 초과 (storage_util=210)
   - 정상 범위(205 이하) 초과
   - 추가 부하 발생 가능

종합 위험도: CRITICAL (surge_risk_score=6)

권장 조치:
- 즉시 허브 용량 모니터링
- Storage 사용률 점검
- 필요 시 긴급 조치 준비
"""


Q11. LLM 설명이 틀릴 수도 있나요?
────────────────────────────────────────────────────────────────────────────
A11.
가능성은 있지만, 다음 방법으로 최소화합니다:

1. 프롬프트 엔지니어링
   - 명확한 지시사항
   - SHAP 값 기반으로만 설명하도록 제한
   - 환각(Hallucination) 방지

2. 검증 레이어
   - 도메인 키워드 필터링
   - 비정상 응답 감지
   - 사람의 최종 검토

3. 지속적인 개선
   - 잘못된 설명 피드백
   - 프롬프트 개선
   - 모델 파인튜닝

4. 백업 시스템
   - LLM 실패 시 템플릿 기반 설명 제공
   - SHAP 값 표로 제공


================================================================================
"""

# RAG 검색 플로우 Q&A
rag_qa = """
================================================================================
RAG 검색 플로우 - 예상 질문 및 답변
================================================================================

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[기본 동작 원리]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Q1. RAG 검색은 어떻게 동작하나요?
────────────────────────────────────────────────────────────────────────────
A1.
RAG (Retrieval Augmented Generation)는 검색 기반 답변 생성 시스템입니다.

1단계: 사용자 질문
   "과거에 비슷한 급증 케이스가 있었나요?"

2단계: 임베딩 변환
   - ko-sroberta-multitask 모델 사용
   - 512차원 벡터로 변환
   - 의미론적 표현 생성

3단계: Vector DB 검색
   - ChromaDB에서 유사도 계산
   - Cosine Similarity 사용
   - Top-5 가장 유사한 문서 추출

4단계: Context 구성
   - 검색된 5개 문서 조합
   - 질문과 함께 Context로 제공

5단계: LLM 답변 생성 (Qwen 80B)
   - Context를 참고하여 답변
   - 정확한 날짜/시간 포함
   - 유사 패턴 설명

6단계: 최종 응답
   """
   유사 케이스 발견:
   
   2024-09-15 14:23:00
   - 예측값: 308.4 (실제: 312.1)
   - 패턴: hub_high=1, storage_util=215
   - 결과: 급증 발생, 5분 후 정상화
   
   2024-10-03 09:47:00
   - 예측값: 301.2 (실제: 305.8)
   - 패턴: target_acceleration=0.9
   - 결과: 급증 발생, 7분 후 정상화
   """


Q2. 어떤 데이터를 검색하나요?
────────────────────────────────────────────────────────────────────────────
A2.
Vector DB에 저장되는 데이터:

1. 과거 예측 이력
   - 예측 시각
   - 예측값 vs 실제값
   - Feature 값들
   - SHAP 설명
   - 결과 (급증/정상)

2. 문서화된 사례
   - 이상 상황 보고서
   - 원인 분석 문서
   - 조치 사항
   - 결과 요약

3. 운영 로그
   - 시스템 이벤트
   - 알람 기록
   - 수동 조치 내역

4. 메타데이터
   - 날짜/시간
   - 장비 ID
   - 운영 조건

데이터 범위:
- 2024년 9월 4일 ~ 현재
- 약 100만 건의 데이터 포인트
- 일일 업데이트


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[검색 성능]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Q3. 검색 정확도는 어떻게 되나요?
────────────────────────────────────────────────────────────────────────────
A3.
검색 성능 지표:

- Top-1 정확도: 78%
  - 가장 유사한 1개 문서가 실제 관련 있는 비율

- Top-5 정확도: 92%
  - 상위 5개 중 관련 문서가 포함된 비율

- 평균 유사도 점수: 0.85
  - 1.0 = 완전히 동일
  - 0.0 = 완전히 다름

검증 방법:
- 100개 테스트 질문으로 평가
- 도메인 전문가가 관련성 판단
- 지속적인 개선


Q4. 검색 속도는 얼마나 걸리나요?
────────────────────────────────────────────────────────────────────────────
A4.
- 임베딩 변환: ~100ms
- Vector DB 검색: ~50ms (100만 건 기준)
- LLM 답변 생성: ~2초
───────────────────────
총 응답 시간: 약 2.2초

확장성:
- 1000만 건: ~200ms
- 1억 건: ~500ms (인덱싱 최적화 필요)

ChromaDB는 HNSW 알고리즘 사용으로 로그 시간 복잡도 보장


Q5. 왜 Top-5를 선택하나요? Top-10은 안 되나요?
────────────────────────────────────────────────────────────────────────────
A5.
Top-5 선택 이유:

1. Context 길이 제한
   - LLM Context Window: 8K 토큰
   - 문서 1개당 평균 1K 토큰
   - 5개 문서 + 질문 = 약 6K 토큰 (여유 있음)

2. 정확도 vs 속도 균형
   - Top-5: 92% 정확도, 2.2초
   - Top-10: 94% 정확도, 3.5초
   → 2% 향상에 1.3초 추가는 비효율

3. 품질 저하 방지
   - Top-10 이후는 유사도 < 0.7
   - 노이즈 문서 포함 가능
   - LLM 혼란 야기

조정 가능:
- 특수한 경우 Top-10까지 확장 가능
- 설정 파일에서 변경


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[임베딩 모델]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Q6. 왜 ko-sroberta를 선택했나요?
────────────────────────────────────────────────────────────────────────────
A6.
ko-sroberta-multitask 선택 이유:

1. 한국어 특화
   - 한국어 코퍼스로 사전 학습
   - 한국어 토큰화 최적화
   - 한국어 의미 이해 우수

2. Sentence-level 임베딩
   - 문장 전체의 의미 파악
   - 문맥 고려

3. 멀티태스크 학습
   - 다양한 NLP 태스크로 학습
   - 범용성 우수

4. 경량화
   - 512차원 (OpenAI: 1536차원)
   - 빠른 검색 속도
   - 적은 저장 공간

5. 검증된 성능
   - KorSTS 벤치마크 1위권
   - 실제 프로덕션 사용 사례 다수

대안 비교:
- OpenAI text-embedding-3: API 비용, 한국어 성능 낮음
- multilingual-e5: 범용성은 높으나 한국어 특화 부족


Q7. 임베딩 모델을 업데이트하면 어떻게 되나요?
────────────────────────────────────────────────────────────────────────────
A7.
임베딩 모델 변경 시:

문제: 
- 기존 Vector DB와 호환 불가
- 벡터 공간이 달라짐

해결책:
1. 전체 재임베딩
   - 모든 문서를 새 모델로 재변환
   - 새 Vector DB 구축
   - 소요 시간: 약 2-3시간 (100만 건 기준)

2. A/B 테스트
   - 기존 DB와 신규 DB 병렬 운영
   - 성능 비교
   - 우수한 모델 선택

3. 점진적 마이그레이션
   - 신규 데이터는 새 모델
   - 기존 데이터는 점진적 변환
   - 하이브리드 검색

권장: 
- 3개월마다 최신 모델 평가
- 성능 향상 10% 이상 시 업데이트


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[Vector DB 관리]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Q8. Vector DB 크기는 얼마나 되나요?
────────────────────────────────────────────────────────────────────────────
A8.
현재 규모:

- 문서 수: 약 100만 건
- 벡터 차원: 512
- 디스크 사용량: 약 2.5 GB
  - 벡터 데이터: 2 GB
  - 메타데이터: 0.5 GB

메모리 사용량:
- 인덱스 로딩: 1.5 GB
- 검색 시: +500 MB (일시적)

성장 예측:
- 월 증가: +10만 건
- 연간 예상: 약 250 GB

스케일링 전략:
1. 파티셔닝
   - 월별/장비