#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¡œê·¸í”„ë ˆì†Œ MD - ê³ ê¸‰ ì²­í¬ ë¶„í• 
(í—¤ë” + ì½”ë“œë¸”ë¡ ë³´ì¡´ + ì¤‘ë³µ ì œê±°)
"""

import os
import re
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer

print("="*60)
print("ë¡œê·¸í”„ë ˆì†Œ MD ê³ ê¸‰ ì²­í¬ ë¶„í• ")
print("="*60)

# 1. MD íŒŒì¼ ë¡œë“œ
print("\nğŸ“‚ MD íŒŒì¼ ë¡œë“œ ì¤‘...")
md_file = "ë¡œê·¸í”„ë ˆì†Œ_ë§¤ë‰´ì–¼.md"

with open(md_file, 'r', encoding='utf-8') as f:
    content = f.read()

print(f"âœ… íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(content):,}ì")

# 2. ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í• 
print("\nâœ‚ï¸ ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í•  ì¤‘...")

def split_markdown_smart(text, max_chunk_size=800):
    """
    ë§ˆí¬ë‹¤ìš´ì„ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ë¶„í• 
    - í—¤ë” êµ¬ì¡° ìœ ì§€
    - ì½”ë“œë¸”ë¡ ë¶„ë¦¬ ì•ˆ í•¨
    - ë¬¸ë§¥ ë³´ì¡´
    """
    chunks = []
    
    # í—¤ë”ë¡œ 1ì°¨ ë¶„í• 
    sections = re.split(r'\n(#{1,3}\s+.+)', text)
    
    current_chunk = ""
    current_header = ""
    
    for i, section in enumerate(sections):
        if not section.strip():
            continue
        
        # í—¤ë”ì¸ì§€ í™•ì¸
        if re.match(r'^#{1,3}\s+', section):
            current_header = section
            current_chunk = section + "\n"
        else:
            # ë‚´ìš© ì¶”ê°€
            current_chunk += section
            
            # ì²­í¬ê°€ ë„ˆë¬´ í¬ë©´ ë¶„í• 
            if len(current_chunk) > max_chunk_size:
                # ì½”ë“œë¸”ë¡ì´ ìˆëŠ”ì§€ í™•ì¸
                code_blocks = re.findall(r'```[\s\S]*?```', current_chunk)
                
                if code_blocks:
                    # ì½”ë“œë¸”ë¡ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
                    chunks.append({
                        'content': current_chunk.strip(),
                        'header': current_header.strip(),
                        'has_code': True
                    })
                else:
                    # ë¬¸ë‹¨ìœ¼ë¡œ ë¶„í• 
                    paragraphs = current_chunk.split('\n\n')
                    temp_chunk = current_header + "\n"
                    
                    for para in paragraphs:
                        if len(temp_chunk + para) > max_chunk_size:
                            if temp_chunk.strip():
                                chunks.append({
                                    'content': temp_chunk.strip(),
                                    'header': current_header.strip(),
                                    'has_code': False
                                })
                            temp_chunk = current_header + "\n" + para
                        else:
                            temp_chunk += "\n\n" + para
                    
                    if temp_chunk.strip():
                        chunks.append({
                            'content': temp_chunk.strip(),
                            'header': current_header.strip(),
                            'has_code': False
                        })
                
                current_chunk = current_header + "\n"
    
    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk.strip() and len(current_chunk) > len(current_header):
        chunks.append({
            'content': current_chunk.strip(),
            'header': current_header.strip(),
            'has_code': '```' in current_chunk
        })
    
    return chunks

documents = split_markdown_smart(content, max_chunk_size=800)

print(f"âœ… ì²­í¬ ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ")
print(f"   - ì½”ë“œ í¬í•¨: {sum(1 for d in documents if d['has_code'])}ê°œ")
print(f"   - í…ìŠ¤íŠ¸ë§Œ: {sum(1 for d in documents if not d['has_code'])}ê°œ")

# 3. ì„ë² ë”© ìƒì„±
print("\nğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")

model = SentenceTransformer('./embeddings/all-MiniLM-L6-v2')

texts = [doc['content'] for doc in documents]
embeddings = model.encode(texts, show_progress_bar=True)

print(f"âœ… ì„ë² ë”© ì™„ë£Œ: {embeddings.shape}")

# 4. ë²¡í„°DB ì €ì¥
print("\nğŸ’¾ ë²¡í„°DB ì €ì¥ ì¤‘...")

os.makedirs('./logpresso_db_smart', exist_ok=True)
db_file = './logpresso_db_smart/vectordb.pkl'

with open(db_file, 'wb') as f:
    pickle.dump({
        'documents': documents,
        'embeddings': embeddings
    }, f)

print(f"âœ… ì €ì¥ ì™„ë£Œ: {db_file}")

# 5. í†µê³„ ì¶œë ¥
print("\nğŸ“Š ì²­í¬ í†µê³„:")
chunk_sizes = [len(doc['content']) for doc in documents]
print(f"   í‰ê·  ê¸¸ì´: {np.mean(chunk_sizes):.0f}ì")
print(f"   ìµœì†Œ ê¸¸ì´: {min(chunk_sizes)}ì")
print(f"   ìµœëŒ€ ê¸¸ì´: {max(chunk_sizes)}ì")

print("\n" + "="*60)
print("âœ… ê³ ê¸‰ ë²¡í„°DB ìƒì„± ì™„ë£Œ!")
print("ë¡œê·¸í”„ë ˆì†Œ_RAG.pyì—ì„œ db_pathë¥¼ './logpresso_db_smart'ë¡œ ë³€ê²½í•˜ì„¸ìš”.")
print("="*60)