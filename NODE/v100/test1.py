#!/usr/bin/env python3
"""
NVIDIA V100 GPU Test Script - TensorFlow Version
GPU ì¸ì‹, ë©”ëª¨ë¦¬, ì—°ì‚° ì„±ëŠ¥ ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
"""

import subprocess
import sys
import os
import time

def check_nvidia_smi():
    """nvidia-smië¡œ GPU ì •ë³´ í™•ì¸"""
    print("="*60)
    print("1. NVIDIA-SMI GPU ì •ë³´")
    print("="*60)
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        print(result.stdout)
    except FileNotFoundError:
        print("âŒ nvidia-smië¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. NVIDIA ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return False
    return True

def check_cuda():
    """CUDA ì„¤ì¹˜ í™•ì¸"""
    print("="*60)
    print("2. CUDA ë²„ì „ í™•ì¸")
    print("="*60)
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
        print(result.stdout)
    except FileNotFoundError:
        print("âš ï¸  nvccë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CUDA toolkitì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("   TensorFlowëŠ” ìì²´ CUDA ëŸ°íƒ€ì„ì„ í¬í•¨í•˜ë¯€ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
    print()

def test_tensorflow():
    """TensorFlowë¡œ GPU í…ŒìŠ¤íŠ¸"""
    print("="*60)
    print("3. TensorFlow GPU í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    try:
        import tensorflow as tf
        print(f"âœ… TensorFlow ë²„ì „: {tf.__version__}")
        
        # GPU ëª©ë¡ í™•ì¸
        gpus = tf.config.list_physical_devices('GPU')
        print(f"âœ… ê°ì§€ëœ GPU ê°œìˆ˜: {len(gpus)}")
        
        if len(gpus) > 0:
            print("\nğŸ“‹ GPU ìƒì„¸ ì •ë³´:")
            for i, gpu in enumerate(gpus):
                print(f"   GPU {i}: {gpu.name}")
                
                # GPU ë©”ëª¨ë¦¬ ì¦ê°€ í—ˆìš© (í•„ìš”ì‹œì—ë§Œ ë©”ëª¨ë¦¬ í• ë‹¹)
                try:
                    tf.config.experimental.set_memory_growth(gpu, True)
                except RuntimeError as e:
                    print(f"   âš ï¸  ë©”ëª¨ë¦¬ ì¦ê°€ ì„¤ì • ì‹¤íŒ¨: {e}")
            
            # CUDA ë° cuDNN ì •ë³´
            print(f"\nâœ… CUDA ì§€ì›: {tf.test.is_built_with_cuda()}")
            print(f"âœ… GPU ê°€ì† ì§€ì›: {tf.test.is_built_with_gpu_support()}")
            
            # ê° GPUì—ì„œ ê°„ë‹¨í•œ ì—°ì‚° í…ŒìŠ¤íŠ¸
            print("\nğŸ“Š ê° GPUë³„ ì—°ì‚° í…ŒìŠ¤íŠ¸:")
            for i in range(len(gpus)):
                with tf.device(f'/GPU:{i}'):
                    print(f"\n   GPU {i} í…ŒìŠ¤íŠ¸:")
                    
                    # ì‘ì€ í–‰ë ¬ë¡œ ì‹œì‘
                    size = 1000
                    a = tf.random.normal([size, size])
                    b = tf.random.normal([size, size])
                    
                    # Warm-up
                    c = tf.matmul(a, b)
                    
                    # ì‹¤ì œ ì¸¡ì •
                    start = time.time()
                    for _ in range(10):
                        c = tf.matmul(a, b)
                    tf.debugging.assert_all_finite(c, "ê²°ê³¼ í™•ì¸")
                    elapsed = time.time() - start
                    
                    print(f"   - 1000x1000 í–‰ë ¬ê³± 10íšŒ: {elapsed:.3f}ì´ˆ")
                    print(f"   - ì²˜ë¦¬ëŸ‰: {10 * 2 * size**3 / elapsed / 1e9:.1f} GFLOPS")
            
            # í° í–‰ë ¬ ì—°ì‚° í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸)
            print("\nğŸ“Š ëŒ€ìš©ëŸ‰ í–‰ë ¬ ì—°ì‚° í…ŒìŠ¤íŠ¸ (GPU 0):")
            with tf.device('/GPU:0'):
                try:
                    size = 10000
                    print(f"   {size}x{size} í–‰ë ¬ ìƒì„± ì¤‘...")
                    a = tf.random.normal([size, size], dtype=tf.float32)
                    b = tf.random.normal([size, size], dtype=tf.float32)
                    
                    print(f"   í–‰ë ¬ê³± ì—°ì‚° ì¤‘...")
                    start = time.time()
                    c = tf.matmul(a, b)
                    tf.debugging.assert_all_finite(c, "ê²°ê³¼ í™•ì¸")
                    elapsed = time.time() - start
                    
                    memory_gb = (3 * size * size * 4) / (1024**3)  # 3ê°œ í–‰ë ¬, float32(4bytes)
                    print(f"   âœ… ì„±ê³µ! ì‹œê°„: {elapsed:.3f}ì´ˆ")
                    print(f"   - ëŒ€ëµì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_gb:.2f} GB")
                    print(f"   - ì²˜ë¦¬ëŸ‰: {2 * size**3 / elapsed / 1e12:.2f} TFLOPS")
                    
                except tf.errors.ResourceExhaustedError:
                    print("   âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±! ë” ì‘ì€ í¬ê¸°ë¡œ ì‹œë„í•˜ì„¸ìš”.")
                except Exception as e:
                    print(f"   âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            # Multi-GPU ì „ëµ í…ŒìŠ¤íŠ¸
            if len(gpus) > 1:
                print("\nğŸ“Š Multi-GPU ì „ëµ í…ŒìŠ¤íŠ¸:")
                strategy = tf.distribute.MirroredStrategy()
                print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: {strategy.num_replicas_in_sync}ê°œ")
                
                with strategy.scope():
                    # ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„±
                    model = tf.keras.Sequential([
                        tf.keras.layers.Dense(1000, activation='relu', input_shape=(1000,)),
                        tf.keras.layers.Dense(1000, activation='relu'),
                        tf.keras.layers.Dense(10, activation='softmax')
                    ])
                    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
                    print("   âœ… Multi-GPU ëª¨ë¸ ìƒì„± ì„±ê³µ!")
                    
                    # ëª¨ë¸ ìš”ì•½
                    print("\n   ëª¨ë¸ êµ¬ì¡°:")
                    model.summary()
            
            # GPU ê°„ í†µì‹  í…ŒìŠ¤íŠ¸
            if len(gpus) > 1:
                print("\nğŸ“Š GPU ê°„ ë°ì´í„° ì „ì†¡ í…ŒìŠ¤íŠ¸:")
                size = 1000
                with tf.device('/GPU:0'):
                    gpu0_data = tf.random.normal([size, size])
                
                start = time.time()
                with tf.device('/GPU:1'):
                    gpu1_data = tf.identity(gpu0_data)  # GPU 0 -> GPU 1 ë³µì‚¬
                    result = tf.matmul(gpu1_data, gpu1_data)
                elapsed = time.time() - start
                
                data_size_mb = (size * size * 4) / (1024**2)
                print(f"   GPU 0 â†’ GPU 1 ì „ì†¡ ë° ì—°ì‚°")
                print(f"   - ë°ì´í„° í¬ê¸°: {data_size_mb:.2f} MB")
                print(f"   - ì „ì†¡ + ì—°ì‚° ì‹œê°„: {elapsed:.3f}ì´ˆ")
                print(f"   - ëŒ€ì—­í­: {data_size_mb / elapsed:.1f} MB/s")
        
        else:
            print("âŒ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            print("   ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:")
            print("   1. NVIDIA ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€")
            print("   2. tensorflow-gpuê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ (pip install tensorflow-gpu)")
            print("   3. CUDAì™€ cuDNNì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€")
            
    except ImportError:
        print("âŒ TensorFlowê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤!")
        print("   ì„¤ì¹˜ ëª…ë ¹ì–´: pip install tensorflow")
        return False
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print(" NVIDIA V100 GPU í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (TensorFlow)")
    print("="*60 + "\n")
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì„ íƒì‚¬í•­)
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # INFO ë©”ì‹œì§€ ìˆ¨ê¹€
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    if check_nvidia_smi():
        check_cuda()
        if test_tensorflow():
            print("\n" + "="*60)
            print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            print("="*60)
        else:
            print("\n" + "="*60)
            print("âš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            print("="*60)

if __name__ == "__main__":
    main()