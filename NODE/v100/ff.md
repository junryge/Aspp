# V100 32GB 2ì¥ GGUF ëª¨ë¸ ì‚¬ìš© ê°€ì´ë“œ

## ğŸ“Š ì‹œìŠ¤í…œ ì‚¬ì–‘
| í•­ëª© | ì‚¬ì–‘ |
|------|------|
| GPU | Tesla V100-SXM2-32GB Ã— 2 |
| ì´ VRAM | 64GB |
| ì‹¤ì‚¬ìš© ê°€ëŠ¥ | ~60GB |
| Compute Capability | 7.0 |

## ğŸ¯ ë‹¨ì¼ GPU (32GB) ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸

| ì–‘ìí™” | íŒŒë¼ë¯¸í„° í•œê³„ | ì‹¤ì œ í¬ê¸° | ì¶”ì²œ ëª¨ë¸ |
|--------|--------------|-----------|-----------|
| **Q8_0** (8bit) | ~30B | ~30GB | Llama-3.1 8B, Qwen2.5 32B |
| **Q6_K** (6bit) | ~40B | ~24GB | Yi-34B, CodeLlama-34B |
| **Q5_K_M** (5bit) | ~50B | ~20GB | Falcon-40B |
| **Q4_K_M** (4bit) | ~70B | ~40GB | Llama-2 70B (íƒ€ì´íŠ¸) |
| **Q3_K_M** (3bit) | ~90B | ~30GB | ì‹¤í—˜ì  ì‚¬ìš© |

## ğŸš€ ë“€ì–¼ GPU (64GB) ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸

| ì–‘ìí™” | íŒŒë¼ë¯¸í„° í•œê³„ | ì‹¤ì œ í¬ê¸° | ì¶”ì²œ ëª¨ë¸ |
|--------|--------------|-----------|-----------|
| **Q8_0** | ~70B | ~70GB | Llama-3.1 70B (íƒ€ì´íŠ¸) |
| **Q6_K** | ~90B | ~54GB | Llama-3.1 70B (ì—¬ìœ ) |
| **Q5_K_M** | ~110B | ~45GB | Qwen2.5 72B |
| **Q4_K_M** | ~140B | ~40GB | Llama-3.1 70B (ìµœì ) |
| **Q3_K_M** | ~180B | ~54GB | Mixtral 8x22B |

## â­ ì¶”ì²œ êµ¬ì„±

### ğŸ† ìµœì  ì„±ëŠ¥ (ì†ë„ + í’ˆì§ˆ)
| ëª¨ë¸ | ì–‘ìí™” | ë©”ëª¨ë¦¬ ì‚¬ìš© | íŠ¹ì§• |
|------|--------|------------|------|
| **Llama-3.1 70B** | Q4_K_M | ~40GB | ìµœê³  ë°¸ëŸ°ìŠ¤ |
| **Qwen2.5 72B** | Q4_K_M | ~42GB | í•œêµ­ì–´ ê°•ì  |
| **Mixtral 8x7B** | Q6_K | ~35GB | MoE, ë¹ ë¥¸ ì†ë„ |
| **DeepSeek-Coder-V2** | Q5_K_M | ~45GB | ì½”ë”© íŠ¹í™” |

### ğŸ’ª ìµœëŒ€ í¬ê¸° ë„ì „
| ëª¨ë¸ | ì–‘ìí™” | ë©”ëª¨ë¦¬ ì‚¬ìš© | íŠ¹ì§• |
|------|--------|------------|------|
| **Llama-3.1 70B** | Q6_K | ~55GB | í’ˆì§ˆ ìš°ì„  |
| **Mixtral 8x22B** | Q3_K_M | ~58GB | ëŒ€ê·œëª¨ MoE |
| **DeepSeek-V2 236B** | Q2_K | ~60GB | ê·¹í•œ ì••ì¶• |

## ğŸ“ˆ ì„±ëŠ¥ ì˜ˆìƒ

### í† í° ìƒì„± ì†ë„ (tokens/sec)
| ëª¨ë¸ í¬ê¸° | Q4_K_M | Q5_K_M | Q6_K |
|-----------|--------|--------|------|
| 7-13B | 80-100 | 70-90 | 60-80 |
| 30-34B | 40-60 | 35-50 | 30-45 |
| 70B | 20-30 | 18-25 | 15-22 |

## âš™ï¸ ì‹¤í–‰ ë„êµ¬ë³„ íŠ¹ì§•

| ë„êµ¬ | ë©€í‹°GPU | íŠ¹ì§• | ì¶”ì²œ ìš©ë„ |
|------|---------|------|-----------|
| **vLLM** | âœ… ìë™ | ìµœê³  ì²˜ë¦¬ëŸ‰ | API ì„œë²„ |
| **llama.cpp** | âš ï¸ ìˆ˜ë™ | CPU+GPU í˜¼ìš© | ì‹¤í—˜/í…ŒìŠ¤íŠ¸ |
| **text-generation-webui** | âœ… ìë™ | GUI ì œê³µ | ëŒ€í™”í˜• ì‚¬ìš© |
| **ExLlamaV2** | âœ… ì§€ì› | ë¹ ë¥¸ ì†ë„ | ê³ ì† ì¶”ë¡  |

## ğŸ’¡ ë©”ëª¨ë¦¬ ìµœì í™” íŒ

### Context ê¸¸ì´ë³„ ì¶”ê°€ ë©”ëª¨ë¦¬
| Context | ì¶”ê°€ ë©”ëª¨ë¦¬ | ìš©ë„ |
|---------|------------|------|
| 4K | +5% | ì¼ë°˜ ëŒ€í™” |
| 8K | +10% | ë¬¸ì„œ ë¶„ì„ |
| 16K | +20% | ê¸´ ë¬¸ì„œ |
| 32K+ | +30% | ì±…/ë…¼ë¬¸ |

### ë°°ì¹˜ ì²˜ë¦¬ ê³ ë ¤ì‚¬í•­
- **ë‹¨ì¼ ìš”ì²­**: ê¸°ë³¸ ë©”ëª¨ë¦¬ë§Œ ì‚¬ìš©
- **ë°°ì¹˜ í¬ê¸° 4**: +20% ë©”ëª¨ë¦¬
- **ë°°ì¹˜ í¬ê¸° 8**: +40% ë©”ëª¨ë¦¬
- **ë™ì‹œ ì‚¬ìš©ì å¤š**: ë©”ëª¨ë¦¬ 50% ì—¬ìœ  ê¶Œì¥

## ğŸ¯ ìµœì¢… ì¶”ì²œ

> **ìµœì  ì„ íƒ: Llama-3.1 70B Q4_K_M**
> - ë©”ëª¨ë¦¬ ì‚¬ìš©: 40GB/64GB (62%)
> - í’ˆì§ˆ: ìš°ìˆ˜ (Q4ëŠ” ì¶©ë¶„í•œ í’ˆì§ˆ)
> - ì†ë„: 20-30 tokens/sec
> - ì—¬ìœ  ë©”ëª¨ë¦¬ë¡œ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥

> **ì‹¤í—˜ì  ì„ íƒ: Mixtral 8x22B Q3_K_M**
> - MoE ì•„í‚¤í…ì²˜ë¡œ íš¨ìœ¨ì 
> - ì „ë¬¸ê°€ ëª¨ë¸ í™œìš©
> - ë‹¤ì–‘í•œ íƒœìŠ¤í¬ ìš°ìˆ˜

## ğŸ“ ì°¸ê³ ì‚¬í•­
- ì˜¨ë„ ì„¤ì •, ìƒ˜í”Œë§ ë°©ë²•ì— ë”°ë¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë³€ë™
- ì‹œìŠ¤í…œ RAMë„ ëª¨ë¸ í¬ê¸°ì˜ 50% ì´ìƒ ê¶Œì¥
- NVLink ì—°ê²°ì‹œ GPUê°„ í†µì‹  ì†ë„ í–¥ìƒ

## ğŸ”§ ì‹¤í–‰ ì˜ˆì‹œ

### vLLM ì‚¬ìš©
```bash
# ë‹¨ì¼ GPU
python -m vllm.entrypoints.openai.api_server \
  --model TheBloke/Llama-2-70B-GGUF \
  --quantization awq \
  --gpu-memory-utilization 0.95

# ë“€ì–¼ GPU
python -m vllm.entrypoints.openai.api_server \
  --model TheBloke/Llama-2-70B-GGUF \
  --tensor-parallel-size 2 \
  --gpu-memory-utilization 0.95
```

### llama.cpp ì‚¬ìš©
```bash
# GPU ë ˆì´ì–´ ì„¤ì •
./main -m llama-70b-q4_k_m.gguf \
  -ngl 80 \  # GPU ë ˆì´ì–´ ìˆ˜
  -c 4096 \  # ì»¨í…ìŠ¤íŠ¸ í¬ê¸°
  -b 512     # ë°°ì¹˜ í¬ê¸°
```

### Text Generation WebUI
```bash
# ìë™ ë©€í‹°GPU ì§€ì›
python server.py \
  --model llama-70b-q4_k_m.gguf \
  --gpu-memory 32 32 \  # ê° GPU ë©”ëª¨ë¦¬
  --auto-devices
```

## ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì˜ˆìƒ

| ëª¨ë¸ | ì–‘ìí™” | Perplexity | ì†ë„ (tok/s) | VRAM |
|------|--------|------------|--------------|------|
| Llama-3.1 70B | Q4_K_M | 5.2 | 25 | 40GB |
| Llama-3.1 70B | Q5_K_M | 5.1 | 22 | 45GB |
| Llama-3.1 70B | Q6_K | 5.0 | 18 | 55GB |
| Qwen2.5 72B | Q4_K_M | 5.3 | 24 | 42GB |
| Mixtral 8x7B | Q6_K | 5.4 | 40 | 35GB |
| Mixtral 8x22B | Q3_K_M | 5.6 | 15 | 58GB |

## ğŸŒŸ ê³ ê¸‰ ì„¤ì •

### CUDA ìµœì í™”
```bash
export CUDA_VISIBLE_DEVICES=0,1
export CUDA_LAUNCH_BLOCKING=0
export TORCH_CUDA_ARCH_LIST="7.0"  # V100
```

### ë©”ëª¨ë¦¬ ê´€ë¦¬
```python
import torch
torch.cuda.set_per_process_memory_fraction(0.95)
torch.cuda.empty_cache()
```

### ëª¨ë¸ ë¶„í•  ì „ëµ
```python
# ë ˆì´ì–´ë³„ ë¶„í• 
device_map = {
    "model.embed_tokens": 0,
    "model.layers.0-39": 0,
    "model.layers.40-79": 1,
    "lm_head": 1
}
```

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤
- [Hugging Face GGUF ëª¨ë¸](https://huggingface.co/models?search=gguf)
- [TheBloke ì–‘ìí™” ëª¨ë¸](https://huggingface.co/TheBloke)
- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- [vLLM Documentation](https://docs.vllm.ai/)

---
*ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2025ë…„ 9ì›”*