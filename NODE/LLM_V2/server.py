#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSV ì§ì ‘ ê²€ìƒ‰ RAG ì„œë²„ (csv_searcher ëª¨ë“ˆ ì‚¬ìš©)
"""

import os
from fastapi import FastAPI
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
import logging
from datetime import datetime
import json

# CSV ê²€ìƒ‰ ëª¨ë“ˆ
import csv_searcher

# M14 ì˜ˆì¸¡ ëª¨ë“ˆ
import m14_predictor

# HUB ì˜ˆì¸¡ ëª¨ë“ˆ
import hub_predictor_numerical
import hub_predictor_categorical

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# ì „ì—­ ë³€ìˆ˜
llm = None
COLUMN_DEFINITIONS = ""

def load_column_definitions():
    """ì»¬ëŸ¼ ì •ì˜ íŒŒì¼ ë¡œë“œ"""
    try:
        with open("column_definitions_short.txt", "r", encoding="utf-8") as f:
            return f.read()
    except:
        try:
            with open("column_definitions.txt", "r", encoding="utf-8") as f:
                return f.read()
        except Exception as e:
            logger.error(f"ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return ""

@app.on_event("startup")
async def startup():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global llm, COLUMN_DEFINITIONS
    
    # 0. ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ
    COLUMN_DEFINITIONS = load_column_definitions()
    logger.info("âœ… ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ ì™„ë£Œ")
    
    # 1. CSV ë¡œë“œ (csv_searcher ì‚¬ìš©)
    CSV_PATH = "./csv/with.csv"
    
    if os.path.exists(CSV_PATH):
        if csv_searcher.load_csv(CSV_PATH):
            logger.info("âœ… CSV ë¡œë“œ ì™„ë£Œ (csv_searcher)")
        else:
            logger.error("âŒ CSV ë¡œë“œ ì‹¤íŒ¨")
    else:
        logger.error(f"âŒ CSV íŒŒì¼ ì—†ìŒ: {CSV_PATH}")
    
    # 2. LLM ë¡œë“œ
    MODEL_PATH = "models/Qwen3-1.7B-Q8_0.gguf"
    
    if os.path.exists(MODEL_PATH):
        logger.info(f"LLM ë¡œë“œ ì‹œì‘: {MODEL_PATH}")
        
        try:
            from llama_cpp import Llama
            
            llm = Llama(
                model_path=MODEL_PATH,
                n_ctx=3000,
                n_batch=256,
                n_gpu_layers=0,
                n_threads=6,
                verbose=False
            )
            
            logger.info("âœ… LLM ë¡œë“œ ì„±ê³µ!")
            
        except Exception as e:
            logger.error(f"âŒ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        logger.warning(f"âš ï¸ LLM ëª¨ë¸ ì—†ìŒ: {MODEL_PATH}")

class Query(BaseModel):
    question: str
    mode: str = "search"

class PredictQuery(BaseModel):
    mode: str
    data: str

@app.get("/")
async def home():
    """ë©”ì¸ í˜ì´ì§€"""
    return FileResponse("index.html")

@app.get("/columns")
async def get_columns():
    """ì»¬ëŸ¼ ëª©ë¡ ë°˜í™˜"""
    return {"columns": csv_searcher.get_columns()}

@app.get("/stats/{column}")
async def get_column_stats(column: str):
    """ì»¬ëŸ¼ í†µê³„ ë°˜í™˜"""
    return csv_searcher.get_statistics(column)

@app.post("/ask")
async def ask(query: Query):
    """RAG ì§ˆë¬¸ ì²˜ë¦¬"""
    global COLUMN_DEFINITIONS
    
    try:
        logger.info(f"ì§ˆë¬¸: {query.question} | ëª¨ë“œ: {query.mode}")
        
        # ëª¨ë“œë³„ ì²˜ë¦¬
        if query.mode == "search":
            # csv_searcherë¡œ ê²€ìƒ‰
            result, data_text = csv_searcher.search_csv(query.question)
            
            if result is None:
                return {"answer": data_text}
            
            # 1. ì •í™•í•œ ë°ì´í„° ë¨¼ì €
            answer = f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼\n{data_text}\n"
            
            # 2. LLM ë¶„ì„ ì¶”ê°€ (ìˆìœ¼ë©´)
            if llm is not None:
                try:
                    prompt = f"""You MUST answer in Korean only. 
ì•„ë˜ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. ë°ì´í„° ê°’ì€ ì ˆëŒ€ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”.

ì»¬ëŸ¼ ì •ì˜:
{COLUMN_DEFINITIONS}

ê²€ìƒ‰ëœ ë°ì´í„°:
{data_text}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ìƒíƒœë¥¼ ê°„ë‹¨íˆ ë¶„ì„í•´ì£¼ì„¸ìš” (2-3ë¬¸ì¥):
- ì •ìƒ/ì£¼ì˜/ìœ„í—˜ ìƒíƒœì¸ì§€
- íŠ¹ì´ì‚¬í•­ì´ ìˆëŠ”ì§€

ë¶„ì„ (í•œêµ­ì–´, ê°„ê²°í•˜ê²Œ):"""
                    
                    response = llm(
                        prompt,
                        max_tokens=150,
                        temperature=0.3,
                        top_p=0.85,
                        repeat_penalty=1.5,
                        stop=["ì§ˆë¬¸:", "ê²€ìƒ‰ëœ", "\n\n\n"]
                    )
                    
                    analysis = response['choices'][0]['text'].strip()
                    
                    # ë°˜ë³µ ì œê±°
                    lines = analysis.split('\n')
                    seen = set()
                    unique_lines = []
                    for line in lines:
                        line_clean = line.strip()
                        if line_clean and line_clean not in seen:
                            seen.add(line_clean)
                            unique_lines.append(line)
                    
                    analysis = '\n'.join(unique_lines[:4])
                    
                    if analysis:
                        answer += f"---\nğŸ¤– ë¶„ì„\n{analysis}"
                    
                except Exception as e:
                    logger.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            return {"answer": answer}
        
        elif query.mode == "m14":
            data_text = "M14 ì˜ˆì¸¡ ê¸°ëŠ¥ì€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.\ní˜„ì¬ëŠ” ë°ì´í„° ê²€ìƒ‰ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            return {"answer": data_text}
        
        elif query.mode == "hub":
            data_text = "HUB ì˜ˆì¸¡ ê¸°ëŠ¥ì€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.\ní˜„ì¬ëŠ” ë°ì´í„° ê²€ìƒ‰ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            return {"answer": data_text}
        
        else:
            # ê¸°ë³¸ê°’: ê²€ìƒ‰
            result, data_text = csv_searcher.search_csv(query.question)
            
            if result is None:
                return {"answer": data_text}
            
            return {"answer": data_text}
        
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {"answer": f"âŒ ì˜¤ë¥˜: {str(e)}"}

@app.post("/predict")
async def predict(query: PredictQuery):
    """M14/HUB ì˜ˆì¸¡ ì²˜ë¦¬"""
    try:
        logger.info(f"ì˜ˆì¸¡ ìš”ì²­: ëª¨ë“œ={query.mode}")
        
        if query.mode == "m14":
            # M14 ì˜ˆì¸¡ ì‹¤í–‰
            result = m14_predictor.predict_m14(query.data)
            
            if 'error' in result:
                return JSONResponse(content=result, status_code=400)
            
            # HTML ëŒ€ì‹œë³´ë“œ ì €ì¥
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            dashboard_filename = f'M14_Dashboard_{timestamp}.html'
            dashboard_path = os.path.join('dashboards', dashboard_filename)
            
            os.makedirs('dashboards', exist_ok=True)
            with open(dashboard_path, 'w', encoding='utf-8') as f:
                f.write(result['dashboard_html'])
            
            logger.info(f"ëŒ€ì‹œë³´ë“œ ì €ì¥: {dashboard_filename}")
            
            # ê°„ë‹¨ ìš”ì•½ ìƒì„±
            summary = generate_prediction_summary(result)
            
            # LLM í•´ì„ (ìˆìœ¼ë©´)
            llm_analysis = ""
            if llm is not None:
                try:
                    llm_analysis = generate_llm_analysis(result)
                except Exception as e:
                    logger.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            return {
                "success": True,
                "summary": summary,
                "llm_analysis": llm_analysis,
                "dashboard_url": f"/dashboard/{dashboard_filename}",
                "predictions": result['predictions'],
                "current_value": result['current_value'],
                "current_status": result['current_status']
            }
        
        elif query.mode == "hub":
            # HUB ì˜ˆì¸¡ ì‹¤í–‰ (ìˆ˜ì¹˜í˜• + ë²”ì£¼í˜•)
            result_numerical = hub_predictor_numerical.predict_hub_numerical(query.data)
            result_categorical = hub_predictor_categorical.predict_hub_categorical(query.data)
            
            if 'error' in result_numerical:
                return JSONResponse(content=result_numerical, status_code=400)
            
            if 'error' in result_categorical:
                return JSONResponse(content=result_categorical, status_code=400)
            
            # HTML ëŒ€ì‹œë³´ë“œ ì €ì¥ (2ê°œ)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            dashboard_numerical_filename = f'HUB_Numerical_{timestamp}.html'
            dashboard_numerical_path = os.path.join('dashboards', dashboard_numerical_filename)
            
            dashboard_categorical_filename = f'HUB_Categorical_{timestamp}.html'
            dashboard_categorical_path = os.path.join('dashboards', dashboard_categorical_filename)
            
            os.makedirs('dashboards', exist_ok=True)
            
            with open(dashboard_numerical_path, 'w', encoding='utf-8') as f:
                f.write(result_numerical['dashboard_html'])
            
            with open(dashboard_categorical_path, 'w', encoding='utf-8') as f:
                f.write(result_categorical['dashboard_html'])
            
            logger.info(f"ëŒ€ì‹œë³´ë“œ ì €ì¥: {dashboard_numerical_filename}, {dashboard_categorical_filename}")
            
            # ê°„ë‹¨ ìš”ì•½ ìƒì„±
            summary = generate_hub_summary(result_numerical, result_categorical)
            
            # LLM í•´ì„ (ìˆìœ¼ë©´)
            llm_analysis = ""
            if llm is not None:
                try:
                    llm_analysis = generate_hub_llm_analysis(result_numerical, result_categorical)
                except Exception as e:
                    logger.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            return {
                "success": True,
                "summary": summary,
                "llm_analysis": llm_analysis,
                "dashboard_numerical_url": f"/dashboard/{dashboard_numerical_filename}",
                "dashboard_categorical_url": f"/dashboard/{dashboard_categorical_filename}",
                "predictions_numerical": result_numerical['predictions'],
                "predictions_categorical": result_categorical['predictions'],
                "current_value": result_numerical['current_value']
            }
        
        else:
            return {
                "error": "Invalid mode",
                "message": "modeëŠ” 'm14' ë˜ëŠ” 'hub'ì—¬ì•¼ í•©ë‹ˆë‹¤."
            }
        
    except Exception as e:
        logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return JSONResponse(
            content={"error": "Prediction failed", "message": str(e)},
            status_code=500
        )

def generate_prediction_summary(result):
    """M14 ì˜ˆì¸¡ ê²°ê³¼ ê°„ë‹¨ ìš”ì•½"""
    predictions = result['predictions']
    current_val = result['current_value']
    current_status = result['current_status']
    
    summary = f"ğŸ“Š í˜„ì¬: {current_val:,} ({current_status})\n\n"
    summary += "ğŸ”® ì˜ˆì¸¡:\n"
    
    for pred in predictions:
        status_emoji = {
            'LOW': 'âœ…',
            'NORMAL': 'ğŸŸ¢',
            'CAUTION': 'âš ï¸',
            'CRITICAL': 'ğŸš¨'
        }.get(pred['status'], 'â“')
        
        summary += f"â€¢ {pred['horizon']}ë¶„: {pred['prediction']:,} {status_emoji} (ìœ„í—˜ {pred['danger_probability']}%)\n"
    
    return summary

def generate_hub_summary(result_numerical, result_categorical):
    """HUB ì˜ˆì¸¡ ê²°ê³¼ ê°„ë‹¨ ìš”ì•½"""
    current_val = result_numerical['current_value']
    
    pred_num = result_numerical['predictions']
    pred_cat = result_categorical['predictions']
    
    summary = f"ğŸ“Š í˜„ì¬: {current_val:,.1f}\n\n"
    summary += "ğŸ”¢ ìˆ˜ì¹˜í˜• ì˜ˆì¸¡:\n"
    
    for pred in pred_num:
        status_emoji = {
            'NORMAL': 'âœ…',
            'CAUTION': 'âš ï¸',
            'WARNING': 'ğŸŸ ',
            'CRITICAL': 'ğŸš¨'
        }.get(pred['status'], 'â“')
        
        summary += f"â€¢ {pred['horizon']}ë¶„: {pred['pred_min']:.1f} ~ {pred['pred_max']:.1f} {status_emoji}\n"
    
    summary += "\nğŸ¯ ë²”ì£¼í˜• ì˜ˆì¸¡:\n"
    
    for pred in pred_cat:
        status_emoji = {
            'LOW': 'âœ…',
            'MEDIUM': 'âš ï¸',
            'HIGH': 'ğŸŸ ',
            'CRITICAL': 'ğŸš¨'
        }.get(pred['status'], 'â“')
        
        summary += f"â€¢ {pred['horizon']}ë¶„: {pred['class_name']} (ê¸‰ì¦ {pred['prob2']:.1f}%) {status_emoji}\n"
    
    return summary

def generate_hub_llm_analysis(result_numerical, result_categorical):
    """LLMìœ¼ë¡œ HUB ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„"""
    current_val = result_numerical['current_value']
    
    pred_num = result_numerical['predictions']
    pred_cat = result_categorical['predictions']
    
    # ìµœëŒ€ ê¸‰ì¦ í™•ë¥ 
    max_surge_prob = max(p['prob2'] for p in pred_cat)
    
    # ìµœëŒ€ ì˜ˆì¸¡ê°’
    max_pred_value = max(p['pred_max'] for p in pred_num)
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    pred_num_text = ""
    for pred in pred_num:
        pred_num_text += f"{pred['horizon']}ë¶„ í›„: {pred['pred_min']:.1f} ~ {pred['pred_max']:.1f} (ìƒíƒœ {pred['status']})\n"
    
    pred_cat_text = ""
    for pred in pred_cat:
        pred_cat_text += f"{pred['horizon']}ë¶„ í›„: {pred['class_name']} (ê¸‰ì¦ {pred['prob2']:.1f}%, ìƒíƒœ {pred['status']})\n"
    
    prompt = f"""You MUST answer in Korean only. Be concise and professional.

í˜„ì¬ HUB ë¬¼ë¥˜ ìƒí™©:
- í˜„ì¬ CURRENT_M16A_3F_JOB_2: {current_val:,.1f}

ìˆ˜ì¹˜í˜• ì˜ˆì¸¡ ê²°ê³¼:
{pred_num_text}

ë²”ì£¼í˜• ì˜ˆì¸¡ ê²°ê³¼:
{pred_cat_text}

ìµœëŒ€ ê¸‰ì¦ í™•ë¥ : {max_surge_prob:.1f}%
ìµœëŒ€ ì˜ˆì¸¡ê°’: {max_pred_value:.1f}

ìœ„ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš” (3-4ë¬¸ì¥):
1. í˜„ì¬ ìƒí™© í‰ê°€
2. ì˜ˆì¸¡ë˜ëŠ” ì¶”ì„¸ (ì¦ê°€/ê°ì†Œ/ì•ˆì •)
3. ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­

ë‹µë³€ (í•œêµ­ì–´):"""
    
    try:
        response = llm(
            prompt,
            max_tokens=200,
            temperature=0.3,
            top_p=0.85,
            repeat_penalty=1.5,
            stop=["ì§ˆë¬¸:", "\n\n\n"]
        )
        
        answer = response['choices'][0]['text'].strip()
        return answer
        
    except Exception as e:
        logger.error(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
        return ""

def generate_llm_analysis(result):
    """LLMìœ¼ë¡œ M14 ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„"""
    predictions = result['predictions']
    current_val = result['current_value']
    current_status = result['current_status']
    
    # ìµœëŒ€ ìœ„í—˜ë„
    max_danger = max(p['danger_probability'] for p in predictions)
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    pred_text = ""
    for pred in predictions:
        pred_text += f"{pred['horizon']}ë¶„ í›„: {pred['prediction']:,} (ìœ„í—˜ë„ {pred['danger_probability']}%, ìƒíƒœ {pred['status']})\n"
    
    prompt = f"""You MUST answer in Korean only. Be concise and professional.

í˜„ì¬ AMHS ë¬¼ë¥˜ ìƒí™©:
- í˜„ì¬ TOTALCNT: {current_val:,} ({current_status})

ì˜ˆì¸¡ ê²°ê³¼:
{pred_text}

ìµœëŒ€ ìœ„í—˜ë„: {max_danger}%

ìœ„ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš” (3-4ë¬¸ì¥):
1. í˜„ì¬ ìƒí™© í‰ê°€
2. ì˜ˆì¸¡ë˜ëŠ” ì¶”ì„¸ (ì¦ê°€/ê°ì†Œ/ì•ˆì •)
3. ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­

ë‹µë³€ (í•œêµ­ì–´):"""
    
    try:
        response = llm(
            prompt,
            max_tokens=200,
            temperature=0.3,
            top_p=0.85,
            repeat_penalty=1.5,
            stop=["ì§ˆë¬¸:", "\n\n\n"]
        )
        
        answer = response['choices'][0]['text'].strip()
        return answer
        
    except Exception as e:
        logger.error(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
        return ""

@app.get("/dashboard/{filename}")
async def get_dashboard(filename: str):
    """ìƒì„±ëœ HTML ëŒ€ì‹œë³´ë“œ ë°˜í™˜"""
    filepath = os.path.join("dashboards", filename)
    
    if not os.path.exists(filepath):
        return JSONResponse(
            content={"error": "File not found"},
            status_code=404
        )
    
    return FileResponse(filepath)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)