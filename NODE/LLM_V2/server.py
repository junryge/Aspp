#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSV ì§ì ‘ ê²€ìƒ‰ RAG ì„œë²„ (csv_searcher ëª¨ë“ˆ ì‚¬ìš©)
"""

import os
from fastapi import FastAPI
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
import logging
from datetime import datetime
import json

# CSV ê²€ìƒ‰ ëª¨ë“ˆ
import csv_searcher

# M14 ì˜ˆì¸¡ ëª¨ë“ˆ
import m14_predictor

# HUB ì˜ˆì¸¡ ëª¨ë“ˆ
import hub_predictor_numerical
import hub_predictor_categorical

# LLM í›„ì²˜ë¦¬ ëª¨ë“ˆ
from llm_postprocessor import clean_llm_response, get_llm_analysis

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# ì „ì—­ ë³€ìˆ˜
llm = None
COLUMN_DEFINITIONS = ""

def load_column_definitions():
    """ì»¬ëŸ¼ ì •ì˜ íŒŒì¼ ë¡œë“œ"""
    try:
        with open("column_definitions_short.txt", "r", encoding="utf-8") as f:
            return f.read()
    except:
        try:
            with open("column_definitions.txt", "r", encoding="utf-8") as f:
                return f.read()
        except Exception as e:
            logger.error(f"ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return ""

@app.on_event("startup")
async def startup():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global llm, COLUMN_DEFINITIONS
    
    # 0. ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ
    COLUMN_DEFINITIONS = load_column_definitions()
    logger.info("âœ… ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ ì™„ë£Œ")
    
    # 1. CSV ë¡œë“œ (csv_searcher ì‚¬ìš©)
    CSV_PATH = "./csv/with.csv"
    
    if os.path.exists(CSV_PATH):
        if csv_searcher.load_csv(CSV_PATH):
            logger.info("âœ… CSV ë¡œë“œ ì™„ë£Œ (csv_searcher)")
        else:
            logger.error("âŒ CSV ë¡œë“œ ì‹¤íŒ¨")
    else:
        logger.error(f"âŒ CSV íŒŒì¼ ì—†ìŒ: {CSV_PATH}")
    
    # 2. LLM ë¡œë“œ
    MODEL_PATH = "models/Qwen3-1.7B-Q8_0.gguf"
    
    if os.path.exists(MODEL_PATH):
        logger.info(f"LLM ë¡œë“œ ì‹œì‘: {MODEL_PATH}")
        
        try:
            from llama_cpp import Llama
            
            llm = Llama(
                model_path=MODEL_PATH,
                n_ctx=3000,
                n_batch=256,
                n_gpu_layers=0,
                n_threads=6,
                verbose=False
            )
            
            logger.info("âœ… LLM ë¡œë“œ ì„±ê³µ!")
            
        except Exception as e:
            logger.error(f"âŒ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        logger.warning(f"âš ï¸ LLM ëª¨ë¸ ì—†ìŒ: {MODEL_PATH}")

class Query(BaseModel):
    question: str
    mode: str = "search"

class PredictQuery(BaseModel):
    mode: str
    data: str

@app.get("/")
async def home():
    """ë©”ì¸ í˜ì´ì§€"""
    return FileResponse("index.html")

@app.get("/columns")
async def get_columns():
    """ì»¬ëŸ¼ ëª©ë¡ ë°˜í™˜"""
    return {"columns": csv_searcher.get_columns()}

@app.get("/stats/{column}")
async def get_column_stats(column: str):
    """ì»¬ëŸ¼ í†µê³„ ë°˜í™˜"""
    return csv_searcher.get_statistics(column)

@app.post("/ask")
async def ask(query: Query):
    """RAG ì§ˆë¬¸ ì²˜ë¦¬"""
    global COLUMN_DEFINITIONS
    
    try:
        logger.info(f"ì§ˆë¬¸: {query.question} | ëª¨ë“œ: {query.mode}")
        
        # ëª¨ë“œë³„ ì²˜ë¦¬
        if query.mode == "search":
            # csv_searcherë¡œ ê²€ìƒ‰
            result, data_text = csv_searcher.search_csv(query.question)
            
            if result is None:
                return {"answer": data_text}
            
            # 1. ì •í™•í•œ ë°ì´í„° ë¨¼ì €
            answer = f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼\n{data_text}\n"
            
            # 2. LLM ë¶„ì„ ì¶”ê°€
            # ë°ì´í„° íƒ€ì… ê°ì§€
            data_type = "hub" if "HUB" in data_text else "m14"
            analysis = get_llm_analysis(data_text, llm, data_type)
            answer += f"\n---\nğŸ¤– LLM ë¶„ì„\n{analysis}"
            
            return {"answer": answer}
        
        elif query.mode == "m14":
            data_text = "M14 ì˜ˆì¸¡ ê¸°ëŠ¥ì€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.\ní˜„ì¬ëŠ” ë°ì´í„° ê²€ìƒ‰ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            return {"answer": data_text}
        
        elif query.mode == "hub":
            data_text = "HUB ì˜ˆì¸¡ ê¸°ëŠ¥ì€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.\ní˜„ì¬ëŠ” ë°ì´í„° ê²€ìƒ‰ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            return {"answer": data_text}
        
        else:
            # ê¸°ë³¸ê°’: ê²€ìƒ‰
            result, data_text = csv_searcher.search_csv(query.question)
            
            if result is None:
                return {"answer": data_text}
            
            return {"answer": data_text}
        
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {"answer": f"âŒ ì˜¤ë¥˜: {str(e)}"}

@app.post("/predict")
async def predict(query: PredictQuery):
    """M14/HUB ì˜ˆì¸¡ ì²˜ë¦¬"""
    try:
        logger.info(f"ì˜ˆì¸¡ ìš”ì²­: ëª¨ë“œ={query.mode}")
        
        if query.mode == "m14":
            # M14 ì˜ˆì¸¡ ì‹¤í–‰
            result = m14_predictor.predict_m14(query.data)
            
            if 'error' in result:
                return JSONResponse(content=result, status_code=400)
            
            # HTML ëŒ€ì‹œë³´ë“œ ì €ì¥
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            dashboard_filename = f'M14_Dashboard_{timestamp}.html'
            dashboard_path = os.path.join('dashboards', dashboard_filename)
            
            os.makedirs('dashboards', exist_ok=True)
            with open(dashboard_path, 'w', encoding='utf-8') as f:
                f.write(result['dashboard_html'])
            
            logger.info(f"ëŒ€ì‹œë³´ë“œ ì €ì¥: {dashboard_filename}")
            
            # ê°„ë‹¨ ìš”ì•½ ìƒì„±
            summary = generate_prediction_summary(result)
            
            # LLM í•´ì„ (ìˆìœ¼ë©´)
            llm_analysis = ""
            if llm is not None:
                try:
                    llm_analysis = generate_llm_analysis(result)
                except Exception as e:
                    logger.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            return {
                "success": True,
                "summary": summary,
                "llm_analysis": llm_analysis,
                "dashboard_url": f"/dashboard/{dashboard_filename}",
                "predictions": result['predictions'],
                "current_value": result['current_value'],
                "current_status": result['current_status']
            }
        
        elif query.mode == "hub":
            # HUB ì˜ˆì¸¡ ì‹¤í–‰ (ìˆ˜ì¹˜í˜• + ë²”ì£¼í˜•)
            result_numerical = hub_predictor_numerical.predict_hub_numerical(query.data)
            result_categorical = hub_predictor_categorical.predict_hub_categorical(query.data)
            
            if 'error' in result_numerical:
                return JSONResponse(content=result_numerical, status_code=400)
            
            if 'error' in result_categorical:
                return JSONResponse(content=result_categorical, status_code=400)
            
            # HTML ëŒ€ì‹œë³´ë“œ ì €ì¥ (2ê°œ)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            dashboard_numerical_filename = f'HUB_Numerical_{timestamp}.html'
            dashboard_numerical_path = os.path.join('dashboards', dashboard_numerical_filename)
            
            dashboard_categorical_filename = f'HUB_Categorical_{timestamp}.html'
            dashboard_categorical_path = os.path.join('dashboards', dashboard_categorical_filename)
            
            os.makedirs('dashboards', exist_ok=True)
            
            with open(dashboard_numerical_path, 'w', encoding='utf-8') as f:
                f.write(result_numerical['dashboard_html'])
            
            with open(dashboard_categorical_path, 'w', encoding='utf-8') as f:
                f.write(result_categorical['dashboard_html'])
            
            logger.info(f"ëŒ€ì‹œë³´ë“œ ì €ì¥: {dashboard_numerical_filename}, {dashboard_categorical_filename}")
            
            # ê°„ë‹¨ ìš”ì•½ ìƒì„±
            summary = generate_hub_summary(result_numerical, result_categorical)
            
            # LLM í•´ì„ (ìˆìœ¼ë©´)
            llm_analysis = ""
            if llm is not None:
                try:
                    llm_analysis = generate_hub_llm_analysis(result_numerical, result_categorical)
                except Exception as e:
                    logger.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            return {
                "success": True,
                "summary": summary,
                "llm_analysis": llm_analysis,
                "dashboard_numerical_url": f"/dashboard/{dashboard_numerical_filename}",
                "dashboard_categorical_url": f"/dashboard/{dashboard_categorical_filename}",
                "predictions_numerical": result_numerical['predictions'],
                "predictions_categorical": result_categorical['predictions'],
                "current_value": result_numerical['current_value']
            }
        
        else:
            return {
                "error": "Invalid mode",
                "message": "modeëŠ” 'm14' ë˜ëŠ” 'hub'ì—¬ì•¼ í•©ë‹ˆë‹¤."
            }
        
    except Exception as e:
        logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return JSONResponse(
            content={"error": "Prediction failed", "message": str(e)},
            status_code=500
        )

def generate_prediction_summary(result):
    """M14 ì˜ˆì¸¡ ê²°ê³¼ ê°„ë‹¨ ìš”ì•½"""
    predictions = result['predictions']
    current_val = result['current_value']
    current_status = result['current_status']
    
    summary = f"ğŸ“Š í˜„ì¬: {current_val:,} ({current_status})\n\n"
    summary += "ğŸ”® ì˜ˆì¸¡:\n"
    
    for pred in predictions:
        status_emoji = {
            'LOW': 'âœ…',
            'NORMAL': 'ğŸŸ¢',
            'CAUTION': 'âš ï¸',
            'CRITICAL': 'ğŸš¨'
        }.get(pred['status'], 'â“')
        
        summary += f"â€¢ {pred['horizon']}ë¶„: {pred['prediction']:,} {status_emoji} (ìœ„í—˜ {pred['danger_probability']}%)\n"
    
    return summary

def generate_hub_summary(result_numerical, result_categorical):
    """HUB ì˜ˆì¸¡ ê²°ê³¼ ê°„ë‹¨ ìš”ì•½"""
    current_val = result_numerical['current_value']
    
    pred_num = result_numerical['predictions']
    pred_cat = result_categorical['predictions']
    
    summary = f"ğŸ“Š í˜„ì¬: {current_val:,.1f}\n\n"
    summary += "ğŸ”¢ ìˆ˜ì¹˜í˜• ì˜ˆì¸¡:\n"
    
    for pred in pred_num:
        status_emoji = {
            'NORMAL': 'âœ…',
            'CAUTION': 'âš ï¸',
            'WARNING': 'ğŸŸ ',
            'CRITICAL': 'ğŸš¨'
        }.get(pred['status'], 'â“')
        
        summary += f"â€¢ {pred['horizon']}ë¶„: {pred['pred_min']:.1f} ~ {pred['pred_max']:.1f} {status_emoji}\n"
    
    summary += "\nğŸ¯ ë²”ì£¼í˜• ì˜ˆì¸¡:\n"
    
    for pred in pred_cat:
        status_emoji = {
            'LOW': 'âœ…',
            'MEDIUM': 'âš ï¸',
            'HIGH': 'ğŸŸ ',
            'CRITICAL': 'ğŸš¨'
        }.get(pred['status'], 'â“')
        
        summary += f"â€¢ {pred['horizon']}ë¶„: {pred['class_name']} (ê¸‰ì¦ {pred['prob2']:.1f}%) {status_emoji}\n"
    
    return summary

def generate_hub_llm_analysis(result_numerical, result_categorical):
    """LLMìœ¼ë¡œ HUB ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„"""
    current_val = result_numerical['current_value']
    
    pred_num = result_numerical['predictions']
    pred_cat = result_categorical['predictions']
    
    # ìµœëŒ€ ê¸‰ì¦ í™•ë¥ 
    max_surge_prob = max(p['prob2'] for p in pred_cat)
    
    # ìµœëŒ€ ì˜ˆì¸¡ê°’
    max_pred_value = max(p['pred_max'] for p in pred_num)
    
    # ìœ„í—˜ ìš”ì¸ ë¶„ì„ (ì˜ˆì¸¡ ê¸°ë°˜)
    risk_factors = []
    
    # ìˆ˜ì¹˜í˜• ì˜ˆì¸¡ ê¸°ë°˜ ìœ„í—˜
    if max_pred_value >= 300:
        risk_factors.append(f"ì˜ˆì¸¡ ìµœëŒ€ê°’({max_pred_value:.0f})ì´ ì‹¬ê° ì„ê³„ê°’(300) ì´ˆê³¼ ì˜ˆìƒ")
    elif max_pred_value >= 280:
        risk_factors.append(f"ì˜ˆì¸¡ ìµœëŒ€ê°’({max_pred_value:.0f})ì´ ì£¼ì˜ ì„ê³„ê°’(280) ì´ˆê³¼ ì˜ˆìƒ")
    
    # ë²”ì£¼í˜• ì˜ˆì¸¡ ê¸°ë°˜ ìœ„í—˜
    if max_surge_prob >= 70:
        risk_factors.append(f"ê¸‰ì¦ í™•ë¥ ({max_surge_prob:.1f}%)ì´ ë§¤ìš° ë†’ìŒ (70% ì´ìƒ)")
    elif max_surge_prob >= 50:
        risk_factors.append(f"ê¸‰ì¦ í™•ë¥ ({max_surge_prob:.1f}%)ì´ ë†’ìŒ (50% ì´ìƒ)")
    elif max_surge_prob >= 30:
        risk_factors.append(f"ê¸‰ì¦ í™•ë¥ ({max_surge_prob:.1f}%)ì´ ì£¼ì˜ ìˆ˜ì¤€ (30% ì´ìƒ)")
    
    # ì‹œê°„ë³„ ì¶”ì„¸ ë¶„ì„
    for pred in pred_cat:
        if pred['prob2'] >= 50:
            risk_factors.append(f"{pred['horizon']}ë¶„ í›„ ê¸‰ì¦ í™•ë¥  {pred['prob2']:.1f}% - {pred['class_name']}")
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    pred_num_text = ""
    for pred in pred_num:
        pred_num_text += f"{pred['horizon']}ë¶„ í›„: {pred['pred_min']:.1f} ~ {pred['pred_max']:.1f} (ìƒíƒœ {pred['status']})\n"
    
    pred_cat_text = ""
    for pred in pred_cat:
        pred_cat_text += f"{pred['horizon']}ë¶„ í›„: {pred['class_name']} (ê¸‰ì¦ {pred['prob2']:.1f}%)\n"
    
    risk_text = "\n- ".join(risk_factors) if risk_factors else "í˜„ì¬ ìœ„í—˜ ìš”ì¸ ì—†ìŒ"
    
    prompt = f"""You MUST answer in Korean only. Be concise and professional.

í˜„ì¬ HUB ë¬¼ë¥˜ ìƒí™©:
- í˜„ì¬ê°’: {current_val:.1f} (ì„ê³„ê°’: 270ì •ìƒ/280ì£¼ì˜/300ì‹¬ê°)
- ìµœëŒ€ ì˜ˆì¸¡ê°’: {max_pred_value:.1f}
- ìµœëŒ€ ê¸‰ì¦ í™•ë¥ : {max_surge_prob:.1f}%

ìˆ˜ì¹˜í˜• ì˜ˆì¸¡:
{pred_num_text}

ë²”ì£¼í˜• ì˜ˆì¸¡:
{pred_cat_text}

âš ï¸ ìœ„í—˜ ìš”ì¸:
- {risk_text}

ìœ„ ë°ì´í„° ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ 3-4ë¬¸ì¥ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”:
1. ì™œ ìœ„í—˜í•œì§€ êµ¬ì²´ì  ì´ìœ  (ì–´ë–¤ ì‹œê°„ëŒ€ì— ê¸‰ì¦ ì˜ˆìƒì¸ì§€)
2. ì˜ˆì¸¡ ì¶”ì„¸ (ì¦ê°€/ê°ì†Œ/ê¸‰ì¦)
3. ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­

ë‹µë³€:"""
    
    try:
        response = llm(
            prompt,
            max_tokens=250,
            temperature=0.2,
            top_p=0.85,
            repeat_penalty=1.5,
            stop=["ì§ˆë¬¸:", "\n\n\n", "ì´ë¯¸ì§€:", "http"]
        )
        
        raw_answer = response['choices'][0]['text'].strip()
        cleaned = clean_llm_response(raw_answer)
        
        # ì˜ì–´/ì´ìƒí•œ ë¬¸ì ê°ì§€ â†’ í…œí”Œë¦¿ ì‚¬ìš©
        english_patterns = ['please', 'answer', 'korean', 'following', 'response', 'analysis', 'the ', 'is ', 'are ', 'this ']
        has_english = any(p in cleaned.lower() for p in english_patterns)
        
        # ì´ìƒí•œ ë¬¸ì ê°ì§€ (ì¤‘êµ­ì–´ ë“±)
        has_weird = any(ord(c) > 0x4E00 and ord(c) < 0x9FFF for c in cleaned)
        
        # ì§€í‘œ ì–¸ê¸‰ í™•ì¸
        indicator_keywords = ['ê¸‰ì¦', 'í™•ë¥ ', '%', 'ë¶„ í›„', 'ì˜ˆì¸¡', 'ì¦ê°€', 'ì„ê³„']
        has_indicator = any(k in cleaned for k in indicator_keywords)
        
        # ë¶€ì ì ˆí•˜ë©´ í…œí”Œë¦¿ ì‚¬ìš©
        if not cleaned or len(cleaned) < 20 or has_english or has_weird or (risk_factors and not has_indicator):
            logger.warning(f"HUB LLM ì‘ë‹µ ë¶€ì ì ˆ, í…œí”Œë¦¿ ì‚¬ìš©")
            return generate_hub_template_analysis(result_numerical, result_categorical, risk_factors, max_surge_prob, max_pred_value)
        
        return cleaned
        
    except Exception as e:
        logger.error(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
        return generate_hub_template_analysis(result_numerical, result_categorical, risk_factors, max_surge_prob, max_pred_value)

def generate_hub_template_analysis(result_numerical, result_categorical, risk_factors, max_surge_prob, max_pred_value):
    """LLM ì‹¤íŒ¨ì‹œ í…œí”Œë¦¿ ê¸°ë°˜ HUB ë¶„ì„ - ìˆ˜ì¹˜í˜• ì˜ˆì¸¡ + ë²”ì£¼í˜• ë’·ë°›ì¹¨"""
    current_val = result_numerical['current_value']
    pred_num = result_numerical['predictions']
    pred_cat = result_categorical['predictions']
    
    # ê°€ì¥ ìœ„í—˜í•œ ì‹œê°„ëŒ€ ì°¾ê¸°
    max_horizon = max(pred_num, key=lambda x: x['pred_max'])
    max_cat = next((p for p in pred_cat if p['horizon'] == max_horizon['horizon']), pred_cat[-1])
    
    if max_pred_value < 280 and max_surge_prob < 30:
        return f"í˜„ì¬ê°’ {current_val:.1f}ë¡œ ì •ìƒ ë²”ìœ„ì…ë‹ˆë‹¤. ê¸‰ì¦ í™•ë¥ ì´ ë‚®ì•„ ì•ˆì •ì ì¸ ìƒíƒœì…ë‹ˆë‹¤. ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    
    analysis = f"âš ï¸ ìœ„í—˜ ë¶„ì„:\n\n"
    
    # 1. ìˆ˜ì¹˜í˜• ì˜ˆì¸¡ (ë©”ì¸)
    analysis += f"ğŸ”¢ ìˆ˜ì¹˜í˜• ì˜ˆì¸¡:\n"
    for p in pred_num:
        if p['pred_max'] >= 300:
            analysis += f"  ğŸš¨ {p['horizon']}ë¶„ í›„: {p['pred_min']:.0f} ~ {p['pred_max']:.0f} (ì‹¬ê°)\n"
        elif p['pred_max'] >= 280:
            analysis += f"  âš ï¸ {p['horizon']}ë¶„ í›„: {p['pred_min']:.0f} ~ {p['pred_max']:.0f} (ì£¼ì˜)\n"
        else:
            analysis += f"  âœ… {p['horizon']}ë¶„ í›„: {p['pred_min']:.0f} ~ {p['pred_max']:.0f} (ì •ìƒ)\n"
    
    # 2. ë²”ì£¼í˜• ë’·ë°›ì¹¨ (ê·¼ê±°)
    analysis += f"\nğŸ¯ ë²”ì£¼í˜• ê·¼ê±° (ë°œìƒ í™•ë¥ ):\n"
    for p in pred_cat:
        if p['prob2'] >= 70:
            analysis += f"  ğŸš¨ {p['horizon']}ë¶„ í›„: ê¸‰ì¦ í™•ë¥  {p['prob2']:.1f}%\n"
        elif p['prob2'] >= 50:
            analysis += f"  âš ï¸ {p['horizon']}ë¶„ í›„: ê¸‰ì¦ í™•ë¥  {p['prob2']:.1f}%\n"
        elif p['prob2'] >= 30:
            analysis += f"  ğŸŸ¡ {p['horizon']}ë¶„ í›„: ê¸‰ì¦ í™•ë¥  {p['prob2']:.1f}%\n"
    
    # 3. ê²°ë¡ 
    analysis += f"\nğŸ“‹ ê²°ë¡ :\n"
    if max_pred_value >= 300 and max_surge_prob >= 70:
        analysis += f"  â†’ {max_horizon['horizon']}ë¶„ í›„ {max_pred_value:.0f}ê¹Œì§€ ìƒìŠ¹ ì˜ˆì¸¡, ë°œìƒ í™•ë¥  {max_surge_prob:.1f}%ë¡œ ë§¤ìš° ë†’ìŒ\n"
        analysis += f"  â†’ HUB ìš©ëŸ‰ í™•ë³´ ë° ìœ ì…ëŸ‰ ì¡°ì ˆ ì¦‰ì‹œ í•„ìš”!"
    elif max_pred_value >= 280 or max_surge_prob >= 50:
        analysis += f"  â†’ {max_horizon['horizon']}ë¶„ í›„ {max_pred_value:.0f}ê¹Œì§€ ìƒìŠ¹ ê°€ëŠ¥, ê¸‰ì¦ í™•ë¥  {max_surge_prob:.1f}%\n"
        analysis += f"  â†’ ë¬¼ë¥˜ íë¦„ ëª¨ë‹ˆí„°ë§ ê°•í™” í•„ìš”"
    else:
        analysis += f"  â†’ í˜„ì¬ ì•ˆì •ì ì´ë‚˜ ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ í•„ìš”"
    
    return analysis

def generate_llm_analysis(result):
    """LLMìœ¼ë¡œ M14 ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„"""
    predictions = result['predictions']
    current_val = result['current_value']
    current_status = result['current_status']
    
    # í˜„ì¬ ì§€í‘œ ë°ì´í„°
    current_m14b = result.get('current_m14b', 0)
    current_m14bsum = result.get('current_m14bsum', 0)
    current_gap = result.get('current_gap', 0)
    current_trans = result.get('current_trans', 0)
    
    # ìµœëŒ€ ìœ„í—˜ë„
    max_danger = max(p['danger_probability'] for p in predictions)
    
    # ìœ„í—˜ ìš”ì¸ ë¶„ì„
    risk_factors = []
    if current_m14b > 520:
        risk_factors.append(f"M14AM14B({current_m14b:.0f})ê°€ ì‹¬ê° ì„ê³„ê°’(520) ì´ˆê³¼")
    elif current_m14b > 517:
        risk_factors.append(f"M14AM14B({current_m14b:.0f})ê°€ ì£¼ì˜ ì„ê³„ê°’(517) ì´ˆê³¼")
    
    if current_m14bsum > 588:
        risk_factors.append(f"M14AM14BSUM({current_m14bsum:.0f})ì´ ì‹¬ê° ì„ê³„ê°’(588) ì´ˆê³¼")
    elif current_m14bsum > 576:
        risk_factors.append(f"M14AM14BSUM({current_m14bsum:.0f})ì´ ì£¼ì˜ ì„ê³„ê°’(576) ì´ˆê³¼")
    
    if current_gap > 300:
        risk_factors.append(f"queue_gap({current_gap:.0f})ì´ ìœ„í—˜ ì„ê³„ê°’(300) ì´ˆê³¼")
    
    if current_trans > 180:
        risk_factors.append(f"TRANSPORT({current_trans:.0f})ê°€ ì‹¬ê° ì„ê³„ê°’(180) ì´ˆê³¼")
    elif current_trans > 151:
        risk_factors.append(f"TRANSPORT({current_trans:.0f})ê°€ ì£¼ì˜ ì„ê³„ê°’(151) ì´ˆê³¼")
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    pred_text = ""
    for pred in predictions:
        pred_text += f"{pred['horizon']}ë¶„ í›„: {pred['prediction']:,} (ìœ„í—˜ë„ {pred['danger_probability']}%)\n"
    
    risk_text = "\n- ".join(risk_factors) if risk_factors else "ëª¨ë“  ì§€í‘œ ì •ìƒ ë²”ìœ„"
    
    prompt = f"""You MUST answer in Korean only. Be concise and professional.

í˜„ì¬ AMHS ë¬¼ë¥˜ ìƒí™©:
- TOTALCNT: {current_val:,} ({current_status})
- M14AM14B: {current_m14b:.0f} (ì„ê³„ê°’: 517ì£¼ì˜/520ì‹¬ê°)
- M14AM14BSUM: {current_m14bsum:.0f} (ì„ê³„ê°’: 576ì£¼ì˜/588ì‹¬ê°)
- queue_gap: {current_gap:.0f} (ì„ê³„ê°’: 300ìœ„í—˜)
- TRANSPORT: {current_trans:.0f} (ì„ê³„ê°’: 151ì£¼ì˜/180ì‹¬ê°)

âš ï¸ í˜„ì¬ ìœ„í—˜ ìš”ì¸:
- {risk_text}

ì˜ˆì¸¡ ê²°ê³¼:
{pred_text}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ 3-4ë¬¸ì¥ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”:
1. ì™œ ìœ„í—˜ë„ê°€ ë†’ì€ì§€ êµ¬ì²´ì  ì´ìœ  (ì–´ë–¤ ì§€í‘œê°€ ì„ê³„ê°’ ì´ˆê³¼í–ˆëŠ”ì§€)
2. ì˜ˆì¸¡ë˜ëŠ” ì¶”ì„¸
3. ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­

ë‹µë³€:"""
    
    try:
        response = llm(
            prompt,
            max_tokens=250,
            temperature=0.2,
            top_p=0.85,
            repeat_penalty=1.5,
            stop=["ì§ˆë¬¸:", "\n\n\n", "ì´ë¯¸ì§€:", "http"]
        )
        
        raw_answer = response['choices'][0]['text'].strip()
        cleaned = clean_llm_response(raw_answer)
        
        # ì˜ì–´ íŒ¨í„´ ê°ì§€ â†’ í…œí”Œë¦¿ ì‚¬ìš©
        english_patterns = ['please', 'answer', 'korean', 'following', 'response', 'analysis', 'the ', 'is ', 'are ', 'this ']
        has_english = any(p in cleaned.lower() for p in english_patterns)
        
        # ì§€í‘œ ì–¸ê¸‰ í™•ì¸ (ìœ„í—˜ ìš”ì¸ ìˆëŠ”ë° ì§€í‘œ ì•ˆ ë§í•˜ë©´ í…œí”Œë¦¿)
        indicator_keywords = ['M14AM14B', 'M14AM14BSUM', 'queue_gap', 'TRANSPORT', 'ì„ê³„ê°’', 'ì´ˆê³¼', 'ë³‘ëª©', 'ì ì²´']
        has_indicator = any(k in cleaned for k in indicator_keywords)
        
        # LLM ì‘ë‹µì´ ì—†ê±°ë‚˜, ì§§ê±°ë‚˜, ì˜ì–´ ì„ì´ê±°ë‚˜, ìœ„í—˜ìš”ì¸ ìˆëŠ”ë° ì§€í‘œ ì•ˆ ë§í•˜ë©´ â†’ í…œí”Œë¦¿
        if not cleaned or len(cleaned) < 20 or has_english or (risk_factors and not has_indicator):
            logger.warning(f"LLM ì‘ë‹µ ë¶€ì ì ˆ, í…œí”Œë¦¿ ì‚¬ìš©: {cleaned[:50] if cleaned else 'empty'}")
            return generate_m14_template_analysis(result, risk_factors, max_danger)
        
        return cleaned
        
    except Exception as e:
        logger.error(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
        return generate_m14_template_analysis(result, risk_factors, max_danger)

def generate_m14_template_analysis(result, risk_factors, max_danger):
    """LLM ì‹¤íŒ¨ì‹œ í…œí”Œë¦¿ ê¸°ë°˜ M14 ë¶„ì„"""
    predictions = result['predictions']
    max_pred = max(p['prediction'] for p in predictions)
    
    if not risk_factors:
        return "í˜„ì¬ ëª¨ë“  ì§€í‘œê°€ ì •ìƒ ë²”ìœ„ì…ë‹ˆë‹¤. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ë©° ìƒí™©ì„ ì§€ì¼œë³´ì„¸ìš”."
    
    analysis = f"âš ï¸ ìœ„í—˜ë„ {max_danger}% ì›ì¸:\n"
    for factor in risk_factors:
        analysis += f"ğŸš¨ {factor}\n"
    
    if max_pred >= 1700:
        analysis += f"\nì˜ˆì¸¡ ìµœëŒ€ê°’ {max_pred:,}ìœ¼ë¡œ CRITICAL ìƒíƒœ ì§„ì… ì˜ˆìƒ. ì¦‰ì‹œ ë¬¼ë¥˜ ë¶„ì‚° ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
    else:
        analysis += f"\nì˜ˆì¸¡ ìµœëŒ€ê°’ {max_pred:,}. ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì´ í•„ìš”í•©ë‹ˆë‹¤."
    
    return analysis

@app.get("/dashboard/{filename}")
async def get_dashboard(filename: str):
    """ìƒì„±ëœ HTML ëŒ€ì‹œë³´ë“œ ë°˜í™˜"""
    filepath = os.path.join("dashboards", filename)
    
    if not os.path.exists(filepath):
        return JSONResponse(
            content={"error": "File not found"},
            status_code=404
        )
    
    return FileResponse(filepath)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)