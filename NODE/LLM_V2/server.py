#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSV ì§ì ‘ ê²€ìƒ‰ RAG ì„œë²„ (ë²¡í„°DB ì—†ìŒ)
"""

import os
from fastapi import FastAPI
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
import logging
import pandas as pd
import re
import numpy as np
import pickle
from datetime import datetime, timedelta
from io import StringIO
import json

# M14 ì˜ˆì¸¡ ëª¨ë“ˆ
import m14_predictor

# HUB ì˜ˆì¸¡ ëª¨ë“ˆ
import hub_predictor_numerical
import hub_predictor_categorical

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# ì „ì—­ ë³€ìˆ˜
llm = None
df = None
COLUMN_DEFINITIONS = ""

def load_column_definitions():
    """ì»¬ëŸ¼ ì •ì˜ íŒŒì¼ ë¡œë“œ"""
    try:
        with open("column_definitions_short.txt", "r", encoding="utf-8") as f:
            return f.read()
    except:
        try:
            with open("column_definitions.txt", "r", encoding="utf-8") as f:
                return f.read()
        except Exception as e:
            logger.error(f"ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return ""

@app.on_event("startup")
async def startup():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global llm, df, COLUMN_DEFINITIONS
    
    # 0. ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ
    COLUMN_DEFINITIONS = load_column_definitions()
    logger.info("âœ… ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ ì™„ë£Œ")
    
    # 1. CSV ë¡œë“œ
    CSV_PATH = "./CSV/2025_DATA.CSV"
    
    if os.path.exists(CSV_PATH):
        logger.info(f"CSV ë¡œë“œ ì¤‘: {CSV_PATH}")
        
        try:
            df = pd.read_csv(CSV_PATH, encoding='utf-8')
            logger.info(f"âœ… CSV ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰, {len(df.columns)}ì»¬ëŸ¼")
            logger.info(f"ì»¬ëŸ¼: {list(df.columns[:5])}...")
            
            # STAT_DTê°€ ìˆëŠ”ì§€ í™•ì¸
            if 'STAT_DT' not in df.columns:
                logger.error("âŒ STAT_DT ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            logger.error(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        logger.error(f"âŒ CSV íŒŒì¼ ì—†ìŒ: {CSV_PATH}")
    
    # 2. LLM ë¡œë“œ
    MODEL_PATH = "models/Qwen3-1.7B-Q8_0.gguf"
    
    if os.path.exists(MODEL_PATH):
        logger.info(f"LLM ë¡œë“œ ì‹œì‘: {MODEL_PATH}")
        
        try:
            from llama_cpp import Llama
            
            llm = Llama(
                model_path=MODEL_PATH,
                n_ctx=3000,
                n_batch=256,
                n_gpu_layers=0,
                n_threads=6,
                verbose=False
            )
            
            logger.info("âœ… LLM ë¡œë“œ ì„±ê³µ!")
            
        except Exception as e:
            logger.error(f"âŒ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        logger.warning(f"âš ï¸ LLM ëª¨ë¸ ì—†ìŒ: {MODEL_PATH}")

def search_csv(query):
    """CSVì—ì„œ ì§ì ‘ ê²€ìƒ‰"""
    if df is None:
        return None, "CSV íŒŒì¼ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    # ì‹œê°„ íŒ¨í„´ ì¶”ì¶œ (202509210013 í˜•ì‹)
    time_pattern = r'(\d{12})'
    time_match = re.search(time_pattern, query)
    
    if time_match:
        stat_dt = time_match.group(1)
        logger.info(f"ì‹œê°„ ê²€ìƒ‰: {stat_dt}")
        
        # STAT_DTë¡œ ì •í™•íˆ ë§¤ì¹­
        result = df[df['STAT_DT'].astype(str) == stat_dt]
        
        if not result.empty:
            # ì²« ë²ˆì§¸ ë§¤ì¹­ í–‰ ë°˜í™˜
            row = result.iloc[0]
            
            # ë°ì´í„° í¬ë§·íŒ… (ì£¼ìš” ì»¬ëŸ¼ë§Œ)
            data_text = f"ì‹œê°„: {stat_dt}\n"
            
            # ì£¼ìš” ì»¬ëŸ¼ë§Œ í‘œì‹œ
            important_cols = [
                'CURRENT_M16A_3F_JOB', 'CURRENT_M16A_3F_JOB_2',
                'M16A_3F_STORAGE_UTIL', 'HUBROOMTOTAL',
                'M16HUB.QUE.ALL.CURRENTQCNT', 'M16HUB.QUE.TIME.AVGTOTALTIME1MIN',
                'M14A_3F_TO_HUB_JOB2', 'M16A_3F_TO_M14A_3F_JOB'
            ]
            
            for col in important_cols:
                if col in row.index:
                    data_text += f"{col}: {row[col]}\n"
            
            return row, data_text
        else:
            return None, f"ì‹œê°„ {stat_dt}ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    # ì‹œê°„ì´ ì—†ìœ¼ë©´ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ê²€ìƒ‰
    col_pattern = r'([A-Z_\.]+)'
    col_matches = re.findall(col_pattern, query)
    
    if col_matches:
        # ìµœê·¼ 5ê°œ ë°ì´í„° ìš”ì•½
        recent_data = df.tail(5)
        data_text = f"ìµœê·¼ 5ê°œ ë°ì´í„°:\n"
        
        for idx, row in recent_data.iterrows():
            stat_dt = row['STAT_DT'] if 'STAT_DT' in row.index else idx
            data_text += f"\n[{stat_dt}]\n"
            
            for col in col_matches:
                if col in row.index:
                    data_text += f"  {col}: {row[col]}\n"
        
        return recent_data, data_text
    
    return None, "ê²€ìƒ‰ ì¡°ê±´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œê°„(ì˜ˆ: 202509210013) ë˜ëŠ” ì»¬ëŸ¼ëª…ì„ í¬í•¨í•´ì£¼ì„¸ìš”."

class Query(BaseModel):
    question: str
    mode: str = "search"  # ê¸°ë³¸ê°’: search

class PredictQuery(BaseModel):
    mode: str  # "m14" or "hub"
    data: str  # CSV ë°ì´í„°

@app.get("/")
async def home():
    """ë©”ì¸ í˜ì´ì§€"""
    return FileResponse("index.html")

@app.post("/ask")
async def ask(query: Query):
    """RAG ì§ˆë¬¸ ì²˜ë¦¬"""
    global COLUMN_DEFINITIONS
    
    if llm is None:
        return {"answer": "âŒ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    
    try:
        logger.info(f"ì§ˆë¬¸: {query.question} | ëª¨ë“œ: {query.mode}")
        
        # ëª¨ë“œë³„ ì²˜ë¦¬
        if query.mode == "search":
            # ë°ì´í„° ê²€ìƒ‰ ëª¨ë“œ
            result, data_text = search_csv(query.question)
            
            if result is None:
                return {"answer": data_text}
        
        elif query.mode == "m14":
            # M14 ì˜ˆì¸¡ ëª¨ë“œ
            data_text = "M14 ì˜ˆì¸¡ ê¸°ëŠ¥ì€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.\ní˜„ì¬ëŠ” ë°ì´í„° ê²€ìƒ‰ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            return {"answer": data_text}
        
        elif query.mode == "hub":
            # HUB ì˜ˆì¸¡ ëª¨ë“œ
            data_text = "HUB ì˜ˆì¸¡ ê¸°ëŠ¥ì€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.\ní˜„ì¬ëŠ” ë°ì´í„° ê²€ìƒ‰ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            return {"answer": data_text}
        
        else:
            # ê¸°ë³¸ê°’: ê²€ìƒ‰
            result, data_text = search_csv(query.question)
            
            if result is None:
                return {"answer": data_text}
        
        # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""You MUST answer in Korean only. Be concise.
ë‹¹ì‹ ì€ AMHS ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ì»¬ëŸ¼ ì •ì˜:
{COLUMN_DEFINITIONS}

ê²€ìƒ‰ëœ ë°ì´í„°:
{data_text}

ì§ˆë¬¸: {query.question}

ë‹µë³€ (í•œêµ­ì–´, ê°„ê²°í•˜ê²Œ):"""
        
        # 3. LLM í˜¸ì¶œ
        response = llm(
            prompt,
            max_tokens=150,
            temperature=0.2,
            top_p=0.85,
            repeat_penalty=1.5,
            frequency_penalty=0.5,
            presence_penalty=0.3,
            stop=["ì§ˆë¬¸:", "Question:", "[", "ì¶”ì •ê°’"]
        )
        
        answer = response['choices'][0]['text'].strip()
        
        # ë°˜ë³µ íŒ¨í„´ ì œê±°
        lines = answer.split('\n')
        seen = set()
        unique_lines = []
        for line in lines:
            line_clean = line.strip()
            if line_clean and line_clean not in seen:
                seen.add(line_clean)
                unique_lines.append(line)
        
        answer = '\n'.join(unique_lines[:5])
        
        logger.info(f"ë‹µë³€ ìƒì„± ì™„ë£Œ")
        
        return {"answer": answer.strip()}
        
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {"answer": f"âŒ ì˜¤ë¥˜: {str(e)}"}

@app.post("/predict")
async def predict(query: PredictQuery):
    """M14/HUB ì˜ˆì¸¡ ì²˜ë¦¬"""
    try:
        logger.info(f"ì˜ˆì¸¡ ìš”ì²­: ëª¨ë“œ={query.mode}")
        
        if query.mode == "m14":
            # M14 ì˜ˆì¸¡ ì‹¤í–‰
            result = m14_predictor.predict_m14(query.data)
            
            if 'error' in result:
                return JSONResponse(content=result, status_code=400)
            
            # HTML ëŒ€ì‹œë³´ë“œ ì €ì¥
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            dashboard_filename = f'M14_Dashboard_{timestamp}.html'
            dashboard_path = os.path.join('dashboards', dashboard_filename)
            
            os.makedirs('dashboards', exist_ok=True)
            with open(dashboard_path, 'w', encoding='utf-8') as f:
                f.write(result['dashboard_html'])
            
            logger.info(f"ëŒ€ì‹œë³´ë“œ ì €ì¥: {dashboard_filename}")
            
            # ê°„ë‹¨ ìš”ì•½ ìƒì„±
            summary = generate_prediction_summary(result)
            
            # LLM í•´ì„ (ìˆìœ¼ë©´)
            llm_analysis = ""
            if llm is not None:
                try:
                    llm_analysis = generate_llm_analysis(result)
                except Exception as e:
                    logger.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            return {
                "success": True,
                "summary": summary,
                "llm_analysis": llm_analysis,
                "dashboard_url": f"/dashboard/{dashboard_filename}",
                "predictions": result['predictions'],
                "current_value": result['current_value'],
                "current_status": result['current_status']
            }
        
        elif query.mode == "hub":
            # HUB ì˜ˆì¸¡ ì‹¤í–‰ (ìˆ˜ì¹˜í˜• + ë²”ì£¼í˜•)
            result_numerical = hub_predictor_numerical.predict_hub_numerical(query.data)
            result_categorical = hub_predictor_categorical.predict_hub_categorical(query.data)
            
            if 'error' in result_numerical:
                return JSONResponse(content=result_numerical, status_code=400)
            
            if 'error' in result_categorical:
                return JSONResponse(content=result_categorical, status_code=400)
            
            # HTML ëŒ€ì‹œë³´ë“œ ì €ì¥ (2ê°œ)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            dashboard_numerical_filename = f'HUB_Numerical_{timestamp}.html'
            dashboard_numerical_path = os.path.join('dashboards', dashboard_numerical_filename)
            
            dashboard_categorical_filename = f'HUB_Categorical_{timestamp}.html'
            dashboard_categorical_path = os.path.join('dashboards', dashboard_categorical_filename)
            
            os.makedirs('dashboards', exist_ok=True)
            
            with open(dashboard_numerical_path, 'w', encoding='utf-8') as f:
                f.write(result_numerical['dashboard_html'])
            
            with open(dashboard_categorical_path, 'w', encoding='utf-8') as f:
                f.write(result_categorical['dashboard_html'])
            
            logger.info(f"ëŒ€ì‹œë³´ë“œ ì €ì¥: {dashboard_numerical_filename}, {dashboard_categorical_filename}")
            
            # ê°„ë‹¨ ìš”ì•½ ìƒì„±
            summary = generate_hub_summary(result_numerical, result_categorical)
            
            # LLM í•´ì„ (ìˆìœ¼ë©´)
            llm_analysis = ""
            if llm is not None:
                try:
                    llm_analysis = generate_hub_llm_analysis(result_numerical, result_categorical)
                except Exception as e:
                    logger.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            return {
                "success": True,
                "summary": summary,
                "llm_analysis": llm_analysis,
                "dashboard_numerical_url": f"/dashboard/{dashboard_numerical_filename}",
                "dashboard_categorical_url": f"/dashboard/{dashboard_categorical_filename}",
                "predictions_numerical": result_numerical['predictions'],
                "predictions_categorical": result_categorical['predictions'],
                "current_value": result_numerical['current_value']
            }
        
        else:
            return {
                "error": "Invalid mode",
                "message": "modeëŠ” 'm14' ë˜ëŠ” 'hub'ì—¬ì•¼ í•©ë‹ˆë‹¤."
            }
        
    except Exception as e:
        logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return JSONResponse(
            content={"error": "Prediction failed", "message": str(e)},
            status_code=500
        )

def generate_prediction_summary(result):
    """ì˜ˆì¸¡ ê²°ê³¼ ê°„ë‹¨ ìš”ì•½"""
    predictions = result['predictions']
    current_val = result['current_value']
    current_status = result['current_status']
    
    summary = f"ğŸ“Š í˜„ì¬: {current_val:,} ({current_status})\n\n"
    summary += "ğŸ”® ì˜ˆì¸¡:\n"
    
    for pred in predictions:
        status_emoji = {
            'LOW': 'âœ…',
            'NORMAL': 'ğŸŸ¢',
            'CAUTION': 'âš ï¸',
            'CRITICAL': 'ğŸš¨'
        }.get(pred['status'], 'â“')
        
        summary += f"â€¢ {pred['horizon']}ë¶„: {pred['prediction']:,} {status_emoji} (ìœ„í—˜ {pred['danger_probability']}%)\n"
    
    return summary

def generate_hub_summary(result_numerical, result_categorical):
    """HUB ì˜ˆì¸¡ ê²°ê³¼ ê°„ë‹¨ ìš”ì•½"""
    current_val = result_numerical['current_value']
    
    pred_num = result_numerical['predictions']
    pred_cat = result_categorical['predictions']
    
    summary = f"ğŸ“Š í˜„ì¬: {current_val:,.1f}\n\n"
    summary += "ğŸ”¢ ìˆ˜ì¹˜í˜• ì˜ˆì¸¡:\n"
    
    for pred in pred_num:
        status_emoji = {
            'NORMAL': 'âœ…',
            'CAUTION': 'âš ï¸',
            'WARNING': 'ğŸŸ ',
            'CRITICAL': 'ğŸš¨'
        }.get(pred['status'], 'â“')
        
        summary += f"â€¢ {pred['horizon']}ë¶„: {pred['pred_min']:.1f} ~ {pred['pred_max']:.1f} {status_emoji}\n"
    
    summary += "\nğŸ¯ ë²”ì£¼í˜• ì˜ˆì¸¡:\n"
    
    for pred in pred_cat:
        status_emoji = {
            'LOW': 'âœ…',
            'MEDIUM': 'âš ï¸',
            'HIGH': 'ğŸŸ ',
            'CRITICAL': 'ğŸš¨'
        }.get(pred['status'], 'â“')
        
        summary += f"â€¢ {pred['horizon']}ë¶„: {pred['class_name']} (ê¸‰ì¦ {pred['prob2']:.1f}%) {status_emoji}\n"
    
    return summary

def generate_hub_llm_analysis(result_numerical, result_categorical):
    """LLMìœ¼ë¡œ HUB ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„"""
    current_val = result_numerical['current_value']
    
    pred_num = result_numerical['predictions']
    pred_cat = result_categorical['predictions']
    
    # ìµœëŒ€ ê¸‰ì¦ í™•ë¥ 
    max_surge_prob = max(p['prob2'] for p in pred_cat)
    
    # ìµœëŒ€ ì˜ˆì¸¡ê°’
    max_pred_value = max(p['pred_max'] for p in pred_num)
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    pred_num_text = ""
    for pred in pred_num:
        pred_num_text += f"{pred['horizon']}ë¶„ í›„: {pred['pred_min']:.1f} ~ {pred['pred_max']:.1f} (ìƒíƒœ {pred['status']})\n"
    
    pred_cat_text = ""
    for pred in pred_cat:
        pred_cat_text += f"{pred['horizon']}ë¶„ í›„: {pred['class_name']} (ê¸‰ì¦ {pred['prob2']:.1f}%, ìƒíƒœ {pred['status']})\n"
    
    prompt = f"""You MUST answer in Korean only. Be concise and professional.

í˜„ì¬ HUB ë¬¼ë¥˜ ìƒí™©:
- í˜„ì¬ CURRENT_M16A_3F_JOB_2: {current_val:,.1f}

ìˆ˜ì¹˜í˜• ì˜ˆì¸¡ ê²°ê³¼:
{pred_num_text}

ë²”ì£¼í˜• ì˜ˆì¸¡ ê²°ê³¼:
{pred_cat_text}

ìµœëŒ€ ê¸‰ì¦ í™•ë¥ : {max_surge_prob:.1f}%
ìµœëŒ€ ì˜ˆì¸¡ê°’: {max_pred_value:.1f}

ìœ„ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš” (3-4ë¬¸ì¥):
1. í˜„ì¬ ìƒí™© í‰ê°€
2. ì˜ˆì¸¡ë˜ëŠ” ì¶”ì„¸ (ì¦ê°€/ê°ì†Œ/ì•ˆì •)
3. ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­

ë‹µë³€ (í•œêµ­ì–´):"""
    
    try:
        response = llm(
            prompt,
            max_tokens=200,
            temperature=0.3,
            top_p=0.85,
            repeat_penalty=1.5,
            stop=["ì§ˆë¬¸:", "\n\n\n"]
        )
        
        answer = response['choices'][0]['text'].strip()
        return answer
        
    except Exception as e:
        logger.error(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
        return ""
    """LLMìœ¼ë¡œ ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„"""
    predictions = result['predictions']
    current_val = result['current_value']
    current_status = result['current_status']
    
    # ìµœëŒ€ ìœ„í—˜ë„
    max_danger = max(p['danger_probability'] for p in predictions)
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    pred_text = ""
    for pred in predictions:
        pred_text += f"{pred['horizon']}ë¶„ í›„: {pred['prediction']:,} (ìœ„í—˜ë„ {pred['danger_probability']}%, ìƒíƒœ {pred['status']})\n"
    
    prompt = f"""You MUST answer in Korean only. Be concise and professional.

í˜„ì¬ AMHS ë¬¼ë¥˜ ìƒí™©:
- í˜„ì¬ TOTALCNT: {current_val:,} ({current_status})

ì˜ˆì¸¡ ê²°ê³¼:
{pred_text}

ìµœëŒ€ ìœ„í—˜ë„: {max_danger}%

ìœ„ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš” (3-4ë¬¸ì¥):
1. í˜„ì¬ ìƒí™© í‰ê°€
2. ì˜ˆì¸¡ë˜ëŠ” ì¶”ì„¸ (ì¦ê°€/ê°ì†Œ/ì•ˆì •)
3. ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­

ë‹µë³€ (í•œêµ­ì–´):"""
    
    try:
        response = llm(
            prompt,
            max_tokens=200,
            temperature=0.3,
            top_p=0.85,
            repeat_penalty=1.5,
            stop=["ì§ˆë¬¸:", "\n\n\n"]
        )
        
        answer = response['choices'][0]['text'].strip()
        return answer
        
    except Exception as e:
        logger.error(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
        return ""

@app.get("/dashboard/{filename}")
async def get_dashboard(filename: str):
    """ìƒì„±ëœ HTML ëŒ€ì‹œë³´ë“œ ë°˜í™˜"""
    filepath = os.path.join("dashboards", filename)
    
    if not os.path.exists(filepath):
        return JSONResponse(
            content={"error": "File not found"},
            status_code=404
        )
    
    return FileResponse(filepath)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)