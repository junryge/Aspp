#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë²¡í„°DB + ì»¬ëŸ¼ ì •ì˜ í†µí•© RAG ì„œë²„
"""

import os
from fastapi import FastAPI
from fastapi.responses import FileResponse
from pydantic import BaseModel
import logging
import pickle
import numpy as np

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# ì „ì—­ ë³€ìˆ˜
llm = None
vectordb = None
embedding_model = None
COLUMN_DEFINITIONS = ""

def load_column_definitions():
    """ì»¬ëŸ¼ ì •ì˜ íŒŒì¼ ë¡œë“œ"""
    try:
        with open("column_definitions.txt", "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        logger.error(f"ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return "ì»¬ëŸ¼ ì •ì˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

@app.on_event("startup")
async def startup():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global llm, vectordb, embedding_model, COLUMN_DEFINITIONS
    
    # 0. ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ
    COLUMN_DEFINITIONS = load_column_definitions()
    logger.info("âœ… ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ ì™„ë£Œ")
    
    # 1. LLM ë¡œë“œ
    MODEL_PATH = "./models/QWEN3-1.7B-18_0.GGUF"
    
    if os.path.exists(MODEL_PATH):
        logger.info(f"LLM ë¡œë“œ ì‹œì‘: {MODEL_PATH}")
        
        try:
            from llama_cpp import Llama
            
            llm = Llama(
                model_path=MODEL_PATH,
                n_ctx=1024,
                n_batch=128,
                n_gpu_layers=0,
                n_threads=6,
                verbose=False
            )
            
            logger.info("âœ… LLM ë¡œë“œ ì„±ê³µ!")
            
        except Exception as e:
            logger.error(f"âŒ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        logger.warning(f"âš ï¸ LLM ëª¨ë¸ ì—†ìŒ: {MODEL_PATH}")
    
    # 2. ë²¡í„°DB ë¡œë“œ
    DB_PATH = "./vector_db/vectordb.pkl"
    
    if os.path.exists(DB_PATH):
        logger.info(f"ë²¡í„°DB ë¡œë“œ ì¤‘: {DB_PATH}")
        
        try:
            with open(DB_PATH, 'rb') as f:
                vectordb = pickle.load(f)
            
            logger.info(f"âœ… ë²¡í„°DB ë¡œë“œ ì™„ë£Œ: {len(vectordb['documents'])}ê°œ ë¬¸ì„œ")
            
        except Exception as e:
            logger.error(f"âŒ ë²¡í„°DB ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        logger.warning(f"âš ï¸ ë²¡í„°DB ì—†ìŒ: {DB_PATH}")
    
    # 3. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    EMB_PATH = "./embeddings/all-MiniLM-L6-v2"
    
    try:
        from sentence_transformers import SentenceTransformer
        
        if os.path.exists(EMB_PATH):
            logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {EMB_PATH}")
            embedding_model = SentenceTransformer(EMB_PATH)
        else:
            logger.info("ì˜¨ë¼ì¸ì—ì„œ ì„ë² ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ...")
            embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

def search_similar(query, k=3):
    """ë²¡í„°DBì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
    if vectordb is None or embedding_model is None:
        return []
    
    try:
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = embedding_model.encode([query])[0]
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i, doc_embedding in enumerate(vectordb['embeddings']):
            sim = np.dot(query_embedding, doc_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
            )
            similarities.append((i, sim))
        
        # ìƒìœ„ kê°œ ì„ íƒ
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_k = similarities[:k]
        
        results = []
        for idx, score in top_k:
            doc = vectordb['documents'][idx]
            results.append({
                'content': doc['content'],
                'stat_dt': doc.get('stat_dt', 'Unknown'),
                'score': float(score)
            })
        
        return results
        
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

class Query(BaseModel):
    question: str

@app.get("/")
async def home():
    """ë©”ì¸ í˜ì´ì§€"""
    return FileResponse("index.html")

@app.post("/ask")
async def ask(query: Query):
    """RAG ì§ˆë¬¸ ì²˜ë¦¬"""
    global COLUMN_DEFINITIONS
    
    if llm is None:
        return {"answer": "âŒ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    
    try:
        logger.info(f"ì§ˆë¬¸: {query.question}")
        
        # 1. ë²¡í„°DB ê²€ìƒ‰
        search_results = search_similar(query.question, k=3)
        
        # 2. ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…
        context = ""
        if search_results:
            context = "\n\nğŸ“ˆ ê´€ë ¨ ë°ì´í„°:\n"
            for i, result in enumerate(search_results, 1):
                context += f"\n[{i}] {result['stat_dt']}\n"
                # ë„ˆë¬´ ê¸¸ë©´ ì¼ë¶€ë§Œ í‘œì‹œ
                content_preview = result['content'][:200]
                context += f"{content_preview}...\n"
        
        # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¹ì‹ ì€ ë°˜ë„ì²´ ì œì¡° AMHS ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.

{COLUMN_DEFINITIONS}
{context}

ì§ˆë¬¸: {query.question}
ë‹µë³€:"""
        
        # 4. LLM í˜¸ì¶œ
        response = llm(
            prompt,
            max_tokens=500,
            temperature=0.7,
            top_p=0.9,
            repeat_penalty=1.1,
            stop=["ì§ˆë¬¸:", "\n\n", "ğŸ“Š"]
        )
        
        answer = response['choices'][0]['text'].strip()
        logger.info(f"ë‹µë³€ ìƒì„± ì™„ë£Œ")
        
        return {"answer": answer}
        
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {"answer": f"âŒ ì˜¤ë¥˜: {str(e)}"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)