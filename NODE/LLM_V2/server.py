#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSV ì§ì ‘ ê²€ìƒ‰ RAG ì„œë²„ (csv_searcher ëª¨ë“ˆ ì‚¬ìš©)
"""

import os
import re
from fastapi import FastAPI
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
import logging
from datetime import datetime
import json

import csv_searcher
import m14_predictor
import hub_predictor_numerical
import hub_predictor_categorical

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

llm = None
COLUMN_DEFINITIONS = ""

def load_column_definitions():
    try:
        with open("column_definitions_short.txt", "r", encoding="utf-8") as f:
            return f.read()
    except:
        try:
            with open("column_definitions.txt", "r", encoding="utf-8") as f:
                return f.read()
        except:
            return ""

def clean_llm_response(text):
    """LLM ì‘ë‹µ ì •ì œ"""
    if not text:
        return ""
    text = re.sub(r'\[+[^\]]*\]+', '', text)
    text = re.sub(r'#{1,6}\s*.*', '', text)
    text = re.sub(r'```[\s\S]*?```', '', text)
    text = re.sub(r'`[^`]*`', '', text)
    text = re.sub(r'\*{1,2}', '', text)
    text = re.sub(r'^\d+[\.\)]\s*', '', text, flags=re.MULTILINE)
    text = re.sub(r'^[-*â€¢]\s*', '', text, flags=re.MULTILINE)
    for word in ['ë¬¸ì œ', 'ìš”ì•½', 'ì„¤ëª…í•˜ì„¸ìš”', 'ë°”íƒ•ìœ¼ë¡œ', 'ê°„ëµíˆ', 'ë¶„ì„ ê²°ê³¼', 'ë‹µë³€:', 'ê²°ê³¼:']:
        text = text.replace(word, '')
    lines = [l.strip() for l in text.split('\n') if l.strip() and len(l.strip()) > 3]
    result = ' '.join(lines)
    sentences = re.split(r'[.ã€‚!]\s*', result)
    valid = [s.strip() for s in sentences if len(s.strip()) > 5][:2]
    return '. '.join(valid) + '.' if valid else ""

@app.on_event("startup")
async def startup():
    global llm, COLUMN_DEFINITIONS
    COLUMN_DEFINITIONS = load_column_definitions()
    logger.info("âœ… ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ ì™„ë£Œ")
    
    CSV_PATH = "./csv/with.csv"
    if os.path.exists(CSV_PATH):
        if csv_searcher.load_csv(CSV_PATH):
            logger.info("âœ… CSV ë¡œë“œ ì™„ë£Œ")
    
    MODEL_PATH = "models/Qwen3-1.7B-Q8_0.gguf"
    if os.path.exists(MODEL_PATH):
        try:
            from llama_cpp import Llama
            llm = Llama(model_path=MODEL_PATH, n_ctx=3000, n_batch=256, n_gpu_layers=0, n_threads=6, verbose=False)
            logger.info("âœ… LLM ë¡œë“œ ì„±ê³µ!")
        except Exception as e:
            logger.error(f"âŒ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")

class Query(BaseModel):
    question: str
    mode: str = "search"

class PredictQuery(BaseModel):
    mode: str
    data: str

@app.get("/")
async def home():
    return FileResponse("index.html")

@app.get("/columns")
async def get_columns():
    return {"columns": csv_searcher.get_columns()}

@app.get("/stats/{column}")
async def get_column_stats(column: str):
    return csv_searcher.get_statistics(column)

@app.post("/ask")
async def ask(query: Query):
    try:
        logger.info(f"ì§ˆë¬¸: {query.question} | ëª¨ë“œ: {query.mode}")
        
        if query.mode == "search":
            result, data_text = csv_searcher.search_csv(query.question)
            
            if result is None:
                return {"answer": data_text}
            
            # HTML ì¶œë ¥ (<br> ìœ ì§€)
            answer = f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼<br>{data_text}"
            
            if llm is not None:
                try:
                    data_plain = data_text.replace('<br>', ' ')
                    prompt = f"ë°ì´í„°: {data_plain}\n\nìœ„ ë°ì´í„° ìƒíƒœë¥¼ í•œêµ­ì–´ 2ë¬¸ì¥ìœ¼ë¡œ í‰ê°€í•´. í¬ë§· ì—†ì´ í‰ë¬¸ë§Œ."
                    response = llm(prompt, max_tokens=60, temperature=0.1, repeat_penalty=2.0,
                                   stop=["\n\n", "[[", "[", "1.", "###", "```", "ë°ì´í„°:"])
                    raw = response['choices'][0]['text'].strip()
                    analysis = clean_llm_response(raw)
                    if analysis and len(analysis) > 10:
                        answer += f"<br><br>---<br>ğŸ¤– ë¶„ì„<br>{analysis}"
                except Exception as e:
                    logger.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            return {"answer": answer}
        
        elif query.mode == "m14":
            return {"answer": "M14 ì˜ˆì¸¡ ê¸°ëŠ¥ì€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.\ní˜„ì¬ëŠ” ë°ì´í„° ê²€ìƒ‰ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."}
        elif query.mode == "hub":
            return {"answer": "HUB ì˜ˆì¸¡ ê¸°ëŠ¥ì€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.\ní˜„ì¬ëŠ” ë°ì´í„° ê²€ìƒ‰ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."}
        else:
            result, data_text = csv_searcher.search_csv(query.question)
            return {"answer": data_text if result else data_text}
        
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {"answer": f"âŒ ì˜¤ë¥˜: {str(e)}"}

@app.post("/predict")
async def predict(query: PredictQuery):
    try:
        logger.info(f"ì˜ˆì¸¡ ìš”ì²­: ëª¨ë“œ={query.mode}")
        
        if query.mode == "m14":
            result = m14_predictor.predict_m14(query.data)
            if 'error' in result:
                return JSONResponse(content=result, status_code=400)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            dashboard_filename = f'M14_Dashboard_{timestamp}.html'
            os.makedirs('dashboards', exist_ok=True)
            with open(f'dashboards/{dashboard_filename}', 'w', encoding='utf-8') as f:
                f.write(result['dashboard_html'])
            
            summary = generate_prediction_summary(result)
            llm_analysis = generate_llm_analysis(result) if llm else ""
            
            return {
                "success": True, "summary": summary, "llm_analysis": llm_analysis,
                "dashboard_url": f"/dashboard/{dashboard_filename}",
                "predictions": result['predictions'],
                "current_value": result['current_value'],
                "current_status": result['current_status']
            }
        
        elif query.mode == "hub":
            result_num = hub_predictor_numerical.predict_hub_numerical(query.data)
            result_cat = hub_predictor_categorical.predict_hub_categorical(query.data)
            
            if 'error' in result_num:
                return JSONResponse(content=result_num, status_code=400)
            if 'error' in result_cat:
                return JSONResponse(content=result_cat, status_code=400)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            os.makedirs('dashboards', exist_ok=True)
            
            with open(f'dashboards/HUB_Numerical_{timestamp}.html', 'w', encoding='utf-8') as f:
                f.write(result_num['dashboard_html'])
            with open(f'dashboards/HUB_Categorical_{timestamp}.html', 'w', encoding='utf-8') as f:
                f.write(result_cat['dashboard_html'])
            
            summary = generate_hub_summary(result_num, result_cat)
            llm_analysis = generate_hub_llm_analysis(result_num, result_cat) if llm else ""
            
            return {
                "success": True, "summary": summary, "llm_analysis": llm_analysis,
                "dashboard_numerical_url": f"/dashboard/HUB_Numerical_{timestamp}.html",
                "dashboard_categorical_url": f"/dashboard/HUB_Categorical_{timestamp}.html",
                "predictions_numerical": result_num['predictions'],
                "predictions_categorical": result_cat['predictions'],
                "current_value": result_num['current_value']
            }
        
        return {"error": "Invalid mode", "message": "modeëŠ” 'm14' ë˜ëŠ” 'hub'"}
        
    except Exception as e:
        logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        return JSONResponse(content={"error": "Prediction failed", "message": str(e)}, status_code=500)

def generate_prediction_summary(result):
    preds = result['predictions']
    summary = f"ğŸ“Š í˜„ì¬: {result['current_value']:,} ({result['current_status']})\n\nğŸ”® ì˜ˆì¸¡:\n"
    for p in preds:
        emoji = {'LOW':'âœ…','NORMAL':'ğŸŸ¢','CAUTION':'âš ï¸','CRITICAL':'ğŸš¨'}.get(p['status'],'â“')
        summary += f"â€¢ {p['horizon']}ë¶„: {p['prediction']:,} {emoji} (ìœ„í—˜ {p['danger_probability']}%)\n"
    return summary

def generate_hub_summary(result_num, result_cat):
    summary = f"ğŸ“Š í˜„ì¬: {result_num['current_value']:,.1f}\n\nğŸ”¢ ìˆ˜ì¹˜í˜•:\n"
    for p in result_num['predictions']:
        emoji = {'NORMAL':'âœ…','CAUTION':'âš ï¸','WARNING':'ğŸŸ ','CRITICAL':'ğŸš¨'}.get(p['status'],'â“')
        summary += f"â€¢ {p['horizon']}ë¶„: {p['pred_min']:.1f}~{p['pred_max']:.1f} {emoji}\n"
    summary += "\nğŸ¯ ë²”ì£¼í˜•:\n"
    for p in result_cat['predictions']:
        emoji = {'LOW':'âœ…','MEDIUM':'âš ï¸','HIGH':'ğŸŸ ','CRITICAL':'ğŸš¨'}.get(p['status'],'â“')
        summary += f"â€¢ {p['horizon']}ë¶„: {p['class_name']} (ê¸‰ì¦ {p['prob2']:.1f}%) {emoji}\n"
    return summary

def generate_hub_llm_analysis(result_num, result_cat):
    try:
        prompt = f"í˜„ì¬:{result_num['current_value']:.1f}, ìµœëŒ€ì˜ˆì¸¡:{max(p['pred_max'] for p in result_num['predictions']):.1f}, ê¸‰ì¦í™•ë¥ :{max(p['prob2'] for p in result_cat['predictions']):.1f}%\ní•œêµ­ì–´ 2ë¬¸ì¥ í‰ê°€. í¬ë§· ì—†ì´ í‰ë¬¸ë§Œ."
        response = llm(prompt, max_tokens=60, temperature=0.1, repeat_penalty=2.0, stop=["\n\n","[[","[","1."])
        return clean_llm_response(response['choices'][0]['text'].strip())
    except:
        return ""

def generate_llm_analysis(result):
    try:
        prompt = f"í˜„ì¬:{result['current_value']:,}({result['current_status']}), ìµœëŒ€ì˜ˆì¸¡:{max(p['prediction'] for p in result['predictions']):,}, ìœ„í—˜ë„:{max(p['danger_probability'] for p in result['predictions'])}%\ní•œêµ­ì–´ 2ë¬¸ì¥ í‰ê°€. í¬ë§· ì—†ì´ í‰ë¬¸ë§Œ."
        response = llm(prompt, max_tokens=60, temperature=0.1, repeat_penalty=2.0, stop=["\n\n","[[","[","1."])
        return clean_llm_response(response['choices'][0]['text'].strip())
    except:
        return ""

@app.get("/dashboard/{filename}")
async def get_dashboard(filename: str):
    filepath = f"dashboards/{filename}"
    if not os.path.exists(filepath):
        return JSONResponse(content={"error": "File not found"}, status_code=404)
    return FileResponse(filepath)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)