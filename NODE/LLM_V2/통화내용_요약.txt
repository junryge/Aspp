통화 정보
통화 일자: 2025년 11월 27일(수)

통화 요약

LLM 서비스 및 장애 예측 모델
"장애 예측 모델이 거의 끝나가서 LLM 서비스를 만든다고 프로젝트를 띄운다고 하더라고요."
• 장애 예측 모델 개발 완료 단계이며 LLM 서비스 구축 계획이 진행 중
    ◦ LLM 서비스는 장애 예측 모델을 기반으로 한다
• 프로젝트 책임자는 이경찬 님으로 확인됨

파인 튜닝 및 커스텀 모델 정책
"파인 튜닝 학습이 동반돼야 하는 파인 튜닝은 현재 지원이 어렵다."
• 파인 튜닝 정의가 명확해야 하며, 실제 웨이트 변경은 GPU 자원 문제로 어려움
    ◦ 단순 RAG 구성이나 프롬프트 엔지니어링은 가능성 있음
• 내년 GPU 투자 상황에 따라 파인 튜닝 환경 구축 검토 예정
• 현재는 HCP 또는 가이아 환경에서 LLM API를 통해 서비스 제공 예정

GPU 자원 및 운영 정책
"외부에서 다운받아 쓸 수 없고, 하이닉스 자체 API만 사용해야 한다."
• GPU 자원은 사용률 기반으로 관리되며 외부 모델 직접 사용 불가
    ◦ HCP가 확보한 GPU로만 서비스 운영 가능
• 단독 모델 요청 시 사용량이 적으면 회수될 수 있음
• 내년 상반기 GPU 추가 확보는 불확실하며 하반기 가능성 있음

모델 서빙 및 운영 주체 역할
"가이아는 특화 서비스용 모델을 결정하고, HCP는 공통 인프라 운영한다."
• HCP는 공통 모델 서빙 및 API 운영 담당
• 가이아는 특화된 모델 선택 및 운영 주체 역할 수행
• 내년 AMHS 장애 판단 시스템에 반영된 4장 모델은 독립 운영 가능성 있음
• 모델 운영은 사용량과 정책에 따라 협의 및 조정

테스트 환경 및 개발 지원
"테스트는 로컬에서 하거나 ATCP 공용 자원, 가이아 협업 필요하다."
• 로컬 GPU 보유 시 테스트 가능하나 전사 자원 사용 아님
• ATCP 공용 자원 또는 가이아 협업을 통한 테스트 권장
• GPU 확보가 시기적으로 어려워 개발자들이 어려움 겪고 있음

향후 계획 및 협의 방향
"HCP나 가이아에서 API 제공은 가능하니 상용화 모델 선택하면 된다."
• 내년 정책 확정은 연말 또는 내년 초에 명확해질 예정
• 모델 추가 요청 시 GPU 확보 상황과 정책에 따라 결정
• 4장 모델 독립 제공 가능하며, 사용량 기반 회수 정책 적용
• 향후 최신 모델 요청 및 운영은 협의 후 진행

통화 후 해야 할 일
이재임(티엘린)
• 다른 TL들과 LLM 서비스 정책 및 운영 방향 공유 및 설득
• 테스트 환경 관련 로컬 또는 공용 자원 활용 방안 검토
강유
• 내년 GPU 투자 및 파인 튜닝 환경 구축 계획 지속 검토
• HCP 및 가이아와 모델 서빙 및 API 제공 관련 협의 진행
• AMHS 장애 판단 시스템 내 4장 모델 운영 방안 준비