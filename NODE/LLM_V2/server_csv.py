#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSV ì§ì ‘ ê²€ìƒ‰ RAG ì„œë²„ (ë²¡í„°DB ì—†ìŒ)
"""

import os
from fastapi import FastAPI
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
import logging
import pandas as pd
import re
import numpy as np
import pickle
from datetime import datetime, timedelta
from io import StringIO
import json

# M14 ì˜ˆì¸¡ ëª¨ë“ˆ
import m14_predictor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# ì „ì—­ ë³€ìˆ˜
llm = None
df = None
COLUMN_DEFINITIONS = ""

def load_column_definitions():
    """ì»¬ëŸ¼ ì •ì˜ íŒŒì¼ ë¡œë“œ"""
    try:
        with open("column_definitions_short.txt", "r", encoding="utf-8") as f:
            return f.read()
    except:
        try:
            with open("column_definitions.txt", "r", encoding="utf-8") as f:
                return f.read()
        except Exception as e:
            logger.error(f"ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return ""

@app.on_event("startup")
async def startup():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global llm, df, COLUMN_DEFINITIONS
    
    # 0. ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ
    COLUMN_DEFINITIONS = load_column_definitions()
    logger.info("âœ… ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ ì™„ë£Œ")
    
    # 1. CSV ë¡œë“œ
    CSV_PATH = "./CSV/2025_DATA.CSV"
    
    if os.path.exists(CSV_PATH):
        logger.info(f"CSV ë¡œë“œ ì¤‘: {CSV_PATH}")
        
        try:
            df = pd.read_csv(CSV_PATH, encoding='utf-8')
            logger.info(f"âœ… CSV ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰, {len(df.columns)}ì»¬ëŸ¼")
            logger.info(f"ì»¬ëŸ¼: {list(df.columns[:5])}...")
            
            # STAT_DTê°€ ìˆëŠ”ì§€ í™•ì¸
            if 'STAT_DT' not in df.columns:
                logger.error("âŒ STAT_DT ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            logger.error(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        logger.error(f"âŒ CSV íŒŒì¼ ì—†ìŒ: {CSV_PATH}")
    
    # 2. LLM ë¡œë“œ
    MODEL_PATH = "models/Qwen3-1.7B-Q8_0.gguf"
    
    if os.path.exists(MODEL_PATH):
        logger.info(f"LLM ë¡œë“œ ì‹œì‘: {MODEL_PATH}")
        
        try:
            from llama_cpp import Llama
            
            llm = Llama(
                model_path=MODEL_PATH,
                n_ctx=3000,
                n_batch=256,
                n_gpu_layers=0,
                n_threads=6,
                verbose=False
            )
            
            logger.info("âœ… LLM ë¡œë“œ ì„±ê³µ!")
            
        except Exception as e:
            logger.error(f"âŒ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        logger.warning(f"âš ï¸ LLM ëª¨ë¸ ì—†ìŒ: {MODEL_PATH}")

def search_csv(query):
    """CSVì—ì„œ ì§ì ‘ ê²€ìƒ‰"""
    if df is None:
        return None, "CSV íŒŒì¼ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    # ì‹œê°„ íŒ¨í„´ ì¶”ì¶œ (202509210013 í˜•ì‹)
    time_pattern = r'(\d{12})'
    time_match = re.search(time_pattern, query)
    
    if time_match:
        stat_dt = time_match.group(1)
        logger.info(f"ì‹œê°„ ê²€ìƒ‰: {stat_dt}")
        
        # STAT_DTë¡œ ì •í™•íˆ ë§¤ì¹­
        result = df[df['STAT_DT'].astype(str) == stat_dt]
        
        if not result.empty:
            # ì²« ë²ˆì§¸ ë§¤ì¹­ í–‰ ë°˜í™˜
            row = result.iloc[0]
            
            # ë°ì´í„° í¬ë§·íŒ… (ì£¼ìš” ì»¬ëŸ¼ë§Œ)
            data_text = f"ì‹œê°„: {stat_dt}\n"
            
            # ì£¼ìš” ì»¬ëŸ¼ë§Œ í‘œì‹œ
            important_cols = [
                'CURRENT_M16A_3F_JOB', 'CURRENT_M16A_3F_JOB_2',
                'M16A_3F_STORAGE_UTIL', 'HUBROOMTOTAL',
                'M16HUB.QUE.ALL.CURRENTQCNT', 'M16HUB.QUE.TIME.AVGTOTALTIME1MIN',
                'M14A_3F_TO_HUB_JOB2', 'M16A_3F_TO_M14A_3F_JOB'
            ]
            
            for col in important_cols:
                if col in row.index:
                    data_text += f"{col}: {row[col]}\n"
            
            return row, data_text
        else:
            return None, f"ì‹œê°„ {stat_dt}ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    # ì‹œê°„ì´ ì—†ìœ¼ë©´ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ê²€ìƒ‰
    col_pattern = r'([A-Z_\.]+)'
    col_matches = re.findall(col_pattern, query)
    
    if col_matches:
        # ìµœê·¼ 5ê°œ ë°ì´í„° ìš”ì•½
        recent_data = df.tail(5)
        data_text = f"ìµœê·¼ 5ê°œ ë°ì´í„°:\n"
        
        for idx, row in recent_data.iterrows():
            stat_dt = row['STAT_DT'] if 'STAT_DT' in row.index else idx
            data_text += f"\n[{stat_dt}]\n"
            
            for col in col_matches:
                if col in row.index:
                    data_text += f"  {col}: {row[col]}\n"
        
        return recent_data, data_text
    
    return None, "ê²€ìƒ‰ ì¡°ê±´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œê°„(ì˜ˆ: 202509210013) ë˜ëŠ” ì»¬ëŸ¼ëª…ì„ í¬í•¨í•´ì£¼ì„¸ìš”."

class Query(BaseModel):
    question: str
    mode: str = "search"  # ê¸°ë³¸ê°’: search

class PredictQuery(BaseModel):
    mode: str  # "m14" or "hub"
    data: str  # CSV ë°ì´í„°

@app.get("/")
async def home():
    """ë©”ì¸ í˜ì´ì§€"""
    return FileResponse("index.html")

@app.post("/ask")
async def ask(query: Query):
    """RAG ì§ˆë¬¸ ì²˜ë¦¬"""
    global COLUMN_DEFINITIONS
    
    if llm is None:
        return {"answer": "âŒ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    
    try:
        logger.info(f"ì§ˆë¬¸: {query.question} | ëª¨ë“œ: {query.mode}")
        
        # ëª¨ë“œë³„ ì²˜ë¦¬
        if query.mode == "search":
            # ë°ì´í„° ê²€ìƒ‰ ëª¨ë“œ
            result, data_text = search_csv(query.question)
            
            if result is None:
                return {"answer": data_text}
        
        elif query.mode == "m14":
            # M14 ì˜ˆì¸¡ ëª¨ë“œ
            data_text = "M14 ì˜ˆì¸¡ ê¸°ëŠ¥ì€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.\ní˜„ì¬ëŠ” ë°ì´í„° ê²€ìƒ‰ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            return {"answer": data_text}
        
        elif query.mode == "hub":
            # HUB ì˜ˆì¸¡ ëª¨ë“œ
            data_text = "HUB ì˜ˆì¸¡ ê¸°ëŠ¥ì€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.\ní˜„ì¬ëŠ” ë°ì´í„° ê²€ìƒ‰ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            return {"answer": data_text}
        
        else:
            # ê¸°ë³¸ê°’: ê²€ìƒ‰
            result, data_text = search_csv(query.question)
            
            if result is None:
                return {"answer": data_text}
        
        # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""You MUST answer in Korean only. Be concise.
ë‹¹ì‹ ì€ AMHS ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ì»¬ëŸ¼ ì •ì˜:
{COLUMN_DEFINITIONS}

ê²€ìƒ‰ëœ ë°ì´í„°:
{data_text}

ì§ˆë¬¸: {query.question}

ë‹µë³€ (í•œêµ­ì–´, ê°„ê²°í•˜ê²Œ):"""
        
        # 3. LLM í˜¸ì¶œ
        response = llm(
            prompt,
            max_tokens=150,
            temperature=0.2,
            top_p=0.85,
            repeat_penalty=1.5,
            frequency_penalty=0.5,
            presence_penalty=0.3,
            stop=["ì§ˆë¬¸:", "Question:", "[", "ì¶”ì •ê°’"]
        )
        
        answer = response['choices'][0]['text'].strip()
        
        # ë°˜ë³µ íŒ¨í„´ ì œê±°
        lines = answer.split('\n')
        seen = set()
        unique_lines = []
        for line in lines:
            line_clean = line.strip()
            if line_clean and line_clean not in seen:
                seen.add(line_clean)
                unique_lines.append(line)
        
        answer = '\n'.join(unique_lines[:5])
        
        logger.info(f"ë‹µë³€ ìƒì„± ì™„ë£Œ")
        
        return {"answer": answer.strip()}
        
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {"answer": f"âŒ ì˜¤ë¥˜: {str(e)}"}

@app.post("/predict")
async def predict(query: PredictQuery):
    """M14/HUB ì˜ˆì¸¡ ì²˜ë¦¬"""
    try:
        logger.info(f"ì˜ˆì¸¡ ìš”ì²­: ëª¨ë“œ={query.mode}")
        
        if query.mode == "m14":
            # M14 ì˜ˆì¸¡ ì‹¤í–‰
            result = m14_predictor.predict_m14(query.data)
            
            if 'error' in result:
                return JSONResponse(content=result, status_code=400)
            
            # ê°„ì†Œí™”ëœ ìš”ì•½ ìƒì„±
            summary = generate_prediction_summary(result)
            
            return {
                "success": True,
                "summary": summary,
                "predictions": result['predictions'],
                "current_value": result['current_value'],
                "current_status": result['current_status']
            }
        
        elif query.mode == "hub":
            # HUB ì˜ˆì¸¡ (ë¯¸êµ¬í˜„)
            return {
                "error": "Not implemented",
                "message": "HUB ì˜ˆì¸¡ ê¸°ëŠ¥ì€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤."
            }
        
        else:
            return {
                "error": "Invalid mode",
                "message": "modeëŠ” 'm14' ë˜ëŠ” 'hub'ì—¬ì•¼ í•©ë‹ˆë‹¤."
            }
        
    except Exception as e:
        logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return JSONResponse(
            content={"error": "Prediction failed", "message": str(e)},
            status_code=500
        )

def generate_prediction_summary(result):
    """ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
    predictions = result['predictions']
    current_val = result['current_value']
    current_status = result['current_status']
    
    summary = f"ğŸ“Š í˜„ì¬ TOTALCNT: {current_val:,} ({current_status})\n\n"
    summary += "ğŸ”® ì˜ˆì¸¡ ê²°ê³¼:\n"
    
    for pred in predictions:
        status_emoji = {
            'LOW': 'âœ…',
            'NORMAL': 'ğŸŸ¢',
            'CAUTION': 'âš ï¸',
            'CRITICAL': 'ğŸš¨'
        }.get(pred['status'], 'â“')
        
        change_sign = '+' if pred['change'] > 0 else ''
        summary += f"â€¢ {pred['horizon']}ë¶„ í›„: {pred['prediction']:,} {status_emoji} "
        summary += f"(ë³€í™”: {change_sign}{pred['change']:,}, ìœ„í—˜ë„: {pred['danger_probability']}%)\n"
    
    # ìµœëŒ€ ìœ„í—˜ë„
    max_danger = max(p['danger_probability'] for p in predictions)
    
    if max_danger >= 85:
        summary += "\nğŸš¨ CRITICAL: ì¦‰ì‹œ ëŒ€ì‘ í•„ìš”!"
    elif max_danger >= 60:
        summary += "\nâš ï¸ WARNING: ì£¼ì˜ í•„ìš”!"
    elif max_danger >= 30:
        summary += "\nâš ï¸ CAUTION: ëª¨ë‹ˆí„°ë§ ê°•í™”!"
    else:
        summary += "\nâœ… NORMAL: ì •ìƒ ë²”ìœ„"
    
    return summary

@app.get("/dashboard/{filename}")
async def get_dashboard(filename: str):
    """ìƒì„±ëœ HTML ëŒ€ì‹œë³´ë“œ ë°˜í™˜"""
    filepath = os.path.join("dashboards", filename)
    
    if not os.path.exists(filepath):
        return JSONResponse(
            content={"error": "File not found"},
            status_code=404
        )
    
    return FileResponse(filepath)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)