#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSV ì§ì ‘ ê²€ìƒ‰ RAG ì„œë²„ (csv_searcher ëª¨ë“ˆ ì‚¬ìš©)
v2.4 - ë¡œì»¬/API LLM ì„ íƒ ê¸°ëŠ¥ ì¶”ê°€
"""

import os
import requests
from fastapi import FastAPI
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
import logging
from datetime import datetime
import json

# CSV ê²€ìƒ‰ ëª¨ë“ˆ
import csv_searcher

# STAR DB ê²€ìƒ‰ ëª¨ë“ˆ
import star_searcher

# MongoDB/Logpresso ê²€ìƒ‰ ëª¨ë“ˆ
import mongo_searcher

# M14 ì˜ˆì¸¡ ëª¨ë“ˆ
import m14_predictor

# HUB ì˜ˆì¸¡ ëª¨ë“ˆ
import hub_predictor_numerical
import hub_predictor_categorical

# LLM í›„ì²˜ë¦¬ ëª¨ë“ˆ
from llm_postprocessor import clean_llm_response, get_llm_analysis, get_prediction_llm_analysis

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# ========================================
# ì „ì—­ ë³€ìˆ˜
# ========================================
llm = None  # ë¡œì»¬ LLM
COLUMN_DEFINITIONS = ""

# LLM ì„¤ì •
LLM_MODE = "api"  # "local" ë˜ëŠ” "api"
API_URL = "http://dev.assistant.llm.skhynix.com/v1/chat/completions"
API_MODEL = "Qwen3-Coder-30B-A3B-Instruct"
API_TOKEN = None

# ========================================
# API LLM í•¨ìˆ˜
# ========================================
def load_api_token():
    """í† í° íŒŒì¼ì—ì„œ API í† í° ë¡œë“œ"""
    global API_TOKEN
    token_path = "token.txt"
    
    if os.path.exists(token_path):
        try:
            with open(token_path, "r") as f:
                API_TOKEN = f.read().strip()
            logger.info("âœ… API í† í° ë¡œë“œ ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"âŒ API í† í° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    else:
        logger.warning(f"âš ï¸ í† í° íŒŒì¼ ì—†ìŒ: {token_path}")
        return False

def call_api_llm(prompt: str, system_prompt: str = "í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.", max_tokens: int = 200) -> str:
    """API LLM í˜¸ì¶œ"""
    global API_TOKEN
    
    if not API_TOKEN:
        logger.warning("API í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")
        return ""
    
    try:
        headers = {
            "Authorization": f"Bearer {API_TOKEN}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": API_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": max_tokens,
            "temperature": 0.3
        }
        
        response = requests.post(API_URL, headers=headers, json=data, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            answer = result["choices"][0]["message"]["content"]
            logger.info(f"âœ… API LLM ì‘ë‹µ: {answer[:100]}...")
            return answer
        else:
            logger.error(f"âŒ API ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return ""
            
    except Exception as e:
        logger.error(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return ""

def call_local_llm(prompt: str, max_tokens: int = 200) -> str:
    """ë¡œì»¬ LLM í˜¸ì¶œ"""
    global llm
    
    if llm is None:
        return ""
    
    try:
        response = llm(
            prompt,
            max_tokens=max_tokens,
            temperature=0.3,
            stop=["<|im_end|>", "\n\n\n"]
        )
        return response['choices'][0]['text'].strip()
    except Exception as e:
        logger.error(f"âŒ ë¡œì»¬ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return ""

def get_llm_response(prompt: str, system_prompt: str = "í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.", max_tokens: int = 200) -> str:
    """LLM ì‘ë‹µ (ëª¨ë“œì— ë”°ë¼ ë¡œì»¬/API ì„ íƒ)"""
    global LLM_MODE
    
    if LLM_MODE == "api":
        return call_api_llm(prompt, system_prompt, max_tokens)
    else:
        # ë¡œì»¬ LLMìš© ChatML í”„ë¡¬í”„íŠ¸
        chatml_prompt = f"""<|im_start|>system
{system_prompt}
<|im_end|>
<|im_start|>user
{prompt}
<|im_end|>
<|im_start|>assistant
"""
        return call_local_llm(chatml_prompt, max_tokens)


def load_column_definitions():
    """ì»¬ëŸ¼ ì •ì˜ íŒŒì¼ ë¡œë“œ"""
    try:
        with open("column_definitions_short.txt", "r", encoding="utf-8") as f:
            return f.read()
    except:
        try:
            with open("column_definitions.txt", "r", encoding="utf-8") as f:
                return f.read()
        except Exception as e:
            logger.error(f"ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return ""

@app.on_event("startup")
async def startup():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global llm, COLUMN_DEFINITIONS, LLM_MODE
    
    # 0. ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ
    COLUMN_DEFINITIONS = load_column_definitions()
    logger.info("âœ… ì»¬ëŸ¼ ì •ì˜ ë¡œë“œ ì™„ë£Œ")
    
    # 1. CSV ë¡œë“œ (csv_searcher ì‚¬ìš©)
    CSV_PATH = "./csv/with.csv"
    
    if os.path.exists(CSV_PATH):
        if csv_searcher.load_csv(CSV_PATH):
            logger.info("âœ… CSV ë¡œë“œ ì™„ë£Œ (csv_searcher)")
        else:
            logger.error("âŒ CSV ë¡œë“œ ì‹¤íŒ¨")
    else:
        logger.error(f"âŒ CSV íŒŒì¼ ì—†ìŒ: {CSV_PATH}")
    
    # 1.5. STAR DB ë¬¸ì„œ ë¡œë“œ
    if star_searcher.load_md():
        logger.info("âœ… STAR DB ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    else:
        logger.warning("âš ï¸ STAR DB ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨ (STAR_READ.md ì—†ìŒ)")
    
    # 1.6. MongoDB/Logpresso ë¬¸ì„œ ë¡œë“œ
    if mongo_searcher.load_md():
        logger.info("âœ… MongoDB/Logpresso ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    else:
        logger.warning("âš ï¸ MongoDB/Logpresso ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨ (MOGO_Read.md ì—†ìŒ)")
    
    # 2. API í† í° ë¡œë“œ (API ëª¨ë“œìš©)
    if load_api_token():
        LLM_MODE = "api"
        logger.info("âœ… LLM ëª¨ë“œ: API")
    else:
        LLM_MODE = "local"
        logger.info("âš ï¸ API í† í° ì—†ìŒ â†’ ë¡œì»¬ ëª¨ë“œ ì‹œë„")
    
    # 3. ë¡œì»¬ LLM ë¡œë“œ (ë¡œì»¬ ëª¨ë“œìš© ë˜ëŠ” ë°±ì—…)
    MODEL_PATH = "Qwen3-4B-Q8_0.gguf"
    
    if os.path.exists(MODEL_PATH):
        logger.info(f"LLM ë¡œë“œ ì‹œì‘: {MODEL_PATH}")
        
        try:
            from llama_cpp import Llama
            
            llm = Llama(
                model_path=MODEL_PATH,
                n_ctx=3000,
                n_batch=512,
                n_gpu_layers=0,
                n_threads=8,
                verbose=False
            )
            
            logger.info("âœ… ë¡œì»¬ LLM ë¡œë“œ ì„±ê³µ!")
            
            # API í† í° ì—†ìœ¼ë©´ ë¡œì»¬ ëª¨ë“œ
            if not API_TOKEN:
                LLM_MODE = "local"
                
        except Exception as e:
            logger.error(f"âŒ ë¡œì»¬ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
            if not API_TOKEN:
                logger.warning("âš ï¸ LLM ì—†ì´ ì‹¤í–‰ (í…œí”Œë¦¿ ê¸°ë°˜)")
    else:
        logger.warning(f"âš ï¸ ë¡œì»¬ LLM ëª¨ë¸ ì—†ìŒ: {MODEL_PATH}")
    
    logger.info(f"ğŸš€ ìµœì¢… LLM ëª¨ë“œ: {LLM_MODE}")


def format_star_result(section_key: str, context: str) -> str:
    """STAR ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…"""
    import re
    
    section_titles = {
        'ì²­ì£¼_ìš´ì˜': 'ğŸ”µ ì²­ì£¼ ìš´ì˜ í™˜ê²½',
        'ì²­ì£¼_QA': 'ğŸŸ¡ ì²­ì£¼ QA í™˜ê²½',
        'ì´ì²œ_ìš´ì˜': 'ğŸ”µ ì´ì²œ ìš´ì˜ í™˜ê²½',
        'ì´ì²œ_QA': 'ğŸŸ¡ ì´ì²œ QA í™˜ê²½',
        'ê³„ì •': 'ğŸ‘¤ ê³µí†µ ê³„ì • ì •ë³´',
        'ìš”ì•½': 'ğŸ“Š ì „ì²´ ìš”ì•½',
        'Failover': 'ğŸ”§ Failover ì„¤ì •'
    }
    
    title = section_titles.get(section_key, f'ğŸ“‚ {section_key}')
    
    result = f"{title}\n"
    result += "=" * 45 + "\n\n"
    
    lines = context.split('\n')
    
    for line in lines:
        line = line.strip()
        
        if not line or line.startswith('|--') or line.startswith('---'):
            continue
        
        if line.startswith('#'):
            continue
        
        if line.startswith('|') and line.endswith('|'):
            cells = [c.strip() for c in line.split('|') if c.strip()]
            if len(cells) >= 2:
                key = cells[0]
                value = cells[1]
                
                if key in ['í•­ëª©', 'ì‚¬ì´íŠ¸'] or value in ['ê°’', 'í™˜ê²½']:
                    continue
                
                if 'Service' in key:
                    result += f"ğŸ“Œ {key}: {value}\n"
                elif 'Node' in key:
                    result += f"   ğŸ–¥ï¸ {key}: {value}\n"
                elif 'ê³„ì •' in key:
                    result += f"ğŸ‘¤ {key}: {value}\n"
                elif 'ë¹„ë°€ë²ˆí˜¸' in key:
                    result += f"ğŸ”‘ {key}: {value}\n"
                elif 'ì‚¬ì´íŠ¸' in key or 'ì²­ì£¼' in key or 'ì´ì²œ' in key:
                    if len(cells) >= 4:
                        result += f"ğŸ“ {cells[0]} {cells[1]}: {cells[2]} ({cells[3]})\n"
                    else:
                        result += f"ğŸ“ {key}: {value}\n"
                else:
                    result += f"   {key}: {value}\n"
        
        elif line.startswith('*'):
            item = line[1:].strip()
            result += f"  â€¢ {item}\n"
    
    return result


class Query(BaseModel):
    question: str
    mode: str = "search"

class PredictQuery(BaseModel):
    mode: str
    data: str

class LLMConfigQuery(BaseModel):
    llm_mode: str  # "local" ë˜ëŠ” "api"

@app.get("/")
async def home():
    """ë©”ì¸ í˜ì´ì§€"""
    return FileResponse("index.html")

@app.get("/columns")
async def get_columns():
    """ì»¬ëŸ¼ ëª©ë¡ ë°˜í™˜"""
    return {"columns": csv_searcher.get_columns()}

@app.get("/stats/{column}")
async def get_column_stats(column: str):
    """ì»¬ëŸ¼ í†µê³„ ë°˜í™˜"""
    return csv_searcher.get_statistics(column)

@app.get("/llm_status")
async def get_llm_status():
    """í˜„ì¬ LLM ìƒíƒœ ë°˜í™˜"""
    global LLM_MODE, llm, API_TOKEN
    
    return {
        "mode": LLM_MODE,
        "local_available": llm is not None,
        "api_available": API_TOKEN is not None,
        "api_model": API_MODEL if API_TOKEN else None
    }

@app.post("/set_llm_mode")
async def set_llm_mode(config: LLMConfigQuery):
    """LLM ëª¨ë“œ ë³€ê²½"""
    global LLM_MODE, llm, API_TOKEN
    
    new_mode = config.llm_mode.lower()
    
    if new_mode == "api":
        if API_TOKEN:
            LLM_MODE = "api"
            logger.info("âœ… LLM ëª¨ë“œ ë³€ê²½: API")
            return {"success": True, "mode": "api", "message": "API ëª¨ë“œë¡œ ë³€ê²½ë¨"}
        else:
            return {"success": False, "mode": LLM_MODE, "message": "API í† í°ì´ ì—†ìŠµë‹ˆë‹¤"}
    
    elif new_mode == "local":
        if llm is not None:
            LLM_MODE = "local"
            logger.info("âœ… LLM ëª¨ë“œ ë³€ê²½: Local")
            return {"success": True, "mode": "local", "message": "ë¡œì»¬ ëª¨ë“œë¡œ ë³€ê²½ë¨"}
        else:
            return {"success": False, "mode": LLM_MODE, "message": "ë¡œì»¬ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
    
    else:
        return {"success": False, "mode": LLM_MODE, "message": "ìœ íš¨í•˜ì§€ ì•Šì€ ëª¨ë“œ"}

@app.post("/ask")
async def ask(query: Query):
    """RAG ì§ˆë¬¸ ì²˜ë¦¬"""
    global COLUMN_DEFINITIONS, LLM_MODE
    
    try:
        logger.info(f"ì§ˆë¬¸: {query.question} | ëª¨ë“œ: {query.mode} | LLM: {LLM_MODE}")
        
        if query.mode == "search":
            
            # â­ MongoDB/Logpresso ì¿¼ë¦¬ ë¨¼ì € ì²´í¬
            if mongo_searcher.is_mongo_query(query.question):
                logger.info("MongoDB/Logpresso ê²€ìƒ‰ ê°ì§€")
                section_key, answer = mongo_searcher.search(query.question)
                
                # LLM ìš”ì•½ ì¶”ê°€
                try:
                    summary = get_llm_response(
                        f"{answer[:500]}\n\nìœ„ ì ‘ì† ì •ë³´ë¥¼ í•œêµ­ì–´ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.",
                        "ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ìƒê° ê³¼ì • ì—†ì´ ë°”ë¡œ ë‹µë³€í•˜ì„¸ìš”.",
                        60
                    )
                    
                    import re
                    summary = re.sub(r'<think>.*?</think>', '', summary, flags=re.DOTALL).strip()
                    
                    if not summary or len(summary) < 5:
                        if 'MongoDB' in answer and 'ì´ì²œ' in answer:
                            summary = "ì´ì²œ MongoDB í´ëŸ¬ìŠ¤í„° ì ‘ì† ì •ë³´ì…ë‹ˆë‹¤."
                        elif 'MongoDB' in answer and 'ì²­ì£¼' in answer:
                            summary = "ì²­ì£¼ MongoDB í´ëŸ¬ìŠ¤í„° ì ‘ì† ì •ë³´ì…ë‹ˆë‹¤."
                        elif 'Logpresso' in answer:
                            summary = "Logpresso ë¡œê·¸ ì„œë²„ ì ‘ì† ì •ë³´ì…ë‹ˆë‹¤."
                        else:
                            summary = "MongoDB/Logpresso ì ‘ì† ì •ë³´ì…ë‹ˆë‹¤."
                    
                    answer += f"\n---\nğŸ¤– ìš”ì•½: {summary}"
                except Exception as e:
                    logger.warning(f"LLM ìš”ì•½ ì‹¤íŒ¨: {e}")
                
                return {"answer": answer}
            
            # â­ STAR DB ì¿¼ë¦¬ ì²´í¬
            if star_searcher.is_star_query(query.question):
                logger.info("STAR DB ê²€ìƒ‰ ê°ì§€")
                section_key, answer = star_searcher.search(query.question)
                
                # LLM ìš”ì•½ ì¶”ê°€
                try:
                    summary = get_llm_response(
                        f"{answer}\n\nìœ„ DB ì ‘ì† ì •ë³´ë¥¼ í•œêµ­ì–´ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.",
                        "ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ìƒê° ê³¼ì • ì—†ì´ ë°”ë¡œ ë‹µë³€í•˜ì„¸ìš”.",
                        60
                    )
                    
                    import re
                    summary = re.sub(r'<think>.*?</think>', '', summary, flags=re.DOTALL).strip()
                    
                    if not summary or len(summary) < 5:
                        if 'ì²­ì£¼' in answer and 'ìš´ì˜' in answer:
                            summary = "ì²­ì£¼ ìš´ì˜ í™˜ê²½ Oracle RAC DB ì ‘ì† ì •ë³´ì…ë‹ˆë‹¤."
                        elif 'ì´ì²œ' in answer:
                            summary = "ì´ì²œ Oracle RAC DB ì ‘ì† ì •ë³´ì…ë‹ˆë‹¤."
                        else:
                            summary = "STAR DB ì ‘ì† ì •ë³´ì…ë‹ˆë‹¤."
                    
                    answer += f"\n---\nğŸ¤– ìš”ì•½: {summary}"
                except Exception as e:
                    logger.warning(f"LLM ìš”ì•½ ì‹¤íŒ¨: {e}")
                
                return {"answer": answer}
            
            # CSV ê²€ìƒ‰
            result, data_text = csv_searcher.search_csv(query.question)
            
            if result is None:
                return {"answer": data_text}
            
            answer = f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼\n{data_text}\n"
            
            # LLM ë¶„ì„ ì¶”ê°€
            if "ğŸ”® ì˜ˆì¸¡ ë¶„ì„" in data_text:
                analysis = get_prediction_llm_analysis(data_text, llm if LLM_MODE == "local" else None)
            else:
                data_type = "hub" if "HUB" in data_text else "m14"
                analysis = get_llm_analysis(data_text, llm if LLM_MODE == "local" else None, data_type)
            
            # API ëª¨ë“œë©´ ì¶”ê°€ ë¶„ì„
            if LLM_MODE == "api" and analysis and "í…œí”Œë¦¿" not in analysis:
                pass  # ì´ë¯¸ ì²˜ë¦¬ë¨
            elif LLM_MODE == "api":
                api_analysis = get_llm_response(
                    f"{data_text[:800]}\n\nìœ„ ë°ì´í„°ë¥¼ í•œêµ­ì–´ 2-3ë¬¸ì¥ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.",
                    "ë‹¹ì‹ ì€ AMHS ë¬¼ë¥˜ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.",
                    150
                )
                if api_analysis and len(api_analysis) > 10:
                    analysis = api_analysis
            
            answer += f"\n---\nğŸ¤– LLM ë¶„ì„ ({LLM_MODE.upper()})\n{analysis}"
            
            return {"answer": answer}
        
        elif query.mode == "m14":
            return {"answer": "M14 ì˜ˆì¸¡ ê¸°ëŠ¥ì€ ë°ì´í„° ì…ë ¥ ì„¹ì…˜ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”."}
        
        elif query.mode == "hub":
            return {"answer": "HUB ì˜ˆì¸¡ ê¸°ëŠ¥ì€ ë°ì´í„° ì…ë ¥ ì„¹ì…˜ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”."}
        
        elif query.mode == "general":
            # ì¼ë°˜ ëŒ€í™”
            if LLM_MODE == "api":
                answer = get_llm_response(
                    query.question,
                    "ë‹¹ì‹ ì€ AMHS ë¬¼ë¥˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.",
                    400
                )
                if answer:
                    return {"answer": answer}
                else:
                    return {"answer": "âŒ API í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}
            
            elif llm is not None:
                # ë¡œì»¬ LLM
                context_parts = []
                
                if star_searcher.is_star_query(query.question):
                    star_content = star_searcher.get_full_content() if hasattr(star_searcher, 'get_full_content') else ""
                    if star_content:
                        context_parts.append(f"[STAR DB ì •ë³´]\n{star_content[:500]}")
                
                if mongo_searcher.is_mongo_query(query.question):
                    mongo_content = mongo_searcher.get_full_content() if hasattr(mongo_searcher, 'get_full_content') else ""
                    if mongo_content:
                        context_parts.append(f"[MongoDB ì •ë³´]\n{mongo_content[:500]}")
                
                if context_parts:
                    data_context = "\n\n".join(context_parts)
                    system_prompt = f"ë‹¹ì‹ ì€ AMHS ë¬¼ë¥˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\nì°¸ê³ :\n{data_context}"
                else:
                    system_prompt = "ë‹¹ì‹ ì€ AMHS ë¬¼ë¥˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì§§ê²Œ ë‹µë³€í•˜ì„¸ìš”."
                
                answer = get_llm_response(query.question, system_prompt, 300)
                
                if answer:
                    import re
                    answer = re.sub(r'<think>.*?</think>', '', answer, flags=re.DOTALL).strip()
                    return {"answer": answer}
                else:
                    return {"answer": "âŒ LLM ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}
            
            else:
                return {"answer": "âŒ LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        else:
            result, data_text = csv_searcher.search_csv(query.question)
            if result is None:
                return {"answer": data_text}
            return {"answer": data_text}
        
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {"answer": f"âŒ ì˜¤ë¥˜: {str(e)}"}

@app.post("/predict")
async def predict(query: PredictQuery):
    """M14/HUB ì˜ˆì¸¡ ì²˜ë¦¬"""
    global LLM_MODE
    
    try:
        logger.info(f"ì˜ˆì¸¡ ìš”ì²­: ëª¨ë“œ={query.mode}")
        
        if query.mode == "m14":
            result = m14_predictor.predict_m14(query.data)
            
            if 'error' in result:
                return JSONResponse(content=result, status_code=400)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            dashboard_filename = f'M14_Dashboard_{timestamp}.html'
            dashboard_path = os.path.join('dashboards', dashboard_filename)
            
            os.makedirs('dashboards', exist_ok=True)
            with open(dashboard_path, 'w', encoding='utf-8') as f:
                f.write(result['dashboard_html'])
            
            logger.info(f"ëŒ€ì‹œë³´ë“œ ì €ì¥: {dashboard_filename}")
            
            summary = generate_prediction_summary(result)
            
            llm_analysis = ""
            try:
                if LLM_MODE == "api":
                    llm_analysis = generate_llm_analysis_api(result)
                elif llm is not None:
                    llm_analysis = generate_llm_analysis(result)
            except Exception as e:
                logger.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            return {
                "success": True,
                "summary": summary,
                "llm_analysis": llm_analysis,
                "dashboard_url": f"/dashboard/{dashboard_filename}",
                "predictions": result['predictions'],
                "current_value": result['current_value'],
                "current_status": result['current_status']
            }
        
        elif query.mode == "hub":
            result_numerical = hub_predictor_numerical.predict_hub_numerical(query.data)
            result_categorical = hub_predictor_categorical.predict_hub_categorical(query.data)
            
            if 'error' in result_numerical:
                return JSONResponse(content=result_numerical, status_code=400)
            
            if 'error' in result_categorical:
                return JSONResponse(content=result_categorical, status_code=400)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            dashboard_numerical_filename = f'HUB_Numerical_{timestamp}.html'
            dashboard_numerical_path = os.path.join('dashboards', dashboard_numerical_filename)
            
            dashboard_categorical_filename = f'HUB_Categorical_{timestamp}.html'
            dashboard_categorical_path = os.path.join('dashboards', dashboard_categorical_filename)
            
            os.makedirs('dashboards', exist_ok=True)
            
            with open(dashboard_numerical_path, 'w', encoding='utf-8') as f:
                f.write(result_numerical['dashboard_html'])
            
            with open(dashboard_categorical_path, 'w', encoding='utf-8') as f:
                f.write(result_categorical['dashboard_html'])
            
            logger.info(f"ëŒ€ì‹œë³´ë“œ ì €ì¥: {dashboard_numerical_filename}, {dashboard_categorical_filename}")
            
            summary = generate_hub_summary(result_numerical, result_categorical)
            
            llm_analysis = ""
            try:
                if LLM_MODE == "api":
                    llm_analysis = generate_hub_llm_analysis_api(result_numerical, result_categorical)
                elif llm is not None:
                    llm_analysis = generate_hub_llm_analysis(result_numerical, result_categorical)
            except Exception as e:
                logger.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            return {
                "success": True,
                "summary": summary,
                "llm_analysis": llm_analysis,
                "dashboard_numerical_url": f"/dashboard/{dashboard_numerical_filename}",
                "dashboard_categorical_url": f"/dashboard/{dashboard_categorical_filename}",
                "predictions_numerical": result_numerical['predictions'],
                "predictions_categorical": result_categorical['predictions'],
                "current_value": result_numerical['current_value']
            }
        
        else:
            return {
                "error": "Invalid mode",
                "message": "modeëŠ” 'm14' ë˜ëŠ” 'hub'ì—¬ì•¼ í•©ë‹ˆë‹¤."
            }
        
    except Exception as e:
        logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return JSONResponse(
            content={"error": "Prediction failed", "message": str(e)},
            status_code=500
        )

def generate_prediction_summary(result):
    """M14 ì˜ˆì¸¡ ê²°ê³¼ ê°„ë‹¨ ìš”ì•½"""
    predictions = result['predictions']
    current_val = result['current_value']
    current_status = result['current_status']
    
    summary = f"ğŸ“Š í˜„ì¬: {current_val:,} ({current_status})\n\n"
    summary += "ğŸ”® ì˜ˆì¸¡:\n"
    
    for pred in predictions:
        status_emoji = {
            'LOW': 'âœ…',
            'NORMAL': 'ğŸŸ¢',
            'CAUTION': 'âš ï¸',
            'CRITICAL': 'ğŸš¨'
        }.get(pred['status'], 'â“')
        
        summary += f"â€¢ {pred['horizon']}ë¶„: {pred['prediction']:,} {status_emoji} (ìœ„í—˜ {pred['danger_probability']}%)\n"
    
    return summary

def generate_hub_summary(result_numerical, result_categorical):
    """HUB ì˜ˆì¸¡ ê²°ê³¼ ê°„ë‹¨ ìš”ì•½"""
    current_val = result_numerical['current_value']
    
    pred_num = result_numerical['predictions']
    pred_cat = result_categorical['predictions']
    
    summary = f"ğŸ“Š í˜„ì¬: {current_val:,.1f}\n\n"
    summary += "ğŸ”¢ ìˆ˜ì¹˜í˜• ì˜ˆì¸¡:\n"
    
    for pred in pred_num:
        status_emoji = {
            'NORMAL': 'âœ…',
            'CAUTION': 'âš ï¸',
            'WARNING': 'ğŸŸ ',
            'CRITICAL': 'ğŸš¨'
        }.get(pred['status'], 'â“')
        
        summary += f"â€¢ {pred['horizon']}ë¶„: {pred['pred_min']:.1f} ~ {pred['pred_max']:.1f} {status_emoji}\n"
    
    summary += "\nğŸ¯ ë²”ì£¼í˜• ì˜ˆì¸¡:\n"
    
    for pred in pred_cat:
        status_emoji = {
            'LOW': 'âœ…',
            'MEDIUM': 'âš ï¸',
            'HIGH': 'ğŸŸ ',
            'CRITICAL': 'ğŸš¨'
        }.get(pred['status'], 'â“')
        
        summary += f"â€¢ {pred['horizon']}ë¶„: {pred['class_name']} (ê¸‰ì¦ {pred['prob2']:.1f}%) {status_emoji}\n"
    
    return summary

def generate_llm_analysis_api(result):
    """API LLMìœ¼ë¡œ M14 ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„"""
    predictions = result['predictions']
    current_val = result['current_value']
    current_status = result['current_status']
    
    current_m14b = result.get('current_m14b', 0)
    current_m14bsum = result.get('current_m14bsum', 0)
    current_gap = result.get('current_gap', 0)
    current_trans = result.get('current_trans', 0)
    
    pred_text = ""
    for pred in predictions:
        pred_text += f"{pred['horizon']}ë¶„ í›„: {pred['prediction']:,} (ìœ„í—˜ë„ {pred['danger_probability']}%)\n"
    
    prompt = f"""í˜„ì¬ AMHS ë¬¼ë¥˜ ìƒí™©:
- TOTALCNT: {current_val:,} ({current_status})
- M14AM14B: {current_m14b:.0f}
- M14AM14BSUM: {current_m14bsum:.0f}
- queue_gap: {current_gap:.0f}
- TRANSPORT: {current_trans:.0f}

ì˜ˆì¸¡ ê²°ê³¼:
{pred_text}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ 3-4ë¬¸ì¥ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”. ìœ„í—˜ë„ê°€ ë†’ì€ ì´ìœ ì™€ ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”."""
    
    analysis = get_llm_response(prompt, "ë‹¹ì‹ ì€ AMHS ë¬¼ë¥˜ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.", 250)
    
    if not analysis or len(analysis) < 20:
        return generate_m14_template_analysis(result, [], max(p['danger_probability'] for p in predictions))
    
    return analysis

def generate_hub_llm_analysis_api(result_numerical, result_categorical):
    """API LLMìœ¼ë¡œ HUB ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„"""
    current_val = result_numerical['current_value']
    pred_num = result_numerical['predictions']
    pred_cat = result_categorical['predictions']
    
    max_surge_prob = max(p['prob2'] for p in pred_cat)
    max_pred_value = max(p['pred_max'] for p in pred_num)
    
    pred_num_text = ""
    for pred in pred_num:
        pred_num_text += f"{pred['horizon']}ë¶„ í›„: {pred['pred_min']:.1f} ~ {pred['pred_max']:.1f}\n"
    
    pred_cat_text = ""
    for pred in pred_cat:
        pred_cat_text += f"{pred['horizon']}ë¶„ í›„: ê¸‰ì¦ í™•ë¥  {pred['prob2']:.1f}%\n"
    
    prompt = f"""í˜„ì¬ HUB ë¬¼ë¥˜ ìƒí™©:
- í˜„ì¬ê°’: {current_val:.1f}
- ìµœëŒ€ ì˜ˆì¸¡ê°’: {max_pred_value:.1f}
- ìµœëŒ€ ê¸‰ì¦ í™•ë¥ : {max_surge_prob:.1f}%

ìˆ˜ì¹˜í˜• ì˜ˆì¸¡:
{pred_num_text}

ë²”ì£¼í˜• ì˜ˆì¸¡:
{pred_cat_text}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ 3-4ë¬¸ì¥ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”. ìœ„í—˜ ì‹œê°„ëŒ€ì™€ ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”."""
    
    analysis = get_llm_response(prompt, "ë‹¹ì‹ ì€ AMHS ë¬¼ë¥˜ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.", 250)
    
    if not analysis or len(analysis) < 20:
        return generate_hub_template_analysis(result_numerical, result_categorical, [], max_surge_prob, max_pred_value)
    
    return analysis

def generate_hub_llm_analysis(result_numerical, result_categorical):
    """ë¡œì»¬ LLMìœ¼ë¡œ HUB ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„"""
    current_val = result_numerical['current_value']
    pred_num = result_numerical['predictions']
    pred_cat = result_categorical['predictions']
    
    max_surge_prob = max(p['prob2'] for p in pred_cat)
    max_pred_value = max(p['pred_max'] for p in pred_num)
    
    risk_factors = []
    
    if max_pred_value >= 300:
        risk_factors.append(f"ì˜ˆì¸¡ ìµœëŒ€ê°’({max_pred_value:.0f})ì´ ì‹¬ê° ì„ê³„ê°’(300) ì´ˆê³¼ ì˜ˆìƒ")
    elif max_pred_value >= 280:
        risk_factors.append(f"ì˜ˆì¸¡ ìµœëŒ€ê°’({max_pred_value:.0f})ì´ ì£¼ì˜ ì„ê³„ê°’(280) ì´ˆê³¼ ì˜ˆìƒ")
    
    if max_surge_prob >= 70:
        risk_factors.append(f"ê¸‰ì¦ í™•ë¥ ({max_surge_prob:.1f}%)ì´ ë§¤ìš° ë†’ìŒ")
    elif max_surge_prob >= 50:
        risk_factors.append(f"ê¸‰ì¦ í™•ë¥ ({max_surge_prob:.1f}%)ì´ ë†’ìŒ")
    
    pred_num_text = ""
    for pred in pred_num:
        pred_num_text += f"{pred['horizon']}ë¶„ í›„: {pred['pred_min']:.1f} ~ {pred['pred_max']:.1f}\n"
    
    pred_cat_text = ""
    for pred in pred_cat:
        pred_cat_text += f"{pred['horizon']}ë¶„ í›„: {pred['class_name']} (ê¸‰ì¦ {pred['prob2']:.1f}%)\n"
    
    risk_text = "\n- ".join(risk_factors) if risk_factors else "í˜„ì¬ ìœ„í—˜ ìš”ì¸ ì—†ìŒ"
    
    prompt = f"""í˜„ì¬ HUB ë¬¼ë¥˜:
- í˜„ì¬ê°’: {current_val:.1f}
- ìµœëŒ€ ì˜ˆì¸¡ê°’: {max_pred_value:.1f}
- ìµœëŒ€ ê¸‰ì¦ í™•ë¥ : {max_surge_prob:.1f}%

ìˆ˜ì¹˜í˜• ì˜ˆì¸¡:
{pred_num_text}

ë²”ì£¼í˜• ì˜ˆì¸¡:
{pred_cat_text}

ìœ„í—˜ ìš”ì¸:
- {risk_text}

í•œêµ­ì–´ 3-4ë¬¸ì¥ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”."""
    
    try:
        response = llm(
            f"<|im_start|>system\ní•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n<|im_end|>\n<|im_start|>user\n{prompt}\n<|im_end|>\n<|im_start|>assistant\n",
            max_tokens=250,
            temperature=0.2,
            stop=["<|im_end|>"]
        )
        
        raw_answer = response['choices'][0]['text'].strip()
        cleaned = clean_llm_response(raw_answer)
        
        if not cleaned or len(cleaned) < 20:
            return generate_hub_template_analysis(result_numerical, result_categorical, risk_factors, max_surge_prob, max_pred_value)
        
        return cleaned
        
    except Exception as e:
        logger.error(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
        return generate_hub_template_analysis(result_numerical, result_categorical, risk_factors, max_surge_prob, max_pred_value)

def generate_hub_template_analysis(result_numerical, result_categorical, risk_factors, max_surge_prob, max_pred_value):
    """í…œí”Œë¦¿ ê¸°ë°˜ HUB ë¶„ì„"""
    current_val = result_numerical['current_value']
    pred_num = result_numerical['predictions']
    pred_cat = result_categorical['predictions']
    
    max_horizon = max(pred_num, key=lambda x: x['pred_max'])
    
    if max_pred_value < 280 and max_surge_prob < 30:
        return f"í˜„ì¬ê°’ {current_val:.1f}ë¡œ ì •ìƒ ë²”ìœ„ì…ë‹ˆë‹¤. ê¸‰ì¦ í™•ë¥ ì´ ë‚®ì•„ ì•ˆì •ì ì¸ ìƒíƒœì…ë‹ˆë‹¤."
    
    analysis = f"âš ï¸ ìœ„í—˜ ë¶„ì„:\n\n"
    
    analysis += f"ğŸ”¢ ìˆ˜ì¹˜í˜• ì˜ˆì¸¡:\n"
    for p in pred_num:
        if p['pred_max'] >= 300:
            analysis += f"  ğŸš¨ {p['horizon']}ë¶„ í›„: {p['pred_min']:.0f} ~ {p['pred_max']:.0f} (ì‹¬ê°)\n"
        elif p['pred_max'] >= 280:
            analysis += f"  âš ï¸ {p['horizon']}ë¶„ í›„: {p['pred_min']:.0f} ~ {p['pred_max']:.0f} (ì£¼ì˜)\n"
        else:
            analysis += f"  âœ… {p['horizon']}ë¶„ í›„: {p['pred_min']:.0f} ~ {p['pred_max']:.0f} (ì •ìƒ)\n"
    
    analysis += f"\nğŸ¯ ë²”ì£¼í˜• ê·¼ê±°:\n"
    for p in pred_cat:
        if p['prob2'] >= 70:
            analysis += f"  ğŸš¨ {p['horizon']}ë¶„ í›„: ê¸‰ì¦ í™•ë¥  {p['prob2']:.1f}%\n"
        elif p['prob2'] >= 50:
            analysis += f"  âš ï¸ {p['horizon']}ë¶„ í›„: ê¸‰ì¦ í™•ë¥  {p['prob2']:.1f}%\n"
        elif p['prob2'] >= 30:
            analysis += f"  ğŸŸ¡ {p['horizon']}ë¶„ í›„: ê¸‰ì¦ í™•ë¥  {p['prob2']:.1f}%\n"
    
    analysis += f"\nğŸ“‹ ê²°ë¡ :\n"
    if max_pred_value >= 300 and max_surge_prob >= 70:
        analysis += f"  â†’ {max_horizon['horizon']}ë¶„ í›„ {max_pred_value:.0f}ê¹Œì§€ ìƒìŠ¹ ì˜ˆì¸¡, ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”!"
    elif max_pred_value >= 280 or max_surge_prob >= 50:
        analysis += f"  â†’ ëª¨ë‹ˆí„°ë§ ê°•í™” í•„ìš”"
    else:
        analysis += f"  â†’ í˜„ì¬ ì•ˆì •ì "
    
    return analysis

def generate_llm_analysis(result):
    """ë¡œì»¬ LLMìœ¼ë¡œ M14 ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„"""
    predictions = result['predictions']
    current_val = result['current_value']
    current_status = result['current_status']
    
    current_m14b = result.get('current_m14b', 0)
    current_m14bsum = result.get('current_m14bsum', 0)
    current_gap = result.get('current_gap', 0)
    current_trans = result.get('current_trans', 0)
    
    max_danger = max(p['danger_probability'] for p in predictions)
    
    risk_factors = []
    if current_m14b > 520:
        risk_factors.append(f"M14AM14B({current_m14b:.0f}) ì„ê³„ê°’ ì´ˆê³¼")
    if current_m14bsum > 588:
        risk_factors.append(f"M14AM14BSUM({current_m14bsum:.0f}) ì„ê³„ê°’ ì´ˆê³¼")
    if current_gap > 300:
        risk_factors.append(f"queue_gap({current_gap:.0f}) ì„ê³„ê°’ ì´ˆê³¼")
    if current_trans > 151:
        risk_factors.append(f"TRANSPORT({current_trans:.0f}) ì„ê³„ê°’ ì´ˆê³¼")
    
    pred_text = ""
    for pred in predictions:
        pred_text += f"{pred['horizon']}ë¶„ í›„: {pred['prediction']:,} (ìœ„í—˜ë„ {pred['danger_probability']}%)\n"
    
    risk_text = "\n- ".join(risk_factors) if risk_factors else "ëª¨ë“  ì§€í‘œ ì •ìƒ"
    
    prompt = f"""í˜„ì¬ AMHS ë¬¼ë¥˜:
- TOTALCNT: {current_val:,} ({current_status})
- M14AM14B: {current_m14b:.0f}
- M14AM14BSUM: {current_m14bsum:.0f}
- queue_gap: {current_gap:.0f}
- TRANSPORT: {current_trans:.0f}

ìœ„í—˜ ìš”ì¸:
- {risk_text}

ì˜ˆì¸¡:
{pred_text}

í•œêµ­ì–´ 3-4ë¬¸ì¥ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”."""
    
    try:
        response = llm(
            f"<|im_start|>system\ní•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n<|im_end|>\n<|im_start|>user\n{prompt}\n<|im_end|>\n<|im_start|>assistant\n",
            max_tokens=250,
            temperature=0.2,
            stop=["<|im_end|>"]
        )
        
        raw_answer = response['choices'][0]['text'].strip()
        cleaned = clean_llm_response(raw_answer)
        
        if not cleaned or len(cleaned) < 20:
            return generate_m14_template_analysis(result, risk_factors, max_danger)
        
        return cleaned
        
    except Exception as e:
        logger.error(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
        return generate_m14_template_analysis(result, risk_factors, max_danger)

def generate_m14_template_analysis(result, risk_factors, max_danger):
    """í…œí”Œë¦¿ ê¸°ë°˜ M14 ë¶„ì„"""
    predictions = result['predictions']
    current_val = result['current_value']
    max_pred = max(p['prediction'] for p in predictions)
    
    analysis = ""
    
    if risk_factors:
        analysis += f"âš ï¸ í˜„ì¬ ì§€í‘œ ìœ„í—˜ ìš”ì¸:\n"
        for factor in risk_factors:
            analysis += f"  ğŸš¨ {factor}\n"
        analysis += "\n"
    
    critical_preds = [p for p in predictions if p['prediction'] >= 1700]
    
    if critical_preds:
        analysis += f"ğŸ”® ì˜ˆì¸¡ ê¸°ë°˜ ìœ„í—˜:\n"
        for p in predictions:
            if p['prediction'] >= 1700:
                analysis += f"  ğŸš¨ {p['horizon']}ë¶„ í›„: {p['prediction']:,} (CRITICAL)\n"
            elif p['prediction'] >= 1650:
                analysis += f"  âš ï¸ {p['horizon']}ë¶„ í›„: {p['prediction']:,} (CAUTION)\n"
    
    analysis += f"\nğŸ“‹ ê²°ë¡ :\n"
    if max_pred >= 1700:
        analysis += f"  â†’ ì˜ˆì¸¡ ìµœëŒ€ê°’ {max_pred:,}ìœ¼ë¡œ CRITICAL ìƒíƒœ ì˜ˆìƒ, ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”!"
    elif max_pred >= 1650:
        analysis += f"  â†’ ì˜ˆì¸¡ ìµœëŒ€ê°’ {max_pred:,}ìœ¼ë¡œ CAUTION ìƒíƒœ ì˜ˆìƒ"
    else:
        analysis += f"  â†’ í˜„ì¬ ì•ˆì •ì , ëª¨ë‹ˆí„°ë§ ê¶Œì¥"
    
    if not analysis.strip():
        return "í˜„ì¬ ëª¨ë“  ì§€í‘œê°€ ì •ìƒ ë²”ìœ„ì…ë‹ˆë‹¤."
    
    return analysis

@app.get("/dashboard/{filename}")
async def get_dashboard(filename: str):
    """ìƒì„±ëœ HTML ëŒ€ì‹œë³´ë“œ ë°˜í™˜"""
    filepath = os.path.join("dashboards", filename)
    
    if not os.path.exists(filepath):
        return JSONResponse(
            content={"error": "File not found"},
            status_code=404
        )
    
    return FileResponse(filepath)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)