#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê°„ë‹¨í•œ FastAPI RAG ì„œë²„
"""

import os
os.environ['USE_TF'] = '0'
os.environ['USE_TORCH'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TRANSFORMERS_NO_TF'] = '1'

import warnings
warnings.filterwarnings('ignore')

from fastapi import FastAPI
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
import pandas as pd
import numpy as np
from datetime import datetime
import re
import pickle
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# TimeSeriesRAG í´ë˜ìŠ¤ ì •ì˜
class TimeSeriesRAG:
    """pickle ë²¡í„°DBë¥¼ ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•˜ëŠ” RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, 
                 csv_path,
                 model_path,
                 db_persist_path="./chroma_db",
                 embedding_model_path="./embeddings/all-MiniLM-L6-v2"):
        
        self.csv_path = csv_path
        self.model_path = model_path
        self.db_persist_path = db_persist_path
        self.embedding_model_path = embedding_model_path
        
        # ì´ˆê¸°í™”
        self._initialize()
    
    def _initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        logger.info("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # 1. CSV ë°ì´í„° ë¡œë“œ (ì¿¼ë¦¬ìš©)
        self._load_csv_data()
        
        # 2. ê¸°ì¡´ ë²¡í„°DB ë¡œë“œ
        self._load_vectorstore()
        
        # 3. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self._load_embedding_model()
        
        # 4. LLM ì„¤ì •
        self._setup_llm()
        
        logger.info("ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _load_csv_data(self):
        """CSV ë°ì´í„° ë¡œë“œ"""
        logger.info(f"CSV ë¡œë“œ: {self.csv_path}")
        
        df = pd.read_csv(self.csv_path, encoding='utf-8')
        self.df = df[['í˜„ì¬ì‹œê°„', 'ì‹¤ì œì‹œì ', 'ì‹¤ì œê°’', 
                     'ë³´ì •ì˜ˆì¸¡', 'ì˜¤ì°¨']].copy()
        
        self.df['í˜„ì¬ì‹œê°„'] = pd.to_datetime(self.df['í˜„ì¬ì‹œê°„'])
        self.df['ì‹¤ì œì‹œì '] = pd.to_datetime(self.df['ì‹¤ì œì‹œì '])
        self.df.set_index('í˜„ì¬ì‹œê°„', inplace=True)
        
        logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê°œ")
    
    def _load_vectorstore(self):
        """ê¸°ì¡´ ë²¡í„°DB ë¡œë“œ"""
        db_file = os.path.join(self.db_persist_path, 'vectordb.pkl')
        
        if not os.path.exists(db_file):
            raise FileNotFoundError(
                f"ë²¡í„°DBê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë²¡í„°DB.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\n"
                f"ê²½ë¡œ: {db_file}"
            )
        
        logger.info(f"ë²¡í„°DB ë¡œë“œ ì¤‘: {db_file}")
        
        with open(db_file, 'rb') as f:
            db_data = pickle.load(f)
        
        self.documents = db_data['documents']
        self.embeddings = db_data['embeddings']
        
        logger.info(f"ë²¡í„°DB ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
    
    def _load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {self.embedding_model_path}")
        
        try:
            import torch
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        except:
            device = 'cpu'
        
        logger.info(f"ë””ë°”ì´ìŠ¤: {device}")
        
        from sentence_transformers import SentenceTransformer
        self.embedding_model = SentenceTransformer(self.embedding_model_path)
        if device == 'cuda':
            self.embedding_model = self.embedding_model.to('cuda')
        
        logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def _setup_llm(self):
        """LLM ì„¤ì •"""
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {self.model_path}")
        
        logger.info(f"LLM ë¡œë“œ: {self.model_path}")
        
        from llama_cpp import Llama
        
        self.llm = Llama(
            model_path=self.model_path,
            n_ctx=1024,
            n_batch=128,
            n_gpu_layers=0,      # CPU
            n_threads=8,
            temperature=0.3,
            max_tokens=150,
            verbose=False
        )
        
        logger.info("LLM ë¡œë“œ ì™„ë£Œ")
    
    def _cosine_similarity(self, a, b):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    def _search_similar(self, query, k=5):
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedding_model.encode([query])[0]
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i, doc_embedding in enumerate(self.embeddings):
            sim = self._cosine_similarity(query_embedding, doc_embedding)
            similarities.append((i, sim))
        
        # ìƒìœ„ kê°œ ì„ íƒ
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_k = similarities[:k]
        
        results = []
        for idx, score in top_k:
            results.append(self.documents[idx]['content'])
        
        return results
    
    def _call_llm(self, prompt, max_tokens=100):
        """LLM í˜¸ì¶œ"""
        logger.info("í† í° ìƒì„± ì‹œì‘...")
        response = self.llm(
            prompt, 
            max_tokens=max_tokens, 
            temperature=0.1,
            top_p=0.9,
            repeat_penalty=1.1,
            stop=["\n\n", "ì§ˆë¬¸:", "ë‹µë³€:", "ë°ì´í„°"]
        )
        result = response['choices'][0]['text'].strip()
        logger.info(f"ìƒì„± ì™„ë£Œ ({len(result)}ì)")
        return result
    
    def get_data_at_time(self, datetime_str):
        """íŠ¹ì • ì‹œê°„ ë°ì´í„° ì¡°íšŒ"""
        try:
            dt = pd.to_datetime(datetime_str)
            
            if dt in self.df.index:
                data = self.df.loc[dt]
                return f"""
ğŸ“Š {dt} ë°ì´í„°:
- ì‹¤ì œê°’: {data['ì‹¤ì œê°’']:.1f}
- ì˜ˆì¸¡ê°’: {data['ë³´ì •ì˜ˆì¸¡']:.1f}  
- ì˜¤ì°¨: {data['ì˜¤ì°¨']:.1f}
                """
            else:
                nearest_idx = self.df.index.get_indexer([dt], method='nearest')[0]
                nearest_time = self.df.index[nearest_idx]
                data = self.df.iloc[nearest_idx]
                
                return f"""
ğŸ“Š ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„: {nearest_time}
- ì‹¤ì œê°’: {data['ì‹¤ì œê°’']:.1f}
- ì˜ˆì¸¡ê°’: {data['ë³´ì •ì˜ˆì¸¡']:.1f}
- ì˜¤ì°¨: {data['ì˜¤ì°¨']:.1f}
                """
        except Exception as e:
            return f"ì˜¤ë¥˜: {e}"
    
    def get_sequence_before(self, datetime_str, length=280):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ì¡°íšŒ"""
        try:
            dt = pd.to_datetime(datetime_str)
            end_idx = self.df.index.get_indexer([dt], method='nearest')[0]
            start_idx = max(0, end_idx - length + 1)
            
            sequence = self.df.iloc[start_idx:end_idx + 1]
            
            return f"""
ğŸ“ˆ ì‹œí€€ìŠ¤ ë¶„ì„ ({len(sequence)}ê°œ):
ê¸°ê°„: {sequence.index[0]} ~ {sequence.index[-1]}

ì‹¤ì œê°’:
- í‰ê· : {sequence['ì‹¤ì œê°’'].mean():.2f}
- ìµœëŒ€: {sequence['ì‹¤ì œê°’'].max():.1f}  
- ìµœì†Œ: {sequence['ì‹¤ì œê°’'].min():.1f}
- í‘œì¤€í¸ì°¨: {sequence['ì‹¤ì œê°’'].std():.2f}

ì˜ˆì¸¡ ì„±ëŠ¥:
- í‰ê·  ì˜¤ì°¨: {sequence['ì˜¤ì°¨'].mean():.2f}
- RMSE: {np.sqrt((sequence['ì˜¤ì°¨']**2).mean()):.2f}

ìµœê·¼ 5ê°œ:
{sequence.tail(5)[['ì‹¤ì œê°’', 'ë³´ì •ì˜ˆì¸¡']].to_string()}
            """
        except Exception as e:
            return f"ì˜¤ë¥˜: {e}"
    
    def analyze_query(self, query):
        """ì¿¼ë¦¬ ë¶„ì„ ë° ì²˜ë¦¬"""
        # ì‹œê°„ íŒ¨í„´ ì¶”ì¶œ
        time_pattern = r'(\d{4}-\d{2}-\d{2}\s+\d{1,2}:\d{2})'
        time_match = re.search(time_pattern, query)
        
        # ì‹¤ì œê°’/ì˜ˆì¸¡ê°’ ì¡°íšŒ
        if time_match and ("ì‹¤ì œê°’" in query or "ì˜ˆì¸¡ê°’" in query):
            return self.get_data_at_time(time_match.group(1))
        
        # ì‹œí€€ìŠ¤ ë¶„ì„
        elif time_match and ("ì‹œí€€ìŠ¤" in query or "280" in query or "ì„¤ëª…" in query):
            logger.info("ì‹œí€€ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
            sequence_info = self.get_sequence_before(time_match.group(1))
            
            logger.info("ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
            similar_docs = self._search_similar(query, k=1)
            
            logger.info("AI ì„¤ëª… ìƒì„± ì¤‘...")
            prompt = f"ë°ì´í„° ìš”ì•½:\ní‰ê· ={sequence_info.split('í‰ê· :')[1].split()[0]}, ìµœëŒ€={sequence_info.split('ìµœëŒ€:')[1].split()[0]}\n\ní•œ ì¤„ë¡œ ê°„ë‹¨íˆ ì„¤ëª…:"
            
            try:
                result = self._call_llm(prompt, max_tokens=100)
                return f"{sequence_info}\n\nğŸ’¡ AI ì„¤ëª…:\n{result}"
            except Exception as e:
                logger.error(f"AI ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {e}")
                return sequence_info
        
        # ì¼ë°˜ RAG ì¿¼ë¦¬
        else:
            similar_docs = self._search_similar(query, k=3)
            context = "\n\n".join(similar_docs)
            
            prompt = f"ì§ˆë¬¸: {query}\n\nì°¸ê³ :\n{context}\n\në‹µë³€:"
            result = self._call_llm(prompt)
            
            return result

# FastAPI ì•±
app = FastAPI()

# RAG ì‹œìŠ¤í…œ (ì „ì—­ ë³€ìˆ˜)
rag = None

@app.on_event("startup")
async def startup():
    """ì„œë²„ ì‹œì‘ ì‹œ RAG ì´ˆê¸°í™”"""
    global rag
    print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    
    rag = TimeSeriesRAG(
        csv_path="./WITH.CSV",
        model_path="./models/Qwen3-Coder-8B-Instruct-Q3_K_M.gguf",
        db_persist_path="./chroma_db"
    )
    
    print("ì¤€ë¹„ ì™„ë£Œ!")

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class Query(BaseModel):
    question: str

@app.get("/")
async def home():
    """ë©”ì¸ í˜ì´ì§€"""
    return HTMLResponse("""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>RAG ì‹œìŠ¤í…œ</title>
    <style>
        body {
            font-family: Arial;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
        }
        input {
            width: 70%;
            padding: 10px;
            font-size: 16px;
        }
        button {
            padding: 10px 20px;
            font-size: 16px;
        }
        #result {
            margin-top: 20px;
            padding: 20px;
            background: #f5f5f5;
            white-space: pre-wrap;
        }
    </style>
</head>
<body>
    <h1>TimeSeriesRAG ì‹œìŠ¤í…œ</h1>
    
    <div>
        <input type="text" id="question" placeholder="ì§ˆë¬¸ ì…ë ¥ (ì˜ˆ: 2025-08-12 02:00 ì‹¤ì œê°’)">
        <button onclick="ask()">ì§ˆë¬¸í•˜ê¸°</button>
    </div>
    
    <div id="result"></div>
    
    <script>
        async function ask() {
            const question = document.getElementById('question').value;
            const resultDiv = document.getElementById('result');
            
            resultDiv.textContent = 'ì²˜ë¦¬ ì¤‘...';
            
            try {
                const response = await fetch('/ask', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({question: question})
                });
                
                const data = await response.json();
                resultDiv.textContent = data.answer;
            } catch (error) {
                resultDiv.textContent = 'ì˜¤ë¥˜: ' + error;
            }
        }
        
        // Enter í‚¤ ì§€ì›
        document.getElementById('question').addEventListener('keypress', function(e) {
            if (e.key === 'Enter') ask();
        });
    </script>
</body>
</html>
    """)

@app.post("/ask")
async def ask(query: Query):
    """ì§ˆë¬¸ ì²˜ë¦¬"""
    if not rag:
        return {"answer": "ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."}
    
    try:
        result = rag.analyze_query(query.question)
        return {"answer": result}
    except Exception as e:
        return {"answer": f"ì˜¤ë¥˜: {str(e)}"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)