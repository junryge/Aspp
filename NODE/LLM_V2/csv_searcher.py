#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSV ê²€ìƒ‰ ëª¨ë“ˆ
server.pyì—ì„œ importí•˜ì—¬ ì‚¬ìš©
"""

import os
import pandas as pd
import re
import json
import logging
from typing import Tuple, Optional, List, Dict, Any

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜
_df = None
_csv_path = None
_column_config = None

# ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ ê²½ë¡œ
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

def load_column_config() -> dict:
    """ì»¬ëŸ¼ ì„¤ì • JSON ë¡œë“œ"""
    global _column_config
    
    config_path = os.path.join(SCRIPT_DIR, 'sort', 'column_config.json')
    
    if not os.path.exists(config_path):
        logger.warning(f"âš ï¸ ì»¬ëŸ¼ ì„¤ì • íŒŒì¼ ì—†ìŒ: {config_path}")
        return {}
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            _column_config = json.load(f)
        logger.info(f"âœ… ì»¬ëŸ¼ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {config_path}")
        return _column_config
    except Exception as e:
        logger.error(f"âŒ ì»¬ëŸ¼ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

def get_column_config() -> dict:
    """í˜„ì¬ ì»¬ëŸ¼ ì„¤ì • ë°˜í™˜"""
    global _column_config
    if _column_config is None:
        load_column_config()
    return _column_config or {}

def detect_data_type(columns: List[str]) -> str:
    """ë°ì´í„° íƒ€ì… ìë™ ê°ì§€ (m14/hub)"""
    config = get_column_config()
    
    if not config:
        return "unknown"
    
    m14_detect = config.get('m14', {}).get('detect_columns', [])
    hub_detect = config.get('hub', {}).get('detect_columns', [])
    
    m14_count = sum(1 for col in m14_detect if col in columns)
    hub_count = sum(1 for col in hub_detect if col in columns)
    
    if m14_count > hub_count:
        return "m14"
    elif hub_count > 0:
        return "hub"
    else:
        return "unknown"

def analyze_status(row: pd.Series, data_type: str) -> str:
    """ì„ê³„ê°’ ê¸°ë°˜ ìƒíƒœ ë¶„ì„"""
    config = get_column_config()
    
    if not config or data_type not in config:
        return ""
    
    thresholds = config[data_type].get('thresholds', {})
    
    if not thresholds:
        return ""
    
    results = []
    warnings = 0
    criticals = 0
    
    for col, limits in thresholds.items():
        if col not in row.index or pd.isna(row[col]):
            continue
        
        value = float(row[col])
        
        # HUBROOMTOTALì€ ë‚®ì„ìˆ˜ë¡ ìœ„í—˜ (ë°˜ëŒ€ ë¡œì§)
        if col == 'HUBROOMTOTAL':
            critical = limits.get('critical', 0)
            caution = limits.get('caution', 0)
            normal = limits.get('normal', 0)
            
            if value < critical:
                results.append(f"ğŸš¨ {col}: {value} â†’ ì‹¬ê° (< {critical})")
                criticals += 1
            elif value < caution:
                results.append(f"âš ï¸ {col}: {value} â†’ ì£¼ì˜ (< {caution})")
                warnings += 1
            else:
                results.append(f"âœ… {col}: {value} â†’ ì •ìƒ")
        else:
            # ì¼ë°˜ ì»¬ëŸ¼: ë†’ì„ìˆ˜ë¡ ìœ„í—˜
            critical = limits.get('critical', float('inf'))
            caution = limits.get('caution', float('inf'))
            normal = limits.get('normal', float('inf'))
            
            if value >= critical:
                results.append(f"ğŸš¨ {col}: {value} â†’ ì‹¬ê° (â‰¥ {critical})")
                criticals += 1
            elif value >= caution:
                results.append(f"âš ï¸ {col}: {value} â†’ ì£¼ì˜ (â‰¥ {caution})")
                warnings += 1
            elif value >= normal:
                results.append(f"ğŸŸ¡ {col}: {value} â†’ ê´€ì‹¬ (â‰¥ {normal})")
            else:
                results.append(f"âœ… {col}: {value} â†’ ì •ìƒ")
    
    if not results:
        return ""
    
    # ì¢…í•© íŒë‹¨
    if criticals > 0:
        summary = f"ğŸš¨ ì¢…í•©: ì‹¬ê° ({criticals}ê°œ í•­ëª© ì„ê³„ê°’ ì´ˆê³¼)"
    elif warnings > 0:
        summary = f"âš ï¸ ì¢…í•©: ì£¼ì˜ ({warnings}ê°œ í•­ëª© ì£¼ì˜ í•„ìš”)"
    else:
        summary = "âœ… ì¢…í•©: ì •ìƒ"
    
    analysis_text = "\nğŸ“Š ìƒíƒœ ë¶„ì„\n"
    analysis_text += "\n".join(results)
    analysis_text += f"\n\n{summary}"
    
    return analysis_text

def load_csv(csv_path: str) -> bool:
    """CSV íŒŒì¼ ë¡œë“œ"""
    global _df, _csv_path
    
    if not os.path.exists(csv_path):
        logger.error(f"âŒ CSV íŒŒì¼ ì—†ìŒ: {csv_path}")
        return False
    
    try:
        _df = pd.read_csv(csv_path, encoding='utf-8')
        _csv_path = csv_path
        logger.info(f"âœ… CSV ë¡œë“œ ì™„ë£Œ: {len(_df)}í–‰, {len(_df.columns)}ì»¬ëŸ¼")
        logger.info(f"ì»¬ëŸ¼: {list(_df.columns[:5])}...")
        return True
    except Exception as e:
        logger.error(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def get_df() -> Optional[pd.DataFrame]:
    """í˜„ì¬ ë¡œë“œëœ DataFrame ë°˜í™˜"""
    return _df

def get_columns() -> List[str]:
    """ì»¬ëŸ¼ ëª©ë¡ ë°˜í™˜"""
    if _df is None:
        return []
    return list(_df.columns)

def search_by_time(time_str: str) -> Tuple[Optional[pd.Series], str]:
    """
    ì‹œê°„ìœ¼ë¡œ ê²€ìƒ‰ (YYYYMMDDHHMM ë˜ëŠ” YYYY-MM-DD HH:MM í˜•ì‹)
    
    Returns:
        (ë§¤ì¹­ëœ í–‰, ì„¤ëª… í…ìŠ¤íŠ¸)
    """
    if _df is None:
        return None, "CSV íŒŒì¼ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    # ì‹œê°„ ì»¬ëŸ¼ í›„ë³´
    time_cols = ['í˜„ì¬ì‹œê°„', 'STAT_DT', 'CURRTIME', 'ì‹œê°„', 'TIME', 'DATETIME']
    time_col = None
    
    for col in time_cols:
        if col in _df.columns:
            time_col = col
            break
    
    if time_col is None:
        return None, "ì‹œê°„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ê²€ìƒ‰ (ì—¬ëŸ¬ ë°©ì‹ ì‹œë„)
    time_col_str = _df[time_col].astype(str)
    
    # 1. ì •í™•íˆ í¬í•¨
    result = _df[time_col_str.str.contains(time_str, na=False, regex=False)]
    
    # 2. ì‹œê°„ ì •ê·œí™” í›„ ë¹„êµ (4:39 vs 04:39)
    if result.empty:
        # ì…ë ¥ ì‹œê°„ì—ì„œ ë‚ ì§œì™€ ì‹œê°„ ë¶„ë¦¬
        if ' ' in time_str:
            date_part, time_part = time_str.rsplit(' ', 1)
            # ì‹œê°„ ë¶€ë¶„ ì •ê·œí™” (4:39 -> 04:39, 04:39 -> 4:39)
            if ':' in time_part:
                h, m = time_part.split(':')
                # ë‘ ê°€ì§€ í˜•ì‹ìœ¼ë¡œ ê²€ìƒ‰
                time_str_padded = f"{date_part} {int(h):02d}:{m}"
                time_str_unpadded = f"{date_part} {int(h)}:{m}"
                
                result = _df[time_col_str.str.contains(time_str_padded, na=False, regex=False) |
                            time_col_str.str.contains(time_str_unpadded, na=False, regex=False)]
    
    if result.empty:
        # ìœ ì‚¬í•œ ì‹œê°„ ì œì•ˆ
        sample_times = time_col_str.head(5).tolist()
        return None, f"ì‹œê°„ '{time_str}'ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\nì˜ˆì‹œ: {sample_times}"
    
    row = result.iloc[0]
    
    # JSON ì„¤ì •ì—ì„œ ì»¬ëŸ¼ ë¡œë“œ
    config = get_column_config()
    
    m14_config = config.get('m14', {})
    hub_config = config.get('hub', {})
    
    m14_display = m14_config.get('display_columns', [])
    hub_display = hub_config.get('display_columns', [])
    m14_detect = m14_config.get('detect_columns', [])
    hub_detect = hub_config.get('detect_columns', [])
    
    # ë°ì´í„° íƒ€ì… ìë™ ê°ì§€
    m14_found = sum(1 for col in m14_detect if col in row.index and pd.notna(row[col]))
    hub_found = sum(1 for col in hub_detect if col in row.index and pd.notna(row[col]))
    
    # ê²°ê³¼ í¬ë§·íŒ…
    data_text = f"ì‹œê°„: {row[time_col]}\n"
    data_text += "-" * 40 + "\n"
    
    if m14_found > hub_found and m14_display:
        # M14 ë°ì´í„°
        data_type = "m14"
        icon = m14_config.get('icon', 'ğŸ“¦')
        name = m14_config.get('name', 'M14')
        data_text += f"{icon} [{name}]\n"
        for col in m14_display:
            if col in row.index and pd.notna(row[col]):
                data_text += f"{col}: {row[col]}\n"
    elif hub_found > 0 and hub_display:
        # HUB ë°ì´í„°
        data_type = "hub"
        icon = hub_config.get('icon', 'ğŸ­')
        name = hub_config.get('name', 'HUB')
        data_text += f"{icon} [{name}]\n"
        for col in hub_display:
            if col in row.index and pd.notna(row[col]):
                data_text += f"{col}: {row[col]}\n"
    else:
        # ë‘˜ ë‹¤ ì•„ë‹ˆë©´ ì „ì²´ í‘œì‹œ
        data_type = "unknown"
        data_text += "ğŸ“Š [ì „ì²´ ë°ì´í„°]\n"
        for col in row.index:
            if col != time_col and pd.notna(row[col]):
                data_text += f"{col}: {row[col]}\n"
    
    # ìƒíƒœ ë¶„ì„ ì¶”ê°€
    analysis = analyze_status(row, data_type)
    if analysis:
        data_text += "\n" + analysis
    
    return row, data_text

def search_by_columns(col_names: List[str], n_rows: int = 5) -> Tuple[Optional[pd.DataFrame], str]:
    """
    ì»¬ëŸ¼ëª…ìœ¼ë¡œ ê²€ìƒ‰ (ìµœê·¼ nê°œ ë°ì´í„°)
    
    Args:
        col_names: ê²€ìƒ‰í•  ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸
        n_rows: ë°˜í™˜í•  í–‰ ìˆ˜
    
    Returns:
        (ë§¤ì¹­ëœ DataFrame, ì„¤ëª… í…ìŠ¤íŠ¸)
    """
    if _df is None:
        return None, "CSV íŒŒì¼ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    # ìœ íš¨í•œ ì»¬ëŸ¼ë§Œ í•„í„°
    valid_cols = [c for c in col_names if c in _df.columns]
    
    if not valid_cols:
        return None, f"ìœ íš¨í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥: {list(_df.columns[:10])}..."
    
    recent = _df.tail(n_rows)
    data_text = f"ìµœê·¼ {n_rows}ê°œ ë°ì´í„°:\n\n"
    
    # ì‹œê°„ ì»¬ëŸ¼
    time_cols = ['í˜„ì¬ì‹œê°„', 'STAT_DT', 'CURRTIME', 'ì‹œê°„']
    time_col = None
    for tc in time_cols:
        if tc in _df.columns:
            time_col = tc
            break
    
    for idx, row in recent.iterrows():
        if time_col:
            data_text += f"[{row[time_col]}]\n"
        else:
            data_text += f"[Row {idx}]\n"
        
        for col in valid_cols:
            data_text += f"  {col}: {row[col]}\n"
        data_text += "\n"
    
    return recent[valid_cols], data_text

def search_csv(query: str) -> Tuple[Optional[Any], str]:
    """
    ìì—°ì–´ ì¿¼ë¦¬ë¡œ CSV ê²€ìƒ‰ (ê¸°ì¡´ server.py í•¨ìˆ˜ì™€ í˜¸í™˜)
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì‹œê°„ ë˜ëŠ” ì»¬ëŸ¼ëª… í¬í•¨)
    
    Returns:
        (ê²°ê³¼ ë°ì´í„°, ì„¤ëª… í…ìŠ¤íŠ¸)
    """
    if _df is None:
        return None, "CSV íŒŒì¼ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    # 1. ì‹œê°„ íŒ¨í„´ ì¶”ì¶œ (202509210013 ë˜ëŠ” 2025-10-14 4:39 í˜•ì‹)
    time_patterns = [
        r'(\d{12})',  # 202509210013
        r'(\d{4}-\d{2}-\d{2}\s+\d{1,2}:\d{2})',  # 2025-10-14 4:39
    ]
    
    time_str = None
    for pattern in time_patterns:
        match = re.search(pattern, query)
        if match:
            time_str = match.group(1)
            break
    
    # 2. ì»¬ëŸ¼ëª… ì¶”ì¶œ (ì‹œê°„ ë¶€ë¶„ ì œì™¸í•˜ê³ )
    query_without_time = query
    if time_str:
        query_without_time = query.replace(time_str, '')
    
    # ì»¬ëŸ¼ íŒ¨í„´: ì˜ë¬¸ëŒ€ë¬¸ì, í•œê¸€, ìˆ«ì, _, ., (, ) í¬í•¨
    col_pattern = r'([A-Zê°€-í£][A-Zê°€-í£0-9_\.\(\)]+)'
    col_matches = re.findall(col_pattern, query_without_time, re.IGNORECASE)
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°
    valid_cols = [c for c in col_matches if c in _df.columns]
    
    # 3. ì‹œê°„ + ì»¬ëŸ¼ ë‘˜ ë‹¤ ìˆìœ¼ë©´ â†’ í•´ë‹¹ ì‹œê°„ì˜ íŠ¹ì • ì»¬ëŸ¼ê°’ë§Œ
    if time_str and valid_cols:
        logger.info(f"ì‹œê°„+ì»¬ëŸ¼ ê²€ìƒ‰: {time_str}, {valid_cols}")
        row, _ = search_by_time(time_str)
        
        if row is None:
            return None, f"ì‹œê°„ '{time_str}'ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # í•´ë‹¹ ì»¬ëŸ¼ê°’ë§Œ ë°˜í™˜
        data_text = f"ì‹œê°„: {time_str}\n"
        for col in valid_cols:
            if col in row.index:
                data_text += f"{col}: {row[col]}\n"
        
        return row, data_text
    
    # 4. ì‹œê°„ë§Œ ìˆìœ¼ë©´ â†’ ì „ì²´ í–‰ ë°ì´í„°
    if time_str:
        logger.info(f"ì‹œê°„ ê²€ìƒ‰: {time_str}")
        return search_by_time(time_str)
    
    # 5. ì»¬ëŸ¼ë§Œ ìˆìœ¼ë©´ â†’ ìµœê·¼ ë°ì´í„°
    if valid_cols:
        logger.info(f"ì»¬ëŸ¼ ê²€ìƒ‰: {valid_cols}")
        return search_by_columns(valid_cols)
    
    return None, "ê²€ìƒ‰ ì¡°ê±´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œê°„(ì˜ˆ: 2025-10-14 4:39) ë˜ëŠ” ì»¬ëŸ¼ëª…ì„ í¬í•¨í•´ì£¼ì„¸ìš”."

def search_range(start_time: str, end_time: str) -> Tuple[Optional[pd.DataFrame], str]:
    """
    ì‹œê°„ ë²”ìœ„ë¡œ ê²€ìƒ‰
    
    Args:
        start_time: ì‹œì‘ ì‹œê°„
        end_time: ì¢…ë£Œ ì‹œê°„
    
    Returns:
        (ê²°ê³¼ DataFrame, ì„¤ëª… í…ìŠ¤íŠ¸)
    """
    if _df is None:
        return None, "CSV íŒŒì¼ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    # ì‹œê°„ ì»¬ëŸ¼ ì°¾ê¸°
    time_cols = ['STAT_DT', 'í˜„ì¬ì‹œê°„', 'CURRTIME', 'ì‹œê°„']
    time_col = None
    for tc in time_cols:
        if tc in _df.columns:
            time_col = tc
            break
    
    if time_col is None:
        return None, "ì‹œê°„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    try:
        df_copy = _df.copy()
        df_copy[time_col] = pd.to_datetime(df_copy[time_col], errors='coerce')
        
        start_dt = pd.to_datetime(start_time)
        end_dt = pd.to_datetime(end_time)
        
        mask = (df_copy[time_col] >= start_dt) & (df_copy[time_col] <= end_dt)
        result = _df[mask]
        
        if result.empty:
            return None, f"{start_time} ~ {end_time} ë²”ìœ„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        data_text = f"ê²€ìƒ‰ ê²°ê³¼: {len(result)}ê±´ ({start_time} ~ {end_time})\n"
        return result, data_text
        
    except Exception as e:
        return None, f"ì‹œê°„ íŒŒì‹± ì˜¤ë¥˜: {e}"

def search_condition(column: str, operator: str, value: Any) -> Tuple[Optional[pd.DataFrame], str]:
    """
    ì¡°ê±´ìœ¼ë¡œ ê²€ìƒ‰
    
    Args:
        column: ì»¬ëŸ¼ëª…
        operator: ì—°ì‚°ì ('>', '<', '>=', '<=', '==', '!=')
        value: ë¹„êµê°’
    
    Returns:
        (ê²°ê³¼ DataFrame, ì„¤ëª… í…ìŠ¤íŠ¸)
    """
    if _df is None:
        return None, "CSV íŒŒì¼ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    if column not in _df.columns:
        return None, f"ì»¬ëŸ¼ '{column}'ì´ ì—†ìŠµë‹ˆë‹¤."
    
    try:
        col_data = pd.to_numeric(_df[column], errors='coerce')
        value = float(value)
        
        if operator == '>':
            mask = col_data > value
        elif operator == '<':
            mask = col_data < value
        elif operator == '>=':
            mask = col_data >= value
        elif operator == '<=':
            mask = col_data <= value
        elif operator == '==':
            mask = col_data == value
        elif operator == '!=':
            mask = col_data != value
        else:
            return None, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—°ì‚°ì: {operator}"
        
        result = _df[mask]
        
        if result.empty:
            return None, f"{column} {operator} {value} ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        data_text = f"ê²€ìƒ‰ ê²°ê³¼: {len(result)}ê±´ ({column} {operator} {value})\n"
        return result, data_text
        
    except Exception as e:
        return None, f"ì¡°ê±´ ê²€ìƒ‰ ì˜¤ë¥˜: {e}"

def get_statistics(column: str) -> Dict[str, Any]:
    """
    ì»¬ëŸ¼ í†µê³„ ì •ë³´
    
    Args:
        column: ì»¬ëŸ¼ëª…
    
    Returns:
        í†µê³„ ë”•ì…”ë„ˆë¦¬
    """
    if _df is None:
        return {'error': 'CSV íŒŒì¼ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}
    
    if column not in _df.columns:
        return {'error': f"ì»¬ëŸ¼ '{column}'ì´ ì—†ìŠµë‹ˆë‹¤."}
    
    try:
        col_data = pd.to_numeric(_df[column], errors='coerce').dropna()
        
        return {
            'count': len(col_data),
            'mean': float(col_data.mean()),
            'std': float(col_data.std()),
            'min': float(col_data.min()),
            'max': float(col_data.max()),
            'median': float(col_data.median()),
            'q25': float(col_data.quantile(0.25)),
            'q75': float(col_data.quantile(0.75))
        }
    except Exception as e:
        return {'error': str(e)}

def format_row(row: pd.Series, important_cols: List[str] = None) -> str:
    """
    í–‰ ë°ì´í„°ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…
    
    Args:
        row: pandas Series
        important_cols: ìš°ì„  í‘œì‹œí•  ì»¬ëŸ¼ (ì—†ìœ¼ë©´ ì „ì²´)
    
    Returns:
        í¬ë§·íŒ…ëœ ë¬¸ìì—´
    """
    text = ""
    
    if important_cols:
        for col in important_cols:
            if col in row.index and pd.notna(row[col]):
                text += f"{col}: {row[col]}\n"
    else:
        for col in row.index:
            if pd.notna(row[col]):
                text += f"{col}: {row[col]}\n"
    
    return text


# í…ŒìŠ¤íŠ¸ëŠ” ë³„ë„ íŒŒì¼ì—ì„œ ì‹¤í–‰
# python -c "import csv_searcher; csv_searcher.load_csv('./csv/with.csv'); print(csv_searcher.get_columns())"