import numpy as np
import pandas as pd
import xgboost as xgb
import pickle
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

warnings.filterwarnings('ignore')
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def train_minimal_drop_support():
    """
    ğŸ¯ ìµœì†Œ ë³€ê²½ìœ¼ë¡œ ê¸‰ê° ëŒ€ì‘ (ê¸°ì¡´ ê¸‰ì¦ ì„±ëŠ¥ ë³´ì¡´)
    
    âœ… ê¸°ì¡´ ìœ ì§€:
    - 1,804ê°œ ê¸‰ì¦ ì¼€ì´ìŠ¤ Feature ì „ë¶€ ìœ ì§€
    - HUBROOMTOTAL, STORAGE_UTIL, CMD, FS_UTIL Feature ê·¸ëŒ€ë¡œ
    
    ğŸ†• ìµœì†Œ ì¶”ê°€ (ë‹¨ 2ê°œ):
    - target_from_max: ê³ ì  ëŒ€ë¹„ í•˜ë½ëŸ‰ (ê¸‰ê° ì§•í›„)
    - target_decreasing: í•˜ë½ ì¶”ì„¸ í”Œë˜ê·¸
    
    âš ï¸ ê²€ì¦:
    - í•™ìŠµ ì „í›„ ê¸‰ì¦ ì˜ˆì¸¡ë¥  ë¹„êµ
    - Feature Importanceë¡œ ê¸°ì¡´ Feature ìˆœìœ„ í™•ì¸
    """
    print("="*80)
    print("ğŸ¯ ìµœì†Œ ë³€ê²½ ê¸‰ê° ëŒ€ì‘ í•™ìŠµ (ê¸°ì¡´ ê¸‰ì¦ ì„±ëŠ¥ ë³´ì¡´)")
    print("="*80)
   
    FEATURE_COLS = {
        'storage': ['M16A_3F_STORAGE_UTIL'],
        'fs_storage': ['CD_M163FSTORAGEUSE', 'CD_M163FSTORAGETOTAL', 'CD_M163FSTORAGEUTIL'],
        'hub': ['HUBROOMTOTAL'],
        'cmd': ['M16A_3F_CMD', 'M16A_6F_TO_HUB_CMD'],
        'inflow': ['M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2', 'M14A_3F_TO_HUB_JOB2'],
        'outflow': ['M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB', 'M16A_3F_TO_M14A_3F_JOB'],
        'maxcapa': ['M16A_6F_LFT_MAXCAPA', 'M16A_2F_LFT_MAXCAPA']
    }
   
    TARGET_COL = 'CURRENT_M16A_3F_JOB_2'
   
    def create_features_optimized(df, start_idx=30):
        """ê¸°ì¡´ Feature + ìµœì†Œ ì¶”ê°€ (2ê°œ)"""
        features_list = []
        labels = []
        seq_max_list = []
        seq_min_list = []
        indices = []
        seq_target_list = []
       
        for i in range(start_idx, len(df) - 10):
            seq_target = df[TARGET_COL].iloc[i-30:i].values
           
            # ============================================================
            # ê¸°ì¡´ íƒ€ê²Ÿ Feature (100% ìœ ì§€)
            # ============================================================
            features = {
                'target_mean': np.mean(seq_target),
                'target_std': np.std(seq_target),
                'target_max': np.max(seq_target),
                'target_min': np.min(seq_target),
                'target_last_value': seq_target[-1],
                'target_last_5_mean': np.mean(seq_target[-5:]),
                'target_slope': np.polyfit(np.arange(30), seq_target, 1)[0],
            }
           
            # ============================================================
            # ğŸ†• ê¸‰ê° ëŒ€ì‘ Feature (ë‹¨ 2ê°œë§Œ ì¶”ê°€)
            # ============================================================
            features['target_from_max'] = np.max(seq_target) - seq_target[-1]  # ê³ ì  ëŒ€ë¹„ í•˜ë½
            features['target_decreasing'] = 1 if features['target_slope'] < -1 else 0  # í•˜ë½ ì¶”ì„¸
           
            # ============================================================
            # ê¸°ì¡´ ì»¬ëŸ¼ Feature (100% ìœ ì§€)
            # ============================================================
            for group_name, cols in FEATURE_COLS.items():
                for col in cols:
                    if col not in df.columns:
                        continue
                   
                    col_seq = df[col].iloc[i-30:i].values
                   
                    if group_name == 'maxcapa':
                        features[f'{col}_last_value'] = col_seq[-1]
                   
                    elif group_name in ['cmd', 'storage', 'fs_storage', 'hub']:
                        # í•µì‹¬ Feature (ê¸°ì¡´ ìœ ì§€)
                        features[f'{col}_mean'] = np.mean(col_seq)
                        features[f'{col}_std'] = np.std(col_seq)
                        features[f'{col}_max'] = np.max(col_seq)
                        features[f'{col}_min'] = np.min(col_seq)
                        features[f'{col}_last_value'] = col_seq[-1]
                        features[f'{col}_last_5_mean'] = np.mean(col_seq[-5:])
                        features[f'{col}_slope'] = np.polyfit(np.arange(30), col_seq, 1)[0]
                   
                    else:
                        # Inflow/Outflow (ê¸°ì¡´ ìœ ì§€)
                        features[f'{col}_mean'] = np.mean(col_seq)
                        features[f'{col}_last_value'] = col_seq[-1]
                        features[f'{col}_slope'] = np.polyfit(np.arange(30), col_seq, 1)[0]
           
            # ============================================================
            # ê¸°ì¡´ ê¸‰ì¦ Feature (100% ìœ ì§€)
            # ============================================================
            if 'CD_M163FSTORAGEUSE' in df.columns and 'CD_M163FSTORAGETOTAL' in df.columns:
                storage_use = df['CD_M163FSTORAGEUSE'].iloc[i-30:i].values
                storage_total = df['CD_M163FSTORAGETOTAL'].iloc[i-30:i].values
                storage_util = df['CD_M163FSTORAGEUTIL'].iloc[i-30:i].values
               
                features['storage_use_rate'] = (storage_use[-1] - storage_use[0]) / 30
                features['storage_remaining'] = storage_total[-1] - storage_use[-1]
                features['storage_util_last'] = storage_util[-1]
                features['storage_util_high'] = 1 if storage_util[-1] >= 7 else 0  # ê¸‰ì¦ ì§•í›„
                features['storage_util_critical'] = 1 if storage_util[-1] >= 10 else 0
           
            # HUBROOMTOTAL Feature (ê¸°ì¡´ ìœ ì§€)
            if 'HUBROOMTOTAL' in df.columns:
                hub_seq = df['HUBROOMTOTAL'].iloc[i-30:i].values
                hub_last = hub_seq[-1]
               
                features['hub_critical'] = 1 if hub_last < 590 else 0
                features['hub_high'] = 1 if hub_last < 610 else 0  # í•µì‹¬!
                features['hub_warning'] = 1 if hub_last < 620 else 0
                features['hub_decrease_rate'] = (hub_seq[0] - hub_last) / 30
               
                # ë³µí•© ìœ„í—˜ (ê¸°ì¡´ ìœ ì§€)
                if 'CD_M163FSTORAGEUTIL' in df.columns:
                    storage_util_last = df['CD_M163FSTORAGEUTIL'].iloc[i-1]
                    features['hub_storage_risk'] = 1 if (hub_last < 610 and storage_util_last >= 7) else 0
           
            # ìœ ì…-ìœ ì¶œ, CMD (ê¸°ì¡´ ìœ ì§€)
            inflow_sum = sum(df[col].iloc[i-1] for col in FEATURE_COLS['inflow'] if col in df.columns)
            outflow_sum = sum(df[col].iloc[i-1] for col in FEATURE_COLS['outflow'] if col in df.columns)
            features['net_flow'] = inflow_sum - outflow_sum
           
            cmd_sum = sum(df[col].iloc[i-1] for col in FEATURE_COLS['cmd'] if col in df.columns)
            features['total_cmd'] = cmd_sum
            features['total_cmd_low'] = 1 if cmd_sum < 220 else 0
            features['total_cmd_very_low'] = 1 if cmd_sum < 200 else 0
           
            # HUB Ã— CMD (ê¸°ì¡´ ìœ ì§€)
            if 'HUBROOMTOTAL' in df.columns:
                hub_last = df['HUBROOMTOTAL'].iloc[i-1]
                features['hub_cmd_bottleneck'] = 1 if (hub_last < 610 and cmd_sum < 220) else 0
           
            # Storage Util (ê¸°ì¡´ ìœ ì§€)
            if 'M16A_3F_STORAGE_UTIL' in df.columns:
                storage_util = df['M16A_3F_STORAGE_UTIL'].iloc[i-1]
                features['storage_util_critical'] = 1 if storage_util >= 205 else 0
                features['storage_util_high_risk'] = 1 if storage_util >= 207 else 0
           
            features_list.append(features)
            labels.append(df[TARGET_COL].iloc[i:i+10].max())
            seq_max_list.append(np.max(seq_target))
            seq_min_list.append(np.min(seq_target))
            indices.append(i)
            seq_target_list.append(seq_target)
       
        return pd.DataFrame(features_list), np.array(labels), seq_max_list, seq_min_list, indices, seq_target_list
   
    # ===== 1. í•™ìŠµ =====
    print("\n[STEP 1] ë°ì´í„° ë¡œë“œ")
    print("-"*40)
   
    try:
        df_train = pd.read_csv('20250904_TO_20251020.csv', on_bad_lines='skip', encoding='utf-8', low_memory=False)
    except:
        try:
            df_train = pd.read_csv('20250904_TO_20251020.csv', on_bad_lines='skip', encoding='cp949', low_memory=False)
        except:
            df_train = pd.read_csv('20250904_TO_20251020.csv', on_bad_lines='skip', encoding='euc-kr', low_memory=False)
   
    df_train[TARGET_COL] = pd.to_numeric(df_train[TARGET_COL], errors='coerce')
    df_train = df_train.dropna(subset=[TARGET_COL])
   
    print(f"í•™ìŠµ ë°ì´í„°: {len(df_train)}ê°œ í–‰")
   
    # Feature ìƒì„±
    X_train, y_train, _, _, _, seq_target_list = create_features_optimized(df_train)
   
    print(f"\nâœ… Feature ìƒì„± ì™„ë£Œ:")
    print(f"  - ê¸°ì¡´ Feature: {len(X_train.columns) - 2}ê°œ")
    print(f"  - ğŸ†• ì¶”ê°€ Feature: 2ê°œ (target_from_max, target_decreasing)")
    print(f"  - ì´ Feature: {len(X_train.columns)}ê°œ")
    print(f"  - í•™ìŠµ ìƒ˜í”Œ: {len(X_train)}ê°œ")
   
    # ê¸‰ì¦/ê¸‰ê° ì¼€ì´ìŠ¤ í™•ì¸
    surge_count = sum(1 for i in range(len(y_train))
                      if X_train.iloc[i]['target_max'] < 300 and y_train[i] >= 300)
    drop_count = sum(1 for i in range(len(y_train))
                     if X_train.iloc[i]['target_max'] >= 300 and y_train[i] < 300)
   
    print(f"\nì¼€ì´ìŠ¤ ë¶„í¬:")
    print(f"  ğŸ”º ê¸‰ì¦: {surge_count}ê°œ ({surge_count/len(y_train)*100:.2f}%)")
    print(f"  ğŸ”» ê¸‰ê°: {drop_count}ê°œ ({drop_count/len(y_train)*100:.2f}%)")
    print(f"  âšª ì •ìƒ: {len(y_train)-surge_count-drop_count}ê°œ ({(len(y_train)-surge_count-drop_count)/len(y_train)*100:.2f}%)")
   
    # í•™ìŠµ/ê²€ì¦ ë¶„í• 
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42
    )
   
    # GPU/CPU ê°ì§€
    print("\nğŸ” í•™ìŠµ í™˜ê²½ ê°ì§€...")
    use_gpu = False
   
    try:
        test_model = xgb.XGBRegressor(
            n_estimators=5, max_depth=3, random_state=42,
            tree_method='gpu_hist', gpu_id=0
        )
        test_model.fit(X_tr[:100], y_tr[:100], verbose=False)
        use_gpu = True
        print("  âœ… GPU ëª¨ë“œ\n")
    except:
        print("  âš ï¸ CPU ëª¨ë“œ\n")
        use_gpu = False
   
    # ëª¨ë¸ ìƒì„± (ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°)
    if use_gpu:
        model = xgb.XGBRegressor(
            n_estimators=200,
            max_depth=7,
            learning_rate=0.04,
            subsample=0.85,
            colsample_bytree=0.85,
            min_child_weight=2,
            gamma=0.05,
            reg_alpha=0.05,
            reg_lambda=0.8,
            random_state=42,
            tree_method='gpu_hist',
            gpu_id=0,
            predictor='gpu_predictor'
        )
    else:
        model = xgb.XGBRegressor(
            n_estimators=200,
            max_depth=7,
            learning_rate=0.04,
            subsample=0.85,
            colsample_bytree=0.85,
            min_child_weight=2,
            gamma=0.05,
            reg_alpha=0.05,
            reg_lambda=0.8,
            random_state=42,
            tree_method='hist',
            n_jobs=-1
        )
   
    print("ëª¨ë¸ í•™ìŠµ ì¤‘...")
    eval_result = model.fit(
        X_tr, y_tr, 
        eval_set=[(X_tr, y_tr), (X_val, y_val)],
        eval_metric=['rmse', 'mae'],
        verbose=100
    )
   
    # í‰ê°€
    y_val_pred = model.predict(X_val)
    train_mae = mean_absolute_error(y_val, y_val_pred)
    train_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
    train_r2 = r2_score(y_val, y_val_pred)
   
    print(f"\ní•™ìŠµ ì„±ëŠ¥:")
    print(f"  MAE:  {train_mae:.4f}")
    print(f"  RMSE: {train_rmse:.4f}")
    print(f"  RÂ²:   {train_r2:.4f}")
    
    # ===== ì‹œê°í™” 1: í•™ìŠµ ê³¡ì„  =====
    print("\nğŸ“Š í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„ ìƒì„± ì¤‘...")
    results = eval_result.evals_result()
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))
    
    # RMSE
    axes[0].plot(results['validation_0']['rmse'], label='Train RMSE', linewidth=2)
    axes[0].plot(results['validation_1']['rmse'], label='Validation RMSE', linewidth=2)
    axes[0].set_xlabel('Iterations', fontsize=12)
    axes[0].set_ylabel('RMSE', fontsize=12)
    axes[0].set_title('Learning Curve - RMSE', fontsize=14, fontweight='bold')
    axes[0].legend(fontsize=11)
    axes[0].grid(True, alpha=0.3)
    
    # MAE
    axes[1].plot(results['validation_0']['mae'], label='Train MAE', linewidth=2)
    axes[1].plot(results['validation_1']['mae'], label='Validation MAE', linewidth=2)
    axes[1].set_xlabel('Iterations', fontsize=12)
    axes[1].set_ylabel('MAE', fontsize=12)
    axes[1].set_title('Learning Curve - MAE', fontsize=14, fontweight='bold')
    axes[1].legend(fontsize=11)
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('í•™ìŠµê³¡ì„ _ê¸‰ê°ëŒ€ì‘.png', dpi=150, bbox_inches='tight')
    print("âœ… ì €ì¥: í•™ìŠµê³¡ì„ _ê¸‰ê°ëŒ€ì‘.png")
    plt.close()
   
    # ëª¨ë¸ ì €ì¥
    with open('xgboost_ê¸‰ê°ëŒ€ì‘_ìµœì†Œë³€ê²½.pkl', 'wb') as f:
        pickle.dump(model, f)
    print("âœ… ëª¨ë¸ ì €ì¥: xgboost_ê¸‰ê°ëŒ€ì‘_ìµœì†Œë³€ê²½.pkl")
   
    # Feature ì¤‘ìš”ë„ (ê¸°ì¡´ Feature ìˆœìœ„ í™•ì¸)
    print("\nğŸ”¥ Feature ì¤‘ìš”ë„ Top 30:")
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False).head(30)
   
    new_features_in_top = []
    for idx, row in feature_importance.iterrows():
        # ì‹ ê·œ Feature ì²´í¬
        if row['feature'] in ['target_from_max', 'target_decreasing']:
            marker = "ğŸ†•"
            new_features_in_top.append(row['feature'])
        # ê¸°ì¡´ í•µì‹¬ Feature ì²´í¬
        elif any(keyword in row['feature'] for keyword in ['hub_high', 'hub_critical', 'storage_util_high',
                                                            'total_cmd_low', 'hub_storage_risk', 'hub_cmd_bottleneck',
                                                            'storage_util_critical']):
            marker = "ğŸ¯"
        elif 'HUBROOM' in row['feature'] or 'storage_util' in row['feature'] or 'CMD' in row['feature']:
            marker = "ğŸ”¥"
        else:
            marker = "  "
        print(f"{marker} {row['feature']}: {row['importance']:.4f}")
   
    if new_features_in_top:
        print(f"\nâš ï¸ ì‹ ê·œ Feature Top 30 ì§„ì…: {new_features_in_top}")
        print(f"   â†’ ê¸°ì¡´ ê¸‰ì¦ Feature ìˆœìœ„ í™•ì¸ í•„ìš”")
    
    # ===== ì‹œê°í™” 2: Feature Importance =====
    print("\nğŸ“Š Feature Importance ê·¸ë˜í”„ ìƒì„± ì¤‘...")
    fig, ax = plt.subplots(figsize=(12, 10))
    
    colors = ['red' if f in ['target_from_max', 'target_decreasing'] else 'steelblue' for f in feature_importance['feature']]
    
    ax.barh(range(len(feature_importance)), feature_importance['importance'], color=colors)
    ax.set_yticks(range(len(feature_importance)))
    ax.set_yticklabels(feature_importance['feature'], fontsize=10)
    ax.set_xlabel('Importance', fontsize=12)
    ax.set_title('Top 30 Feature Importance (Red=New Features)', fontsize=14, fontweight='bold')
    ax.grid(True, axis='x', alpha=0.3)
    ax.invert_yaxis()
    
    plt.tight_layout()
    plt.savefig('Featureì¤‘ìš”ë„_ê¸‰ê°ëŒ€ì‘.png', dpi=150, bbox_inches='tight')
    print("âœ… ì €ì¥: Featureì¤‘ìš”ë„_ê¸‰ê°ëŒ€ì‘.png")
    plt.close()
   
    # ===== 2. í‰ê°€ =====
    print("\n[STEP 2] í‰ê°€")
    print("-"*40)
   
    split_idx = int(len(df_train) * 0.8)
    df_test = df_train.iloc[split_idx:].copy()
   
    X_test, y_test, seq_max_list, seq_min_list, indices, seq_target_list = create_features_optimized(df_test, start_idx=30)
   
    y_pred = model.predict(X_test)
   
    test_mae = mean_absolute_error(y_test, y_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    test_r2 = r2_score(y_test, y_pred)
   
    print(f"\ní‰ê°€ ì„±ëŠ¥:")
    print(f"  MAE:  {test_mae:.4f}")
    print(f"  RMSE: {test_rmse:.4f}")
    print(f"  RÂ²:   {test_r2:.4f}")
   
    # ===== 3. ê¸‰ì¦/ê¸‰ê° ì„±ëŠ¥ ë¹„êµ (í•µì‹¬!) =====
    print("\n[STEP 3] âš ï¸ ê¸°ì¡´ ê¸‰ì¦ ì„±ëŠ¥ ë³´ì¡´ í™•ì¸")
    print("-"*40)
   
    surge_count = sum(1 for i in range(len(y_test))
                      if seq_target_list[i][-1] < 300 and y_test[i] >= 300)
    surge_detected = sum(1 for i in range(len(y_test))
                         if seq_target_list[i][-1] < 300 and y_test[i] >= 300 and y_pred[i] >= 290)
   
    print(f"ğŸ”º ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥:")
    print(f"  ë°œìƒ: {surge_count}ê°œ")
    print(f"  ì˜ˆì¸¡ ì„±ê³µ: {surge_detected}/{surge_count}ê°œ ({surge_detected/surge_count*100 if surge_count > 0 else 0:.1f}%)")
    print(f"  âœ… ê¸°ì¡´ ëŒ€ë¹„: í™•ì¸ í•„ìš” (ê¸°ì¡´ 78% ìœ ì§€ë˜ì–´ì•¼ í•¨)")
   
    drop_count = sum(1 for i in range(len(y_test))
                     if seq_target_list[i][-1] >= 300 and y_test[i] < 300)
    drop_detected = sum(1 for i in range(len(y_test))
                        if seq_target_list[i][-1] >= 300 and y_test[i] < 300 and y_pred[i] < 310)
   
    print(f"\nğŸ”» ê¸‰ê° ì˜ˆì¸¡ ì„±ëŠ¥ (ì‹ ê·œ):")
    print(f"  ë°œìƒ: {drop_count}ê°œ")
    print(f"  ì˜ˆì¸¡ ì„±ê³µ: {drop_detected}/{drop_count}ê°œ ({drop_detected/drop_count*100 if drop_count > 0 else 0:.1f}%)")
   
    # ê·¹ë‹¨ê°’ ì„±ëŠ¥
    extreme_mask = y_test >= 300
    extreme_count = extreme_mask.sum()
    extreme_detected = ((y_pred >= 290) & extreme_mask).sum()
   
    print(f"\nê·¹ë‹¨ê°’ (300+) ê°ì§€:")
    print(f"  ë°œìƒ: {extreme_count}ê°œ")
    print(f"  ê°ì§€: {extreme_detected}/{extreme_count}ê°œ ({extreme_detected/extreme_count*100 if extreme_count > 0 else 0:.1f}%)")
   
    # ê²°ê³¼ ì €ì¥
    results = []
    for i, idx in enumerate(indices):
        is_surge = (seq_target_list[i][-1] < 300 and y_test[i] >= 300)
        is_drop = (seq_target_list[i][-1] >= 300 and y_test[i] < 300)
        
        results.append({
            'ì‹¤ì œê°’': y_test[i],
            'ì˜ˆì¸¡ê°’': round(y_pred[i], 2),
            'ì˜¤ì°¨': round(abs(y_test[i] - y_pred[i]), 2),
            'ì‹œí€€ìŠ¤MAX': seq_max_list[i],
            'ê¸‰ì¦': 'O' if is_surge else '-',
            'ê¸‰ì¦ê°ì§€': 'O' if (is_surge and y_pred[i] >= 290) else '-',
            'ê¸‰ê°': 'O' if is_drop else '-',
            'ê¸‰ê°ê°ì§€': 'O' if (is_drop and y_pred[i] < 310) else '-',
            'ê·¹ë‹¨ê°’': 'O' if y_test[i] >= 300 else '-',
            'ê°ì§€': 'O' if (y_test[i] >= 300 and y_pred[i] >= 290) else '-'
        })
   
    df_results = pd.DataFrame(results)
    df_results.to_csv('ê¸‰ê°ëŒ€ì‘_ìµœì†Œë³€ê²½_ê²°ê³¼.csv', index=False, encoding='utf-8-sig')
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: ê¸‰ê°ëŒ€ì‘_ìµœì†Œë³€ê²½_ê²°ê³¼.csv")
    
    # ===== ì‹œê°í™” 3: ì˜ˆì¸¡ vs ì‹¤ì œ ì‚°ì ë„ =====
    print("\nğŸ“Š ì˜ˆì¸¡ vs ì‹¤ì œ ì‚°ì ë„ ìƒì„± ì¤‘...")
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # ì „ì²´ ë°ì´í„°
    axes[0].scatter(y_test, y_pred, alpha=0.5, s=20)
    axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')
    axes[0].set_xlabel('Actual Value', fontsize=12)
    axes[0].set_ylabel('Predicted Value', fontsize=12)
    axes[0].set_title(f'Prediction vs Actual (RÂ²={test_r2:.3f}, MAE={test_mae:.2f})', fontsize=14, fontweight='bold')
    axes[0].legend(fontsize=11)
    axes[0].grid(True, alpha=0.3)
    
    # ê¸‰ì¦/ê¸‰ê° ì¼€ì´ìŠ¤ ê°•ì¡°
    surge_mask = np.array([results[i]['ê¸‰ì¦'] == 'O' for i in range(len(results))])
    drop_mask = np.array([results[i]['ê¸‰ê°'] == 'O' for i in range(len(results))])
    normal_mask = ~(surge_mask | drop_mask)
    
    axes[1].scatter(y_test[normal_mask], y_pred[normal_mask], alpha=0.3, s=20, c='gray', label='Normal')
    axes[1].scatter(y_test[surge_mask], y_pred[surge_mask], alpha=0.7, s=40, c='red', marker='^', label=f'Surge ({surge_mask.sum()})')
    axes[1].scatter(y_test[drop_mask], y_pred[drop_mask], alpha=0.7, s=40, c='blue', marker='v', label=f'Drop ({drop_mask.sum()})')
    axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
    axes[1].set_xlabel('Actual Value', fontsize=12)
    axes[1].set_ylabel('Predicted Value', fontsize=12)
    axes[1].set_title('Surge/Drop Cases Highlighted', fontsize=14, fontweight='bold')
    axes[1].legend(fontsize=11)
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('ì˜ˆì¸¡vsì‹¤ì œ_ê¸‰ê°ëŒ€ì‘.png', dpi=150, bbox_inches='tight')
    print("âœ… ì €ì¥: ì˜ˆì¸¡vsì‹¤ì œ_ê¸‰ê°ëŒ€ì‘.png")
    plt.close()
    
    # ===== ì‹œê°í™” 4: ì˜¤ì°¨ ë¶„í¬ ë° ì¼€ì´ìŠ¤ ë¶„ì„ =====
    print("\nğŸ“Š ì˜¤ì°¨ ë¶„í¬ ë° ì¼€ì´ìŠ¤ ë¶„ì„ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # ì˜¤ì°¨ íˆìŠ¤í† ê·¸ë¨
    errors = y_test - y_pred
    axes[0, 0].hist(errors, bins=50, edgecolor='black', alpha=0.7)
    axes[0, 0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')
    axes[0, 0].set_xlabel('Prediction Error', fontsize=12)
    axes[0, 0].set_ylabel('Frequency', fontsize=12)
    axes[0, 0].set_title(f'Error Distribution (Mean={errors.mean():.2f}, Std={errors.std():.2f})', fontsize=14, fontweight='bold')
    axes[0, 0].legend(fontsize=11)
    axes[0, 0].grid(True, alpha=0.3)
    
    # ì¼€ì´ìŠ¤ë³„ ë¶„í¬
    case_types = ['Normal', 'Surge', 'Drop']
    case_counts = [normal_mask.sum(), surge_mask.sum(), drop_mask.sum()]
    case_colors = ['gray', 'red', 'blue']
    
    axes[0, 1].bar(case_types, case_counts, color=case_colors, alpha=0.7, edgecolor='black')
    axes[0, 1].set_ylabel('Count', fontsize=12)
    axes[0, 1].set_title('Case Type Distribution', fontsize=14, fontweight='bold')
    for i, v in enumerate(case_counts):
        axes[0, 1].text(i, v + max(case_counts)*0.02, f'{v}\n({v/len(y_test)*100:.1f}%)', 
                       ha='center', fontsize=11, fontweight='bold')
    axes[0, 1].grid(True, axis='y', alpha=0.3)
    
    # ê¸‰ì¦ ì˜ˆì¸¡ ì„±ê³µë¥ 
    surge_detected_mask = np.array([results[i]['ê¸‰ì¦ê°ì§€'] == 'O' for i in range(len(results))])
    surge_success_rate = surge_detected_mask.sum() / surge_mask.sum() * 100 if surge_mask.sum() > 0 else 0
    
    drop_detected_mask = np.array([results[i]['ê¸‰ê°ê°ì§€'] == 'O' for i in range(len(results))])
    drop_success_rate = drop_detected_mask.sum() / drop_mask.sum() * 100 if drop_mask.sum() > 0 else 0
    
    success_types = ['Surge\nDetection', 'Drop\nDetection']
    success_rates = [surge_success_rate, drop_success_rate]
    success_colors = ['red', 'blue']
    
    bars = axes[1, 0].bar(success_types, success_rates, color=success_colors, alpha=0.7, edgecolor='black')
    axes[1, 0].set_ylabel('Success Rate (%)', fontsize=12)
    axes[1, 0].set_title('Detection Success Rate', fontsize=14, fontweight='bold')
    axes[1, 0].set_ylim([0, 105])
    axes[1, 0].axhline(78, color='green', linestyle='--', linewidth=2, label='Target: 78%')
    for i, (bar, rate) in enumerate(zip(bars, success_rates)):
        axes[1, 0].text(bar.get_x() + bar.get_width()/2, rate + 2, f'{rate:.1f}%', 
                       ha='center', fontsize=12, fontweight='bold')
    axes[1, 0].legend(fontsize=11)
    axes[1, 0].grid(True, axis='y', alpha=0.3)
    
    # ì‹¤ì œê°’ ë²”ìœ„ë³„ ì˜¤ì°¨
    value_bins = [0, 250, 300, 350, 400, y_test.max()]
    value_labels = ['<250', '250-300', '300-350', '350-400', '400+']
    bin_errors = []
    
    for i in range(len(value_bins)-1):
        mask = (y_test >= value_bins[i]) & (y_test < value_bins[i+1])
        if mask.sum() > 0:
            bin_errors.append(np.abs(errors[mask]).mean())
        else:
            bin_errors.append(0)
    
    axes[1, 1].bar(value_labels, bin_errors, alpha=0.7, edgecolor='black', color='steelblue')
    axes[1, 1].set_xlabel('Actual Value Range', fontsize=12)
    axes[1, 1].set_ylabel('Mean Absolute Error', fontsize=12)
    axes[1, 1].set_title('Error by Value Range', fontsize=14, fontweight='bold')
    axes[1, 1].grid(True, axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('ì˜¤ì°¨ë¶„ì„_ê¸‰ê°ëŒ€ì‘.png', dpi=150, bbox_inches='tight')
    print("âœ… ì €ì¥: ì˜¤ì°¨ë¶„ì„_ê¸‰ê°ëŒ€ì‘.png")
    plt.close()
   
    # ìµœì¢… ìš”ì•½
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… ìš”ì•½")
    print("="*80)
   
    print(f"\n1. Feature ë³€ê²½:")
    print(f"   - ê¸°ì¡´: {len(X_train.columns) - 2}ê°œ (100% ìœ ì§€)")
    print(f"   - ì¶”ê°€: 2ê°œ (target_from_max, target_decreasing)")
    print(f"   - ì´: {len(X_train.columns)}ê°œ")
   
    print(f"\n2. ì„±ëŠ¥:")
    print(f"   - MAE: {test_mae:.2f}")
    print(f"   - RÂ²: {test_r2:.3f}")
   
    print(f"\n3. ê¸‰ì¦ ì˜ˆì¸¡ (ê¸°ì¡´ ì„±ëŠ¥):")
    print(f"   - ì„±ê³µë¥ : {surge_detected/surge_count*100 if surge_count > 0 else 0:.1f}%")
    print(f"   - âš ï¸ 78% ì´ìƒ ìœ ì§€ í™•ì¸ í•„ìš”")
   
    print(f"\n4. ê¸‰ê° ì˜ˆì¸¡ (ì‹ ê·œ):")
    print(f"   - ì„±ê³µë¥ : {drop_detected/drop_count*100 if drop_count > 0 else 0:.1f}%")
   
    if surge_detected/surge_count < 0.75 if surge_count > 0 else False:
        print(f"\nâŒ ê²½ê³ : ê¸‰ì¦ ì˜ˆì¸¡ë¥ ì´ 75% ë¯¸ë§Œìœ¼ë¡œ í•˜ë½!")
        print(f"   â†’ ì´ì „ ëª¨ë¸ë¡œ ë¡¤ë°± ê¶Œì¥")
    else:
        print(f"\nâœ… ê¸°ì¡´ ê¸‰ì¦ ì„±ëŠ¥ ìœ ì§€ë¨")
    
    print(f"\n5. ìƒì„±ëœ ì‹œê°í™” íŒŒì¼:")
    print(f"   ğŸ“Š í•™ìŠµê³¡ì„ _ê¸‰ê°ëŒ€ì‘.png")
    print(f"   ğŸ“Š Featureì¤‘ìš”ë„_ê¸‰ê°ëŒ€ì‘.png")
    print(f"   ğŸ“Š ì˜ˆì¸¡vsì‹¤ì œ_ê¸‰ê°ëŒ€ì‘.png")
    print(f"   ğŸ“Š ì˜¤ì°¨ë¶„ì„_ê¸‰ê°ëŒ€ì‘.png")
   
    return model, df_results, feature_importance

if __name__ == '__main__':
    model, results, importance = train_minimal_drop_support()