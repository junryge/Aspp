ì•Œê² ìŠµë‹ˆë‹¤. ModelCheckpoint íŒŒì¼ëª… ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•œ ì „ì²´ ì½”ë“œì…ë‹ˆë‹¤:#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ğŸ”¥ HUBROOM V4 ë“€ì–¼ ëª¨ë¸ í•™ìŠµ ì‹œìŠ¤í…œ - 30ë¶„â†’10ë¶„ ì˜ˆì¸¡ (ìˆ˜ì • ë²„ì „)
================================================================================
âœ¨ ì£¼ìš” ìˆ˜ì •ì‚¬í•­:
   - ì¸ë±ìŠ¤ ë§¤í•‘ ë¬¸ì œ í•´ê²°
   - ì¤‘ë‹¨/ì¬ê°œ ê¸°ëŠ¥ ê°•í™”
   - ì‹œí€€ìŠ¤ ìƒì„± ì§„í–‰ìƒí™© ì €ì¥
   - ModelCheckpoint íŒŒì¼ëª… ìˆ˜ì • (.weights.h5)
   
ğŸ¯ ëª©í‘œ:
   - Model 1 (ì•ˆì •í˜•): 80-299 ë²”ìœ„ ì „ë¬¸, False Positive ìµœì†Œí™”
   - Model 2 (ê·¹ë‹¨í˜•): 300+ ì „ë¬¸, ì í”„ ê°ì§€ íŠ¹í™”
================================================================================
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.model_selection import train_test_split
import os
import pickle
import warnings
from datetime import datetime
import signal
import sys
import joblib
import h5py
from tqdm import tqdm
import gc  # ë©”ëª¨ë¦¬ ê´€ë¦¬

warnings.filterwarnings('ignore')
np.random.seed(42)
tf.random.set_seed(42)

print("="*80)
print("ğŸ”¥ HUBROOM V4 ë“€ì–¼ ëª¨ë¸ í•™ìŠµ ì‹œìŠ¤í…œ (ìˆ˜ì • ë²„ì „)")
print("ğŸ“Š 30ë¶„ ì‹œí€€ìŠ¤ â†’ 10ë¶„ í›„ ì˜ˆì¸¡")
print("ğŸ¯ Model1(ì•ˆì •í˜•) + Model2(ê·¹ë‹¨í˜•)")
print("âœ… ì¸ë±ìŠ¤ ë¬¸ì œ í•´ê²° + ì¤‘ë‹¨/ì¬ê°œ ê°•í™”")
print("="*80)
print(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"ğŸ”§ TensorFlow Version: {tf.__version__}")
print("="*80)

# ==============================================================================
# ğŸ’¾ ê°•í™”ëœ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
# ==============================================================================

class EnhancedCheckpointManager:
    """ê°•í™”ëœ í•™ìŠµ ì¤‘ë‹¨/ì¬ê°œ ê´€ë¦¬"""
    
    def __init__(self, checkpoint_dir='./checkpoints_dual_30min_v2'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        self.state_file = os.path.join(checkpoint_dir, 'training_state.pkl')
        self.sequence_file = os.path.join(checkpoint_dir, 'sequences_complete.h5')
        self.scaler_dir = os.path.join(checkpoint_dir, 'scalers')
        os.makedirs(self.scaler_dir, exist_ok=True)
        
        # ì‹œí€€ìŠ¤ ìƒì„± ì§„í–‰ìƒí™© íŒŒì¼
        self.sequence_progress_file = os.path.join(checkpoint_dir, 'sequence_progress.pkl')
        
        self.interrupted = False
        signal.signal(signal.SIGINT, self._signal_handler)
    
    def _signal_handler(self, sig, frame):
        """Ctrl+C ì‹œê·¸ë„ ì²˜ë¦¬"""
        print('\n\nâš ï¸ ì¤‘ë‹¨ ê°ì§€! í˜„ì¬ ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤...')
        self.interrupted = True
    
    def save_state(self, state):
        """í˜„ì¬ ìƒíƒœ ì €ì¥"""
        with open(self.state_file, 'wb') as f:
            pickle.dump(state, f)
        print(f"ğŸ’¾ ìƒíƒœ ì €ì¥ ì™„ë£Œ: Step {state.get('step', 0)}")
    
    def load_state(self):
        """ì €ì¥ëœ ìƒíƒœ ë¡œë“œ"""
        if os.path.exists(self.state_file):
            with open(self.state_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def save_sequence_progress(self, progress_data):
        """ì‹œí€€ìŠ¤ ìƒì„± ì§„í–‰ìƒí™© ì €ì¥"""
        with open(self.sequence_progress_file, 'wb') as f:
            pickle.dump(progress_data, f)
        print(f"ğŸ’¾ ì‹œí€€ìŠ¤ ì§„í–‰ìƒí™© ì €ì¥: {progress_data.get('current_idx', 0)}ê°œ ì™„ë£Œ")
    
    def load_sequence_progress(self):
        """ì‹œí€€ìŠ¤ ìƒì„± ì§„í–‰ìƒí™© ë¡œë“œ"""
        if os.path.exists(self.sequence_progress_file):
            with open(self.sequence_progress_file, 'rb') as f:
                return pickle.load(f)
        return None

# ==============================================================================
# ğŸ“Š ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤ (ìˆ˜ì •)
# ==============================================================================

class V4DataProcessor:
    """V4 ë“€ì–¼ ëª¨ë¸ìš© ë°ì´í„° ì²˜ë¦¬ - ì¸ë±ìŠ¤ ë¬¸ì œ í•´ê²°"""
    
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        self.seq_len = 30  # 30ë¶„ ì‹œí€€ìŠ¤
        
        # V4 í•„ìˆ˜ 21ê°œ ì»¬ëŸ¼
        self.v4_cols = [
            'CURRENT_M16A_3F_JOB_2',
            'M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2', 
            'M14A_3F_TO_HUB_JOB2', 'M14B_7F_TO_HUB_JOB2', 'M16B_10F_TO_HUB_JOB',
            'M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB', 'M16A_3F_TO_M14B_7F_JOB', 'M16A_3F_TO_3F_MLUD_JOB',
            'M16A_3F_CMD', 'M16A_6F_TO_HUB_CMD', 'M16A_2F_TO_HUB_CMD',
            'M14A_3F_TO_HUB_CMD', 'M14B_7F_TO_HUB_CMD',
            'M16A_6F_LFT_MAXCAPA', 'M16A_2F_LFT_MAXCAPA',
            'M16A_3F_STORAGE_UTIL',
            'M14_TO_M16_OFS_CUR', 'M16_TO_M14_OFS_CUR'
        ]
        
        # í™•ë¥  ë§µ - ë§¤ìš° ì¤‘ìš”!
        self.probability_map = {
            0: 0.003, 1: 0.15, 2: 0.25, 3: 0.31, 4: 0.43, 5: 0.43,
            6: 0.35, 7: 0.42, 8: 0.53, 9: 0.49, 10: 0.42,
            11: 0.47, 12: 0.52, 13: 0.60, 14: 0.54, 15: 0.66,
            16: 0.62, 17: 0.71, 18: 0.79, 19: 0.83, 20: 0.987,
            21: 0.99, 22: 0.99, 23: 0.99, 24: 0.99, 25: 0.99,
            26: 0.99, 27: 0.99, 28: 0.99, 29: 0.99, 30: 0.99
        }
        
        self.scaler_X = RobustScaler()
        self.scaler_y = RobustScaler()
        self.scaler_physics = StandardScaler()
    
    def load_and_merge_data(self):
        """ë°ì´í„° ë¡œë“œ ë° BRIDGE_TIME ë³‘í•©"""
        print("\n[1ë‹¨ê³„] ë°ì´í„° ë¡œë“œ")
        
        # ë©”ì¸ ë°ì´í„°
        df = pd.read_csv('data/HUB_0509_TO_0730_DATA.CSV')
        print(f"âœ… ë©”ì¸ ë°ì´í„°: {df.shape}")
        
        # ì‹œê°„ ì²˜ë¦¬
        time_col = df.columns[0]
        df['datetime'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M')
        
        # BRIDGE_TIME ë°ì´í„°
        bridge_df = pd.read_csv('data/BRTIME_0509_TO_0730.CSV')
        print(f"âœ… BRIDGE_TIME ë°ì´í„°: {bridge_df.shape}")
        
        # BRIDGE_TIME ë³‘í•©
        if 'IDC_VAL' in bridge_df.columns:
            bridge_df['BRIDGE_TIME'] = bridge_df['IDC_VAL']
            bridge_df['datetime'] = pd.to_datetime(bridge_df['CRT_TM'])
            
            # ì‹œê°„ëŒ€ ì •ë³´ ì œê±°
            if hasattr(bridge_df['datetime'].dtype, 'tz'):
                bridge_df['datetime'] = bridge_df['datetime'].dt.tz_localize(None)
            if hasattr(df['datetime'].dtype, 'tz'):
                df['datetime'] = df['datetime'].dt.tz_localize(None)
            
            # ë¶„ ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
            bridge_df['datetime'] = bridge_df['datetime'].dt.floor('min')
            df['datetime'] = df['datetime'].dt.floor('min')
            
            # ë³‘í•©
            df = pd.merge(df, bridge_df[['datetime', 'BRIDGE_TIME']], 
                         on='datetime', how='left')
            
            # BRIDGE_TIME ë³´ê°„ ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬
            df['BRIDGE_TIME'] = df['BRIDGE_TIME'].interpolate(method='linear', limit_direction='both')
            df['BRIDGE_TIME'] = df['BRIDGE_TIME'].fillna(3.5)
            
            print(f"âœ… BRIDGE_TIME ë³‘í•© ì™„ë£Œ")
            
            # V4 ì»¬ëŸ¼ì— ì¶”ê°€
            if 'BRIDGE_TIME' not in self.v4_cols:
                self.v4_cols.append('BRIDGE_TIME')
        
        # ì¸ë±ìŠ¤ ë¦¬ì…‹ (ì¤‘ìš”!)
        df = df.reset_index(drop=True)
        
        return df
    
    def add_consecutive_patterns(self, df):
        """ì—°ì† 300+ íŒ¨í„´ ì¶”ê°€ (í™•ë¥  ë§µ í™œìš©)"""
        consecutive_counts = []
        consecutive_probs = []
        
        for i in tqdm(range(len(df)), desc="ì—°ì† íŒ¨í„´ ê³„ì‚°"):
            if i < self.seq_len:
                count = 0
                prob = 0.003
            else:
                window = df[self.target_col].iloc[i-self.seq_len:i].values
                count = sum(1 for v in window if v >= 300)
                # í™•ë¥  ë§µì—ì„œ ì •í™•í•œ ê°’ ì°¾ê¸°
                if count in self.probability_map:
                    prob = self.probability_map[count]
                else:
                    # ê°€ì¥ ê°€ê¹Œìš´ ê°’ ì°¾ê¸°
                    closest_key = min(self.probability_map.keys(), 
                                    key=lambda x: abs(x - count))
                    prob = self.probability_map[closest_key]
            
            consecutive_counts.append(count)
            consecutive_probs.append(prob)
        
        df['consecutive_300_count'] = consecutive_counts
        df['consecutive_300_prob'] = consecutive_probs
        
        return df
    
    def create_sequences_with_resume(self, df, ckpt, pred_len=10):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± - ì¤‘ë‹¨/ì¬ê°œ ì§€ì›"""
        print(f"\n[3ë‹¨ê³„] ì‹œí€€ìŠ¤ ìƒì„± ({self.seq_len}ë¶„ â†’ {pred_len}ë¶„ í›„)")
        
        # ì§„í–‰ìƒí™© í™•ì¸
        progress = ckpt.load_sequence_progress()
        if progress and progress.get('completed', False) == False:
            print(f"âš ï¸ ì´ì „ ì§„í–‰ìƒí™© ë°œê²¬: {progress['current_idx']}/{progress['total']} ì™„ë£Œ")
            resume = input("ì´ì–´ì„œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
            
            if resume == 'y':
                # ì´ì „ ë°ì´í„° ë¡œë“œ
                X = progress['X']
                y = progress['y']
                physics_features = progress['physics_features']
                sequence_indices = progress['sequence_indices']
                start_idx = progress['current_idx'] + self.seq_len
            else:
                X, y, physics_features, sequence_indices = [], [], [], []
                start_idx = self.seq_len
                progress = None
        else:
            X, y, physics_features, sequence_indices = [], [], [], []
            start_idx = self.seq_len
            progress = None
        
        # í•„ìˆ˜ ì»¬ëŸ¼ + consecutive ì»¬ëŸ¼
        feature_cols = self.v4_cols + ['consecutive_300_count', 'consecutive_300_prob']
        
        total = len(df) - pred_len - self.seq_len
        
        try:
            for i in range(start_idx, len(df) - pred_len):
                if ckpt.interrupted:
                    # ì¤‘ë‹¨ ì‹œ ì§„í–‰ìƒí™© ì €ì¥
                    progress_data = {
                        'X': X,
                        'y': y,
                        'physics_features': physics_features,
                        'sequence_indices': sequence_indices,
                        'current_idx': i - self.seq_len,
                        'total': total,
                        'completed': False
                    }
                    ckpt.save_sequence_progress(progress_data)
                    print(f"\nğŸ’¾ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘ë‹¨! {len(X)}ê°œ ì €ì¥")
                    sys.exit(0)
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if (i - start_idx) % 1000 == 0:
                    print(f"  ì§„í–‰: {i - self.seq_len}/{total} ({(i - self.seq_len)/total*100:.1f}%)")
                
                # ì…ë ¥: ê³¼ê±° 30ë¶„
                X.append(df[feature_cols].iloc[i-self.seq_len:i].values)
                
                # íƒ€ê²Ÿ: 10ë¶„ í›„
                target_idx = i + pred_len - 1
                y.append(df[self.target_col].iloc[target_idx])
                
                # ì›ë³¸ ì¸ë±ìŠ¤ ì €ì¥ (íƒ€ê²Ÿ ì‹œì ì˜ df ì¸ë±ìŠ¤)
                sequence_indices.append(target_idx)
                
                # ë¬¼ë¦¬ íŠ¹ì§• (í˜„ì¬ ì‹œì  ê¸°ì¤€)
                physics = self.create_physics_features(df, i-1)
                physics_features.append(physics)
            
            # ì™„ë£Œ í‘œì‹œ
            X = np.array(X)
            y = np.array(y)
            physics_features = np.array(physics_features)
            sequence_indices = np.array(sequence_indices)
            
            # ì™„ë£Œ ìƒíƒœ ì €ì¥
            progress_data = {
                'completed': True,
                'total_sequences': len(X)
            }
            ckpt.save_sequence_progress(progress_data)
            
            print(f"\nâœ… {len(X)}ê°œ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ")
            print(f"  X shape: {X.shape}")
            print(f"  y shape: {y.shape}")
            print(f"  Physics shape: {physics_features.shape}")
            print(f"  Indices shape: {sequence_indices.shape}")
            
            return X, y, physics_features, sequence_indices
        
        except Exception as e:
            print(f"\nâŒ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œì—ë„ ì§„í–‰ìƒí™© ì €ì¥
            if len(X) > 0:
                progress_data = {
                    'X': X,
                    'y': y,
                    'physics_features': physics_features,
                    'sequence_indices': sequence_indices,
                    'current_idx': i - self.seq_len if 'i' in locals() else 0,
                    'total': total,
                    'completed': False
                }
                ckpt.save_sequence_progress(progress_data)
            raise
    
    def create_physics_features(self, df, idx):
        """ë¬¼ë¦¬ íŠ¹ì§• ìƒì„± (11ì°¨ì›) - í‰ê°€ ì½”ë“œì™€ ë™ì¼"""
        physics = []
        
        # 1. í˜„ì¬ HUBROOM ê°’
        physics.append(df[self.target_col].iloc[idx])
        
        # 2. ìœ ì… í•©ê³„
        inflow_cols = ['M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2', 
                      'M14A_3F_TO_HUB_JOB2', 'M14B_7F_TO_HUB_JOB2', 'M16B_10F_TO_HUB_JOB']
        inflow_sum = df[inflow_cols].iloc[idx].sum()
        physics.append(inflow_sum)
        
        # 3. ìœ ì¶œ í•©ê³„
        outflow_cols = ['M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB',
                       'M16A_3F_TO_M14A_3F_JOB', 'M16A_3F_TO_M14B_7F_JOB', 
                       'M16A_3F_TO_3F_MLUD_JOB']
        outflow_sum = df[outflow_cols].iloc[idx].sum()
        physics.append(outflow_sum)
        
        # 4. BRIDGE_TIME
        physics.append(df.get('BRIDGE_TIME', pd.Series([3.5])).iloc[idx])
        
        # 5. ì—°ì† 300+ ì¹´ìš´íŠ¸
        physics.append(df['consecutive_300_count'].iloc[idx])
        
        # 6. ì—°ì† 300+ í™•ë¥ 
        physics.append(df['consecutive_300_prob'].iloc[idx])
        
        # 7. STORAGE_UTIL
        physics.append(df.get('M16A_3F_STORAGE_UTIL', pd.Series([0])).iloc[idx])
        
        # 8. CMD í•©ê³„
        cmd_cols = ['M16A_3F_CMD', 'M16A_6F_TO_HUB_CMD', 'M16A_2F_TO_HUB_CMD',
                   'M14A_3F_TO_HUB_CMD', 'M14B_7F_TO_HUB_CMD']
        cmd_sum = df[cmd_cols].iloc[idx].sum()
        physics.append(cmd_sum)
        
        # 9. ìµœê·¼ 5ë¶„ í‰ê·  (ìƒìŠ¹/í•˜ë½ íŒë‹¨ìš©)
        if idx >= 4:
            recent_avg = df[self.target_col].iloc[idx-4:idx+1].mean()
        else:
            recent_avg = df[self.target_col].iloc[idx]
        physics.append(recent_avg)
        
        # 10. 30ë¶„ ì¥ê¸° ì¶”ì„¸ (first 10 vs last 10)
        if idx >= 29:
            first_10_avg = df[self.target_col].iloc[idx-29:idx-19].mean()
            last_10_avg = df[self.target_col].iloc[idx-9:idx+1].mean()
            long_trend = last_10_avg - first_10_avg
        else:
            long_trend = 0
        physics.append(long_trend)
        
        # 11. ë³€ë™ì„± ë³€í™”
        if idx >= 29:
            first_10_std = df[self.target_col].iloc[idx-29:idx-19].std()
            last_10_std = df[self.target_col].iloc[idx-9:idx+1].std()
            if first_10_std > 0:
                volatility_change = last_10_std / first_10_std
            else:
                volatility_change = 1
        else:
            volatility_change = 1
        physics.append(volatility_change)
        
        return np.array(physics)

# ==============================================================================
# ğŸ“Š ë°ì´í„° ë¶„ë¦¬ (ìˆ˜ì •) - ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ ê¸°ë°˜
# ==============================================================================

def create_model_specific_data_v2(df_expanded, sequence_indices):
    """ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ ê¸°ë°˜ ëª¨ë¸ë³„ ë°ì´í„° ë¶„ë¦¬"""
    print("\n[4ë‹¨ê³„] ëª¨ë¸ë³„ ë°ì´í„° ë¶„ë¦¬ (ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ ê¸°ë°˜)")
    
    # Model1 ë°ì´í„° ì„ íƒ ì¡°ê±´ (ì•ˆì •í˜•)
    model1_mask = (
        (df_expanded['consecutive_300_prob'] < 0.5) &  # ë‚®ì€ í™•ë¥ 
        (df_expanded['consecutive_300_count'] < 10) &  # 300+ ì ìŒ
        (df_expanded['past_30min_max'] < 275) &  # ì í”„ ìœ„í—˜ ì—†ìŒ
        (df_expanded['BRIDGE_TIME'] <= 4) &  # ë¬¼ë¦¬ ì§€í‘œ ì•ˆì •
        (df_expanded['acceleration'] <= 2) &  # ê¸‰ë³€ ì—†ìŒ
        (df_expanded['volatility_change'] <= 1.5) &  # ë³€ë™ì„± ì•ˆì •
        (df_expanded['long_trend'].between(-30, 30))  # ê·¹ë‹¨ ì¶”ì„¸ ì—†ìŒ
    )
    
    # Model2 ë°ì´í„° ì„ íƒ ì¡°ê±´ (ê·¹ë‹¨í˜•)
    model2_mask = (
        # í™•ë¥  ë§µ ê¸°ë°˜
        (df_expanded['consecutive_300_prob'] >= 0.6) |  # ë†’ì€ í™•ë¥  (15ê°œ+)
        (df_expanded['consecutive_300_count'] >= 15) |  # ê·¹ë‹¨ê°’ ì§€ì†
        
        # 277 êµ¬ê°„ íŠ¹ë³„ ì²˜ë¦¬
        ((df_expanded['past_30min_max'] >= 275) & (df_expanded['past_30min_max'] <= 279)) |
        ((df_expanded['recent_5min_max'] >= 275) & (df_expanded['recent_5min_max'] <= 279)) |
        ((df_expanded['recent_3min_max'] >= 275) & (df_expanded['recent_3min_max'] <= 279)) |
        
        # ì í”„ ê°€ëŠ¥ì„± êµ¬ê°„
        ((df_expanded['past_30min_max'] >= 260) & (df_expanded['past_30min_max'] <= 280)) |
        (df_expanded['CURRENT_M16A_3F_JOB_2'] >= 300) |  # ì´ë¯¸ ê·¹ë‹¨ê°’
        
        # ë¬¼ë¦¬ ì§€í‘œ
        (df_expanded['BRIDGE_TIME'] > 4.2) |  # ë¬¼ë¦¬ ì§€í‘œ ìœ„í—˜
        
        # ê°€ì†ë„ ë° ì¶”ì„¸
        (df_expanded['acceleration'] > 2) |  # ê¸‰ë³€ ì‹ í˜¸
        (df_expanded['long_trend'] > 10) |  # ì¥ê¸° ìƒìŠ¹
        (df_expanded['recent_5min_trend'] > 5) |  # ìµœê·¼ ê¸‰ë“±
        
        # ë³€ë™ì„±
        (df_expanded['volatility_change'] > 1.8) |  # ë³€ë™ì„± ì¦ê°€
        
        # íŠ¹ë³„ ì¼€ì´ìŠ¤
        (df_expanded['is_277_zone']) |  # 277 íŠ¹ë³„ êµ¬ê°„
        (df_expanded['is_jump_case']) |  # ì‹¤ì œ ì í”„ ì¼€ì´ìŠ¤
        
        # ìƒìŠ¹ íŒ¨í„´
        (df_expanded['consecutive_rises'] >= 20)  # ì—°ì† ìƒìŠ¹
    )
    
    # ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ ì¶”ì¶œ
    model1_sequence_idx = np.where(model1_mask)[0]
    model2_sequence_idx = np.where(model2_mask)[0]
    
    print(f"âœ… Model1 ì‹œí€€ìŠ¤: {len(model1_sequence_idx)}ê°œ ({len(model1_sequence_idx)/len(sequence_indices)*100:.1f}%)")
    print(f"âœ… Model2 ì‹œí€€ìŠ¤: {len(model2_sequence_idx)}ê°œ ({len(model2_sequence_idx)/len(sequence_indices)*100:.1f}%)")
    
    # ê²¹ì¹˜ëŠ” ë°ì´í„° í™•ì¸
    overlap = set(model1_sequence_idx) & set(model2_sequence_idx)
    print(f"âš ï¸ ê²¹ì¹˜ëŠ” ì‹œí€€ìŠ¤: {len(overlap)}ê°œ")
    
    # ê°€ì¤‘ì¹˜ ê³„ì‚°
    model1_weights = calculate_weights_v2(df_expanded.iloc[model1_sequence_idx], 'stable')
    model2_weights = calculate_weights_v2(df_expanded.iloc[model2_sequence_idx], 'extreme')
    
    return model1_sequence_idx, model2_sequence_idx, model1_weights, model2_weights

def calculate_weights_v2(df_subset, model_type):
    """ë°ì´í„° ê°€ì¤‘ì¹˜ ê³„ì‚° (ìˆ˜ì •)"""
    weights = np.ones(len(df_subset))
    
    if model_type == 'extreme':
        # ì í”„ ì¼€ì´ìŠ¤ì— ë†’ì€ ê°€ì¤‘ì¹˜
        jump_mask = df_subset['is_jump_case'].values == True
        weights[jump_mask] = 10.0
        
        # 277 êµ¬ê°„ì— ë†’ì€ ê°€ì¤‘ì¹˜  
        zone_277_mask = df_subset['is_277_zone'].values == True
        weights[zone_277_mask] = 8.0
        
        # ê·¹ê³ ê°’(335+)ì— ë†’ì€ ê°€ì¤‘ì¹˜
        extreme_high = df_subset['CURRENT_M16A_3F_JOB_2'].values >= 335
        weights[extreme_high] = 5.0
        
        # 300-335 êµ¬ê°„
        high_value = (df_subset['CURRENT_M16A_3F_JOB_2'].values >= 300) & \
                    (df_subset['CURRENT_M16A_3F_JOB_2'].values < 335)
        weights[high_value] = 3.0
        
        # 260-280 ì í”„ ê°€ëŠ¥ êµ¬ê°„
        jump_possible = (df_subset['past_30min_max'].values >= 260) & \
                       (df_subset['past_30min_max'].values < 280)
        weights[jump_possible] = 2.0
    
    elif model_type == 'stable':
        # ì•ˆì •í˜•ì€ ê· ë“± ê°€ì¤‘ì¹˜
        # ë‹¨, 250-270 ê²½ê³„ êµ¬ê°„ì€ ì¡°ê¸ˆ ë” ì¤‘ìš”
        boundary = (df_subset['CURRENT_M16A_3F_JOB_2'].values >= 250) & \
                  (df_subset['CURRENT_M16A_3F_JOB_2'].values < 270)
        weights[boundary] = 1.5
    
    return weights

# íŠ¹ì§• ê³„ì‚° í•¨ìˆ˜ë“¤ì€ ì´ì „ê³¼ ë™ì¼...
def calculate_advanced_features(df, target_col):
    """í‰ê°€ ì½”ë“œì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë“  íŠ¹ì§• ê³„ì‚°"""
    
    # ê³¼ê±° 30ë¶„ ìµœëŒ€ê°’/ìµœì†Œê°’/í‰ê· 
    df['past_30min_max'] = df[target_col].rolling(30, min_periods=1).max()
    df['past_30min_min'] = df[target_col].rolling(30, min_periods=1).min()
    df['past_30min_mean'] = df[target_col].rolling(30, min_periods=1).mean()
    
    # ìµœê·¼ 5ë¶„ í†µê³„ (ìƒìŠ¹/í•˜ë½ íŒë‹¨ìš©)
    df['recent_5min_mean'] = df[target_col].rolling(5, min_periods=1).mean()
    df['recent_5min_max'] = df[target_col].rolling(5, min_periods=1).max()
    df['recent_5min_min'] = df[target_col].rolling(5, min_periods=1).min()
    df['recent_5min_trend'] = df[target_col] - df[target_col].shift(5)
    
    # ìµœê·¼ 3ë¶„ í†µê³„ (277 ê°ì§€ìš©)
    df['recent_3min_max'] = df[target_col].rolling(3, min_periods=1).max()
    df['recent_3min_mean'] = df[target_col].rolling(3, min_periods=1).mean()
    
    # 10ë¶„ êµ¬ê°„ë³„ í‰ê·  (30ë¶„ ë°ì´í„° 3ë“±ë¶„)
    df['first_10min_avg'] = df[target_col].rolling(10, min_periods=1).mean().shift(20)
    df['middle_10min_avg'] = df[target_col].rolling(10, min_periods=1).mean().shift(10)
    df['last_10min_avg'] = df[target_col].rolling(10, min_periods=1).mean()
    
    # ì¥ê¸° ì¶”ì„¸ (30ë¶„)
    df['long_trend'] = df['last_10min_avg'] - df['first_10min_avg']
    
    # ê°€ì†ë„
    df['mid_trend'] = df['last_10min_avg'] - df['middle_10min_avg']
    df['early_trend'] = df['middle_10min_avg'] - df['first_10min_avg']
    df['acceleration'] = df['mid_trend'] - df['early_trend']
    
    # ë³€ë™ì„± ë³€í™”
    df['first_10min_std'] = df[target_col].rolling(10, min_periods=1).std().shift(20)
    df['last_10min_std'] = df[target_col].rolling(10, min_periods=1).std()
    df['volatility_change'] = df['last_10min_std'] / (df['first_10min_std'] + 1e-6)
    
    # 277 íŠ¹ë³„ êµ¬ê°„
    df['is_277_zone'] = (
        ((df['past_30min_max'] >= 275) & (df['past_30min_max'] <= 279)) |
        ((df['recent_5min_max'] >= 275) & (df['recent_5min_max'] <= 279)) |
        ((df['recent_3min_max'] >= 275) & (df['recent_3min_max'] <= 279))
    )
    
    # ì í”„ ì¼€ì´ìŠ¤ (ê³¼ê±° ìµœëŒ€ < 280, í˜„ì¬ >= 300)
    df['is_jump_case'] = (df['past_30min_max'].shift(10) < 280) & (df[target_col] >= 300)
    
    # ì—°ì† ìƒìŠ¹ ì¹´ìš´íŠ¸
    df['consecutive_rises'] = (df[target_col] > df[target_col].shift(1)).rolling(30).sum()
    
    # ìƒìŠ¹/í•˜ë½ íŒ¨í„´
    df['is_rising'] = df['recent_5min_trend'] > 0
    df['is_rapid_rising'] = df['recent_5min_trend'] > 10
    
    # NaN ì²˜ë¦¬
    df = df.fillna(0)
    
    return df

# PatchTSTì™€ PatchTSTPINN ëª¨ë¸ í´ë˜ìŠ¤ëŠ” ì´ì „ê³¼ ë™ì¼...
class PatchTST(keras.Model):
    """ì•ˆì •í˜• ëª¨ë¸ - False Positive ìµœì†Œí™”"""
    
    def __init__(self, config):
        super().__init__()
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.d_model = config.get('d_model', 128)
        self.n_heads = config.get('n_heads', 8)
        self.d_ff = config.get('d_ff', 256)
        self.n_layers = config.get('n_layers', 3)
        self.dropout = config.get('dropout', 0.1)
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(self.d_model)
        
        # Positional Encoding
        self.pos_encoding = self.create_positional_encoding()
        
        # Transformer Encoder Layers
        self.encoder_layers = [
            layers.MultiHeadAttention(
                num_heads=self.n_heads,
                key_dim=self.d_model // self.n_heads,
                dropout=self.dropout
            ) for _ in range(self.n_layers)
        ]
        
        # Feed Forward Networks
        self.ff_layers = [
            keras.Sequential([
                layers.Dense(self.d_ff, activation='relu'),
                layers.Dropout(self.dropout),
                layers.Dense(self.d_model),
                layers.Dropout(self.dropout)
            ]) for _ in range(self.n_layers)
        ]
        
        # Layer Normalization
        self.ln_layers = [
            [layers.LayerNormalization(epsilon=1e-6),
             layers.LayerNormalization(epsilon=1e-6)]
            for _ in range(self.n_layers)
        ]
        
        # Output layers
        self.global_pool = layers.GlobalAveragePooling1D()
        self.output_dense = layers.Dense(1)
    
    def create_positional_encoding(self):
        num_patches = self.seq_len // self.patch_len
        positions = tf.range(start=0, limit=num_patches, delta=1)
        positions = tf.expand_dims(positions, 0)
        return layers.Embedding(input_dim=num_patches, output_dim=self.d_model)(positions)
    
    def create_patches(self, x):
        batch_size = tf.shape(x)[0]
        patches = tf.reshape(x, [batch_size, -1, self.patch_len * self.n_features])
        return patches
    
    def call(self, inputs, training=False):
        # íŒ¨ì¹˜ ìƒì„±
        x = self.create_patches(inputs)
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x = self.patch_embedding(x)
        
        # Positional encoding ì¶”ê°€
        x += self.pos_encoding
        
        # Transformer layers
        for i in range(self.n_layers):
            # Multi-head attention
            attn_output = self.encoder_layers[i](x, x, training=training)
            x = self.ln_layers[i][0](x + attn_output)
            
            # Feed forward
            ff_output = self.ff_layers[i](x, training=training)
            x = self.ln_layers[i][1](x + ff_output)
        
        # Global pooling and output
        x = self.global_pool(x)
        output = self.output_dense(x)
        
        return output

class PatchTSTPINN(keras.Model):
    """ê·¹ë‹¨í˜• ëª¨ë¸ - ì í”„ ê°ì§€ íŠ¹í™”"""
    
    def __init__(self, config):
        super().__init__()
        self.patchtst = PatchTST(config)
        
        # Physics-Informed Network (11ì°¨ì› ë¬¼ë¦¬ íŠ¹ì§•)
        self.physics_net = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.2),
            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.2),
            layers.Dense(16, activation='relu')
        ])
        
        # í†µí•© ë ˆì´ì–´
        self.fusion_layer = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.2),
            layers.Dense(16, activation='relu'),
            layers.Dense(1)
        ])
    
    def call(self, inputs, training=False):
        seq_input, physics_input = inputs
        
        # PatchTST ì¶œë ¥
        patchtst_out = self.patchtst(seq_input, training=training)
        
        # Physics network ì¶œë ¥
        physics_out = self.physics_net(physics_input, training=training)
        
        # ê²°í•©
        combined = tf.concat([patchtst_out, physics_out], axis=1)
        output = self.fusion_layer(combined, training=training)
        
        return output

# ==============================================================================
# ğŸ¯ í•™ìŠµ í•¨ìˆ˜ (ìˆ˜ì •)
# ==============================================================================

def train_dual_models_v2(processor, X, y, physics, model1_idx, model2_idx, 
                        model1_weights, model2_weights, ckpt):
    """ë“€ì–¼ ëª¨ë¸ í•™ìŠµ - ì¸ë±ìŠ¤ ë¬¸ì œ í•´ê²°"""
    
    # ëª¨ë¸ë³„ ë°ì´í„° ì¤€ë¹„ (ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ ì‚¬ìš©)
    X_model1 = X[model1_idx]
    y_model1 = y[model1_idx]
    physics_model1 = physics[model1_idx]
    weights_model1 = model1_weights
    
    X_model2 = X[model2_idx]
    y_model2 = y[model2_idx]
    physics_model2 = physics[model2_idx]
    weights_model2 = model2_weights
    
    print(f"\nğŸ“Š Model1 í•™ìŠµ ë°ì´í„°: {len(X_model1)}ê°œ")
    print(f"ğŸ“Š Model2 í•™ìŠµ ë°ì´í„°: {len(X_model2)}ê°œ")
    
    # ê° ëª¨ë¸ë³„ train/val ë¶„ë¦¬
    # Model 1
    split1 = int(0.8 * len(X_model1))
    X_train1, X_val1 = X_model1[:split1], X_model1[split1:]
    y_train1, y_val1 = y_model1[:split1], y_model1[split1:]
    weights_train1 = weights_model1[:split1]
    
    # Model 2  
    split2 = int(0.8 * len(X_model2))
    X_train2, X_val2 = X_model2[:split2], X_model2[split2:]
    y_train2, y_val2 = y_model2[:split2], y_model2[split2:]
    physics_train2, physics_val2 = physics_model2[:split2], physics_model2[split2:]
    weights_train2 = weights_model2[:split2]
    
    # ì •ê·œí™”
    # Model 1
    n_features = X_train1.shape[2]
    X_train1_scaled = processor.scaler_X.fit_transform(
        X_train1.reshape(-1, n_features)
    ).reshape(X_train1.shape)
    X_val1_scaled = processor.scaler_X.transform(
        X_val1.reshape(-1, n_features)
    ).reshape(X_val1.shape)
    
    y_train1_scaled = processor.scaler_y.fit_transform(y_train1.reshape(-1, 1)).flatten()
    y_val1_scaled = processor.scaler_y.transform(y_val1.reshape(-1, 1)).flatten()
    
    # Model 2 (ë³„ë„ ìŠ¤ì¼€ì¼ëŸ¬)
    scaler_X2 = RobustScaler()
    scaler_y2 = RobustScaler()
    
    X_train2_scaled = scaler_X2.fit_transform(
        X_train2.reshape(-1, n_features)
    ).reshape(X_train2.shape)
    X_val2_scaled = scaler_X2.transform(
        X_val2.reshape(-1, n_features)
    ).reshape(X_val2.shape)
    
    y_train2_scaled = scaler_y2.fit_transform(y_train2.reshape(-1, 1)).flatten()
    y_val2_scaled = scaler_y2.transform(y_val2.reshape(-1, 1)).flatten()
    
    physics_train2_scaled = processor.scaler_physics.fit_transform(physics_train2)
    physics_val2_scaled = processor.scaler_physics.transform(physics_val2)
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    joblib.dump(processor.scaler_X, f'{ckpt.scaler_dir}/scaler_X.pkl')
    joblib.dump(processor.scaler_y, f'{ckpt.scaler_dir}/scaler_y.pkl')
    joblib.dump(scaler_X2, f'{ckpt.scaler_dir}/scaler_X2.pkl')
    joblib.dump(scaler_y2, f'{ckpt.scaler_dir}/scaler_y2.pkl')
    joblib.dump(processor.scaler_physics, f'{ckpt.scaler_dir}/scaler_physics.pkl')
    
    # ëª¨ë¸ ì„¤ì •
    config = {
        'seq_len': 30,
        'n_features': n_features,
        'patch_len': 6,  # 30/6 = 5 patches
        'd_model': 128,
        'n_heads': 8,
        'd_ff': 256,
        'n_layers': 3,
        'dropout': 0.1
    }
    
    # Model 1 í•™ìŠµ (ì•ˆì •í˜•)
    if not ckpt.load_state().get('model1_trained', False):
        print("\n" + "="*60)
        print("ğŸ¤– Model 1: PatchTST (ì•ˆì •í˜•) í•™ìŠµ")
        print("="*60)
        
        model1 = PatchTST(config)
        
        # ì•ˆì •í˜• ì†ì‹¤í•¨ìˆ˜ (ê³¼ëŒ€ì˜ˆì¸¡ í˜ë„í‹°)
        def stable_loss(y_true, y_pred):
            mae = tf.abs(y_true - y_pred)
            
            # ê³¼ëŒ€ì˜ˆì¸¡ í˜ë„í‹° (ì‹¤ì œ < 300ì¸ë° 300+ ì˜ˆì¸¡)
            y_true_original = y_true * processor.scaler_y.scale_ + processor.scaler_y.center_
            y_pred_original = y_pred * processor.scaler_y.scale_ + processor.scaler_y.center_
            
            overpredict_penalty = tf.where(
                (y_true_original < 300) & (y_pred_original >= 300),
                mae * 5.0,  # 5ë°° í˜ë„í‹°
                mae
            )
            
            # 250-270 ê²½ê³„ êµ¬ê°„ì€ ë” ì •í™•í•˜ê²Œ
            boundary_penalty = tf.where(
                (y_true_original >= 250) & (y_true_original < 270),
                mae * 1.5,
                mae
            )
            
            return tf.reduce_mean(tf.maximum(overpredict_penalty, boundary_penalty))
        
        model1.compile(
            optimizer=Adam(learning_rate=0.0001, clipnorm=1.0),
            loss=stable_loss,
            metrics=['mae']
        )
        
        callbacks1 = [
            EarlyStopping(patience=20, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-6),
            ModelCheckpoint(f'{ckpt.checkpoint_dir}/model1_stable.weights.h5',  # ìˆ˜ì •!
                           save_best_only=True, save_weights_only=True)
        ]
        
        history1 = model1.fit(
            X_train1_scaled, y_train1_scaled,
            validation_data=(X_val1_scaled, y_val1_scaled),
            epochs=100,
            batch_size=64,
            sample_weight=weights_train1,
            callbacks=callbacks1,
            verbose=1
        )
        
        # í•™ìŠµ ìƒíƒœ ì €ì¥
        state = ckpt.load_state()
        state['model1_trained'] = True
        ckpt.save_state(state)
        
        print("âœ… Model 1 í•™ìŠµ ì™„ë£Œ")
    
    # Model 2 í•™ìŠµ (ê·¹ë‹¨í˜•)
    if not ckpt.load_state().get('model2_trained', False):
        print("\n" + "="*60)
        print("ğŸ¤– Model 2: PatchTST + PINN (ê·¹ë‹¨í˜•) í•™ìŠµ")
        print("="*60)
        
        model2 = PatchTSTPINN(config)
        
        # ê·¹ë‹¨í˜• ì†ì‹¤í•¨ìˆ˜ (ì í”„ ê°ì§€ ì¤‘ì‹œ)
        def extreme_loss(y_true, y_pred):
            mae = tf.abs(y_true - y_pred)
            
            # ê·¹ë‹¨ê°’ ë†“ì¹¨ í˜ë„í‹° (ì‹¤ì œ 300+ì¸ë° 300 ë¯¸ë§Œ ì˜ˆì¸¡)
            y_true_original = y_true * scaler_y2.scale_ + scaler_y2.center_
            y_pred_original = y_pred * scaler_y2.scale_ + scaler_y2.center_
            
            miss_penalty = tf.where(
                (y_true_original >= 300) & (y_pred_original < 300),
                mae * 10.0,  # 10ë°° í˜ë„í‹°
                mae
            )
            
            # 277 êµ¬ê°„ íŠ¹ë³„ ì²˜ë¦¬
            zone_277_penalty = tf.where(
                (y_true_original >= 275) & (y_true_original <= 279),
                mae * 0.5,  # í˜ë„í‹° ê°ì†Œ (ë” ê´€ëŒ€í•˜ê²Œ)
                mae
            )
            
            # ê·¹ê³ ê°’(335+) ë†“ì¹¨ ì¶”ê°€ í˜ë„í‹°
            extreme_miss_penalty = tf.where(
                (y_true_original >= 335) & (y_pred_original < 320),
                mae * 15.0,  # 15ë°° í˜ë„í‹°
                mae
            )
            
            return tf.reduce_mean(tf.maximum(tf.maximum(miss_penalty, zone_277_penalty), extreme_miss_penalty))
        
        model2.compile(
            optimizer=Adam(learning_rate=0.0001, clipnorm=1.0),
            loss=extreme_loss,
            metrics=['mae']
        )
        
        callbacks2 = [
            EarlyStopping(patience=25, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=12, min_lr=1e-6),
            ModelCheckpoint(f'{ckpt.checkpoint_dir}/model2_extreme.weights.h5',  # ìˆ˜ì •!
                           save_best_only=True, save_weights_only=True)
        ]
        
        history2 = model2.fit(
            [X_train2_scaled, physics_train2_scaled], y_train2_scaled,
            validation_data=([X_val2_scaled, physics_val2_scaled], y_val2_scaled),
            epochs=120,
            batch_size=64,
            sample_weight=weights_train2,
            callbacks=callbacks2,
            verbose=1
        )
        
        # í•™ìŠµ ìƒíƒœ ì €ì¥
        state = ckpt.load_state()
        state['model2_trained'] = True
        ckpt.save_state(state)
        
        print("âœ… Model 2 í•™ìŠµ ì™„ë£Œ")
    
    return True

# ==============================================================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ (ìˆ˜ì •)
# ==============================================================================

def main():
    """ë©”ì¸ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ - ì¸ë±ìŠ¤ ë¬¸ì œ í•´ê²° ë²„ì „"""
    
    ckpt = EnhancedCheckpointManager()
    processor = V4DataProcessor()
    
    # ì´ì „ ìƒíƒœ í™•ì¸
    state = ckpt.load_state()
    if state:
        print(f"\nğŸ“‚ ì´ì „ í•™ìŠµ ìƒíƒœ ë°œê²¬! (Step {state.get('step', 1)}/7)")
        resume = input("ì´ì–´ì„œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
        
        if resume != 'y':
            state = {}
            step = 1
        else:
            step = state.get('step', 1)
    else:
        state = {}
        step = 1
    
    try:
        # Step 1: ë°ì´í„° ë¡œë“œ
        if step <= 1:
            print(f"\n[Step 1/7] ë°ì´í„° ë¡œë“œ ë° ë³‘í•©")
            print("-"*60)
            
            df = processor.load_and_merge_data()
            
            state['step'] = 2
            ckpt.save_state(state)
            
            if ckpt.interrupted:
                sys.exit(0)
        
        # Step 2: ì—°ì† íŒ¨í„´ ì¶”ê°€
        if step <= 2:
            print(f"\n[Step 2/7] ì—°ì† íŒ¨í„´ ê³„ì‚°")
            print("-"*60)
            
            if step < 2:
                df = processor.load_and_merge_data()
            
            df = processor.add_consecutive_patterns(df)
            
            state['step'] = 3
            state['df_shape'] = df.shape
            ckpt.save_state(state)
            
            # ë°ì´í„° ì„ì‹œ ì €ì¥
            df.to_pickle(f'{ckpt.checkpoint_dir}/df_with_patterns.pkl')
            
            if ckpt.interrupted:
                sys.exit(0)
        
        # Step 3: ì‹œí€€ìŠ¤ ìƒì„± (ì¤‘ë‹¨/ì¬ê°œ ì§€ì›)
        if step <= 3:
            print(f"\n[Step 3/7] ì‹œí€€ìŠ¤ ìƒì„±")
            print("-"*60)
            
            # ë°ì´í„° ë¡œë“œ
            if os.path.exists(f'{ckpt.checkpoint_dir}/df_with_patterns.pkl'):
                df = pd.read_pickle(f'{ckpt.checkpoint_dir}/df_with_patterns.pkl')
            else:
                df = processor.load_and_merge_data()
                df = processor.add_consecutive_patterns(df)
            
            # ì‹œí€€ìŠ¤ ìƒì„± (ì¤‘ë‹¨/ì¬ê°œ ì§€ì›)
            X, y, physics, sequence_indices = processor.create_sequences_with_resume(df, ckpt)
            
            # HDF5ë¡œ ì €ì¥
            with h5py.File(ckpt.sequence_file, 'w') as f:
                f.create_dataset('X', data=X, compression='gzip')
                f.create_dataset('y', data=y, compression='gzip')
                f.create_dataset('physics', data=physics, compression='gzip')
                f.create_dataset('sequence_indices', data=sequence_indices, compression='gzip')
                f.attrs['n_features'] = X.shape[2]
            
            state['step'] = 4
            state['sequences_shape'] = X.shape
            ckpt.save_state(state)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del X, y, physics, sequence_indices
            gc.collect()
            
            if ckpt.interrupted:
                sys.exit(0)
        
        # Step 4: ëª¨ë¸ë³„ ë°ì´í„° ë¶„ë¦¬ (ìˆ˜ì •)
        if step <= 4:
            print(f"\n[Step 4/7] ëª¨ë¸ë³„ ë°ì´í„° ë¶„ë¦¬")
            print("-"*60)
            
            # ì‹œí€€ìŠ¤ ë¡œë“œ
            with h5py.File(ckpt.sequence_file, 'r') as f:
                sequence_indices = f['sequence_indices'][:]
            
            # df ë¡œë“œ (íŠ¹ì§• ê³„ì‚°ìš©)
            df = pd.read_pickle(f'{ckpt.checkpoint_dir}/df_with_patterns.pkl')
            
            # íŠ¹ì§• ê³„ì‚°
            df = calculate_advanced_features(df, processor.target_col)
            
            # ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” íŠ¹ì§•ë§Œ ì¶”ì¶œ
            df_expanded = df.iloc[sequence_indices].reset_index(drop=True)
            
            # ëª¨ë¸ë³„ ë°ì´í„° ë¶„ë¦¬
            model1_idx, model2_idx, model1_weights, model2_weights = \
                create_model_specific_data_v2(df_expanded, sequence_indices)
            
            # ì¸ë±ìŠ¤ ì €ì¥
            np.save(f'{ckpt.checkpoint_dir}/model1_idx.npy', model1_idx)
            np.save(f'{ckpt.checkpoint_dir}/model2_idx.npy', model2_idx)
            np.save(f'{ckpt.checkpoint_dir}/model1_weights.npy', model1_weights)
            np.save(f'{ckpt.checkpoint_dir}/model2_weights.npy', model2_weights)
            
            state['step'] = 5
            state['model1_samples'] = len(model1_idx)
            state['model2_samples'] = len(model2_idx)
            ckpt.save_state(state)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del df, df_expanded
            gc.collect()
            
            if ckpt.interrupted:
                sys.exit(0)
        
        # Step 5: ëª¨ë¸ í•™ìŠµ
        if step <= 5:
            print(f"\n[Step 5/7] ë“€ì–¼ ëª¨ë¸ í•™ìŠµ")
            print("-"*60)
            
            # ë°ì´í„° ë¡œë“œ
            with h5py.File(ckpt.sequence_file, 'r') as f:
                X = f['X'][:]
                y = f['y'][:]
                physics = f['physics'][:]
            
            # ì¸ë±ìŠ¤ì™€ ê°€ì¤‘ì¹˜ ë¡œë“œ
            model1_idx = np.load(f'{ckpt.checkpoint_dir}/model1_idx.npy')
            model2_idx = np.load(f'{ckpt.checkpoint_dir}/model2_idx.npy')
            model1_weights = np.load(f'{ckpt.checkpoint_dir}/model1_weights.npy')
            model2_weights = np.load(f'{ckpt.checkpoint_dir}/model2_weights.npy')
            
            # í•™ìŠµ
            success = train_dual_models_v2(
                processor, X, y, physics,
                model1_idx, model2_idx,
                model1_weights, model2_weights,
                ckpt
            )
            
            if success:
                state['step'] = 6
                ckpt.save_state(state)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del X, y, physics
            gc.collect()
            
            if ckpt.interrupted:
                sys.exit(0)
        
        # Step 6: ìµœì¢… ì €ì¥
        if step <= 6:
            print(f"\n[Step 6/7] ìµœì¢… ì €ì¥")
            print("-"*60)
            
            # í•™ìŠµ ì„¤ì • ì €ì¥
            with h5py.File(f'{ckpt.checkpoint_dir}/scaled_data.h5', 'w') as f:
                with h5py.File(ckpt.sequence_file, 'r') as sf:
                    f.attrs['n_features'] = sf.attrs['n_features']
                f.attrs['seq_len'] = 30
                f.attrs['patch_len'] = 6
                f.attrs['model1_samples'] = state.get('model1_samples', 0)
                f.attrs['model2_samples'] = state.get('model2_samples', 0)
            
            print("âœ… í•™ìŠµ ì™„ë£Œ!")
            print(f"\nğŸ“ ì €ì¥ ìœ„ì¹˜: {ckpt.checkpoint_dir}")
            print("  - model1_stable.weights.h5: ì•ˆì •í˜• ëª¨ë¸")  # ìˆ˜ì •!
            print("  - model2_extreme.weights.h5: ê·¹ë‹¨í˜• ëª¨ë¸")  # ìˆ˜ì •!
            print("  - scalers/: ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬")
            print("  - scaled_data.h5: í•™ìŠµ ì„¤ì •")
            
            state['step'] = 7
            state['completed'] = True
            ckpt.save_state(state)
        
        print("\n" + "="*80)
        print("ğŸ‰ V4 ë“€ì–¼ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        print("="*80)
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("1. í‰ê°€ ì½”ë“œë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        print("2. 202509 ë°ì´í„°ë¡œ ìµœì¢… ê²€ì¦")
        print("="*80)
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
        print("ğŸ’¾ ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()ModelCheckpoint íŒŒì¼ëª…ì„ ìˆ˜ì •í•œ ì „ì²´ ì½”ë“œì…ë‹ˆë‹¤. ì´ì œ ì˜¤ë¥˜ ì—†ì´ ì‹¤í–‰ë  ê²ƒì…ë‹ˆë‹¤!