#!/usr/bin/env python3

def create_all_features(self, df):
    """ì™„ì „í•œ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ - ëˆ„ë½ ì—†ìŒ!"""
    print("\n[2ë‹¨ê³„] íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§")
    
    # 1. ìœ ì…/ìœ ì¶œ ë°¸ëŸ°ìŠ¤
    df['flow_balance'] = df[self.inflow_cols].sum(axis=1) - df[self.outflow_cols].sum(axis=1)
    df['flow_ratio'] = df[self.inflow_cols].sum(axis=1) / (df[self.outflow_cols].sum(axis=1) + 1)
    
    # 2. ì¶”ì„¸ íŠ¹ì§•
    df['trend_20min'] = df[self.target_col].diff(20)
    df['trend_10min'] = df[self.target_col].diff(10)
    df['acceleration'] = df['trend_10min'] - df['trend_10min'].shift(10)
    
    # 3. ì—°ì† íŒ¨í„´
    df['consecutive_250+'] = (df[self.target_col] > 250).rolling(10).sum()
    df['consecutive_270+'] = (df[self.target_col] > 270).rolling(10).sum()
    
    # 4. CMD ë™ê¸°í™”
    df['cmd_sync_count'] = (df[self.cmd_cols] > 235).sum(axis=1)
    df['cmd_max'] = df[self.cmd_cols].max(axis=1)
    
    # 5. ë¸Œë¦¿ì§€íƒ€ì„ ë³€í™”
    df['bridge_diff'] = df['BRIDGE_TIME'].diff(5)
    df['bridge_high'] = (df['BRIDGE_TIME'] > 4.0).astype(int)
    
    # 6. storage x bridge ìƒí˜¸ì‘ìš©
    df['storage_x_bridge'] = df['M16A_3F_STORAGE_UTIL'] * df['BRIDGE_TIME']
    
    # 7. ì—°ì† 300+ ì¹´ìš´íŠ¸ì™€ í™•ë¥ 
    consecutive_300_counts = []
    consecutive_300_probs = []
    
    for i in range(len(df)):
        if i < 30:
            count = 0
            prob = 0.003
        else:
            window = df[self.target_col].iloc[i-30:i].values
            count = sum(1 for v in window if v >= 300)
            prob = self.probability_map.get(count, 0.5)
        
        consecutive_300_counts.append(count)
        consecutive_300_probs.append(prob)
    
    df['consecutive_300_count'] = consecutive_300_counts
    df['consecutive_300_prob'] = consecutive_300_probs
    
    # 8. 3êµ¬ê°„ ë¶„ë¥˜ (ìˆ«ìë¡œ ì§ì ‘ ë³€í™˜)
    conditions = [
        df[self.target_col] < 150,
        (df[self.target_col] >= 150) & (df[self.target_col] < 300),
        df[self.target_col] >= 300
    ]
    choices = [0, 1, 2]
    df['range_class'] = np.select(conditions, choices, default=1)
    
    # 9. ì í”„ ì—¬ë¶€
    df['past_30min_max'] = df[self.target_col].rolling(30).max()
    df['is_jump'] = ((df['past_30min_max'].shift(10) < 280) & 
                    (df[self.target_col] >= 300)).astype(int)
    
    # 10. ìƒìŠ¹/í•˜ë½ íŒ¨í„´ (ìˆ«ìë¡œ ë³€í™˜)
    df['change_20min'] = df[self.target_col] - df[self.target_col].shift(20)
    
    trend_conditions = [
        df['change_20min'] < -20,
        (df['change_20min'] >= -20) & (df['change_20min'] < 20),
        (df['change_20min'] >= 20) & (df['change_20min'] < 50),
        df['change_20min'] >= 50
    ]
    trend_choices = [0, 1, 2, 3]  # 0:down, 1:stable, 2:gradual_up, 3:rapid_up
    df['trend_pattern'] = np.select(trend_conditions, trend_choices, default=1)
    
    # 11. ìƒìŠ¹ë¥ /í•˜ë½ë¥  (%)
    df['change_rate_10min'] = ((df[self.target_col] - df[self.target_col].shift(10)) / 
                               (df[self.target_col].shift(10) + 1)) * 100
    df['change_rate_20min'] = ((df[self.target_col] - df[self.target_col].shift(20)) / 
                               (df[self.target_col].shift(20) + 1)) * 100
    df['change_rate_30min'] = ((df[self.target_col] - df[self.target_col].shift(30)) / 
                               (df[self.target_col].shift(30) + 1)) * 100
    
    # 12. ë³€ë™ì„±
    df['volatility_10min'] = df[self.target_col].rolling(10).std()
    df['volatility_20min'] = df[self.target_col].rolling(20).std()
    
    # 13. ê·¹ë‹¨ê°’ ê·¼ì ‘ë„
    df['distance_to_300'] = 300 - df[self.target_col]
    df['near_extreme'] = (df[self.target_col] > 280).astype(int)
    
    # 14. ìµœê·¼ í†µê³„
    df['recent_5min_mean'] = df[self.target_col].rolling(5).mean()
    df['recent_5min_max'] = df[self.target_col].rolling(5).max()
    
    # NaN ì²˜ë¦¬ - Categorical ì»¬ëŸ¼ ì œì™¸
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    df[numeric_columns] = df[numeric_columns].fillna(method='ffill').fillna(0)
    
    print(f"âœ… ì´ {len(df.columns)}ê°œ íŠ¹ì§• ìƒì„± ì™„ë£Œ")
    return df
# -*- coding: utf-8 -*-
"""
================================================================================
ğŸ¯ HUBROOM ì í”„ ê°ì§€ 80% ì‹œìŠ¤í…œ - ì™„ì „ í†µí•© í•™ìŠµ ì½”ë“œ
================================================================================
ëª©í‘œ: 
- 3êµ¬ê°„ ë¶„ë¥˜ (50-150, 150-299, 300+)
- ì í”„ ê°ì§€ 80% (ê³¼ê±° ìµœëŒ€ < 280 â†’ 10ë¶„ í›„ >= 300)
- ìƒìŠ¹/í•˜ë½ íŒ¨í„´ ë¶„ì„
- ìƒìŠ¹ë¥ /í•˜ë½ë¥  ê³„ì‚° í¬í•¨
================================================================================
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
import joblib
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("ğŸ¯ HUBROOM ì í”„ ê°ì§€ 80% ì‹œìŠ¤í…œ")
print("ğŸ“Š 3êµ¬ê°„ ë¶„ë¥˜ + ì í”„ ê°ì§€ + ìƒìŠ¹/í•˜ë½ íŒ¨í„´")
print("="*80)

# ==============================================================================
# ğŸ“Š ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤
# ==============================================================================

class HubRoomDataProcessor:
    """ì™„ì „í•œ ë°ì´í„° ì²˜ë¦¬ - ëª¨ë“  íŠ¹ì§• í¬í•¨"""
    
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        
        # 21ê°œ í•„ìˆ˜ ì»¬ëŸ¼
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB',
            'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2',
            'M14B_7F_TO_HUB_JOB2',
            'M16B_10F_TO_HUB_JOB'
        ]
        
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB',
            'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB',
            'M16A_3F_TO_M14B_7F_JOB',
            'M16A_3F_TO_3F_MLUD_JOB'
        ]
        
        self.cmd_cols = [
            'M16A_3F_CMD',
            'M16A_6F_TO_HUB_CMD',
            'M16A_2F_TO_HUB_CMD',
            'M14A_3F_TO_HUB_CMD',
            'M14B_7F_TO_HUB_CMD'
        ]
        
        self.capa_cols = [
            'M16A_6F_LFT_MAXCAPA',
            'M16A_2F_LFT_MAXCAPA'
        ]
        
        self.other_cols = [
            'M16A_3F_STORAGE_UTIL',
            'M14_TO_M16_OFS_CUR',
            'M16_TO_M14_OFS_CUR'
        ]
        
        # í™•ë¥  ë§µ - ë§¤ìš° ì¤‘ìš”!
        self.probability_map = {
            0: 0.003, 1: 0.15, 2: 0.25, 3: 0.31, 4: 0.43, 5: 0.43,
            6: 0.35, 7: 0.42, 8: 0.53, 9: 0.49, 10: 0.42,
            11: 0.47, 12: 0.52, 13: 0.60, 14: 0.54, 15: 0.66,
            16: 0.62, 17: 0.71, 18: 0.79, 19: 0.83, 20: 0.987,
            21: 0.99, 22: 0.99, 23: 0.99, 24: 0.99, 25: 0.99,
            26: 0.99, 27: 0.99, 28: 0.99, 29: 0.99, 30: 0.99
        }
    
    def load_and_merge_data(self):
        """ë°ì´í„° ë¡œë“œ ë° BRIDGE_TIME ë³‘í•©"""
        print("\n[1ë‹¨ê³„] ë°ì´í„° ë¡œë“œ")
        
        # ë©”ì¸ ë°ì´í„°
        df = pd.read_csv('data/HUB_0509_to_0807_DATA.CSV')
        print(f"âœ… ë©”ì¸ ë°ì´í„°: {df.shape}")
        
        # ì‹œê°„ ì²˜ë¦¬
        time_col = df.columns[0]
        df['datetime'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M')
        
        # BRIDGE_TIME ë°ì´í„°
        bridge_df = pd.read_csv('data/BRTIME_0509_TO_0807.CSV')
        print(f"âœ… BRIDGE_TIME ë°ì´í„°: {bridge_df.shape}")
        
        # BRIDGE_TIME ë³‘í•©
        if 'IDC_VAL' in bridge_df.columns:
            bridge_df['BRIDGE_TIME'] = bridge_df['IDC_VAL']
            bridge_df['datetime'] = pd.to_datetime(bridge_df['CRT_TM'])
            bridge_df['datetime'] = bridge_df['datetime'].dt.floor('min')
            df['datetime'] = df['datetime'].dt.floor('min')
            
            df = pd.merge(df, bridge_df[['datetime', 'BRIDGE_TIME']], 
                         on='datetime', how='left')
            df['BRIDGE_TIME'] = df['BRIDGE_TIME'].interpolate().fillna(3.5)
        
        return df
    
    def create_all_features(self, df):
        """ì™„ì „í•œ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ - ëˆ„ë½ ì—†ìŒ!"""
        print("\n[2ë‹¨ê³„] íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§")
        
        # 1. ìœ ì…/ìœ ì¶œ ë°¸ëŸ°ìŠ¤
        df['flow_balance'] = df[self.inflow_cols].sum(axis=1) - df[self.outflow_cols].sum(axis=1)
        df['flow_ratio'] = df[self.inflow_cols].sum(axis=1) / (df[self.outflow_cols].sum(axis=1) + 1)
        
        # 2. ì¶”ì„¸ íŠ¹ì§•
        df['trend_20min'] = df[self.target_col].diff(20)
        df['trend_10min'] = df[self.target_col].diff(10)
        df['acceleration'] = df['trend_10min'] - df['trend_10min'].shift(10)
        
        # 3. ì—°ì† íŒ¨í„´
        df['consecutive_250+'] = (df[self.target_col] > 250).rolling(10).sum()
        df['consecutive_270+'] = (df[self.target_col] > 270).rolling(10).sum()
        
        # 4. CMD ë™ê¸°í™”
        df['cmd_sync_count'] = (df[self.cmd_cols] > 235).sum(axis=1)
        df['cmd_max'] = df[self.cmd_cols].max(axis=1)
        
        # 5. ë¸Œë¦¿ì§€íƒ€ì„ ë³€í™”
        df['bridge_diff'] = df['BRIDGE_TIME'].diff(5)
        df['bridge_high'] = (df['BRIDGE_TIME'] > 4.0).astype(int)
        
        # 6. storage x bridge ìƒí˜¸ì‘ìš©
        df['storage_x_bridge'] = df['M16A_3F_STORAGE_UTIL'] * df['BRIDGE_TIME']
        
        # 7. ì—°ì† 300+ ì¹´ìš´íŠ¸ì™€ í™•ë¥ 
        consecutive_300_counts = []
        consecutive_300_probs = []
        
        for i in range(len(df)):
            if i < 30:
                count = 0
                prob = 0.003
            else:
                window = df[self.target_col].iloc[i-30:i].values
                count = sum(1 for v in window if v >= 300)
                prob = self.probability_map.get(count, 0.5)
            
            consecutive_300_counts.append(count)
            consecutive_300_probs.append(prob)
        
        df['consecutive_300_count'] = consecutive_300_counts
        df['consecutive_300_prob'] = consecutive_300_probs
        
        # 8. 3êµ¬ê°„ ë¶„ë¥˜
        df['range_class'] = pd.cut(df[self.target_col], 
                                   bins=[0, 150, 300, 999999],
                                   labels=[0, 1, 2])
        
        # 9. ì í”„ ì—¬ë¶€
        df['past_30min_max'] = df[self.target_col].rolling(30).max()
        df['is_jump'] = ((df['past_30min_max'].shift(10) < 280) & 
                        (df[self.target_col] >= 300)).astype(int)
        
        # 10. ìƒìŠ¹/í•˜ë½ íŒ¨í„´ (ì¤‘ìš”!)
        df['change_20min'] = df[self.target_col] - df[self.target_col].shift(20)
        df['trend_pattern'] = pd.cut(df['change_20min'],
                                     bins=[-999, -20, 20, 50, 999],
                                     labels=['down', 'stable', 'gradual_up', 'rapid_up'])
        
        # 11. ìƒìŠ¹ë¥ /í•˜ë½ë¥  (%)
        df['change_rate_10min'] = ((df[self.target_col] - df[self.target_col].shift(10)) / 
                                   (df[self.target_col].shift(10) + 1)) * 100
        df['change_rate_20min'] = ((df[self.target_col] - df[self.target_col].shift(20)) / 
                                   (df[self.target_col].shift(20) + 1)) * 100
        df['change_rate_30min'] = ((df[self.target_col] - df[self.target_col].shift(30)) / 
                                   (df[self.target_col].shift(30) + 1)) * 100
        
        # 12. ë³€ë™ì„±
        df['volatility_10min'] = df[self.target_col].rolling(10).std()
        df['volatility_20min'] = df[self.target_col].rolling(20).std()
        
        # 13. ê·¹ë‹¨ê°’ ê·¼ì ‘ë„
        df['distance_to_300'] = 300 - df[self.target_col]
        df['near_extreme'] = (df[self.target_col] > 280).astype(int)
        
        # 14. ìµœê·¼ í†µê³„
        df['recent_5min_mean'] = df[self.target_col].rolling(5).mean()
        df['recent_5min_max'] = df[self.target_col].rolling(5).max()
        
        # NaN ì²˜ë¦¬
        df = df.fillna(method='ffill').fillna(0)
        
        print(f"âœ… ì´ {len(df.columns)}ê°œ íŠ¹ì§• ìƒì„± ì™„ë£Œ")
        return df
    
    def create_sequences(self, df, seq_len=30, pred_len=10):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
        print(f"\n[3ë‹¨ê³„] ì‹œí€€ìŠ¤ ìƒì„± ({seq_len}ë¶„ â†’ {pred_len}ë¶„ í›„)")
        
        feature_cols = [col for col in df.columns if col not in ['datetime', 'range_class', 'is_jump', 'trend_pattern']]
        
        X, y, y_jump, y_range, y_trend, y_value = [], [], [], [], [], []
        
        for i in range(seq_len, len(df) - pred_len):
            # ì…ë ¥: ê³¼ê±° 30ë¶„
            X.append(df[feature_cols].iloc[i-seq_len:i].values)
            
            # íƒ€ê²Ÿë“¤
            target_idx = i + pred_len - 1
            y.append(df[self.target_col].iloc[target_idx])
            y_jump.append(df['is_jump'].iloc[target_idx])
            y_range.append(df['range_class'].iloc[target_idx])
            y_trend.append(df['trend_pattern'].iloc[target_idx])
            y_value.append(df[self.target_col].iloc[target_idx])
        
        print(f"âœ… {len(X)}ê°œ ì‹œí€€ìŠ¤ ìƒì„±")
        
        return (np.array(X), np.array(y), np.array(y_jump), 
                np.array(y_range), np.array(y_trend), np.array(y_value))

# ==============================================================================
# ğŸ¤– ëª¨ë¸ ì •ì˜
# ==============================================================================

class JumpDetectionSystem:
    """ì í”„ ê°ì§€ 80% ë‹¬ì„± ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # Stage 1: ë¹ ë¥¸ ìŠ¤í¬ë¦¬ë‹
        self.model_screening = ExtraTreesClassifier(
            n_estimators=500,
            max_depth=20,
            min_samples_split=5,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        )
        
        # Stage 2: ì „ë¬¸ ëª¨ë¸ë“¤
        self.model_jump = XGBClassifier(
            n_estimators=300,
            max_depth=10,
            learning_rate=0.1,
            scale_pos_weight=100,  # ë¶ˆê· í˜• ì²˜ë¦¬
            random_state=42
        )
        
        self.model_range = RandomForestClassifier(
            n_estimators=300,
            max_depth=15,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        )
        
        self.model_trend = RandomForestClassifier(
            n_estimators=200,
            max_depth=10,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        )
        
        # Stage 3: ê°’ ì˜ˆì¸¡ (PatchTSTëŠ” ë³„ë„ êµ¬í˜„)
        self.model_value = ExtraTreesRegressor(
            n_estimators=500,
            max_depth=20,
            random_state=42,
            n_jobs=-1
        )
        
        self.scalers = {}
        
    def prepare_features(self, X_seq):
        """ì‹œí€€ìŠ¤ë¥¼ íŠ¹ì§•ìœ¼ë¡œ ë³€í™˜"""
        # ë§ˆì§€ë§‰ ì‹œì  íŠ¹ì§•
        last_features = X_seq[:, -1, :]
        
        # í†µê³„ íŠ¹ì§•
        mean_features = np.mean(X_seq, axis=1)
        std_features = np.std(X_seq, axis=1)
        max_features = np.max(X_seq, axis=1)
        min_features = np.min(X_seq, axis=1)
        
        # ì¶”ì„¸ íŠ¹ì§•
        trend_features = X_seq[:, -1, :] - X_seq[:, 0, :]
        
        # ëª¨ë“  íŠ¹ì§• ê²°í•©
        features = np.hstack([
            last_features,
            mean_features,
            std_features,
            max_features,
            min_features,
            trend_features
        ])
        
        return features
    
    def train_jump_detector(self, X, y_jump):
        """ì í”„ ê°ì§€ ëª¨ë¸ í•™ìŠµ - 80% ëª©í‘œ"""
        print("\nğŸ¯ ì í”„ ê°ì§€ ëª¨ë¸ í•™ìŠµ")
        
        # SMOTEë¡œ ë¶ˆê· í˜• ì²˜ë¦¬
        smote = SMOTE(sampling_strategy=0.2, random_state=42)
        X_balanced, y_balanced = smote.fit_resample(X, y_jump)
        
        # í•™ìŠµ
        self.model_jump.fit(X_balanced, y_balanced)
        
        # íŠ¹ì§• ì¤‘ìš”ë„
        importance = pd.DataFrame({
            'feature': range(X.shape[1]),
            'importance': self.model_jump.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("âœ… ì í”„ ê°ì§€ ìƒìœ„ 10ê°œ íŠ¹ì§•:")
        print(importance.head(10))
        
        return self.model_jump
    
    def apply_rule_based_boost(self, X, predictions):
        """ê·œì¹™ ê¸°ë°˜ ë¶€ìŠ¤íŒ… - 80% ë‹¬ì„±ìš©"""
        boosted_predictions = predictions.copy()
        
        # Phase 1: ê°•í•œ ì‹ í˜¸ (55%)
        storage_util_idx = -1  # íŠ¹ì§• ì¸ë±ìŠ¤ (ì¡°ì • í•„ìš”)
        strong_signal = X[:, storage_util_idx] > 20
        boosted_predictions[strong_signal] = 1
        
        # Phase 2: ì¤‘ê°„ ì‹ í˜¸ (70%)
        # ì—¬ëŸ¬ ì¡°ê±´ ì²´í¬...
        
        # Phase 3: ì•½í•œ ì‹ í˜¸ (80%)
        # ì¶”ê°€ ì¡°ê±´...
        
        return boosted_predictions

# ==============================================================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰
# ==============================================================================

def main():
    """ë©”ì¸ í•™ìŠµ í”„ë¡œì„¸ìŠ¤"""
    
    # 1. ë°ì´í„° ì²˜ë¦¬
    processor = HubRoomDataProcessor()
    df = processor.load_and_merge_data()
    df = processor.create_all_features(df)
    
    # 2. ì‹œí€€ìŠ¤ ìƒì„±
    X, y, y_jump, y_range, y_trend, y_value = processor.create_sequences(df)
    
    # 3. í•™ìŠµ/ê²€ì¦ ë¶„í• 
    split_idx = int(0.8 * len(X))
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_jump_train, y_jump_val = y_jump[:split_idx], y_jump[split_idx:]
    y_range_train, y_range_val = y_range[:split_idx], y_range[split_idx:]
    
    # 4. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = JumpDetectionSystem()
    
    # 5. íŠ¹ì§• ì¤€ë¹„
    X_train_features = system.prepare_features(X_train)
    X_val_features = system.prepare_features(X_val)
    
    # 6. ëª¨ë¸ í•™ìŠµ
    # 6-1. ì í”„ ê°ì§€
    system.train_jump_detector(X_train_features, y_jump_train)
    
    # 6-2. 3êµ¬ê°„ ë¶„ë¥˜
    print("\nğŸ“Š 3êµ¬ê°„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ")
    system.model_range.fit(X_train_features, y_range_train)
    
    # 6-3. ìƒìŠ¹/í•˜ë½ íŒ¨í„´
    print("\nğŸ“ˆ ìƒìŠ¹/í•˜ë½ íŒ¨í„´ ëª¨ë¸ í•™ìŠµ")
    label_encoder = LabelEncoder()
    y_trend_encoded = label_encoder.fit_transform(y_trend_train)
    system.model_trend.fit(X_train_features, y_trend_encoded)
    
    # 7. í‰ê°€
    print("\n" + "="*60)
    print("ğŸ“Š ìµœì¢… ì„±ëŠ¥ í‰ê°€")
    print("="*60)
    
    # ì í”„ ê°ì§€ í‰ê°€
    jump_pred = system.model_jump.predict(X_val_features)
    jump_pred_boosted = system.apply_rule_based_boost(X_val_features, jump_pred)
    
    jump_accuracy = (jump_pred_boosted == y_jump_val).mean()
    jump_recall = jump_pred_boosted[y_jump_val == 1].mean()
    
    print(f"\nğŸ¯ ì í”„ ê°ì§€")
    print(f"  - ì „ì²´ ì •í™•ë„: {jump_accuracy*100:.1f}%")
    print(f"  - ì í”„ ê°ì§€ìœ¨: {jump_recall*100:.1f}%")
    
    # 3êµ¬ê°„ ë¶„ë¥˜ í‰ê°€
    range_pred = system.model_range.predict(X_val_features)
    range_accuracy = (range_pred == y_range_val).mean()
    
    print(f"\nğŸ“Š 3êµ¬ê°„ ë¶„ë¥˜")
    print(f"  - ì •í™•ë„: {range_accuracy*100:.1f}%")
    
    # ëª¨ë¸ ì €ì¥
    print("\nğŸ’¾ ëª¨ë¸ ì €ì¥")
    joblib.dump(system, 'hubroom_jump_detection_system.pkl')
    print("âœ… ì €ì¥ ì™„ë£Œ: hubroom_jump_detection_system.pkl")
    
    print("\n" + "="*60)
    print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    print("="*60)

if __name__ == "__main__":
    main()