# -*- coding: utf-8 -*-

def integrated_loss(y_true, y_pred):
    """í†µí•© ì†ì‹¤ í•¨ìˆ˜ - bool íƒ€ì… ì˜¤ë¥˜ ìˆ˜ì •"""
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    
    if len(y_true.shape) > 1:
        y_true = tf.squeeze(y_true)
    if len(y_pred.shape) > 1:
        y_pred = tf.squeeze(y_pred)
    
    # ê¸°ë³¸ MAE ì†ì‹¤
    mae_loss = tf.reduce_mean(tf.abs(y_true - y_pred))
    
    # ê·¹ë‹¨ê°’ íŠ¹í™” ì†ì‹¤ ì¶”ê°€ - bool íƒ€ì… ìœ ì§€!
    is_extreme = y_true >= 300.0  # bool íƒ€ì…ìœ¼ë¡œ ìœ ì§€ (ìˆ˜ì •ë¨!)
    extreme_penalty = tf.where(is_extreme, 2.0, 1.0)  
    
    # False Positive ë°©ì§€ - ì´ë¯¸ bool íƒ€ì…
    fp_condition = tf.logical_and(y_true < 300.0, y_pred >= 300.0)
    fp_penalty = tf.where(fp_condition, 5.0, 1.0)
    
    # ì´ í˜ë„í‹° ì ìš©
    total_penalty = extreme_penalty * fp_penalty
    weighted_loss = tf.reduce_mean(tf.abs(y_true - y_pred) * total_penalty)
    
    return weighted_loss
"""
================================================================================
ğŸ”¥ HUBROOM V4 ULTIMATE ì™„ì „ í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ - 30ë¶„â†’10ë¶„ ì˜ˆì¸¡ (ì™„ì „ ìˆ˜ì •íŒ)
================================================================================
âœ¨ í•µì‹¬ ê¸°ëŠ¥:
   1. 30ë¶„ ì‹œí€€ìŠ¤ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡
   2. Base Models: PatchTST (ì•ˆì •í˜•) + PatchTST+PINN (ê·¹ë‹¨í˜•)
   3. Fine-tuning Network: ë¯¸ì„¸ì¡°ì • ë¡œì§ì„ í•™ìŠµìœ¼ë¡œ ë³€í™˜
   4. Jump Detection Network: 277â†’300 ì í”„ íŠ¹í™” í•™ìŠµ
   5. Decision Fusion Network: ìµœì¢… í†µí•© ì˜ˆì¸¡
   6. ì™„ë²½í•œ ì¤‘ë‹¨/ì¬ê°œ ì‹œìŠ¤í…œ
   
ğŸ¯ ëª¨ë“  ì˜¤ë¥˜ ìˆ˜ì •:
   - TensorFlow 2.16.1 í˜¸í™˜ (.weights.h5)
   - Tensor shape ì˜¤ë¥˜ ìˆ˜ì •
   - ì†ì‹¤ í•¨ìˆ˜ í˜¸í™˜ì„± ìˆ˜ì •
   - ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ì‘ë™
================================================================================
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.model_selection import train_test_split
import os
import pickle
import warnings
from datetime import datetime
import signal
import sys
import joblib
import h5py
from tqdm import tqdm
from typing import Dict, List, Tuple, Optional, Any

warnings.filterwarnings('ignore')
np.random.seed(42)
tf.random.set_seed(42)

print("="*80)
print("ğŸ”¥ HUBROOM V4 ULTIMATE í†µí•© ì‹œìŠ¤í…œ (ì™„ì „ ìˆ˜ì •íŒ)")
print("ğŸ“Š 30ë¶„ ì‹œí€€ìŠ¤ â†’ 10ë¶„ í›„ ì˜ˆì¸¡")
print("ğŸ¯ Base Models + Fine-tuning + Jump Detection + Decision Fusion")
print("="*80)
print(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"ğŸ”§ TensorFlow Version: {tf.__version__}")
print("="*80)

# ==============================================================================
# ğŸ’¾ ì™„ì „ í†µí•© ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
# ==============================================================================

class IntegratedCheckpointManager:
    """ì™„ì „ í†µí•© ì‹œìŠ¤í…œìš© ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬"""
    
    def __init__(self, checkpoint_dir='./checkpoints_integrated_30min'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ìƒíƒœ íŒŒì¼ë“¤
        self.state_file = os.path.join(checkpoint_dir, 'integrated_state.pkl')
        self.sequence_file = os.path.join(checkpoint_dir, 'sequences_integrated.h5')
        self.scaler_dir = os.path.join(checkpoint_dir, 'scalers')
        self.model_dir = os.path.join(checkpoint_dir, 'models')
        os.makedirs(self.scaler_dir, exist_ok=True)
        os.makedirs(self.model_dir, exist_ok=True)
        
        self.interrupted = False
        
        # Ctrl+C í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
    
    def _signal_handler(self, sig, frame):
        """Ctrl+C ì‹œê·¸ë„ ì²˜ë¦¬"""
        print('\n\nâš ï¸ ì¤‘ë‹¨ ê°ì§€! í˜„ì¬ ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤...')
        self.interrupted = True
    
    def save_state(self, state: Dict[str, Any]):
        """í˜„ì¬ ìƒíƒœ ì €ì¥"""
        with open(self.state_file, 'wb') as f:
            pickle.dump(state, f)
        print(f"ğŸ’¾ ìƒíƒœ ì €ì¥ ì™„ë£Œ: Stage {state.get('stage', 0)}")
    
    def load_state(self) -> Optional[Dict[str, Any]]:
        """ì €ì¥ëœ ìƒíƒœ ë¡œë“œ"""
        if os.path.exists(self.state_file):
            with open(self.state_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def save_sequences(self, X, y, X_physics, weights, features, progress, total):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ì €ì¥"""
        with h5py.File(self.sequence_file, 'w') as f:
            f.create_dataset('X', data=X, compression='gzip')
            f.create_dataset('y', data=y, compression='gzip')
            f.create_dataset('X_physics', data=X_physics, compression='gzip')
            f.create_dataset('weights', data=weights, compression='gzip')
            f.create_dataset('features', data=features, compression='gzip')
            f.attrs['progress'] = progress
            f.attrs['total'] = total
        print(f"ğŸ’¾ í†µí•© ì‹œí€€ìŠ¤ ì €ì¥ ì™„ë£Œ: {progress}/{total}")
    
    def load_sequences(self):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ë¡œë“œ"""
        if os.path.exists(self.sequence_file):
            with h5py.File(self.sequence_file, 'r') as f:
                return {
                    'X': f['X'][:],
                    'y': f['y'][:],
                    'X_physics': f['X_physics'][:],
                    'weights': f['weights'][:],
                    'features': f['features'][:],
                    'progress': f.attrs['progress'],
                    'total': f.attrs['total']
                }
        return None
    
    def clear_state(self):
        """ìƒíƒœ ì´ˆê¸°í™”"""
        if os.path.exists(self.state_file):
            os.remove(self.state_file)
        if os.path.exists(self.sequence_file):
            os.remove(self.sequence_file)
        print("ğŸ§¹ ì´ì „ ìƒíƒœ ì œê±° ì™„ë£Œ")

# ==============================================================================
# ğŸ§  í†µí•© ë°ì´í„° ì²˜ë¦¬ê¸° (30ë¶„ ì‹œí€€ìŠ¤ + íŠ¹ì§• ìƒì„±)
# ==============================================================================

class IntegratedDataProcessor:
    """í†µí•© ì‹œìŠ¤í…œìš© ë°ì´í„° ì²˜ë¦¬ - 30ë¶„ ì‹œí€€ìŠ¤ + ê³ ê¸‰ íŠ¹ì§•"""
    
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        self.bridge_time_col = 'BRIDGE_TIME'
        self.seq_len = 30  # 30ë¶„ ì‹œí€€ìŠ¤
        self.pred_len = 10  # 10ë¶„ í›„ ì˜ˆì¸¡
        
        # V4 í•„ìˆ˜ ì»¬ëŸ¼ ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼)
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB',
            'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2',
            'M14B_7F_TO_HUB_JOB2',
            'M16B_10F_TO_HUB_JOB'
        ]
        
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB',
            'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB',
            'M16A_3F_TO_M14B_7F_JOB',
            'M16A_3F_TO_3F_MLUD_JOB'
        ]
        
        self.cmd_cols = [
            'M16A_3F_CMD',
            'M16A_6F_TO_HUB_CMD',
            'M16A_2F_TO_HUB_CMD',
            'M14A_3F_TO_HUB_CMD',
            'M14B_7F_TO_HUB_CMD'
        ]
        
        self.maxcapa_cols = [
            'M16A_6F_LFT_MAXCAPA',
            'M16A_2F_LFT_MAXCAPA'
        ]
        
        self.extreme_cols = [
            'M16A_3F_STORAGE_UTIL'
        ]
        
        self.ofs_cols = [
            'M14_TO_M16_OFS_CUR',
            'M16_TO_M14_OFS_CUR'
        ]
        
        # ì „ì²´ V4 í•„ìˆ˜ ì»¬ëŸ¼
        self.v4_essential_cols = ([self.target_col] + self.inflow_cols + self.outflow_cols + 
                                  self.cmd_cols + self.maxcapa_cols + self.extreme_cols + self.ofs_cols)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ë“¤
        self.scaler_X = RobustScaler()
        self.scaler_y = RobustScaler()
        self.scaler_physics = RobustScaler()
        self.scaler_features = StandardScaler()
        
        # ì—°ì† íŒ¨í„´ í™•ë¥  (30ê°œê¹Œì§€)
        self.pattern_probability = {
            0: 0.003, 1: 0.15, 2: 0.25, 3: 0.31, 4: 0.43, 5: 0.43,
            6: 0.35, 7: 0.42, 8: 0.53, 9: 0.49, 10: 0.42,
            11: 0.47, 12: 0.52, 13: 0.60, 14: 0.54, 15: 0.66,
            16: 0.62, 17: 0.71, 18: 0.79, 19: 0.83, 20: 0.987,
            21: 0.99, 22: 0.99, 23: 0.99, 24: 0.99, 25: 0.99,
            26: 0.99, 27: 0.99, 28: 0.99, 29: 0.99, 30: 0.99
        }
    
    def load_and_merge_data(self) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ ë° BRIDGE_TIME ë³‘í•©"""
        print("\n[ë°ì´í„° ë¡œë“œ - í†µí•© ì‹œìŠ¤í…œ]")
        
        # ë©”ì¸ ë°ì´í„° ë¡œë“œ
        df_raw = pd.read_csv('data/HUB_0509_TO_0807_DATA.CSV')
        print(f"ğŸ“Š ì›ë³¸ ë°ì´í„°: {df_raw.shape}")
        
        # ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬
        time_col = df_raw.columns[0]  
        df_raw['timestamp'] = pd.to_datetime(df_raw[time_col], format='%Y%m%d%H%M', errors='coerce')
        
        # V4 í•„ìˆ˜ ì»¬ëŸ¼ë§Œ ì„ íƒ
        available_cols = [time_col, 'timestamp']
        missing_cols = []
        
        for col in self.v4_essential_cols:
            if col in df_raw.columns:
                available_cols.append(col)
            else:
                missing_cols.append(col)
        
        df = df_raw[available_cols].copy()
        print(f"âœ… V4 ì»¬ëŸ¼ ì„ íƒ: {len(available_cols)-2}/{len(self.v4_essential_cols)} ì»¬ëŸ¼ ì‚¬ìš©")
        
        if missing_cols:
            print(f"âš ï¸ ëˆ„ë½ëœ V4 ì»¬ëŸ¼ ({len(missing_cols)}ê°œ): {missing_cols[:5]}...")
            for col in missing_cols:
                if col == self.target_col:
                    print(f"âŒ íƒ€ê²Ÿ ì»¬ëŸ¼ {self.target_col} ì—†ìŒ!")
                    raise ValueError(f"íƒ€ê²Ÿ ì»¬ëŸ¼ {self.target_col}ì´ ì—†ìŠµë‹ˆë‹¤!")
                else:
                    df[col] = 0
        
        # BRIDGE_TIME ë°ì´í„° ë³‘í•©
        bridge_path = 'data/BRTIME_0509_TO_0807.CSV'
        if os.path.exists(bridge_path):
            print("\nğŸ“Š BRIDGE_TIME íŒŒì¼ ì²˜ë¦¬")
            try:
                bridge_df = pd.read_csv(bridge_path)
                print(f"  ì›ë³¸ shape: {bridge_df.shape}")
                
                if 'IDC_VAL' in bridge_df.columns:
                    bridge_df['BRIDGE_TIME'] = pd.to_numeric(bridge_df['IDC_VAL'], errors='coerce')
                    
                    if 'CRT_TM' in bridge_df.columns:
                        bridge_df['timestamp'] = pd.to_datetime(bridge_df['CRT_TM'])
                        if hasattr(bridge_df['timestamp'].dtype, 'tz'):
                            bridge_df['timestamp'] = bridge_df['timestamp'].dt.tz_localize(None)
                    
                    if hasattr(df['timestamp'].dtype, 'tz'):
                        df['timestamp'] = df['timestamp'].dt.tz_localize(None)
                    
                    bridge_df['timestamp'] = bridge_df['timestamp'].dt.floor('min')
                    df['timestamp'] = df['timestamp'].dt.floor('min')
                    
                    df = pd.merge(df, bridge_df[['timestamp', 'BRIDGE_TIME']], 
                                on='timestamp', how='left')
                    
                    merged_count = df['BRIDGE_TIME'].notna().sum()
                    print(f"âœ… BRIDGE_TIME ë³‘í•©: {merged_count}/{len(df)} í–‰ ë§¤ì¹­")
                    
                    if merged_count > 0:
                        df['BRIDGE_TIME'] = df['BRIDGE_TIME'].interpolate(method='linear', limit_direction='both')
                    df['BRIDGE_TIME'] = df['BRIDGE_TIME'].fillna(3.5)
                    
                    print(f"  ìµœì¢… í†µê³„: min={df['BRIDGE_TIME'].min():.2f}, "
                          f"max={df['BRIDGE_TIME'].max():.2f}, "
                          f"mean={df['BRIDGE_TIME'].mean():.2f}")
                else:
                    df['BRIDGE_TIME'] = 3.5
                    
            except Exception as e:
                print(f"âš ï¸ BRIDGE_TIME ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                df['BRIDGE_TIME'] = 3.5
        else:
            print("âš ï¸ BRIDGE_TIME.CSV ì—†ìŒ - ê¸°ë³¸ê°’ 3.5")
            df['BRIDGE_TIME'] = 3.5
        
        df = df.fillna(method='ffill').fillna(0)
        
        if 'timestamp' in df.columns:
            df = df.drop('timestamp', axis=1)
        
        print(f"\nâœ… ìµœì¢… ë°ì´í„°: {df.shape}")
        
        return df
    
    def create_advanced_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê³ ê¸‰ íŠ¹ì§• ìƒì„± (30ë¶„ ì‹œí€€ìŠ¤ í™œìš©)"""
        print("\n[ê³ ê¸‰ íŠ¹ì§• ìƒì„± - 30ë¶„ ë¶„ì„]")
        
        # ì—°ì† íŒ¨í„´ ë¶„ì„
        consecutive_counts = []
        consecutive_probs = []
        
        # 30ë¶„ ì¶”ì„¸ ë¶„ì„
        long_trends = []
        mid_trends = []
        accelerations = []
        volatility_changes = []
        
        # ì í”„ ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´
        jump_risks = []
        extreme_risks = []
        
        for i in range(len(df)):
            if i < self.seq_len:
                # ì´ˆê¸° ë°ì´í„° ì²˜ë¦¬
                count = 0
                prob = 0
                long_trend = 0
                mid_trend = 0
                acceleration = 0
                volatility_change = 1
                jump_risk = 0
                extreme_risk = 0
            else:
                # 30ë¶„ ë°ì´í„° ë¶„ì„
                window_30 = df[self.target_col].iloc[i-self.seq_len:i].values
                
                # ì—°ì† 300+ ì¹´ìš´íŠ¸
                count = sum(1 for v in window_30 if v >= 300)
                prob = self.pattern_probability.get(count, 0.5)
                
                # 30ë¶„ì„ 3êµ¬ê°„ìœ¼ë¡œ ë¶„í• 
                first_10 = window_30[:10]
                middle_10 = window_30[10:20]
                last_10 = window_30[20:30]
                
                # ì¶”ì„¸ ê³„ì‚°
                long_trend = np.mean(last_10) - np.mean(first_10)
                mid_trend = np.mean(last_10) - np.mean(middle_10)
                acceleration = mid_trend - (np.mean(middle_10) - np.mean(first_10))
                
                # ë³€ë™ì„± ë³€í™”
                first_std = np.std(first_10) if np.std(first_10) > 0 else 1
                last_std = np.std(last_10)
                volatility_change = last_std / first_std
                
                # ì í”„ ë¦¬ìŠ¤í¬ ê³„ì‚° (270-280 êµ¬ê°„ ìœ„í—˜ë„)
                max_val = np.max(window_30)
                recent_5 = window_30[-5:]
                mean_recent = np.mean(recent_5)
                
                jump_risk = 0
                if 270 <= max_val <= 280:
                    jump_risk += 20
                    if acceleration > 2:
                        jump_risk += 25
                    if long_trend > 5:
                        jump_risk += 25
                    if volatility_change > 1.5:
                        jump_risk += 15
                    if mean_recent > 275:
                        jump_risk += 30
                
                # ê·¹ë‹¨ê°’ ë¦¬ìŠ¤í¬
                extreme_risk = 0
                if mean_recent > 285:
                    extreme_risk += 50
                elif mean_recent > 270:
                    extreme_risk += 30
                if count > 15:
                    extreme_risk += 20
                if long_trend > 15:
                    extreme_risk += 25
            
            consecutive_counts.append(count)
            consecutive_probs.append(prob)
            long_trends.append(long_trend)
            mid_trends.append(mid_trend)
            accelerations.append(acceleration)
            volatility_changes.append(volatility_change)
            jump_risks.append(jump_risk)
            extreme_risks.append(extreme_risk)
        
        # íŠ¹ì§• ì¶”ê°€
        df['consecutive_300_count'] = consecutive_counts
        df['consecutive_300_prob'] = consecutive_probs
        df['long_trend'] = long_trends
        df['mid_trend'] = mid_trends
        df['acceleration'] = accelerations
        df['volatility_change'] = volatility_changes
        df['jump_risk'] = jump_risks
        df['extreme_risk'] = extreme_risks
        
        print(f"âœ… ê³ ê¸‰ íŠ¹ì§• ìƒì„± ì™„ë£Œ (30ë¶„ ë¶„ì„)")
        return df
    
    def create_integrated_sequences(self, df: pd.DataFrame, ckpt_manager: Optional[IntegratedCheckpointManager] = None) -> Tuple:
        """í†µí•© ì‹œí€€ìŠ¤ ìƒì„± with ì¤‘ë‹¨/ì¬ê°œ (30ë¶„â†’10ë¶„)"""
        print(f"\n[í†µí•© ì‹œí€€ìŠ¤ ìƒì„± - {self.seq_len}ë¶„â†’{self.pred_len}ë¶„ ì˜ˆì¸¡]")
        
        saved_sequences = None
        if ckpt_manager:
            saved_sequences = ckpt_manager.load_sequences()
        
        if saved_sequences:
            resume = input(f"\nì´ì „ ì‹œí€€ìŠ¤ ë°œê²¬ ({saved_sequences['progress']}/{saved_sequences['total']}). ì´ì–´ì„œ? (y/n): ")
            if resume.lower() == 'y':
                print("âœ… ì´ì „ ì‹œí€€ìŠ¤ ì‚¬ìš©")
                return (saved_sequences['X'], saved_sequences['y'], 
                       saved_sequences['X_physics'], saved_sequences['weights'], 
                       saved_sequences['features'])
        
        X_list, y_list, X_physics_list, weights_list, features_list = [], [], [], [], []
        
        # ì‚¬ìš©í•  ì»¬ëŸ¼ë“¤
        all_cols = self.v4_essential_cols + ['BRIDGE_TIME'] + [
            'consecutive_300_count', 'consecutive_300_prob', 'long_trend', 
            'mid_trend', 'acceleration', 'volatility_change', 'jump_risk', 'extreme_risk'
        ]
        available_cols = [col for col in all_cols if col in df.columns]
        
        total_sequences = len(df) - self.seq_len - self.pred_len
        
        print(f"ğŸ“Š ì´ {total_sequences}ê°œ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘ (30ë¶„â†’10ë¶„)...")
        
        for i in tqdm(range(total_sequences)):
            if ckpt_manager and ckpt_manager.interrupted:
                print("\nğŸ’¾ ì¤‘ë‹¨! í˜„ì¬ê¹Œì§€ ì €ì¥...")
                ckpt_manager.save_sequences(
                    np.array(X_list), np.array(y_list),
                    np.array(X_physics_list), np.array(weights_list),
                    np.array(features_list), i, total_sequences
                )
                sys.exit(0)
            
            # 30ë¶„ ì…ë ¥ ì‹œí€€ìŠ¤
            X = df[available_cols].iloc[i:i+self.seq_len].values
            
            # 10ë¶„ í›„ íƒ€ê²Ÿ
            y = df[self.target_col].iloc[i+self.seq_len+self.pred_len-1]
            
            # ë¬¼ë¦¬ íŠ¹ì§• (11ì°¨ì›)
            physics = self.create_physics_features(df, i+self.seq_len-1)
            
            # í†µí•© íŠ¹ì§• (ë¯¸ì„¸ì¡°ì •ìš©, 15ì°¨ì›)
            features = self.create_fine_tuning_features(df, i+self.seq_len-1)
            
            # ìƒ˜í”Œ ê°€ì¤‘ì¹˜
            weight = self.calculate_sample_weight(y)
            
            X_list.append(X)
            y_list.append(y)
            X_physics_list.append(physics)
            weights_list.append(weight)
            features_list.append(features)
        
        print(f"âœ… í†µí•© ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {len(X_list)}ê°œ (30ë¶„â†’10ë¶„)")
        
        return (np.array(X_list), np.array(y_list), 
               np.array(X_physics_list), np.array(weights_list), np.array(features_list))
    
    def create_physics_features(self, df: pd.DataFrame, idx: int) -> np.ndarray:
        """ë¬¼ë¦¬ íŠ¹ì§• ìƒì„± (11ì°¨ì› - ê¸°ì¡´ê³¼ ë™ì¼)"""
        physics = []
        
        # ê¸°ë³¸ 9ê°œ íŠ¹ì§•
        physics.append(df[self.target_col].iloc[idx])
        
        inflow_sum = sum(df[col].iloc[idx] for col in self.inflow_cols if col in df.columns)
        physics.append(inflow_sum)
        
        outflow_sum = sum(df[col].iloc[idx] for col in self.outflow_cols if col in df.columns)
        physics.append(outflow_sum)
        
        physics.append(df.get('BRIDGE_TIME', pd.Series([3.5])).iloc[idx])
        physics.append(df.get('consecutive_300_count', pd.Series([0])).iloc[idx])
        physics.append(df.get('consecutive_300_prob', pd.Series([0.5])).iloc[idx])
        physics.append(df.get('M16A_3F_STORAGE_UTIL', pd.Series([0])).iloc[idx])
        
        cmd_sum = sum(df[col].iloc[idx] for col in self.cmd_cols if col in df.columns)
        physics.append(cmd_sum)
        
        if idx >= 4:
            recent_avg = df[self.target_col].iloc[idx-4:idx+1].mean()
        else:
            recent_avg = df[self.target_col].iloc[idx]
        physics.append(recent_avg)
        
        # 30ë¶„ íŠ¹ì§• (2ê°œ)
        if idx >= 30:
            first_10_avg = df[self.target_col].iloc[idx-29:idx-19].mean()
            last_10_avg = df[self.target_col].iloc[idx-9:idx+1].mean()
            long_trend = last_10_avg - first_10_avg
            
            first_10_std = df[self.target_col].iloc[idx-29:idx-19].std()
            last_10_std = df[self.target_col].iloc[idx-9:idx+1].std()
            volatility_change = last_10_std / max(first_10_std, 1)
        else:
            long_trend = 0
            volatility_change = 1
            
        physics.append(long_trend)
        physics.append(volatility_change)
        
        return np.array(physics)
    
    def create_fine_tuning_features(self, df: pd.DataFrame, idx: int) -> np.ndarray:
        """ë¯¸ì„¸ì¡°ì •ìš© íŠ¹ì§• ìƒì„± (15ì°¨ì›) - ë£° ê¸°ë°˜ ë¡œì§ì„ íŠ¹ì§•ìœ¼ë¡œ ë³€í™˜"""
        features = []
        
        if idx < self.seq_len:
            return np.zeros(15)
        
        # 30ë¶„ ë°ì´í„°
        past_30 = df[self.target_col].iloc[idx-self.seq_len+1:idx+1].values
        recent_3 = past_30[-3:]
        recent_5 = past_30[-5:]
        
        # 1. ì í”„ ê°ì§€ íŠ¹ì§•ë“¤ (277 êµ¬ê°„)
        features.append(1.0 if any(275 <= v <= 279 for v in recent_3) else 0.0)  # 277 ì§ê²©
        features.append(1.0 if any(274 <= v < 275 for v in recent_3) else 0.0)   # 274-275
        features.append(1.0 if any(272 <= v < 274 for v in recent_5) else 0.0)   # 272-274
        features.append(1.0 if any(270 <= v < 272 for v in recent_5) else 0.0)   # 270-272
        
        # 2. êµ¬ê°„ë³„ íŠ¹ì§•ë“¤
        features.append(1.0 if any(255 <= v <= 260 for v in recent_5) else 0.0)  # 255-260
        features.append(1.0 if any(260 <= v < 270 for v in recent_5) else 0.0)   # 260-270
        features.append(1.0 if any(250 <= v < 260 for v in past_30) else 0.0)    # 250-260
        
        # 3. ì¶”ì„¸ íŠ¹ì§•ë“¤
        features.append(df.get('long_trend', pd.Series([0])).iloc[idx])
        features.append(df.get('acceleration', pd.Series([0])).iloc[idx])
        features.append(df.get('volatility_change', pd.Series([1])).iloc[idx])
        
        # 4. ê·¹ë‹¨ê°’ íŠ¹ì§•ë“¤
        features.append(np.max(past_30))  # ìµœëŒ€ê°’
        features.append(np.mean(recent_3))  # ìµœê·¼ 3ë¶„ í‰ê· 
        features.append(np.mean(recent_5))  # ìµœê·¼ 5ë¶„ í‰ê· 
        
        # 5. ìœ„í—˜ ìŠ¤ì½”ì–´ë“¤
        features.append(df.get('jump_risk', pd.Series([0])).iloc[idx])
        features.append(df.get('extreme_risk', pd.Series([0])).iloc[idx])
        
        return np.array(features)
    
    def calculate_sample_weight(self, y_value: float) -> float:
        """ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ê³„ì‚° (ê¸°ì¡´ê³¼ ë™ì¼)"""
        if y_value < 200:
            return 3.0
        elif y_value < 300:
            return 1.0
        elif y_value < 310:
            return 5.0
        elif y_value < 335:
            return 15.0
        else:
            return 25.0

# ==============================================================================
# ğŸ—ï¸ í†µí•© ëª¨ë¸ ì •ì˜ - Base Models + Fine-tuning + Jump Detection + Decision Fusion
# ==============================================================================

class PatchTSTModel(keras.Model):
    """ì•ˆì • êµ¬ê°„ ì „ë¬¸ PatchTST (30ë¶„ ì‹œí€€ìŠ¤)"""
    
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']  # 30
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']  # 6
        self.n_patches = self.seq_len // self.patch_len  # 5
        
        self.patch_embedding = layers.Dense(128, activation='relu')
        
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm1 = layers.LayerNormalization()
        self.norm2 = layers.LayerNormalization()
        
        self.ffn = keras.Sequential([
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(128)
        ])
        
        self.flatten = layers.Flatten()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dropout = layers.Dropout(0.3)
        self.dense2 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(1)
    
    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        
        x = tf.reshape(x, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        
        attn = self.attention(x, x, training=training)
        x = self.norm1(x + attn)
        
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        
        x = self.flatten(x)
        x = self.dense1(x)
        x = self.dropout(x, training=training)
        x = self.dense2(x)
        output = self.output_layer(x)
        
        return tf.squeeze(output, axis=-1)

class PatchTSTPINN(keras.Model):
    """ê·¹ë‹¨ê°’ ì „ë¬¸ PatchTST + PINN (30ë¶„ ì‹œí€€ìŠ¤)"""
    
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']  # 30
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']  # 6
        self.n_patches = self.seq_len // self.patch_len  # 5
        
        self.patch_embedding = layers.Dense(128, activation='relu')
        
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm = layers.LayerNormalization()
        
        self.flatten = layers.Flatten()
        self.temporal_dense = layers.Dense(64, activation='relu')
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬ (11ì°¨ì›)
        self.physics_net = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.BatchNormalization(),
            layers.Dense(32, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(16, activation='relu')
        ])
        
        self.fusion = keras.Sequential([
            layers.Dense(128, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(64, activation='relu'),
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(1)
        ])
        
        self.extreme_boost = layers.Dense(1, activation='sigmoid')
    
    def call(self, inputs, training=False):
        x_seq, x_physics = inputs
        batch_size = tf.shape(x_seq)[0]
        
        x = tf.reshape(x_seq, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        
        attn = self.attention(x, x, training=training)
        x = self.norm(x + attn)
        
        x = self.flatten(x)
        x_temporal = self.temporal_dense(x)
        
        x_physics = self.physics_net(x_physics)
        
        x_combined = tf.concat([x_temporal, x_physics], axis=-1)
        output = self.fusion(x_combined)
        
        boost_factor = self.extreme_boost(x_physics)
        output = output * (1 + boost_factor * 0.1)
        
        return tf.squeeze(output, axis=-1)

class FineTuningNetwork(keras.Model):
    """ë¯¸ì„¸ì¡°ì • ë„¤íŠ¸ì›Œí¬ - ë£° ê¸°ë°˜ ë¡œì§ì„ í•™ìŠµìœ¼ë¡œ ëŒ€ì²´"""
    
    def __init__(self):
        super().__init__()
        
        # íŠ¹ì§• ì²˜ë¦¬ ë ˆì´ì–´
        self.feature_net = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu')
        ])
        
        # ì í”„ ê°ì§€ ë¸Œëœì¹˜
        self.jump_branch = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(1, activation='sigmoid', name='jump_prob')
        ])
        
        # ê·¹ë‹¨ê°’ ê°ì§€ ë¸Œëœì¹˜
        self.extreme_branch = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(1, activation='sigmoid', name='extreme_prob')
        ])
        
        # ë³´ì • ì¸ìˆ˜ ë¸Œëœì¹˜
        self.adjustment_branch = keras.Sequential([
            layers.Dense(16, activation='relu'),
            layers.Dense(8, activation='relu'),
            layers.Dense(1, activation='tanh', name='adjustment_factor')  # -1 ~ 1
        ])
    
    def call(self, features, training=False):
        x = self.feature_net(features, training=training)
        
        jump_prob = self.jump_branch(x, training=training)
        extreme_prob = self.extreme_branch(x, training=training)
        adjustment = self.adjustment_branch(x, training=training)
        
        return {
            'jump_prob': tf.squeeze(jump_prob, axis=-1),
            'extreme_prob': tf.squeeze(extreme_prob, axis=-1),
            'adjustment': tf.squeeze(adjustment, axis=-1) * 0.2  # -0.2 ~ 0.2
        }

class JumpDetectionNetwork(keras.Model):
    """ì í”„ ê°ì§€ íŠ¹í™” ë„¤íŠ¸ì›Œí¬ (277â†’300 ì í”„)"""
    
    def __init__(self):
        super().__init__()
        
        self.jump_detector = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.4),
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(1, activation='sigmoid', name='jump_detection')
        ])
        
        self.jump_magnitude = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(1, activation='relu', name='jump_size')  # ì í”„ í¬ê¸°
        ])
    
    def call(self, features, training=False):
        jump_prob = self.jump_detector(features, training=training)
        jump_size = self.jump_magnitude(features, training=training)
        
        return {
            'jump_prob': tf.squeeze(jump_prob, axis=-1),
            'jump_size': tf.squeeze(jump_size, axis=-1) * 50 + 250  # 250~300 ë²”ìœ„
        }

class DecisionFusionNetwork(keras.Model):
    """ìµœì¢… ì˜ì‚¬ê²°ì • ìœµí•© ë„¤íŠ¸ì›Œí¬ - Tensor Shape ì˜¤ë¥˜ ìˆ˜ì •"""
    
    def __init__(self):
        super().__init__()
        
        # ëª¨ë¸ ì„ íƒ ë„¤íŠ¸ì›Œí¬
        self.model_selector = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(1, activation='sigmoid', name='model_choice')  # 0: Model1, 1: Model2
        ])
        
        # ìµœì¢… ì˜ˆì¸¡ ì¡°í•© ë„¤íŠ¸ì›Œí¬
        self.fusion_net = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(8, activation='relu'),
            layers.Dense(1, name='final_prediction')
        ])
    
    def call(self, inputs, training=False):
        # inputs: [features, pred1, pred2, fine_tune_outputs, jump_outputs]
        features, pred1, pred2, fine_tune, jump_detect = inputs
        
        # ëª¨ë¸ ì„ íƒ í™•ë¥ 
        model_choice_prob = self.model_selector(features, training=training)
        model_choice_squeezed = tf.squeeze(model_choice_prob, axis=-1)  # [?, 1] -> [?]
        
        # íŠ¹ì§•ë“¤ ê²°í•© - ëª¨ë“  í…ì„œë¥¼ [?, 1] í˜•íƒœë¡œ í†µì¼
        combined_features = tf.concat([
            features[:, :8],  # ì£¼ìš” íŠ¹ì§•ë“¤ë§Œ [?, 8]
            tf.expand_dims(pred1, axis=-1),  # [?] -> [?, 1]
            tf.expand_dims(pred2, axis=-1),  # [?] -> [?, 1]
            tf.expand_dims(fine_tune['jump_prob'], axis=-1),  # [?] -> [?, 1]
            tf.expand_dims(fine_tune['extreme_prob'], axis=-1),  # [?] -> [?, 1]
            tf.expand_dims(jump_detect['jump_prob'], axis=-1),  # [?] -> [?, 1]
            tf.expand_dims(model_choice_squeezed, axis=-1)  # [?] -> [?, 1]
        ], axis=-1)
        
        # ìµœì¢… ì˜ˆì¸¡
        final_pred = self.fusion_net(combined_features, training=training)
        
        return {
            'model_choice': model_choice_squeezed,
            'final_prediction': tf.squeeze(final_pred, axis=-1)
        }

class IntegratedV4Model(keras.Model):
    """í†µí•© V4 ëª¨ë¸ - í•™ìŠµ/ì¶”ë¡  ëª¨ë“œ ë¶„ë¦¬ë¡œ ì†ì‹¤ í•¨ìˆ˜ í˜¸í™˜ì„± í•´ê²°"""
    
    def __init__(self, config):
        super().__init__()
        
        self.model1 = PatchTSTModel(config)
        self.model2 = PatchTSTPINN(config)
        self.fine_tuning = FineTuningNetwork()
        self.jump_detection = JumpDetectionNetwork()
        self.decision_fusion = DecisionFusionNetwork()
    
    def call(self, inputs, training=False):
        # inputs: [x_seq, x_physics, x_features]
        x_seq, x_physics, x_features = inputs
        
        # Base ëª¨ë¸ ì˜ˆì¸¡
        pred1 = self.model1(x_seq, training=training)
        pred2 = self.model2([x_seq, x_physics], training=training)
        
        # ë¯¸ì„¸ì¡°ì • ë„¤íŠ¸ì›Œí¬
        fine_tune_outputs = self.fine_tuning(x_features, training=training)
        
        # ì í”„ ê°ì§€ ë„¤íŠ¸ì›Œí¬
        jump_outputs = self.jump_detection(x_features, training=training)
        
        # ìµœì¢… ìœµí•©
        fusion_outputs = self.decision_fusion(
            [x_features, pred1, pred2, fine_tune_outputs, jump_outputs], 
            training=training
        )
        
        # í•™ìŠµ ì‹œì—ëŠ” final_predë§Œ ë°˜í™˜ (ì†ì‹¤ í•¨ìˆ˜ í˜¸í™˜)
        if training:
            return fusion_outputs['final_prediction']
        else:
            # ì¶”ë¡  ì‹œì—ëŠ” ì „ì²´ ì •ë³´ ë°˜í™˜
            return {
                'pred1': pred1,
                'pred2': pred2,
                'fine_tune': fine_tune_outputs,
                'jump_detect': jump_outputs,
                'fusion': fusion_outputs,
                'final_pred': fusion_outputs['final_prediction']
            }

# ==============================================================================
# ğŸ“‰ í†µí•© ì†ì‹¤ í•¨ìˆ˜ - ì™„ì „ ìˆ˜ì •
# ==============================================================================

def stable_loss(y_true, y_pred):
    """Model 1ìš© - ì•ˆì • êµ¬ê°„ ì „ë¬¸"""
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    
    if len(y_true.shape) > 1:
        y_true = tf.squeeze(y_true)
    if len(y_pred.shape) > 1:
        y_pred = tf.squeeze(y_pred)
    
    error = tf.abs(y_true - y_pred)
    
    penalty = tf.ones_like(error)
    penalty = tf.where(y_true < 300.0, 1.0, 0.01)  # 300+ ëŠ” ê±°ì˜ ë¬´ì‹œ
    
    # False Positive ê°•ë ¥ ë°©ì§€
    condition_fp = tf.logical_and(y_true < 300.0, y_pred >= 300.0)
    penalty = tf.where(condition_fp, 50.0, penalty)
    
    condition_extreme_fp = tf.logical_and(y_true < 250.0, y_pred >= 300.0)
    penalty = tf.where(condition_extreme_fp, 100.0, penalty)
    
    weighted_error = error * penalty
    
    result = tf.reduce_mean(weighted_error)
    result = tf.where(tf.math.is_nan(result), 0.0, result)
    
    return result

def extreme_loss(y_true, y_pred):
    """Model 2ìš© - ê·¹ë‹¨ê°’ íŠ¹í™”"""
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    
    if len(y_true.shape) > 1:
        y_true = tf.squeeze(y_true)
    if len(y_pred.shape) > 1:
        y_pred = tf.squeeze(y_pred)
    
    error = tf.abs(y_true - y_pred)
    
    missed_335 = tf.logical_and(y_true >= 335.0, y_pred < 335.0)
    missed_310 = tf.logical_and(
        tf.logical_and(y_true >= 310.0, y_true < 335.0),
        y_pred < 310.0
    )
    in_300_range = tf.logical_and(y_true >= 300.0, y_true < 310.0)
    
    penalty = tf.ones_like(error)
    penalty = tf.where(y_true < 300.0, 0.3, penalty)
    penalty = tf.where(in_300_range, 5.0, penalty)
    penalty = tf.where(missed_310, 15.0, penalty)
    penalty = tf.where(missed_335, 30.0, penalty)
    
    weighted_error = error * penalty
    
    result = tf.reduce_mean(weighted_error)
    result = tf.where(tf.math.is_nan(result), 0.0, result)
    
    return result

def integrated_loss(y_true, y_pred):
    """í†µí•© ì†ì‹¤ í•¨ìˆ˜ - ì™„ì „ ìˆ˜ì • (final_predë§Œ ë°›ìŒ)"""
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    
    if len(y_true.shape) > 1:
        y_true = tf.squeeze(y_true)
    if len(y_pred.shape) > 1:
        y_pred = tf.squeeze(y_pred)
    
    # ê¸°ë³¸ MAE ì†ì‹¤
    mae_loss = tf.reduce_mean(tf.abs(y_true - y_pred))
    
    # ê·¹ë‹¨ê°’ íŠ¹í™” ì†ì‹¤ ì¶”ê°€
    is_extreme = tf.cast(y_true >= 300.0, tf.float32)
    extreme_penalty = tf.where(is_extreme, 2.0, 1.0)  # ê·¹ë‹¨ê°’ì€ 2ë°° ê°€ì¤‘ì¹˜
    
    # False Positive ë°©ì§€
    fp_condition = tf.logical_and(y_true < 300.0, y_pred >= 300.0)
    fp_penalty = tf.where(fp_condition, 5.0, 1.0)
    
    # ì´ í˜ë„í‹° ì ìš©
    total_penalty = extreme_penalty * fp_penalty
    weighted_loss = tf.reduce_mean(tf.abs(y_true - y_pred) * total_penalty)
    
    return weighted_loss

# ==============================================================================
# ğŸ“Š í†µí•© ì½œë°± - ìˆ˜ì •
# ==============================================================================

class IntegratedCallback(Callback):
    """í†µí•© ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ - ìˆ˜ì •"""
    
    def __init__(self, X_val, y_val, scaler_y, model_name="Integrated"):
        self.X_val = X_val
        self.y_val = y_val
        self.scaler_y = scaler_y
        self.model_name = model_name
    
    def on_epoch_end(self, epoch, logs=None):
        # ì¶”ë¡  ëª¨ë“œë¡œ ì˜ˆì¸¡ (ì „ì²´ ì¶œë ¥)
        self.model.training = False
        outputs = self.model.predict(self.X_val, verbose=0)
        self.model.training = True  # í•™ìŠµ ëª¨ë“œë¡œ ë³µì›
        
        if isinstance(outputs, dict):
            y_pred_scaled = outputs['final_pred']
        else:
            y_pred_scaled = outputs
        
        y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
        y_true = self.scaler_y.inverse_transform(self.y_val.reshape(-1, 1)).flatten()
        
        # ì „ì²´ MAE
        mae = np.mean(np.abs(y_true - y_pred))
        
        # ê·¹ë‹¨ê°’ ì„±ëŠ¥
        mask_300 = y_true >= 300
        if mask_300.sum() > 0:
            extreme_recall = (y_pred[mask_300] >= 300).sum() / mask_300.sum() * 100
        else:
            extreme_recall = 0
        
        # ì í”„ ì¼€ì´ìŠ¤ ì„±ëŠ¥
        mask_jump = (y_true >= 300) & (y_true < 350)
        if mask_jump.sum() > 0:
            jump_recall = (y_pred[mask_jump] >= 290).sum() / mask_jump.sum() * 100
        else:
            jump_recall = 0
        
        # FP ìœ¨
        mask_stable = y_true < 300
        if mask_stable.sum() > 0:
            fp_rate = (y_pred[mask_stable] >= 300).sum() / mask_stable.sum() * 100
        else:
            fp_rate = 0
        
        print(f"\n[{self.model_name} Epoch {epoch+1}] "
              f"MAE: {mae:.1f} | "
              f"300+ ê°ì§€: {extreme_recall:.1f}% | "
              f"ì í”„ ê°ì§€: {jump_recall:.1f}% | "
              f"FP: {fp_rate:.1f}%")

# ==============================================================================
# ğŸ¯ í†µí•© í•™ìŠµ ë©”ì¸ í•¨ìˆ˜
# ==============================================================================

def main():
    """í†µí•© V4 ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰"""
    
    print("\n" + "="*80)
    print("ğŸš€ V4 ULTIMATE í†µí•© ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*80)
    
    ckpt = IntegratedCheckpointManager()
    
    state = ckpt.load_state()
    
    if state:
        print(f"\nğŸ“‚ ì´ì „ í•™ìŠµ ìƒíƒœ ë°œê²¬! (Stage {state.get('stage', 1)}/5)")
        resume = input("ì´ì–´ì„œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y: ì´ì–´ì„œ, n: ì²˜ìŒë¶€í„°): ").lower()
        
        if resume != 'y':
            ckpt.clear_state()
            state = {}
            stage = 1
        else:
            stage = state.get('stage', 1)
            print(f"âœ… Stage {stage}ë¶€í„° ì¬ê°œí•©ë‹ˆë‹¤.")
    else:
        state = {}
        stage = 1
        print("\nìƒˆë¡œìš´ í†µí•© í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    processor = IntegratedDataProcessor()
    
    try:
        # Stage 1: ë°ì´í„° ì¤€ë¹„
        if stage <= 1:
            print(f"\n[Stage 1/5] ë°ì´í„° ë¡œë“œ ë° íŠ¹ì§• ìƒì„±")
            print("-"*60)
            
            df = processor.load_and_merge_data()
            df = processor.create_advanced_features(df)
            
            state['stage'] = 2
            state['data_shape'] = df.shape
            ckpt.save_state(state)
            print("âœ… Stage 1 ì™„ë£Œ")
        
        # Stage 2: ì‹œí€€ìŠ¤ ìƒì„±
        if stage <= 2:
            print(f"\n[Stage 2/5] í†µí•© ì‹œí€€ìŠ¤ ìƒì„± (30ë¶„â†’10ë¶„)")
            print("-"*60)
            
            if stage < 2:
                df = processor.load_and_merge_data()
                df = processor.create_advanced_features(df)
            
            X, y, X_physics, weights, X_features = processor.create_integrated_sequences(df, ckpt_manager=ckpt)
            
            with h5py.File('./checkpoints_integrated_30min/sequences_integrated_complete.h5', 'w') as f:
                f.create_dataset('X', data=X, compression='gzip')
                f.create_dataset('y', data=y, compression='gzip')
                f.create_dataset('X_physics', data=X_physics, compression='gzip')
                f.create_dataset('weights', data=weights, compression='gzip')
                f.create_dataset('X_features', data=X_features, compression='gzip')
            
            state['stage'] = 3
            state['sequence_shape'] = X.shape
            ckpt.save_state(state)
            print("âœ… Stage 2 ì™„ë£Œ")
        
        # Stage 3: ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§
        if stage <= 3:
            print(f"\n[Stage 3/5] ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§")
            print("-"*60)
            
            with h5py.File('./checkpoints_integrated_30min/sequences_integrated_complete.h5', 'r') as f:
                X = f['X'][:]
                y = f['y'][:]
                X_physics = f['X_physics'][:]
                weights = f['weights'][:]
                X_features = f['X_features'][:]
            
            indices = np.arange(len(X))
            np.random.shuffle(indices)
            
            train_size = int(0.7 * len(X))
            val_size = int(0.15 * len(X))
            
            train_idx = indices[:train_size]
            val_idx = indices[train_size:train_size+val_size]
            test_idx = indices[train_size+val_size:]
            
            X_train, y_train = X[train_idx], y[train_idx]
            X_val, y_val = X[val_idx], y[val_idx]
            X_test, y_test = X[test_idx], y[test_idx]
            
            X_physics_train = X_physics[train_idx]
            X_physics_val = X_physics[val_idx]
            X_physics_test = X_physics[test_idx]
            
            X_features_train = X_features[train_idx]
            X_features_val = X_features[val_idx]
            X_features_test = X_features[test_idx]
            
            weights_train = weights[train_idx]
            
            print(f"âœ… Train: {len(X_train):,} samples")
            print(f"âœ… Valid: {len(X_val):,} samples")  
            print(f"âœ… Test: {len(X_test):,} samples")
            
            # ìŠ¤ì¼€ì¼ë§
            n_features = X.shape[2]
            
            X_train_scaled = processor.scaler_X.fit_transform(X_train.reshape(-1, n_features))
            X_train_scaled = X_train_scaled.reshape(len(X_train), 30, n_features)
            
            X_val_scaled = processor.scaler_X.transform(X_val.reshape(-1, n_features))
            X_val_scaled = X_val_scaled.reshape(len(X_val), 30, n_features)
            
            X_test_scaled = processor.scaler_X.transform(X_test.reshape(-1, n_features))
            X_test_scaled = X_test_scaled.reshape(len(X_test), 30, n_features)
            
            y_train_scaled = processor.scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
            y_val_scaled = processor.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
            
            X_physics_train_scaled = processor.scaler_physics.fit_transform(X_physics_train)
            X_physics_val_scaled = processor.scaler_physics.transform(X_physics_val)
            X_physics_test_scaled = processor.scaler_physics.transform(X_physics_test)
            
            X_features_train_scaled = processor.scaler_features.fit_transform(X_features_train)
            X_features_val_scaled = processor.scaler_features.transform(X_features_val)
            X_features_test_scaled = processor.scaler_features.transform(X_features_test)
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
            os.makedirs('./checkpoints_integrated_30min/scalers', exist_ok=True)
            joblib.dump(processor.scaler_X, './checkpoints_integrated_30min/scalers/scaler_X.pkl')
            joblib.dump(processor.scaler_y, './checkpoints_integrated_30min/scalers/scaler_y.pkl')
            joblib.dump(processor.scaler_physics, './checkpoints_integrated_30min/scalers/scaler_physics.pkl')
            joblib.dump(processor.scaler_features, './checkpoints_integrated_30min/scalers/scaler_features.pkl')
            
            # ìŠ¤ì¼€ì¼ëœ ë°ì´í„° ì €ì¥
            with h5py.File('./checkpoints_integrated_30min/scaled_integrated_data.h5', 'w') as f:
                f.create_dataset('X_train_scaled', data=X_train_scaled, compression='gzip')
                f.create_dataset('X_val_scaled', data=X_val_scaled, compression='gzip')
                f.create_dataset('X_test_scaled', data=X_test_scaled, compression='gzip')
                f.create_dataset('y_train_scaled', data=y_train_scaled, compression='gzip')
                f.create_dataset('y_val_scaled', data=y_val_scaled, compression='gzip')
                f.create_dataset('y_test', data=y_test, compression='gzip')
                f.create_dataset('X_physics_train_scaled', data=X_physics_train_scaled, compression='gzip')
                f.create_dataset('X_physics_val_scaled', data=X_physics_val_scaled, compression='gzip')
                f.create_dataset('X_physics_test_scaled', data=X_physics_test_scaled, compression='gzip')
                f.create_dataset('X_features_train_scaled', data=X_features_train_scaled, compression='gzip')
                f.create_dataset('X_features_val_scaled', data=X_features_val_scaled, compression='gzip')
                f.create_dataset('X_features_test_scaled', data=X_features_test_scaled, compression='gzip')
                f.create_dataset('weights_train', data=weights_train, compression='gzip')
                f.attrs['n_features'] = n_features
            
            state['stage'] = 4
            state['n_features'] = n_features
            ckpt.save_state(state)
            print("âœ… Stage 3 ì™„ë£Œ")
        
        # Stage 4: Base Models í•™ìŠµ
        if stage <= 4:
            print(f"\n[Stage 4/5] Base Models í•™ìŠµ")
            print("-"*60)
            
            with h5py.File('./checkpoints_integrated_30min/scaled_integrated_data.h5', 'r') as f:
                X_train_scaled = f['X_train_scaled'][:]
                X_val_scaled = f['X_val_scaled'][:]
                y_train_scaled = f['y_train_scaled'][:]
                y_val_scaled = f['y_val_scaled'][:]
                X_physics_train_scaled = f['X_physics_train_scaled'][:]
                X_physics_val_scaled = f['X_physics_val_scaled'][:]
                weights_train = f['weights_train'][:]
                n_features = f.attrs['n_features']
            
            processor.scaler_y = joblib.load('./checkpoints_integrated_30min/scalers/scaler_y.pkl')
            
            config = {
                'seq_len': 30,  # 30ë¶„ ì‹œí€€ìŠ¤
                'n_features': n_features,
                'patch_len': 6
            }
            
            # Model 1 í•™ìŠµ
            if not state.get('model1_trained', False):
                print("\nğŸ¤– Model 1: PatchTST (ì•ˆì •í˜•) í•™ìŠµ")
                
                model1 = PatchTSTModel(config)
                
                optimizer1 = Adam(learning_rate=0.0001, clipnorm=1.0)
                
                model1.compile(
                    optimizer=optimizer1,
                    loss=stable_loss,
                    metrics=['mae']
                )
                
                callbacks1 = [
                    EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss'),
                    ReduceLROnPlateau(factor=0.5, patience=8, min_lr=1e-6, monitor='val_loss'),
                    ModelCheckpoint('./checkpoints_integrated_30min/models/model1_stable.weights.h5', 
                                  save_best_only=True, save_weights_only=True, monitor='val_loss')
                ]
                
                history1 = model1.fit(
                    X_train_scaled, y_train_scaled,
                    validation_data=(X_val_scaled, y_val_scaled),
                    epochs=50,
                    batch_size=64,
                    callbacks=callbacks1,
                    verbose=1
                )
                
                state['model1_trained'] = True
                ckpt.save_state(state)
                print("âœ… Model 1 í•™ìŠµ ì™„ë£Œ")
            
            # Model 2 í•™ìŠµ
            if not state.get('model2_trained', False):
                print("\nğŸ¤– Model 2: PatchTST+PINN (ê·¹ë‹¨í˜•) í•™ìŠµ")
                
                model2 = PatchTSTPINN(config)
                
                optimizer2 = Adam(learning_rate=0.0001, clipnorm=1.0)
                
                model2.compile(
                    optimizer=optimizer2,
                    loss=extreme_loss,
                    metrics=['mae']
                )
                
                callbacks2 = [
                    EarlyStopping(patience=20, restore_best_weights=True, monitor='val_loss'),
                    ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-6, monitor='val_loss'),
                    ModelCheckpoint('./checkpoints_integrated_30min/models/model2_extreme.weights.h5', 
                                  save_best_only=True, save_weights_only=True, monitor='val_loss')
                ]
                
                history2 = model2.fit(
                    [X_train_scaled, X_physics_train_scaled], y_train_scaled,
                    validation_data=([X_val_scaled, X_physics_val_scaled], y_val_scaled),
                    epochs=60,
                    batch_size=64,
                    callbacks=callbacks2,
                    verbose=1
                )
                
                state['model2_trained'] = True
                ckpt.save_state(state)
                print("âœ… Model 2 í•™ìŠµ ì™„ë£Œ")
            
            state['stage'] = 5
            ckpt.save_state(state)
            print("âœ… Stage 4 ì™„ë£Œ")
        
        # Stage 5: í†µí•© ëª¨ë¸ í•™ìŠµ
        if stage <= 5:
            print(f"\n[Stage 5/5] í†µí•© ëª¨ë¸ End-to-End í•™ìŠµ")
            print("-"*60)
            
            with h5py.File('./checkpoints_integrated_30min/scaled_integrated_data.h5', 'r') as f:
                X_train_scaled = f['X_train_scaled'][:]
                X_val_scaled = f['X_val_scaled'][:]
                X_test_scaled = f['X_test_scaled'][:]
                y_train_scaled = f['y_train_scaled'][:]
                y_val_scaled = f['y_val_scaled'][:]
                y_test = f['y_test'][:]
                X_physics_train_scaled = f['X_physics_train_scaled'][:]
                X_physics_val_scaled = f['X_physics_val_scaled'][:]
                X_physics_test_scaled = f['X_physics_test_scaled'][:]
                X_features_train_scaled = f['X_features_train_scaled'][:]
                X_features_val_scaled = f['X_features_val_scaled'][:]
                X_features_test_scaled = f['X_features_test_scaled'][:]
                weights_train = f['weights_train'][:]
                n_features = f.attrs['n_features']
            
            processor.scaler_y = joblib.load('./checkpoints_integrated_30min/scalers/scaler_y.pkl')
            
            config = {
                'seq_len': 30,
                'n_features': n_features,
                'patch_len': 6
            }
            
            # í†µí•© ëª¨ë¸ ìƒì„±
            integrated_model = IntegratedV4Model(config)
            
            # Base ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ (ì‚¬ì „ í•™ìŠµëœ) - TensorFlow 2.16.1 í˜¸í™˜
            integrated_model.model1.build(input_shape=(None, 30, n_features))
            integrated_model.model1.load_weights('./checkpoints_integrated_30min/models/model1_stable.weights.h5')
            
            integrated_model.model2.build([(None, 30, n_features), (None, 11)])
            integrated_model.model2.load_weights('./checkpoints_integrated_30min/models/model2_extreme.weights.h5')
            
            # Base ëª¨ë¸ ê°€ì¤‘ì¹˜ ê³ ì • (ì„ íƒì‚¬í•­)
            for layer in integrated_model.model1.layers:
                layer.trainable = False
            for layer in integrated_model.model2.layers:
                layer.trainable = False
            
            print("âœ… Base ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ë° ê³ ì •")
            
            # í†µí•© ëª¨ë¸ ì»´íŒŒì¼
            optimizer_integrated = Adam(learning_rate=0.0005, clipnorm=1.0)
            
            integrated_model.compile(
                optimizer=optimizer_integrated,
                loss=integrated_loss,
                metrics=['mae']
            )
            
            # í†µí•© ëª¨ë¸ í•™ìŠµ
            callbacks_integrated = [
                EarlyStopping(patience=25, restore_best_weights=True, monitor='val_loss'),
                ReduceLROnPlateau(factor=0.7, patience=12, min_lr=1e-6, monitor='val_loss'),
                ModelCheckpoint('./checkpoints_integrated_30min/models/integrated_model.weights.h5', 
                              save_best_only=True, save_weights_only=True, monitor='val_loss'),
                IntegratedCallback([X_val_scaled, X_physics_val_scaled, X_features_val_scaled], 
                                 y_val_scaled, processor.scaler_y, "Integrated")
            ]
            
            print("\nğŸš€ í†µí•© ëª¨ë¸ End-to-End í•™ìŠµ ì‹œì‘")
            
            history_integrated = integrated_model.fit(
                [X_train_scaled, X_physics_train_scaled, X_features_train_scaled], 
                y_train_scaled,
                validation_data=([X_val_scaled, X_physics_val_scaled, X_features_val_scaled], 
                               y_val_scaled),
                sample_weight=weights_train,
                epochs=40,
                batch_size=32,
                callbacks=callbacks_integrated,
                verbose=1
            )
            
            print("âœ… í†µí•© ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            
            # ìµœì¢… í‰ê°€
            print("\nğŸ“Š ìµœì¢… ì„±ëŠ¥ í‰ê°€")
            print("="*60)
            
            # ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜
            integrated_model.training = False
            test_outputs = integrated_model.predict([X_test_scaled, X_physics_test_scaled, X_features_test_scaled], 
                                                   verbose=1)
            
            if isinstance(test_outputs, dict):
                y_pred = processor.scaler_y.inverse_transform(
                    test_outputs['final_pred'].reshape(-1, 1)
                ).flatten()
            else:
                y_pred = processor.scaler_y.inverse_transform(
                    test_outputs.reshape(-1, 1)
                ).flatten()
            
            y_true = y_test
            
            # ì„±ëŠ¥ ì§€í‘œ
            mae = np.mean(np.abs(y_true - y_pred))
            
            # ê·¹ë‹¨ê°’ ì„±ëŠ¥
            mask_300 = y_true >= 300
            extreme_recall = (y_pred[mask_300] >= 300).sum() / max(1, mask_300.sum()) * 100
            
            # ì í”„ ì¼€ì´ìŠ¤ ì„±ëŠ¥
            mask_jump = (y_true >= 300) & (y_true < 350)
            jump_recall = (y_pred[mask_jump] >= 290).sum() / max(1, mask_jump.sum()) * 100
            
            # FP ìœ¨
            mask_stable = y_true < 300
            fp_rate = (y_pred[mask_stable] >= 300).sum() / max(1, mask_stable.sum()) * 100
            
            print(f"\nğŸ“ˆ ìµœì¢… í†µí•© ëª¨ë¸ ì„±ëŠ¥:")
            print(f"  ì „ì²´ MAE: {mae:.2f}")
            print(f"  300+ ê°ì§€ìœ¨: {extreme_recall:.1f}%")
            print(f"  ì í”„ ê°ì§€ìœ¨: {jump_recall:.1f}%") 
            print(f"  FP ìœ¨: {fp_rate:.1f}%")
            
            # ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼
            print(f"\nğŸ“‹ ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ (ì²˜ìŒ 20ê°œ):")
            for i in range(min(20, len(y_true))):
                print(f"[{i+1:2d}] ì‹¤ì œ: {y_true[i]:6.1f}, ì˜ˆì¸¡: {y_pred[i]:6.1f}, "
                      f"ì˜¤ì°¨: {abs(y_true[i] - y_pred[i]):5.1f}")
            
            print("\n" + "="*80)
            print("âœ… V4 ULTIMATE í†µí•© ì‹œìŠ¤í…œ ì™„ë£Œ! (ëª¨ë“  ì˜¤ë¥˜ í•´ê²°)")
            print("ğŸ“Š 30ë¶„ ì‹œí€€ìŠ¤ â†’ 10ë¶„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ êµ¬ì¶• ì„±ê³µ!")
            print("="*80)
            
            remove = input("\nìƒíƒœ íŒŒì¼ì„ ì œê±°í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
            if remove == 'y':
                ckpt.clear_state()
                print("ğŸ§¹ ìƒíƒœ íŒŒì¼ ì œê±° ì™„ë£Œ")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨ ê°ì§€")
        print("ğŸ’¾ ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        print("ğŸ’¾ ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()