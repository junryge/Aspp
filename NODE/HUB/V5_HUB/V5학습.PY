#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ğŸ¯ HUBROOM ì í”„ ê°ì§€ 80% â†’ 81% ì‹œìŠ¤í…œ - ì™„ì „ í†µí•© í•™ìŠµ ì½”ë“œ
================================================================================
ëª¨ë¸ êµ¬ì„±:
1. ê¸°ì¡´ ëª¨ë¸ë“¤ (ExtraTrees, XGBoost, RandomForest)
2. PatchTST (ì•ˆì •í˜•)
3. PatchTST+PINN (ê·¹ë‹¨í˜•)
4. ExtraTrees + ê·œì¹™
5. ExtraTrees + PatchTST í•˜ì´ë¸Œë¦¬ë“œ
================================================================================
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, ExtraTreesRegressor
from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
import joblib
import os
import pickle
import signal
import sys
from datetime import datetime
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("ğŸ¯ HUBROOM ì í”„ ê°ì§€ 80% â†’ 81% ì‹œìŠ¤í…œ")
print("ğŸ“Š ëª¨ë“  ëª¨ë¸ í¬í•¨: ExtraTrees, PatchTST, í•˜ì´ë¸Œë¦¬ë“œ")
print("âœ… ì¤‘ë‹¨/ì¬ê°œ ì§€ì› - Ctrl+Cë¡œ ì–¸ì œë“  ì¤‘ë‹¨ ê°€ëŠ¥!")
print("="*80)

# ==============================================================================
# ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
# ==============================================================================

class CheckpointManager:
    """í•™ìŠµ ì¤‘ë‹¨/ì¬ê°œë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬"""
    
    def __init__(self, checkpoint_dir='./checkpoints_jump80'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        self.state_file = os.path.join(checkpoint_dir, 'training_state.pkl')
        self.data_file = os.path.join(checkpoint_dir, 'processed_data.pkl')
        self.sequences_file = os.path.join(checkpoint_dir, 'sequences.pkl')
        self.models_dir = os.path.join(checkpoint_dir, 'models')
        os.makedirs(self.models_dir, exist_ok=True)
        
        self.interrupted = False
        signal.signal(signal.SIGINT, self._signal_handler)
    
    def _signal_handler(self, sig, frame):
        """Ctrl+C ì‹œê·¸ë„ ì²˜ë¦¬"""
        print('\n\nâš ï¸ ì¤‘ë‹¨ ê°ì§€! í˜„ì¬ ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤...')
        self.interrupted = True
    
    def save_state(self, state):
        """í˜„ì¬ ìƒíƒœ ì €ì¥"""
        with open(self.state_file, 'wb') as f:
            pickle.dump(state, f)
        print(f"ğŸ’¾ ìƒíƒœ ì €ì¥ ì™„ë£Œ: Step {state.get('step', 0)}")
    
    def load_state(self):
        """ì €ì¥ëœ ìƒíƒœ ë¡œë“œ"""
        if os.path.exists(self.state_file):
            with open(self.state_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def save_data(self, data_dict):
        """ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        with open(self.data_file, 'wb') as f:
            pickle.dump(data_dict, f)
        print("ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
    
    def load_data(self):
        """ì €ì¥ëœ ë°ì´í„° ë¡œë“œ"""
        if os.path.exists(self.data_file):
            with open(self.data_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def save_sequences(self, sequences_dict):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ì €ì¥"""
        with open(self.sequences_file, 'wb') as f:
            pickle.dump(sequences_dict, f)
        print("ğŸ’¾ ì‹œí€€ìŠ¤ ì €ì¥ ì™„ë£Œ")
    
    def load_sequences(self):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ë¡œë“œ"""
        if os.path.exists(self.sequences_file):
            with open(self.sequences_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def save_model(self, model, model_name):
        """ëª¨ë¸ ì €ì¥"""
        model_path = os.path.join(self.models_dir, f'{model_name}.pkl')
        joblib.dump(model, model_path)
        print(f"ğŸ’¾ {model_name} ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
    
    def save_keras_model(self, model, model_name):
        """ì¼€ë¼ìŠ¤ ëª¨ë¸ ì €ì¥"""
        model_path = os.path.join(self.models_dir, f'{model_name}.h5')
        model.save_weights(model_path)
        print(f"ğŸ’¾ {model_name} ì¼€ë¼ìŠ¤ ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
    
    def load_model(self, model_name):
        """ëª¨ë¸ ë¡œë“œ"""
        model_path = os.path.join(self.models_dir, f'{model_name}.pkl')
        if os.path.exists(model_path):
            return joblib.load(model_path)
        return None
    
    def clear_state(self):
        """ìƒíƒœ ì´ˆê¸°í™”"""
        if os.path.exists(self.state_file):
            os.remove(self.state_file)
        if os.path.exists(self.data_file):
            os.remove(self.data_file)
        if os.path.exists(self.sequences_file):
            os.remove(self.sequences_file)
        print("ğŸ§¹ ì´ì „ ìƒíƒœ ì œê±° ì™„ë£Œ")

# ==============================================================================
# ğŸ“Š ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤
# ==============================================================================

class HubRoomDataProcessor:
    """ì™„ì „í•œ ë°ì´í„° ì²˜ë¦¬ - ëª¨ë“  íŠ¹ì§• í¬í•¨"""
    
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        
        # 21ê°œ í•„ìˆ˜ ì»¬ëŸ¼
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB',
            'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2',
            'M14B_7F_TO_HUB_JOB2',
            'M16B_10F_TO_HUB_JOB'
        ]
        
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB',
            'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB',
            'M16A_3F_TO_M14B_7F_JOB',
            'M16A_3F_TO_3F_MLUD_JOB'
        ]
        
        self.cmd_cols = [
            'M16A_3F_CMD',
            'M16A_6F_TO_HUB_CMD',
            'M16A_2F_TO_HUB_CMD',
            'M14A_3F_TO_HUB_CMD',
            'M14B_7F_TO_HUB_CMD'
        ]
        
        self.capa_cols = [
            'M16A_6F_LFT_MAXCAPA',
            'M16A_2F_LFT_MAXCAPA'
        ]
        
        self.other_cols = [
            'M16A_3F_STORAGE_UTIL',
            'M14_TO_M16_OFS_CUR',
            'M16_TO_M14_OFS_CUR'
        ]
        
        # í™•ë¥  ë§µ - ë§¤ìš° ì¤‘ìš”!
        self.probability_map = {
            0: 0.003, 1: 0.15, 2: 0.25, 3: 0.31, 4: 0.43, 5: 0.43,
            6: 0.35, 7: 0.42, 8: 0.53, 9: 0.49, 10: 0.42,
            11: 0.47, 12: 0.52, 13: 0.60, 14: 0.54, 15: 0.66,
            16: 0.62, 17: 0.71, 18: 0.79, 19: 0.83, 20: 0.987,
            21: 0.99, 22: 0.99, 23: 0.99, 24: 0.99, 25: 0.99,
            26: 0.99, 27: 0.99, 28: 0.99, 29: 0.99, 30: 0.99
        }
    
    def load_and_merge_data(self):
        """ë°ì´í„° ë¡œë“œ ë° BRIDGE_TIME ë³‘í•©"""
        print("\n[1ë‹¨ê³„] ë°ì´í„° ë¡œë“œ")
        
        # ë©”ì¸ ë°ì´í„°
        df = pd.read_csv('data/HUB_0509_to_0807_DATA.CSV')
        print(f"âœ… ë©”ì¸ ë°ì´í„°: {df.shape}")
        
        # ì‹œê°„ ì²˜ë¦¬
        time_col = df.columns[0]
        df['datetime'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M')
        
        # BRIDGE_TIME ë°ì´í„°
        bridge_df = pd.read_csv('data/BRTIME_0509_TO_0807.CSV')
        print(f"âœ… BRIDGE_TIME ë°ì´í„°: {bridge_df.shape}")
        
        # BRIDGE_TIME ë³‘í•© - ì‹œê°„ëŒ€ ë¬¸ì œ í•´ê²°
        if 'IDC_VAL' in bridge_df.columns:
            bridge_df['BRIDGE_TIME'] = bridge_df['IDC_VAL']
            bridge_df['datetime'] = pd.to_datetime(bridge_df['CRT_TM'])
            
            # ì‹œê°„ëŒ€ ì •ë³´ ì œê±°
            if hasattr(bridge_df['datetime'].dtype, 'tz'):
                bridge_df['datetime'] = bridge_df['datetime'].dt.tz_localize(None)
            if hasattr(df['datetime'].dtype, 'tz'):
                df['datetime'] = df['datetime'].dt.tz_localize(None)
            
            # ë¶„ ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
            bridge_df['datetime'] = bridge_df['datetime'].dt.floor('min')
            df['datetime'] = df['datetime'].dt.floor('min')
            
            # ë³‘í•©
            df = pd.merge(df, bridge_df[['datetime', 'BRIDGE_TIME']], 
                         on='datetime', how='left')
            
            # BRIDGE_TIME ë³´ê°„ ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬
            df['BRIDGE_TIME'] = df['BRIDGE_TIME'].interpolate(method='linear', limit_direction='both')
            df['BRIDGE_TIME'] = df['BRIDGE_TIME'].fillna(3.5)
            
            print(f"âœ… BRIDGE_TIME ë³‘í•© ì™„ë£Œ: {df['BRIDGE_TIME'].notna().sum()}/{len(df)} ë§¤ì¹­")
        
        return df
    
    def create_all_features(self, df):
        """ì™„ì „í•œ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ - ëˆ„ë½ ì—†ìŒ!"""
        print("\n[2ë‹¨ê³„] íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§")
        
        # 1. ìœ ì…/ìœ ì¶œ ë°¸ëŸ°ìŠ¤
        df['flow_balance'] = df[self.inflow_cols].sum(axis=1) - df[self.outflow_cols].sum(axis=1)
        df['flow_ratio'] = df[self.inflow_cols].sum(axis=1) / (df[self.outflow_cols].sum(axis=1) + 1)
        
        # 2. ì¶”ì„¸ íŠ¹ì§•
        df['trend_20min'] = df[self.target_col].diff(20)
        df['trend_10min'] = df[self.target_col].diff(10)
        df['acceleration'] = df['trend_10min'] - df['trend_10min'].shift(10)
        
        # 3. ì—°ì† íŒ¨í„´
        df['consecutive_250+'] = (df[self.target_col] > 250).rolling(10).sum()
        df['consecutive_270+'] = (df[self.target_col] > 270).rolling(10).sum()
        
        # 4. CMD ë™ê¸°í™”
        df['cmd_sync_count'] = (df[self.cmd_cols] > 235).sum(axis=1)
        df['cmd_max'] = df[self.cmd_cols].max(axis=1)
        
        # 5. ë¸Œë¦¿ì§€íƒ€ì„ ë³€í™”
        df['bridge_diff'] = df['BRIDGE_TIME'].diff(5)
        df['bridge_high'] = (df['BRIDGE_TIME'] > 4.0).astype(int)
        
        # 6. storage x bridge ìƒí˜¸ì‘ìš©
        df['storage_x_bridge'] = df['M16A_3F_STORAGE_UTIL'] * df['BRIDGE_TIME']
        
        # 7. ì—°ì† 300+ ì¹´ìš´íŠ¸ì™€ í™•ë¥ 
        consecutive_300_counts = []
        consecutive_300_probs = []
        
        for i in tqdm(range(len(df)), desc="300+ íŒ¨í„´ ê³„ì‚°"):
            if i < 30:
                count = 0
                prob = 0.003
            else:
                window = df[self.target_col].iloc[i-30:i].values
                count = sum(1 for v in window if v >= 300)
                prob = self.probability_map.get(count, 0.5)
            
            consecutive_300_counts.append(count)
            consecutive_300_probs.append(prob)
        
        df['consecutive_300_count'] = consecutive_300_counts
        df['consecutive_300_prob'] = consecutive_300_probs
        
        # 8. 3êµ¬ê°„ ë¶„ë¥˜ (ìˆ«ìë¡œ ì§ì ‘ ë³€í™˜)
        conditions = [
            df[self.target_col] < 150,
            (df[self.target_col] >= 150) & (df[self.target_col] < 300),
            df[self.target_col] >= 300
        ]
        choices = [0, 1, 2]
        df['range_class'] = np.select(conditions, choices, default=1)
        
        # 9. ì í”„ ì—¬ë¶€
        df['past_30min_max'] = df[self.target_col].rolling(30).max()
        df['is_jump'] = ((df['past_30min_max'].shift(10) < 280) & 
                        (df[self.target_col] >= 300)).astype(int)
        
        # 10. ìƒìŠ¹/í•˜ë½ íŒ¨í„´ (ìˆ«ìë¡œ ë³€í™˜)
        df['change_20min'] = df[self.target_col] - df[self.target_col].shift(20)
        
        trend_conditions = [
            df['change_20min'] < -20,
            (df['change_20min'] >= -20) & (df['change_20min'] < 20),
            (df['change_20min'] >= 20) & (df['change_20min'] < 50),
            df['change_20min'] >= 50
        ]
        trend_choices = [0, 1, 2, 3]  # 0:down, 1:stable, 2:gradual_up, 3:rapid_up
        df['trend_pattern'] = np.select(trend_conditions, trend_choices, default=1)
        
        # 11. ìƒìŠ¹ë¥ /í•˜ë½ë¥  (%)
        df['change_rate_10min'] = ((df[self.target_col] - df[self.target_col].shift(10)) / 
                                   (df[self.target_col].shift(10) + 1)) * 100
        df['change_rate_20min'] = ((df[self.target_col] - df[self.target_col].shift(20)) / 
                                   (df[self.target_col].shift(20) + 1)) * 100
        df['change_rate_30min'] = ((df[self.target_col] - df[self.target_col].shift(30)) / 
                                   (df[self.target_col].shift(30) + 1)) * 100
        
        # 12. ë³€ë™ì„±
        df['volatility_10min'] = df[self.target_col].rolling(10).std()
        df['volatility_20min'] = df[self.target_col].rolling(20).std()
        df['volatility_30min'] = df[self.target_col].rolling(30).std()
        
        # 13. ê·¹ë‹¨ê°’ ê·¼ì ‘ë„
        df['distance_to_300'] = 300 - df[self.target_col]
        df['near_extreme'] = (df[self.target_col] > 280).astype(int)
        
        # 14. ìµœê·¼ í†µê³„
        df['recent_5min_mean'] = df[self.target_col].rolling(5).mean()
        df['recent_5min_max'] = df[self.target_col].rolling(5).max()
        df['recent_10min_mean'] = df[self.target_col].rolling(10).mean()
        
        # 15. 277 êµ¬ê°„ íŠ¹ë³„ ì§€í‘œ
        df['in_jump_zone'] = ((df[self.target_col] >= 275) & (df[self.target_col] <= 279)).astype(int)
        
        # NaN ì²˜ë¦¬
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].fillna(method='ffill').fillna(0)
        
        print(f"âœ… ì´ {len(df.columns)}ê°œ íŠ¹ì§• ìƒì„± ì™„ë£Œ")
        return df
    
    def create_sequences(self, df, seq_len=30, pred_len=10, ckpt=None):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± - ì¤‘ë‹¨/ì¬ê°œ ì§€ì›"""
        print(f"\n[3ë‹¨ê³„] ì‹œí€€ìŠ¤ ìƒì„± ({seq_len}ë¶„ â†’ {pred_len}ë¶„ í›„)")
        
        # íŠ¹ì§• ì»¬ëŸ¼ ì„ íƒ
        feature_cols = [col for col in df.columns 
                       if col not in ['datetime', 'range_class', 'is_jump', 'trend_pattern']]
        
        X, y, y_jump, y_range, y_trend, y_value = [], [], [], [], [], []
        
        total_sequences = len(df) - seq_len - pred_len
        
        for i in tqdm(range(seq_len, len(df) - pred_len), desc="ì‹œí€€ìŠ¤ ìƒì„±"):
            if ckpt and ckpt.interrupted:
                print("\nğŸ’¾ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘ë‹¨! í˜„ì¬ê¹Œì§€ ì €ì¥...")
                sequences_dict = {
                    'X': np.array(X),
                    'y': np.array(y),
                    'y_jump': np.array(y_jump),
                    'y_range': np.array(y_range),
                    'y_trend': np.array(y_trend),
                    'y_value': np.array(y_value),
                    'progress': i,
                    'feature_cols': feature_cols
                }
                ckpt.save_sequences(sequences_dict)
                sys.exit(0)
            
            # ì…ë ¥: ê³¼ê±° 30ë¶„
            X.append(df[feature_cols].iloc[i-seq_len:i].values)
            
            # íƒ€ê²Ÿë“¤
            target_idx = i + pred_len - 1
            y.append(df[self.target_col].iloc[target_idx])
            y_jump.append(df['is_jump'].iloc[target_idx])
            y_range.append(df['range_class'].iloc[target_idx])
            y_trend.append(df['trend_pattern'].iloc[target_idx])
            y_value.append(df[self.target_col].iloc[target_idx])
        
        print(f"âœ… {len(X)}ê°œ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        return (np.array(X), np.array(y), np.array(y_jump), 
                np.array(y_range), np.array(y_trend), np.array(y_value), feature_cols)

# ==============================================================================
# ğŸ¤– PatchTST ëª¨ë¸ ì •ì˜
# ==============================================================================

class PatchTSTModel(keras.Model):
    """ì•ˆì • êµ¬ê°„ ì „ë¬¸ PatchTST"""
    
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(128, activation='relu')
        
        # Transformer ë¸”ë¡
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm1 = layers.LayerNormalization()
        self.norm2 = layers.LayerNormalization()
        
        # Feed Forward
        self.ffn = keras.Sequential([
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(128)
        ])
        
        # ì¶œë ¥ì¸µ
        self.flatten = layers.Flatten()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dropout = layers.Dropout(0.3)
        self.dense2 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(1)
    
    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        
        # íŒ¨ì¹˜í™”
        x = tf.reshape(x, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        
        # Transformer
        attn = self.attention(x, x, training=training)
        x = self.norm1(x + attn)
        
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        
        # ì¶œë ¥
        x = self.flatten(x)
        x = self.dense1(x)
        x = self.dropout(x, training=training)
        x = self.dense2(x)
        output = self.output_layer(x)
        
        return tf.squeeze(output, axis=-1)

class PatchTSTPINN(keras.Model):
    """ê·¹ë‹¨ê°’ ì „ë¬¸ PatchTST + PINN"""
    
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # PatchTST ë¶€ë¶„
        self.patch_embedding = layers.Dense(128, activation='relu')
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm = layers.LayerNormalization()
        
        self.flatten = layers.Flatten()
        self.temporal_dense = layers.Dense(64, activation='relu')
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬
        self.physics_net = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.BatchNormalization(),
            layers.Dense(32, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(16, activation='relu')
        ])
        
        # ìœµí•©
        self.fusion = keras.Sequential([
            layers.Dense(128, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(64, activation='relu'),
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(1)
        ])
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŠ¸
        self.extreme_boost = layers.Dense(1, activation='sigmoid')
    
    def call(self, inputs, training=False):
        x_seq, x_physics = inputs
        batch_size = tf.shape(x_seq)[0]
        
        # PatchTST ì²˜ë¦¬
        x = tf.reshape(x_seq, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        
        attn = self.attention(x, x, training=training)
        x = self.norm(x + attn)
        
        x = self.flatten(x)
        x_temporal = self.temporal_dense(x)
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬
        x_physics = self.physics_net(x_physics)
        
        # ìœµí•©
        x_combined = tf.concat([x_temporal, x_physics], axis=-1)
        output = self.fusion(x_combined)
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ…
        boost_factor = self.extreme_boost(x_physics)
        output = output * (1 + boost_factor * 0.1)
        
        return tf.squeeze(output, axis=-1)

# ==============================================================================
# ğŸ¤– ëª¨ë¸ ì‹œìŠ¤í…œ (ê¸°ì¡´ + ì‹ ê·œ)
# ==============================================================================

class JumpDetectionSystem:
    """ì í”„ ê°ì§€ 80% ë‹¬ì„± ì‹œìŠ¤í…œ - ëª¨ë“  ëª¨ë¸ í¬í•¨"""
    
    def __init__(self):
        # ê¸°ì¡´ ëª¨ë¸ë“¤
        self.model_screening = ExtraTreesClassifier(
            n_estimators=500,
            max_depth=20,
            min_samples_split=5,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        )
        
        self.model_jump = XGBClassifier(
            n_estimators=300,
            max_depth=10,
            learning_rate=0.1,
            scale_pos_weight=100,
            random_state=42
        )
        
        self.model_range = RandomForestClassifier(
            n_estimators=300,
            max_depth=15,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        )
        
        self.model_trend = RandomForestClassifier(
            n_estimators=200,
            max_depth=10,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        )
        
        self.model_value = ExtraTreesRegressor(
            n_estimators=500,
            max_depth=20,
            random_state=42,
            n_jobs=-1
        )
        
        # ExtraTrees + ê·œì¹™ (ì í”„ ì „ìš©)
        self.model_extra_rules = ExtraTreesClassifier(
            n_estimators=700,
            max_depth=25,
            min_samples_split=3,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        )
        
        # PatchTST ëª¨ë¸ë“¤
        self.model_patchtst = None
        self.model_patchtst_pinn = None
        
        # ìŠ¤ì¼€ì¼ëŸ¬
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.scaler_physics = StandardScaler()
        
        self.feature_names = None
        self.feature_indices = {}
    
    def build_patchtst_models(self, n_features):
        """PatchTST ëª¨ë¸ ìƒì„±"""
        config = {
            'seq_len': 30,
            'n_features': n_features,
            'patch_len': 6  # 5ê°œ íŒ¨ì¹˜
        }
        
        # ì•ˆì •í˜• ëª¨ë¸
        self.model_patchtst = PatchTSTModel(config)
        
        # ê·¹ë‹¨í˜• ëª¨ë¸
        self.model_patchtst_pinn = PatchTSTPINN(config)
        
        print("âœ… PatchTST ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    
    def prepare_features(self, X_seq):
        """ì‹œí€€ìŠ¤ë¥¼ íŠ¹ì§•ìœ¼ë¡œ ë³€í™˜"""
        # ë§ˆì§€ë§‰ ì‹œì  íŠ¹ì§•
        last_features = X_seq[:, -1, :]
        
        # í†µê³„ íŠ¹ì§•
        mean_features = np.mean(X_seq, axis=1)
        std_features = np.std(X_seq, axis=1)
        max_features = np.max(X_seq, axis=1)
        min_features = np.min(X_seq, axis=1)
        
        # ì¶”ì„¸ íŠ¹ì§•
        trend_features = X_seq[:, -1, :] - X_seq[:, 0, :]
        
        # ëª¨ë“  íŠ¹ì§• ê²°í•©
        features = np.hstack([
            last_features,
            mean_features,
            std_features,
            max_features,
            min_features,
            trend_features
        ])
        
        return features
    
    def create_physics_features(self, X_seq, feature_cols):
        """ë¬¼ë¦¬ íŠ¹ì§• ìƒì„± (PatchTST-PINNìš©)"""
        # íŠ¹ì§• ì¸ë±ìŠ¤ ì°¾ê¸°
        idx_storage = None
        idx_bridge = None
        idx_flow_balance = None
        
        for i, col in enumerate(feature_cols):
            if 'M16A_3F_STORAGE_UTIL' in col:
                idx_storage = i
            elif col == 'BRIDGE_TIME':
                idx_bridge = i
            elif 'flow_balance' in col:
                idx_flow_balance = i
        
        physics_features = []
        
        for seq in X_seq:
            features = []
            
            # ë§ˆì§€ë§‰ ì‹œì  ê°’ë“¤
            if idx_storage is not None:
                features.append(seq[-1, idx_storage])
            else:
                features.append(0)
                
            if idx_bridge is not None:
                features.append(seq[-1, idx_bridge])
            else:
                features.append(3.5)
                
            if idx_flow_balance is not None:
                features.append(seq[-1, idx_flow_balance])
            else:
                features.append(0)
            
            # ì¶”ì„¸
            features.append(np.mean(seq[-10:, 0]) - np.mean(seq[:10, 0]))  # target ì¶”ì„¸
            features.append(np.std(seq[:, 0]))  # ë³€ë™ì„±
            
            physics_features.append(features)
        
        return np.array(physics_features)
    
    def get_feature_indices(self, feature_cols):
        """ì¤‘ìš” íŠ¹ì§•ë“¤ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°"""
        indices = {}
        for i, col in enumerate(feature_cols):
            if 'M16A_3F_STORAGE_UTIL' in col:
                indices['storage_util'] = i
            elif col == 'BRIDGE_TIME':
                indices['bridge_time'] = i
            elif 'flow_balance' in col:
                indices['flow_balance'] = i
            elif 'consecutive_250+' in col:
                indices['consecutive_250'] = i
            elif 'cmd_sync_count' in col:
                indices['cmd_sync'] = i
            elif 'trend_20min' in col:
                indices['trend_20min'] = i
            elif 'flow_ratio' in col:
                indices['flow_ratio'] = i
            elif 'acceleration' in col:
                indices['acceleration'] = i
            elif 'cmd_max' in col:
                indices['cmd_max'] = i
            elif 'bridge_high' in col:
                indices['bridge_high'] = i
            elif 'consecutive_300_prob' in col:
                indices['prob_extreme'] = i
        
        self.feature_indices = indices
        return indices
    
    def train_jump_detector(self, X, y_jump):
        """ê¸°ì¡´ ì í”„ ê°ì§€ ëª¨ë¸ í•™ìŠµ"""
        print("\nğŸ¯ ì í”„ ê°ì§€ ëª¨ë¸ í•™ìŠµ (XGBoost)")
        
        # SMOTEë¡œ ë¶ˆê· í˜• ì²˜ë¦¬
        jump_count = np.sum(y_jump)
        print(f"  ì í”„ ì¼€ì´ìŠ¤: {jump_count}/{len(y_jump)} ({jump_count/len(y_jump)*100:.2f}%)")
        
        smote = SMOTE(sampling_strategy=0.2, random_state=42)
        X_balanced, y_balanced = smote.fit_resample(X, y_jump)
        
        print(f"  SMOTE í›„: {np.sum(y_balanced)}/{len(y_balanced)} ({np.sum(y_balanced)/len(y_balanced)*100:.2f}%)")
        
        # í•™ìŠµ
        self.model_jump.fit(X_balanced, y_balanced)
        
        return self.model_jump
    
    def train_extra_rules_jump(self, X, y_jump):
        """ExtraTrees + ê·œì¹™ ì í”„ ëª¨ë¸ í•™ìŠµ"""
        print("\nğŸŒ³ ExtraTrees + ê·œì¹™ ì í”„ ëª¨ë¸ í•™ìŠµ")
        
        # ë” ê³µê²©ì ì¸ SMOTE
        smote = SMOTE(sampling_strategy=0.3, random_state=42)
        X_balanced, y_balanced = smote.fit_resample(X, y_jump)
        
        # í•™ìŠµ
        self.model_extra_rules.fit(X_balanced, y_balanced)
        
        # íŠ¹ì§• ì¤‘ìš”ë„
        importance = pd.DataFrame({
            'feature_idx': range(X.shape[1]),
            'importance': self.model_extra_rules.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\nâœ… ExtraTrees ìƒìœ„ 10ê°œ íŠ¹ì§•:")
        for idx, row in importance.head(10).iterrows():
            print(f"  íŠ¹ì§• {int(row['feature_idx'])}: {row['importance']:.4f}")
        
        return self.model_extra_rules
    
    def apply_rule_based_boost(self, X, predictions, prob_scores=None):
        """ê·œì¹™ ê¸°ë°˜ ë¶€ìŠ¤íŒ… - 80% ë‹¬ì„±ìš©"""
        boosted_predictions = predictions.copy()
        
        if not self.feature_indices:
            return boosted_predictions
        
        idx = self.feature_indices
        
        # Phase 1: ê°•í•œ ì‹ í˜¸ (55%)
        if 'storage_util' in idx:
            strong_signal = X[:, idx['storage_util']] > 20
            boosted_predictions[strong_signal] = 1
        
        # Phase 2: ì¤‘ê°„ ì‹ í˜¸ (70%)
        medium_conditions = []
        
        if 'storage_util' in idx:
            medium_conditions.append(X[:, idx['storage_util']] > 10)
        if 'flow_balance' in idx:
            medium_conditions.append(X[:, idx['flow_balance']] > 50)
        if 'bridge_time' in idx:
            medium_conditions.append(X[:, idx['bridge_time']] > 4.0)
        if 'consecutive_250' in idx:
            medium_conditions.append(X[:, idx['consecutive_250']] >= 5)
        if 'cmd_sync' in idx:
            medium_conditions.append(X[:, idx['cmd_sync']] >= 3)
        if 'trend_20min' in idx:
            medium_conditions.append(X[:, idx['trend_20min']] > 30)
        
        if len(medium_conditions) >= 2:
            medium_signal = np.sum(medium_conditions, axis=0) >= 2
            if 'storage_util' in idx:
                storage_medium = X[:, idx['storage_util']] > 10
                final_medium = medium_signal & storage_medium & (boosted_predictions == 0)
                boosted_predictions[final_medium] = 1
        
        # Phase 3: ì•½í•œ ì‹ í˜¸ (80%)
        weak_conditions = []
        
        if 'bridge_time' in idx:
            weak_conditions.append(X[:, idx['bridge_time']] > 3.85)
        if 'flow_ratio' in idx:
            weak_conditions.append(X[:, idx['flow_ratio']] > 1.5)
        if 'acceleration' in idx:
            weak_conditions.append(X[:, idx['acceleration']] > 20)
        if 'cmd_max' in idx:
            weak_conditions.append(X[:, idx['cmd_max']] > 238)
        if 'consecutive_250' in idx:
            weak_conditions.append(X[:, idx['consecutive_250']] >= 3)
        
        if len(weak_conditions) >= 3:
            weak_signal = np.sum(weak_conditions, axis=0) >= 3
            final_weak = weak_signal & (boosted_predictions == 0)
            
            if prob_scores is not None:
                final_weak = final_weak & (prob_scores > 0.3)
            
            boosted_predictions[final_weak] = 1
        
        return boosted_predictions

# ==============================================================================
# ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡ê¸°
# ==============================================================================

class HybridJumpPredictor:
    """ExtraTrees + PatchTST í•˜ì´ë¸Œë¦¬ë“œ - 81% ëª©í‘œ"""
    
    def __init__(self, system):
        self.system = system
    
    def predict(self, X_seq, X_features, X_physics=None):
        """í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡"""
        predictions = []
        
        # 1. ExtraTrees í™•ë¥ 
        if hasattr(self.system.model_extra_rules, 'predict_proba'):
            et_proba = self.system.model_extra_rules.predict_proba(X_features)[:, 1]
        else:
            et_proba = self.system.model_extra_rules.predict(X_features)
        
        for i in range(len(X_seq)):
            # ExtraTrees í™•ë¥  í™•ì¸
            if et_proba[i] > 0.8:
                # í™•ì‹¤í•œ ì í”„
                predictions.append(1)
            elif et_proba[i] < 0.2:
                # í™•ì‹¤í•œ ë¹„ì í”„
                predictions.append(0)
            else:
                # ì• ë§¤í•œ ê²½ìš° - PatchTST ì‚¬ìš©
                if self.system.model_patchtst_pinn is not None and X_physics is not None:
                    # ê·¹ë‹¨í˜• ëª¨ë¸ë¡œ ì •ë°€ ì˜ˆì¸¡
                    x_seq_single = X_seq[i:i+1]
                    x_physics_single = X_physics[i:i+1]
                    
                    # ìŠ¤ì¼€ì¼ë§
                    x_seq_scaled = self.system.scaler_X.transform(
                        x_seq_single.reshape(-1, x_seq_single.shape[-1])
                    ).reshape(x_seq_single.shape)
                    x_physics_scaled = self.system.scaler_physics.transform(x_physics_single)
                    
                    # ì˜ˆì¸¡
                    pred_value = self.system.model_patchtst_pinn.predict(
                        [x_seq_scaled, x_physics_scaled], 
                        verbose=0
                    )[0]
                    
                    # 300+ ì˜ˆì¸¡ì´ë©´ ì í”„
                    if pred_value > 300:
                        predictions.append(1)
                    else:
                        predictions.append(0)
                else:
                    # PatchTST ì—†ìœ¼ë©´ í™•ë¥  ê¸°ë°˜
                    predictions.append(1 if et_proba[i] > 0.5 else 0)
        
        return np.array(predictions)

# ==============================================================================
# ğŸ“Š í‰ê°€ í•¨ìˆ˜
# ==============================================================================

def evaluate_jump_detection(y_true, y_pred, model_name="Model"):
    """ì í”„ ê°ì§€ ì„±ëŠ¥ í‰ê°€"""
    print(f"\nğŸ“Š {model_name} ì í”„ ê°ì§€ ì„±ëŠ¥")
    print("-" * 50)
    
    total = len(y_true)
    jump_true = np.sum(y_true)
    jump_pred = np.sum(y_pred)
    
    print(f"ì „ì²´ ìƒ˜í”Œ: {total}")
    print(f"ì‹¤ì œ ì í”„: {jump_true} ({jump_true/total*100:.2f}%)")
    print(f"ì˜ˆì¸¡ ì í”„: {jump_pred} ({jump_pred/total*100:.2f}%)")
    
    # ì í”„ ê°ì§€ìœ¨ (Recall)
    if jump_true > 0:
        tp = np.sum((y_true == 1) & (y_pred == 1))
        recall = tp / jump_true
        print(f"\nğŸ¯ ì í”„ ê°ì§€ìœ¨: {recall*100:.1f}% ({tp}/{jump_true})")
    else:
        recall = 0
    
    # False Positive Rate
    non_jump_true = total - jump_true
    if non_jump_true > 0:
        fp = np.sum((y_true == 0) & (y_pred == 1))
        fpr = fp / non_jump_true
        print(f"False Positive Rate: {fpr*100:.1f}% ({fp}/{non_jump_true})")
    else:
        fpr = 0
    
    # ì •í™•ë„
    accuracy = np.sum(y_true == y_pred) / total
    print(f"\nì „ì²´ ì •í™•ë„: {accuracy*100:.1f}%")
    
    return {
        'recall': recall,
        'fpr': fpr,
        'accuracy': accuracy
    }

# ==============================================================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰
# ==============================================================================

def main():
    """ë©”ì¸ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ - ì¤‘ë‹¨/ì¬ê°œ ì§€ì›"""
    
    # ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    ckpt = CheckpointManager()
    
    # ì´ì „ ìƒíƒœ í™•ì¸
    state = ckpt.load_state()
    if state:
        print(f"\nğŸ“‚ ì´ì „ í•™ìŠµ ìƒíƒœ ë°œê²¬! (Step {state.get('step', 1)}/10)")
        resume = input("ì´ì–´ì„œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
        
        if resume != 'y':
            ckpt.clear_state()
            state = {}
            step = 1
        else:
            step = state.get('step', 1)
            print(f"âœ… Step {step}ë¶€í„° ì¬ê°œí•©ë‹ˆë‹¤.")
    else:
        state = {}
        step = 1
    
    processor = HubRoomDataProcessor()
    system = JumpDetectionSystem()
    
    try:
        # Step 1: ë°ì´í„° ë¡œë“œ
        if step <= 1:
            print(f"\n[Step 1/10] ë°ì´í„° ë¡œë“œ ë° ë³‘í•©")
            print("-"*60)
            
            df = processor.load_and_merge_data()
            
            state['step'] = 2
            state['data_shape'] = df.shape
            ckpt.save_state(state)
            
            if ckpt.interrupted:
                print("\nğŸ’¾ Step 1 ì €ì¥ í›„ ì¢…ë£Œ")
                sys.exit(0)
        
        # Step 2: íŠ¹ì§• ìƒì„±
        if step <= 2:
            print(f"\n[Step 2/10] íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§")
            print("-"*60)
            
            if step < 2:
                df = processor.load_and_merge_data()
            
            df = processor.create_all_features(df)
            
            # ë°ì´í„° ì €ì¥
            ckpt.save_data({'df': df})
            
            state['step'] = 3
            state['feature_count'] = len(df.columns)
            ckpt.save_state(state)
            
            if ckpt.interrupted:
                print("\nğŸ’¾ Step 2 ì €ì¥ í›„ ì¢…ë£Œ")
                sys.exit(0)
        
        # Step 3: ì‹œí€€ìŠ¤ ìƒì„±
        if step <= 3:
            print(f"\n[Step 3/10] ì‹œí€€ìŠ¤ ìƒì„±")
            print("-"*60)
            
            # ë°ì´í„° ë¡œë“œ
            saved_data = ckpt.load_data()
            if saved_data:
                df = saved_data['df']
            else:
                df = processor.load_and_merge_data()
                df = processor.create_all_features(df)
            
            # ì‹œí€€ìŠ¤ ìƒì„±
            X, y, y_jump, y_range, y_trend, y_value, feature_cols = processor.create_sequences(df, ckpt=ckpt)
            
            # ì‹œí€€ìŠ¤ ì €ì¥
            sequences_dict = {
                'X': X,
                'y': y,
                'y_jump': y_jump,
                'y_range': y_range,
                'y_trend': y_trend,
                'y_value': y_value,
                'feature_cols': feature_cols,
                'progress': len(X)
            }
            ckpt.save_sequences(sequences_dict)
            
            # íŠ¹ì§• ì¸ë±ìŠ¤ ì €ì¥
            system.get_feature_indices(feature_cols)
            
            state['step'] = 4
            state['sequence_shape'] = X.shape
            state['n_features'] = X.shape[2]
            ckpt.save_state(state)
            
            if ckpt.interrupted:
                print("\nğŸ’¾ Step 3 ì €ì¥ í›„ ì¢…ë£Œ")
                sys.exit(0)
        
        # Step 4: ë°ì´í„° ë¶„í• 
        if step <= 4:
            print(f"\n[Step 4/10] í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• ")
            print("-"*60)
            
            # ì‹œí€€ìŠ¤ ë¡œë“œ
            saved_seq = ckpt.load_sequences()
            X = saved_seq['X']
            y = saved_seq['y']
            y_jump = saved_seq['y_jump']
            y_range = saved_seq['y_range']
            y_trend = saved_seq['y_trend']
            y_value = saved_seq['y_value']
            feature_cols = saved_seq['feature_cols']
            
            # ë°ì´í„° ë¶„í• 
            split_idx = int(0.8 * len(X))
            
            X_train, X_val = X[:split_idx], X[split_idx:]
            y_train, y_val = y[:split_idx], y[split_idx:]
            y_jump_train, y_jump_val = y_jump[:split_idx], y_jump[split_idx:]
            y_range_train, y_range_val = y_range[:split_idx], y_range[split_idx:]
            y_trend_train, y_trend_val = y_trend[:split_idx], y_trend[split_idx:]
            y_value_train, y_value_val = y_value[:split_idx], y_value[split_idx:]
            
            print(f"âœ… í•™ìŠµ ë°ì´í„°: {len(X_train)}")
            print(f"âœ… ê²€ì¦ ë°ì´í„°: {len(X_val)}")
            
            state['step'] = 5
            state['split_idx'] = split_idx
            ckpt.save_state(state)
            
            if ckpt.interrupted:
                print("\nğŸ’¾ Step 4 ì €ì¥ í›„ ì¢…ë£Œ")
                sys.exit(0)
        
        # Step 5: íŠ¹ì§• ì¤€ë¹„ ë° ìŠ¤ì¼€ì¼ë§
        if step <= 5:
            print(f"\n[Step 5/10] íŠ¹ì§• ì¤€ë¹„ ë° ìŠ¤ì¼€ì¼ë§")
            print("-"*60)
            
            # ë°ì´í„° ì¬ë¡œë“œ
            saved_seq = ckpt.load_sequences()
            X = saved_seq['X']
            y_value = saved_seq['y_value']
            feature_cols = saved_seq['feature_cols']
            split_idx = state.get('split_idx', int(0.8 * len(X)))
            
            X_train, X_val = X[:split_idx], X[split_idx:]
            y_value_train, y_value_val = y_value[:split_idx], y_value[split_idx:]
            
            # íŠ¹ì§• ì¤€ë¹„
            X_train_features = system.prepare_features(X_train)
            X_val_features = system.prepare_features(X_val)
            
            # ë¬¼ë¦¬ íŠ¹ì§• (PatchTST-PINNìš©)
            X_physics_train = system.create_physics_features(X_train, feature_cols)
            X_physics_val = system.create_physics_features(X_val, feature_cols)
            
            # ìŠ¤ì¼€ì¼ë§
            # ì‹œí€€ìŠ¤ ë°ì´í„°
            X_train_scaled = system.scaler_X.fit_transform(
                X_train.reshape(-1, X_train.shape[-1])
            ).reshape(X_train.shape)
            X_val_scaled = system.scaler_X.transform(
                X_val.reshape(-1, X_val.shape[-1])
            ).reshape(X_val.shape)
            
            # íƒ€ê²Ÿ
            y_value_train_scaled = system.scaler_y.fit_transform(y_value_train.reshape(-1, 1)).flatten()
            y_value_val_scaled = system.scaler_y.transform(y_value_val.reshape(-1, 1)).flatten()
            
            # ë¬¼ë¦¬ íŠ¹ì§•
            X_physics_train_scaled = system.scaler_physics.fit_transform(X_physics_train)
            X_physics_val_scaled = system.scaler_physics.transform(X_physics_val)
            
            print(f"âœ… í•™ìŠµ íŠ¹ì§• shape: {X_train_features.shape}")
            print(f"âœ… ê²€ì¦ íŠ¹ì§• shape: {X_val_features.shape}")
            print(f"âœ… ë¬¼ë¦¬ íŠ¹ì§• shape: {X_physics_train.shape}")
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
            joblib.dump(system.scaler_X, ckpt.models_dir + '/scaler_X.pkl')
            joblib.dump(system.scaler_y, ckpt.models_dir + '/scaler_y.pkl')
            joblib.dump(system.scaler_physics, ckpt.models_dir + '/scaler_physics.pkl')
            
            # ì¸ë±ìŠ¤ ì„¤ì •
            system.get_feature_indices(feature_cols)
            
            state['step'] = 6
            state['n_features'] = X.shape[2]
            ckpt.save_state(state)
            
            if ckpt.interrupted:
                print("\nğŸ’¾ Step 5 ì €ì¥ í›„ ì¢…ë£Œ")
                sys.exit(0)
        
        # Step 6: ê¸°ì¡´ ëª¨ë¸ í•™ìŠµ
        if step <= 6:
            print(f"\n[Step 6/10] ê¸°ì¡´ ëª¨ë¸ í•™ìŠµ")
            print("-"*60)
            
            # ë°ì´í„° ì¬ë¡œë“œ
            saved_seq = ckpt.load_sequences()
            X = saved_seq['X']
            y_jump = saved_seq['y_jump']
            y_range = saved_seq['y_range']
            y_trend = saved_seq['y_trend']
            y_value = saved_seq['y_value']
            feature_cols = saved_seq['feature_cols']
            
            split_idx = state.get('split_idx', int(0.8 * len(X)))
            
            X_train = X[:split_idx]
            y_jump_train = y_jump[:split_idx]
            y_range_train = y_range[:split_idx]
            y_trend_train = y_trend[:split_idx]
            y_value_train = y_value[:split_idx]
            
            # íŠ¹ì§• ì¤€ë¹„
            X_train_features = system.prepare_features(X_train)
            
            # íŠ¹ì§• ì¸ë±ìŠ¤ ì„¤ì •
            system.get_feature_indices(feature_cols)
            
            # 6-1. XGBoost ì í”„ ëª¨ë¸
            if not state.get('jump_trained', False):
                system.train_jump_detector(X_train_features, y_jump_train)
                ckpt.save_model(system.model_jump, 'model_jump')
                state['jump_trained'] = True
                ckpt.save_state(state)
                
                if ckpt.interrupted:
                    sys.exit(0)
            
            # 6-2. 3êµ¬ê°„ ë¶„ë¥˜
            if not state.get('range_trained', False):
                print("\nğŸ“Š 3êµ¬ê°„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ")
                system.model_range.fit(X_train_features, y_range_train)
                ckpt.save_model(system.model_range, 'model_range')
                state['range_trained'] = True
                ckpt.save_state(state)
                
                if ckpt.interrupted:
                    sys.exit(0)
            
            # 6-3. ìƒìŠ¹/í•˜ë½ íŒ¨í„´
            if not state.get('trend_trained', False):
                print("\nğŸ“ˆ ìƒìŠ¹/í•˜ë½ íŒ¨í„´ ëª¨ë¸ í•™ìŠµ")
                system.model_trend.fit(X_train_features, y_trend_train)
                ckpt.save_model(system.model_trend, 'model_trend')
                state['trend_trained'] = True
                ckpt.save_state(state)
            
            # 6-4. ê°’ ì˜ˆì¸¡
            if not state.get('value_trained', False):
                print("\nğŸ’° ê°’ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ")
                system.model_value.fit(X_train_features, y_value_train)
                ckpt.save_model(system.model_value, 'model_value')
                state['value_trained'] = True
                ckpt.save_state(state)
            
            state['step'] = 7
            ckpt.save_state(state)
            
            if ckpt.interrupted:
                print("\nğŸ’¾ ê¸°ì¡´ ëª¨ë¸ ì €ì¥ í›„ ì¢…ë£Œ")
                sys.exit(0)
        
        # Step 7: ExtraTrees + ê·œì¹™ ëª¨ë¸
        if step <= 7:
            print(f"\n[Step 7/10] ExtraTrees + ê·œì¹™ ëª¨ë¸ í•™ìŠµ")
            print("-"*60)
            
            # ë°ì´í„° ì¬ë¡œë“œ
            saved_seq = ckpt.load_sequences()
            X = saved_seq['X']
            y_jump = saved_seq['y_jump']
            feature_cols = saved_seq['feature_cols']
            
            split_idx = state.get('split_idx', int(0.8 * len(X)))
            X_train = X[:split_idx]
            y_jump_train = y_jump[:split_idx]
            
            # íŠ¹ì§• ì¤€ë¹„
            X_train_features = system.prepare_features(X_train)
            system.get_feature_indices(feature_cols)
            
            if not state.get('extra_rules_trained', False):
                system.train_extra_rules_jump(X_train_features, y_jump_train)
                ckpt.save_model(system.model_extra_rules, 'model_extra_rules')
                state['extra_rules_trained'] = True
                ckpt.save_state(state)
            
            state['step'] = 8
            ckpt.save_state(state)
            
            if ckpt.interrupted:
                print("\nğŸ’¾ ExtraTrees ëª¨ë¸ ì €ì¥ í›„ ì¢…ë£Œ")
                sys.exit(0)
        
        # Step 8: PatchTST ëª¨ë¸ í•™ìŠµ
        if step <= 8:
            print(f"\n[Step 8/10] PatchTST ëª¨ë¸ í•™ìŠµ")
            print("-"*60)
            
            # ë°ì´í„° ì¬ë¡œë“œ
            saved_seq = ckpt.load_sequences()
            X = saved_seq['X']
            y_value = saved_seq['y_value']
            feature_cols = saved_seq['feature_cols']
            n_features = state.get('n_features', X.shape[2])
            
            split_idx = state.get('split_idx', int(0.8 * len(X)))
            
            X_train, X_val = X[:split_idx], X[split_idx:]
            y_value_train, y_value_val = y_value[:split_idx], y_value[split_idx:]
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            system.scaler_X = joblib.load(ckpt.models_dir + '/scaler_X.pkl')
            system.scaler_y = joblib.load(ckpt.models_dir + '/scaler_y.pkl')
            system.scaler_physics = joblib.load(ckpt.models_dir + '/scaler_physics.pkl')
            
            # ìŠ¤ì¼€ì¼ë§
            X_train_scaled = system.scaler_X.transform(
                X_train.reshape(-1, X_train.shape[-1])
            ).reshape(X_train.shape)
            X_val_scaled = system.scaler_X.transform(
                X_val.reshape(-1, X_val.shape[-1])
            ).reshape(X_val.shape)
            
            y_value_train_scaled = system.scaler_y.transform(y_value_train.reshape(-1, 1)).flatten()
            y_value_val_scaled = system.scaler_y.transform(y_value_val.reshape(-1, 1)).flatten()
            
            # PatchTST ëª¨ë¸ ìƒì„±
            if system.model_patchtst is None:
                system.build_patchtst_models(n_features)
            
            # 8-1. PatchTST ì•ˆì •í˜•
            if not state.get('patchtst_trained', False):
                print("\nğŸ”· PatchTST ì•ˆì •í˜• ëª¨ë¸ í•™ìŠµ")
                
                system.model_patchtst.compile(
                    optimizer=Adam(learning_rate=0.001),
                    loss='mse',
                    metrics=['mae']
                )
                
                history_stable = system.model_patchtst.fit(
                    X_train_scaled, y_value_train_scaled,
                    validation_data=(X_val_scaled, y_value_val_scaled),
                    epochs=30,
                    batch_size=32,
                    callbacks=[
                        EarlyStopping(patience=5, restore_best_weights=True),
                        ReduceLROnPlateau(factor=0.5, patience=3)
                    ],
                    verbose=1
                )
                
                ckpt.save_keras_model(system.model_patchtst, 'model_patchtst')
                state['patchtst_trained'] = True
                ckpt.save_state(state)
                
                if ckpt.interrupted:
                    sys.exit(0)
            
            # 8-2. PatchTST-PINN ê·¹ë‹¨í˜•
            if not state.get('patchtst_pinn_trained', False):
                print("\nğŸ”¶ PatchTST-PINN ê·¹ë‹¨í˜• ëª¨ë¸ í•™ìŠµ")
                
                # ë¬¼ë¦¬ íŠ¹ì§•
                X_physics_train = system.create_physics_features(X_train, feature_cols)
                X_physics_val = system.create_physics_features(X_val, feature_cols)
                
                X_physics_train_scaled = system.scaler_physics.transform(X_physics_train)
                X_physics_val_scaled = system.scaler_physics.transform(X_physics_val)
                
                system.model_patchtst_pinn.compile(
                    optimizer=Adam(learning_rate=0.001),
                    loss='mse',
                    metrics=['mae']
                )
                
                history_extreme = system.model_patchtst_pinn.fit(
                    [X_train_scaled, X_physics_train_scaled], 
                    y_value_train_scaled,
                    validation_data=(
                        [X_val_scaled, X_physics_val_scaled], 
                        y_value_val_scaled
                    ),
                    epochs=40,
                    batch_size=32,
                    callbacks=[
                        EarlyStopping(patience=7, restore_best_weights=True),
                        ReduceLROnPlateau(factor=0.5, patience=4)
                    ],
                    verbose=1
                )
                
                ckpt.save_keras_model(system.model_patchtst_pinn, 'model_patchtst_pinn')
                state['patchtst_pinn_trained'] = True
                ckpt.save_state(state)
            
            state['step'] = 9
            ckpt.save_state(state)
            
            if ckpt.interrupted:
                print("\nğŸ’¾ PatchTST ëª¨ë¸ ì €ì¥ í›„ ì¢…ë£Œ")
                sys.exit(0)
        
        # Step 9: í‰ê°€
        if step <= 9:
            print(f"\n[Step 9/10] ëª¨ë¸ í‰ê°€")
            print("-"*60)
            
            # ëª¨ë¸ ë¡œë“œ
            system.model_jump = ckpt.load_model('model_jump')
            system.model_extra_rules = ckpt.load_model('model_extra_rules')
            
            # ë°ì´í„° ì¬ë¡œë“œ
            saved_seq = ckpt.load_sequences()
            X = saved_seq['X']
            y_jump = saved_seq['y_jump']
            feature_cols = saved_seq['feature_cols']
            
            split_idx = state.get('split_idx', int(0.8 * len(X)))
            X_val = X[split_idx:]
            y_jump_val = y_jump[split_idx:]
            
            # íŠ¹ì§• ì¤€ë¹„
            X_val_features = system.prepare_features(X_val)
            system.get_feature_indices(feature_cols)
            
            # ë¬¼ë¦¬ íŠ¹ì§•
            X_physics_val = system.create_physics_features(X_val, feature_cols)
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            system.scaler_X = joblib.load(ckpt.models_dir + '/scaler_X.pkl')
            system.scaler_physics = joblib.load(ckpt.models_dir + '/scaler_physics.pkl')
            
            print("\n" + "="*60)
            print("ğŸ“Š ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
            print("="*60)
            
            # 1. XGBoost ê¸°ë³¸
            jump_pred_xgb = system.model_jump.predict(X_val_features)
            metrics_xgb = evaluate_jump_detection(y_jump_val, jump_pred_xgb, "XGBoost ê¸°ë³¸")
            
            # 2. XGBoost + ê·œì¹™
            jump_pred_xgb_rules = system.apply_rule_based_boost(X_val_features, jump_pred_xgb)
            metrics_xgb_rules = evaluate_jump_detection(y_jump_val, jump_pred_xgb_rules, "XGBoost + ê·œì¹™")
            
            # 3. ExtraTrees
            jump_pred_extra = system.model_extra_rules.predict(X_val_features)
            metrics_extra = evaluate_jump_detection(y_jump_val, jump_pred_extra, "ExtraTrees")
            
            # 4. ExtraTrees + ê·œì¹™
            jump_pred_extra_rules = system.apply_rule_based_boost(X_val_features, jump_pred_extra)
            metrics_extra_rules = evaluate_jump_detection(y_jump_val, jump_pred_extra_rules, "ExtraTrees + ê·œì¹™")
            
            # 5. í•˜ì´ë¸Œë¦¬ë“œ (ExtraTrees + PatchTST)
            if state.get('patchtst_pinn_trained', False):
                # PatchTST ëª¨ë¸ ì¬ìƒì„±
                n_features = state.get('n_features', X_val.shape[2])
                system.build_patchtst_models(n_features)
                
                # ê°€ì¤‘ì¹˜ ë¡œë“œ
                system.model_patchtst_pinn.load_weights(ckpt.models_dir + '/model_patchtst_pinn.h5')
                
                # í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡
                hybrid_predictor = HybridJumpPredictor(system)
                jump_pred_hybrid = hybrid_predictor.predict(X_val, X_val_features, X_physics_val)
                metrics_hybrid = evaluate_jump_detection(y_jump_val, jump_pred_hybrid, "ExtraTrees + PatchTST í•˜ì´ë¸Œë¦¬ë“œ")
            
            state['step'] = 10
            ckpt.save_state(state)
        
        # Step 10: ìµœì¢… ì •ë¦¬
        if step <= 10:
            print(f"\n[Step 10/10] ìµœì¢… ì •ë¦¬ ë° ì €ì¥")
            print("-"*60)
            
            # ì „ì²´ ì‹œìŠ¤í…œ ì €ì¥
            print("\nğŸ’¾ ì „ì²´ ì‹œìŠ¤í…œ ì €ì¥")
            joblib.dump(system, 'hubroom_jump_detection_system_complete.pkl')
            
            # ìµœì¢… ê²°ê³¼ ìš”ì•½
            print("\n" + "="*60)
            print("ğŸ‰ í•™ìŠµ ì™„ë£Œ! ìµœì¢… ì„±ëŠ¥ ìš”ì•½")
            print("="*60)
            
            print("\nğŸ“Š ì í”„ ê°ì§€ìœ¨ ë¹„êµ:")
            print(f"  - XGBoost ê¸°ë³¸: {metrics_xgb['recall']*100:.1f}%")
            print(f"  - XGBoost + ê·œì¹™: {metrics_xgb_rules['recall']*100:.1f}%")
            print(f"  - ExtraTrees: {metrics_extra['recall']*100:.1f}%")
            print(f"  - ExtraTrees + ê·œì¹™: {metrics_extra_rules['recall']*100:.1f}%")
            if 'metrics_hybrid' in locals():
                print(f"  - ExtraTrees + PatchTST í•˜ì´ë¸Œë¦¬ë“œ: {metrics_hybrid['recall']*100:.1f}% âœ¨")
            
            # ìƒíƒœ íŒŒì¼ ì •ë¦¬
            remove = input("\ní•™ìŠµ ì™„ë£Œ! ìƒíƒœ íŒŒì¼ì„ ì œê±°í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            if remove.lower() == 'y':
                ckpt.clear_state()
                print("ğŸ§¹ ìƒíƒœ íŒŒì¼ ì œê±° ì™„ë£Œ")
            
            print("\n" + "="*60)
            print("âœ… ëª¨ë“  ê³¼ì • ì™„ë£Œ!")
            print("="*60)
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨ ê°ì§€")
        print("ğŸ’¾ ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        print("ğŸ’¾ ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()