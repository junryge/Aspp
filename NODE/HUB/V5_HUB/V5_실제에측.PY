#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ğŸ¯ HUBROOM ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
================================================================================
ëª©í‘œ:
- ìµœì‹  30ë¶„ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡
- ì˜ˆì¸¡ê°’ ë° ìƒíƒœ(ì •ìƒ/ì£¼ì˜/ì‹¬ê°) ë°˜í™˜
================================================================================
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, ExtraTreesRegressor
from sklearn.preprocessing import RobustScaler, LabelEncoder
from xgboost import XGBClassifier
import joblib
import os
import pickle
from datetime import datetime, timedelta
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ==============================================================================
# ğŸ“Š ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤
# ==============================================================================

class HubRoomDataProcessor:
    """ì™„ì „í•œ ë°ì´í„° ì²˜ë¦¬ - ëª¨ë“  íŠ¹ì§• í¬í•¨"""
    
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        
        # 21ê°œ í•„ìˆ˜ ì»¬ëŸ¼
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB',
            'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2',
            'M14B_7F_TO_HUB_JOB2',
            'M16B_10F_TO_HUB_JOB'
        ]
        
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB',
            'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB',
            'M16A_3F_TO_M14B_7F_JOB',
            'M16A_3F_TO_3F_MLUD_JOB'
        ]
        
        self.cmd_cols = [
            'M16A_3F_CMD',
            'M16A_6F_TO_HUB_CMD',
            'M16A_2F_TO_HUB_CMD',
            'M14A_3F_TO_HUB_CMD',
            'M14B_7F_TO_HUB_CMD'
        ]
        
        self.capa_cols = [
            'M16A_6F_LFT_MAXCAPA',
            'M16A_2F_LFT_MAXCAPA'
        ]
        
        self.other_cols = [
            'M16A_3F_STORAGE_UTIL',
            'M14_TO_M16_OFS_CUR',
            'M16_TO_M14_OFS_CUR'
        ]
        
        # í™•ë¥  ë§µ
        self.probability_map = {
            0: 0.003, 1: 0.15, 2: 0.25, 3: 0.31, 4: 0.43, 5: 0.43,
            6: 0.35, 7: 0.42, 8: 0.53, 9: 0.49, 10: 0.42,
            11: 0.47, 12: 0.52, 13: 0.60, 14: 0.54, 15: 0.66,
            16: 0.62, 17: 0.71, 18: 0.79, 19: 0.83, 20: 0.987,
            21: 0.99, 22: 0.99, 23: 0.99, 24: 0.99, 25: 0.99,
            26: 0.99, 27: 0.99, 28: 0.99, 29: 0.99, 30: 0.99
        }
    
    def load_and_merge_data(self, data_path):
        """ë°ì´í„° ë¡œë“œ ë° BRIDGE_TIME ë³‘í•©"""
        print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ë©”ì¸ ë°ì´í„°
        df = pd.read_csv(data_path)
        print(f"âœ… ë°ì´í„° ë¡œë“œ: {df.shape}")
        
        # ì‹œê°„ ì²˜ë¦¬
        time_col = df.columns[0]
        df['datetime'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M')
        
        # BRIDGE_TIME ë°ì´í„° í™•ì¸
        bridge_path = data_path.replace('.csv', '_BRIDGE.csv')
        if os.path.exists(bridge_path):
            bridge_df = pd.read_csv(bridge_path)
            
            if 'IDC_VAL' in bridge_df.columns:
                bridge_df['BRIDGE_TIME'] = bridge_df['IDC_VAL']
                bridge_df['datetime'] = pd.to_datetime(bridge_df['CRT_TM'])
                
                # ì‹œê°„ëŒ€ ì •ë³´ ì œê±°
                if hasattr(bridge_df['datetime'].dtype, 'tz'):
                    bridge_df['datetime'] = bridge_df['datetime'].dt.tz_localize(None)
                if hasattr(df['datetime'].dtype, 'tz'):
                    df['datetime'] = df['datetime'].dt.tz_localize(None)
                
                # ë¶„ ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
                bridge_df['datetime'] = bridge_df['datetime'].dt.floor('min')
                df['datetime'] = df['datetime'].dt.floor('min')
                
                # ë³‘í•©
                df = pd.merge(df, bridge_df[['datetime', 'BRIDGE_TIME']], 
                             on='datetime', how='left')
                
                # BRIDGE_TIME ë³´ê°„
                df['BRIDGE_TIME'] = df['BRIDGE_TIME'].interpolate(method='linear', limit_direction='both')
        
        # BRIDGE_TIMEì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
        if 'BRIDGE_TIME' not in df.columns:
            df['BRIDGE_TIME'] = 3.5
            
        df['BRIDGE_TIME'] = df['BRIDGE_TIME'].fillna(3.5)
        
        return df
    
    def create_all_features(self, df):
        """ì™„ì „í•œ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§"""
        print("ğŸ”§ íŠ¹ì§• ìƒì„± ì¤‘...")
        
        # 1. ìœ ì…/ìœ ì¶œ ë°¸ëŸ°ìŠ¤
        df['flow_balance'] = df[self.inflow_cols].sum(axis=1) - df[self.outflow_cols].sum(axis=1)
        df['flow_ratio'] = df[self.inflow_cols].sum(axis=1) / (df[self.outflow_cols].sum(axis=1) + 1)
        
        # 2. ì¶”ì„¸ íŠ¹ì§•
        df['trend_20min'] = df[self.target_col].diff(20)
        df['trend_10min'] = df[self.target_col].diff(10)
        df['acceleration'] = df['trend_10min'] - df['trend_10min'].shift(10)
        
        # 3. ì—°ì† íŒ¨í„´
        df['consecutive_250+'] = (df[self.target_col] > 250).rolling(10).sum()
        df['consecutive_270+'] = (df[self.target_col] > 270).rolling(10).sum()
        
        # 4. CMD ë™ê¸°í™”
        df['cmd_sync_count'] = (df[self.cmd_cols] > 235).sum(axis=1)
        df['cmd_max'] = df[self.cmd_cols].max(axis=1)
        
        # 5. ë¸Œë¦¿ì§€íƒ€ì„ ë³€í™”
        df['bridge_diff'] = df['BRIDGE_TIME'].diff(5)
        df['bridge_high'] = (df['BRIDGE_TIME'] > 4.0).astype(int)
        
        # 6. storage x bridge ìƒí˜¸ì‘ìš©
        df['storage_x_bridge'] = df['M16A_3F_STORAGE_UTIL'] * df['BRIDGE_TIME']
        
        # 7. ì—°ì† 300+ ì¹´ìš´íŠ¸ì™€ í™•ë¥ 
        consecutive_300_counts = []
        consecutive_300_probs = []
        
        for i in range(len(df)):
            if i < 30:
                count = 0
                prob = 0.003
            else:
                window = df[self.target_col].iloc[i-30:i].values
                count = sum(1 for v in window if v >= 300)
                prob = self.probability_map.get(count, 0.5)
            
            consecutive_300_counts.append(count)
            consecutive_300_probs.append(prob)
        
        df['consecutive_300_count'] = consecutive_300_counts
        df['consecutive_300_prob'] = consecutive_300_probs
        
        # 8. 3êµ¬ê°„ ë¶„ë¥˜
        conditions = [
            df[self.target_col] < 150,
            (df[self.target_col] >= 150) & (df[self.target_col] < 300),
            df[self.target_col] >= 300
        ]
        choices = [0, 1, 2]
        df['range_class'] = np.select(conditions, choices, default=1)
        
        # 9. ì í”„ ì—¬ë¶€
        df['past_30min_max'] = df[self.target_col].rolling(30).max()
        df['is_jump'] = ((df['past_30min_max'].shift(10) < 280) & 
                        (df[self.target_col] >= 300)).astype(int)
        
        # 10. ìƒìŠ¹/í•˜ë½ íŒ¨í„´
        df['change_20min'] = df[self.target_col] - df[self.target_col].shift(20)
        
        trend_conditions = [
            df['change_20min'] < -20,
            (df['change_20min'] >= -20) & (df['change_20min'] < 20),
            (df['change_20min'] >= 20) & (df['change_20min'] < 50),
            df['change_20min'] >= 50
        ]
        trend_choices = [0, 1, 2, 3]
        df['trend_pattern'] = np.select(trend_conditions, trend_choices, default=1)
        
        # 11. ìƒìŠ¹ë¥ /í•˜ë½ë¥ 
        df['change_rate_10min'] = ((df[self.target_col] - df[self.target_col].shift(10)) / 
                                   (df[self.target_col].shift(10) + 1)) * 100
        df['change_rate_20min'] = ((df[self.target_col] - df[self.target_col].shift(20)) / 
                                   (df[self.target_col].shift(20) + 1)) * 100
        df['change_rate_30min'] = ((df[self.target_col] - df[self.target_col].shift(30)) / 
                                   (df[self.target_col].shift(30) + 1)) * 100
        
        # 12. ë³€ë™ì„±
        df['volatility_10min'] = df[self.target_col].rolling(10).std()
        df['volatility_20min'] = df[self.target_col].rolling(20).std()
        df['volatility_30min'] = df[self.target_col].rolling(30).std()
        
        # 13. ê·¹ë‹¨ê°’ ê·¼ì ‘ë„
        df['distance_to_300'] = 300 - df[self.target_col]
        df['near_extreme'] = (df[self.target_col] > 280).astype(int)
        
        # 14. ìµœê·¼ í†µê³„
        df['recent_5min_mean'] = df[self.target_col].rolling(5).mean()
        df['recent_5min_max'] = df[self.target_col].rolling(5).max()
        df['recent_10min_mean'] = df[self.target_col].rolling(10).mean()
        
        # 15. 277 êµ¬ê°„ íŠ¹ë³„ ì§€í‘œ
        df['in_jump_zone'] = ((df[self.target_col] >= 275) & (df[self.target_col] <= 279)).astype(int)
        
        # NaN ì²˜ë¦¬
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].fillna(method='ffill').fillna(0)
        
        print(f"âœ… {len(df.columns)}ê°œ íŠ¹ì§• ìƒì„± ì™„ë£Œ")
        return df

# ==============================================================================
# ğŸ¤– ì í”„ ê°ì§€ ì‹œìŠ¤í…œ
# ==============================================================================

class JumpDetectionSystem:
    """ì í”„ ê°ì§€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.model_jump = None
        self.model_range = None
        self.model_trend = None
        self.model_value = None
        self.feature_indices = {}
    
    def load_models(self, models_dir):
        """ëª¨ë¸ ë¡œë“œ"""
        print("\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        self.model_jump = joblib.load(os.path.join(models_dir, 'model_jump.pkl'))
        self.model_range = joblib.load(os.path.join(models_dir, 'model_range.pkl'))
        self.model_trend = joblib.load(os.path.join(models_dir, 'model_trend.pkl'))
        self.model_value = joblib.load(os.path.join(models_dir, 'model_value.pkl'))
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def prepare_features(self, X_seq):
        """ì‹œí€€ìŠ¤ë¥¼ íŠ¹ì§•ìœ¼ë¡œ ë³€í™˜"""
        # ë§ˆì§€ë§‰ ì‹œì  íŠ¹ì§•
        last_features = X_seq[:, -1, :]
        
        # í†µê³„ íŠ¹ì§•
        mean_features = np.mean(X_seq, axis=1)
        std_features = np.std(X_seq, axis=1)
        max_features = np.max(X_seq, axis=1)
        min_features = np.min(X_seq, axis=1)
        
        # ì¶”ì„¸ íŠ¹ì§•
        trend_features = X_seq[:, -1, :] - X_seq[:, 0, :]
        
        # ëª¨ë“  íŠ¹ì§• ê²°í•©
        features = np.hstack([
            last_features,
            mean_features,
            std_features,
            max_features,
            min_features,
            trend_features
        ])
        
        return features
    
    def get_expanded_feature_indices(self, df):
        """í™•ì¥ëœ íŠ¹ì§• ì¸ë±ìŠ¤ ê³„ì‚°"""
        feature_cols = [col for col in df.columns 
                       if col not in ['datetime', 'range_class', 'is_jump', 'trend_pattern']]
        
        n_base_features = len(feature_cols)
        expanded_indices = {}
        
        # ì›ë³¸ íŠ¹ì§•ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
        base_indices = {}
        for i, col in enumerate(feature_cols):
            if 'STORAGE_UTIL' in col:
                base_indices['storage_util'] = i
            elif col == 'BRIDGE_TIME':
                base_indices['bridge_time'] = i
            elif 'flow_balance' in col:
                base_indices['flow_balance'] = i
            elif 'consecutive_250+' in col:
                base_indices['consecutive_250'] = i
            elif 'cmd_sync_count' in col:
                base_indices['cmd_sync'] = i
            elif 'trend_20min' in col:
                base_indices['trend_20min'] = i
            elif 'acceleration' in col:
                base_indices['acceleration'] = i
            elif 'consecutive_300_prob' in col:
                base_indices['prob_extreme'] = i
            elif 'in_jump_zone' in col:
                base_indices['in_jump_zone'] = i
            elif 'recent_5min_max' in col:
                base_indices['recent_5min_max'] = i
        
        # í™•ì¥ëœ ì¸ë±ìŠ¤ ê³„ì‚°
        for key, base_idx in base_indices.items():
            expanded_indices[f'{key}_last'] = base_idx
            expanded_indices[f'{key}_mean'] = base_idx + n_base_features
            expanded_indices[f'{key}_std'] = base_idx + 2 * n_base_features
            expanded_indices[f'{key}_max'] = base_idx + 3 * n_base_features
            expanded_indices[f'{key}_min'] = base_idx + 4 * n_base_features
            expanded_indices[f'{key}_trend'] = base_idx + 5 * n_base_features
        
        self.feature_indices = expanded_indices
        return expanded_indices
    
    def apply_rule_based_boost_v3(self, X, predictions, prob_scores=None):
        """ê·œì¹™ ê¸°ë°˜ ë¶€ìŠ¤íŒ…"""
        boosted_predictions = predictions.copy()
        
        if not self.feature_indices:
            return boosted_predictions
        
        idx = self.feature_indices
        
        # Phase 1: storage_util ê¸°ë°˜
        conditions = []
        if 'storage_util_last' in idx:
            conditions.append(X[:, idx['storage_util_last']] > 15)
        if 'storage_util_max' in idx:
            conditions.append(X[:, idx['storage_util_max']] > 20)
        if 'storage_util_mean' in idx:
            conditions.append(X[:, idx['storage_util_mean']] > 10)
        
        if conditions:
            strong_signal = np.any(conditions, axis=0)
            boosted_predictions[strong_signal] = 1
        
        # Phase 2: bridge_time ê¸°ë°˜
        conditions = []
        if 'bridge_time_last' in idx:
            conditions.append(X[:, idx['bridge_time_last']] > 3.8)
        if 'bridge_time_max' in idx:
            conditions.append(X[:, idx['bridge_time_max']] > 4.0)
        if 'bridge_time_mean' in idx:
            conditions.append(X[:, idx['bridge_time_mean']] > 3.7)
        
        if conditions:
            bridge_signal = np.any(conditions, axis=0)
            boosted_predictions[bridge_signal] = 1
        
        # Phase 3: ë³µí•© ì¡°ê±´
        complex_conditions = []
        if 'flow_balance_mean' in idx:
            complex_conditions.append(X[:, idx['flow_balance_mean']] > 30)
        if 'acceleration_last' in idx:
            complex_conditions.append(X[:, idx['acceleration_last']] > 10)
        if 'consecutive_250_max' in idx:
            complex_conditions.append(X[:, idx['consecutive_250_max']] >= 5)
        if 'trend_20min_last' in idx:
            complex_conditions.append(X[:, idx['trend_20min_last']] > 20)
        
        if len(complex_conditions) >= 2:
            complex_signal = np.sum(complex_conditions, axis=0) >= 2
            boosted_predictions[complex_signal] = 1
        
        # Phase 4: í™•ë¥  ê¸°ë°˜
        if prob_scores is not None:
            prob_boost = (prob_scores > 0.1) & (boosted_predictions == 0)
            boosted_predictions[prob_boost] = 1
        
        # Phase 5: ì í”„ì¡´ ê°ì§€
        if 'in_jump_zone_last' in idx:
            jump_zone = X[:, idx['in_jump_zone_last']] > 0
            boosted_predictions[jump_zone] = 1
        
        # Phase 6: recent_5min_max
        if 'recent_5min_max_last' in idx:
            recent_high = X[:, idx['recent_5min_max_last']] >= 275
            boosted_predictions[recent_high] = 1
        
        return boosted_predictions
    
    def apply_emergency_boost(self, X_seq, X_features, predictions, value_pred):
        """ê¸´ê¸‰ ë¶€ìŠ¤íŒ…"""
        boosted = predictions.copy()
        
        for i in range(len(X_seq)):
            seq_max = np.max(X_seq[i, :, 0])
            
            # ê·œì¹™ 1: ê°’ ì˜ˆì¸¡ì´ 290 ì´ìƒì´ê³  ê³¼ê±° ìµœëŒ€ê°’ì´ 280 ë¯¸ë§Œ
            if value_pred[i] >= 290 and seq_max < 280:
                boosted[i] = 1
            
            # ê·œì¹™ 2: ìµœê·¼ 5ë¶„ í‰ê· ì´ 275 ì´ìƒì´ê³  ê³¼ê±° ìµœëŒ€ê°’ì´ 279 ì´í•˜
            recent_5min = np.mean(X_seq[i, -5:, 0])
            if recent_5min >= 275 and seq_max <= 279:
                boosted[i] = 1
            
            # ê·œì¹™ 3: ê°€ì†ë„ê°€ í¬ê³  í˜„ì¬ê°’ì´ 270 ì´ìƒ
            if len(X_seq[i]) >= 20:
                accel = (np.mean(X_seq[i, -5:, 0]) - np.mean(X_seq[i, -10:-5, 0])) - \
                       (np.mean(X_seq[i, -10:-5, 0]) - np.mean(X_seq[i, -15:-10, 0]))
                current = X_seq[i, -1, 0]
                
                if accel > 15 and current >= 270:
                    boosted[i] = 1
        
        return boosted

# ==============================================================================
# ğŸ”® ì‹¤ì‹œê°„ ì˜ˆì¸¡ í•¨ìˆ˜
# ==============================================================================

def predict_realtime():
    """ì‹¤ì‹œê°„ ì˜ˆì¸¡ ë©”ì¸ í•¨ìˆ˜"""
    
    print("="*80)
    print("ğŸ¯ HUBROOM ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
    print("ğŸ“Š ìµœì‹  30ë¶„ ë°ì´í„° â†’ 10ë¶„ í›„ ì˜ˆì¸¡")
    print("="*80)
    
    # 1. ì²´í¬í¬ì¸íŠ¸ í™•ì¸
    checkpoint_dir = './checkpoints_jump80'
    models_dir = os.path.join(checkpoint_dir, 'models')
    
    if not os.path.exists(models_dir):
        print("âŒ í•™ìŠµëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # 2. ë°ì´í„° ì²˜ë¦¬ê¸° ë° ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    processor = HubRoomDataProcessor()
    system = JumpDetectionSystem()
    
    # 3. ëª¨ë¸ ë¡œë“œ
    system.load_models(models_dir)
    
    # 4. ë°ì´í„° ë¡œë“œ
    data_path = 'data/HUBROOM_PIVOT_DATA.csv'
    if not os.path.exists(data_path):
        print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        return None
    
    df = processor.load_and_merge_data(data_path)
    df = processor.create_all_features(df)
    
    # 5. íŠ¹ì§• ì¸ë±ìŠ¤ ì„¤ì •
    system.get_expanded_feature_indices(df)
    
    # 6. ìµœì‹  30ë¶„ ì‹œí€€ìŠ¤ ìƒì„±
    seq_len = 30
    
    if len(df) < seq_len:
        print(f"âŒ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ {seq_len}ê°œ í•„ìš” (í˜„ì¬: {len(df)}ê°œ)")
        return None
    
    # íŠ¹ì§• ì»¬ëŸ¼ ì„ íƒ
    feature_cols = [col for col in df.columns 
                   if col not in ['datetime', 'range_class', 'is_jump', 'trend_pattern']]
    
    # ìµœì‹  30ë¶„ ë°ì´í„°
    X_seq = df[feature_cols].iloc[-seq_len:].values
    X_seq = X_seq.reshape(1, seq_len, -1)  # (1, 30, features)
    
    current_time = df['datetime'].iloc[-1]
    predict_time = current_time + timedelta(minutes=10)
    
    print(f"\nâ° í˜„ì¬ ì‹œê°„: {current_time}")
    print(f"ğŸ”® ì˜ˆì¸¡ ì‹œê°„: {predict_time} (10ë¶„ í›„)")
    
    # 7. íŠ¹ì§• ì¤€ë¹„
    X_features = system.prepare_features(X_seq)
    
    # 8. ì˜ˆì¸¡ ìˆ˜í–‰
    print("\nğŸ“ˆ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
    
    # ì í”„ ì˜ˆì¸¡
    jump_pred = system.model_jump.predict(X_features)
    jump_pred_proba = system.model_jump.predict_proba(X_features)[:, 1]
    
    # ê°’ ì˜ˆì¸¡
    value_pred = system.model_value.predict(X_features)
    
    # ê·œì¹™ ê¸°ë°˜ ë¶€ìŠ¤íŒ…
    jump_pred_boosted = system.apply_rule_based_boost_v3(X_features, jump_pred, jump_pred_proba)
    
    # ê¸´ê¸‰ ë¶€ìŠ¤íŒ…
    jump_pred_final = system.apply_emergency_boost(X_seq, X_features, jump_pred_boosted, value_pred)
    
    # êµ¬ê°„ ì˜ˆì¸¡
    range_pred = system.model_range.predict(X_features)
    
    # íŒ¨í„´ ì˜ˆì¸¡
    trend_pred = system.model_trend.predict(X_features)
    
    # 9. ì˜ˆì¸¡ê°’ ì¡°ì •
    predicted_value = value_pred[0]
    is_jump = jump_pred_final[0] == 1
    
    if is_jump:
        adjusted_value = predicted_value + 40
        print(f"\nğŸš€ ì í”„ ê°ì§€! ì˜ˆì¸¡ê°’ ì¡°ì •: {predicted_value:.1f} â†’ {adjusted_value:.1f}")
        predicted_value = adjusted_value
    
    # 10. ìƒíƒœ íŒì •
    if predicted_value < 250:
        status = "ì •ìƒ"
        status_percent = {"ì •ìƒ": 80, "ì£¼ì˜": 15, "ì‹¬ê°": 5}
    elif predicted_value < 280:
        status = "ì£¼ì˜"
        status_percent = {"ì •ìƒ": 20, "ì£¼ì˜": 60, "ì‹¬ê°": 20}
    else:
        status = "ì‹¬ê°"
        status_percent = {"ì •ìƒ": 5, "ì£¼ì˜": 15, "ì‹¬ê°": 80}
    
    # ì í”„ ì˜ˆì¸¡ì´ë©´ ì‹¬ê°ë„ ìƒí–¥
    if is_jump:
        status = "ì‹¬ê°"
        status_percent = {"ì •ìƒ": 2, "ì£¼ì˜": 8, "ì‹¬ê°": 90}
    
    # 11. ê²°ê³¼ ìƒì„±
    result = {
        "í˜„ì¬ì‹œê°„": current_time.strftime('%Y-%m-%d %H:%M'),
        "ì˜ˆì¸¡ì‹œê°„": predict_time.strftime('%Y-%m-%d %H:%M'),
        "ì˜ˆì¸¡ê°’": round(predicted_value, 1),
        "ìƒíƒœ": status,
        "ìƒíƒœí™•ë¥ ": status_percent,
        "ì í”„ì˜ˆì¸¡": "ì˜ˆìƒ" if is_jump else "ì •ìƒ",
        "ì í”„í™•ë¥ ": round(jump_pred_proba[0] * 100, 1),
        "êµ¬ê°„ì˜ˆì¸¡": ['50-150', '150-299', '300+'][range_pred[0]],
        "íŒ¨í„´ì˜ˆì¸¡": ['í•˜ë½', 'ì•ˆì •', 'ì ì§„ìƒìŠ¹', 'ê¸‰ìƒìŠ¹'][trend_pred[0]]
    }
    
    # 12. ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼")
    print("="*80)
    print(f"ğŸ”® 10ë¶„ í›„ ì˜ˆì¸¡ê°’: {result['ì˜ˆì¸¡ê°’']}")
    print(f"âš ï¸ ìƒíƒœ: {result['ìƒíƒœ']}")
    print(f"ğŸ“Š ìƒíƒœ í™•ë¥ : ì •ìƒ {result['ìƒíƒœí™•ë¥ ']['ì •ìƒ']}% | ì£¼ì˜ {result['ìƒíƒœí™•ë¥ ']['ì£¼ì˜']}% | ì‹¬ê° {result['ìƒíƒœí™•ë¥ ']['ì‹¬ê°']}%")
    print(f"ğŸš€ ì í”„ ì˜ˆì¸¡: {result['ì í”„ì˜ˆì¸¡']} (í™•ë¥ : {result['ì í”„í™•ë¥ ']}%)")
    print(f"ğŸ“ˆ íŒ¨í„´: {result['íŒ¨í„´ì˜ˆì¸¡']}")
    print(f"ğŸ“ êµ¬ê°„: {result['êµ¬ê°„ì˜ˆì¸¡']}")
    print("="*80)
    
    return result

# ==============================================================================
# ğŸ¯ ì‹¤í–‰
# ==============================================================================

if __name__ == "__main__":
    result = predict_realtime()
    
    if result:
        print("\nâœ… ì˜ˆì¸¡ ì™„ë£Œ!")
        print(f"\nğŸ”„ ë°˜í™˜ê°’:")
        print(f"  ì˜ˆì¸¡ê°’: {result['ì˜ˆì¸¡ê°’']}")
        print(f"  ìƒíƒœí™•ë¥ : {result['ìƒíƒœí™•ë¥ ']}")