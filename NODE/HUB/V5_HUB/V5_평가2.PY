#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ğŸ¯ HUBROOM ì í”„ ê°ì§€ ì‹œìŠ¤í…œ - ì™„ì „í•œ í‰ê°€ ì½”ë“œ
================================================================================
í•™ìŠµëœ ëª¨ë“  ëª¨ë¸ ë¡œë“œ:
1. XGBoost (model_jump)
2. ExtraTrees (model_extra_rules)
3. RandomForest (model_range, model_trend)
4. ExtraTreesRegressor (model_value)
5. PatchTST (model_patchtst)
6. PatchTST-PINN (model_patchtst_pinn)
================================================================================
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import joblib
import os
from datetime import datetime
from tqdm import tqdm
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("ğŸ¯ HUBROOM ì í”„ ê°ì§€ ì‹œìŠ¤í…œ - 2025ë…„ 8ì›” í‰ê°€")
print("ğŸ“Š í•™ìŠµ ì½”ë“œì˜ ëª¨ë“  ëª¨ë¸ ì •í™•íˆ ë¡œë“œ")
print("="*80)

# ==============================================================================
# ğŸ“Š ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼)
# ==============================================================================

class HubRoomDataProcessor:
    """ì™„ì „í•œ ë°ì´í„° ì²˜ë¦¬ - í•™ìŠµ ì½”ë“œì™€ ë™ì¼"""
    
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        
        # 21ê°œ í•„ìˆ˜ ì»¬ëŸ¼
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB',
            'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2',
            'M14B_7F_TO_HUB_JOB2',
            'M16B_10F_TO_HUB_JOB'
        ]
        
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB',
            'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB',
            'M16A_3F_TO_M14B_7F_JOB',
            'M16A_3F_TO_3F_MLUD_JOB'
        ]
        
        self.cmd_cols = [
            'M16A_3F_CMD',
            'M16A_6F_TO_HUB_CMD',
            'M16A_2F_TO_HUB_CMD',
            'M14A_3F_TO_HUB_CMD',
            'M14B_7F_TO_HUB_CMD'
        ]
        
        self.capa_cols = [
            'M16A_6F_LFT_MAXCAPA',
            'M16A_2F_LFT_MAXCAPA'
        ]
        
        self.other_cols = [
            'M16A_3F_STORAGE_UTIL',
            'M14_TO_M16_OFS_CUR',
            'M16_TO_M14_OFS_CUR'
        ]
        
        # í™•ë¥  ë§µ - ë§¤ìš° ì¤‘ìš”!
        self.probability_map = {
            0: 0.003, 1: 0.15, 2: 0.25, 3: 0.31, 4: 0.43, 5: 0.43,
            6: 0.35, 7: 0.42, 8: 0.53, 9: 0.49, 10: 0.42,
            11: 0.47, 12: 0.52, 13: 0.60, 14: 0.54, 15: 0.66,
            16: 0.62, 17: 0.71, 18: 0.79, 19: 0.83, 20: 0.987,
            21: 0.99, 22: 0.99, 23: 0.99, 24: 0.99, 25: 0.99,
            26: 0.99, 27: 0.99, 28: 0.99, 29: 0.99, 30: 0.99
        }
    
    def load_evaluation_data(self):
        """8ì›” í‰ê°€ ë°ì´í„° ë¡œë“œ"""
        print("\n[1ë‹¨ê³„] í‰ê°€ ë°ì´í„° ë¡œë“œ")
        
        # 8ì›” ë°ì´í„°
        df = pd.read_csv('data/20250801_to_20250831.csv')
        print(f"âœ… 8ì›” í‰ê°€ ë°ì´í„°: {df.shape}")
        
        # ì‹œê°„ ì²˜ë¦¬
        time_col = df.columns[0]
        df['datetime'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M')
        
        # BRIDGE_TIME ì²˜ë¦¬ (8ì›” BRIDGE_TIME íŒŒì¼ì´ ìˆë‹¤ë©´)
        if os.path.exists('data/BRTIME_20250801_to_20250831.csv'):
            bridge_df = pd.read_csv('data/BRTIME_20250801_to_20250831.csv')
            print(f"âœ… 8ì›” BRIDGE_TIME ë°ì´í„°: {bridge_df.shape}")
            
            if 'IDC_VAL' in bridge_df.columns:
                bridge_df['BRIDGE_TIME'] = bridge_df['IDC_VAL']
                bridge_df['datetime'] = pd.to_datetime(bridge_df['CRT_TM'])
                
                # ì‹œê°„ëŒ€ ì •ë³´ ì œê±°
                if hasattr(bridge_df['datetime'].dtype, 'tz'):
                    bridge_df['datetime'] = bridge_df['datetime'].dt.tz_localize(None)
                if hasattr(df['datetime'].dtype, 'tz'):
                    df['datetime'] = df['datetime'].dt.tz_localize(None)
                
                # ë¶„ ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
                bridge_df['datetime'] = bridge_df['datetime'].dt.floor('min')
                df['datetime'] = df['datetime'].dt.floor('min')
                
                # ë³‘í•©
                df = pd.merge(df, bridge_df[['datetime', 'BRIDGE_TIME']], 
                             on='datetime', how='left')
        
        # BRIDGE_TIME ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
        if 'BRIDGE_TIME' not in df.columns:
            print("âš ï¸ BRIDGE_TIME ë°ì´í„° ì—†ìŒ - ê¸°ë³¸ê°’ 3.5 ì‚¬ìš©")
            df['BRIDGE_TIME'] = 3.5
        else:
            df['BRIDGE_TIME'] = df['BRIDGE_TIME'].interpolate(method='linear', limit_direction='both')
            df['BRIDGE_TIME'] = df['BRIDGE_TIME'].fillna(3.5)
        
        return df
    
    def create_all_features(self, df):
        """ì™„ì „í•œ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ - í•™ìŠµê³¼ ë™ì¼"""
        print("\n[2ë‹¨ê³„] íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§")
        
        # 1. ìœ ì…/ìœ ì¶œ ë°¸ëŸ°ìŠ¤
        df['flow_balance'] = df[self.inflow_cols].sum(axis=1) - df[self.outflow_cols].sum(axis=1)
        df['flow_ratio'] = df[self.inflow_cols].sum(axis=1) / (df[self.outflow_cols].sum(axis=1) + 1)
        
        # 2. ì¶”ì„¸ íŠ¹ì§•
        df['trend_20min'] = df[self.target_col].diff(20)
        df['trend_10min'] = df[self.target_col].diff(10)
        df['acceleration'] = df['trend_10min'] - df['trend_10min'].shift(10)
        
        # 3. ì—°ì† íŒ¨í„´
        df['consecutive_250+'] = (df[self.target_col] > 250).rolling(10).sum()
        df['consecutive_270+'] = (df[self.target_col] > 270).rolling(10).sum()
        
        # 4. CMD ë™ê¸°í™”
        df['cmd_sync_count'] = (df[self.cmd_cols] > 235).sum(axis=1)
        df['cmd_max'] = df[self.cmd_cols].max(axis=1)
        
        # 5. ë¸Œë¦¿ì§€íƒ€ì„ ë³€í™”
        df['bridge_diff'] = df['BRIDGE_TIME'].diff(5)
        df['bridge_high'] = (df['BRIDGE_TIME'] > 4.0).astype(int)
        
        # 6. storage x bridge ìƒí˜¸ì‘ìš©
        df['storage_x_bridge'] = df['M16A_3F_STORAGE_UTIL'] * df['BRIDGE_TIME']
        
        # 7. ì—°ì† 300+ ì¹´ìš´íŠ¸ì™€ í™•ë¥ 
        consecutive_300_counts = []
        consecutive_300_probs = []
        
        for i in tqdm(range(len(df)), desc="300+ íŒ¨í„´ ê³„ì‚°"):
            if i < 30:
                count = 0
                prob = 0.003
            else:
                window = df[self.target_col].iloc[i-30:i].values
                count = sum(1 for v in window if v >= 300)
                prob = self.probability_map.get(count, 0.5)
            
            consecutive_300_counts.append(count)
            consecutive_300_probs.append(prob)
        
        df['consecutive_300_count'] = consecutive_300_counts
        df['consecutive_300_prob'] = consecutive_300_probs
        
        # 8. 3êµ¬ê°„ ë¶„ë¥˜ (ìˆ«ìë¡œ ì§ì ‘ ë³€í™˜)
        conditions = [
            df[self.target_col] < 150,
            (df[self.target_col] >= 150) & (df[self.target_col] < 300),
            df[self.target_col] >= 300
        ]
        choices = [0, 1, 2]
        df['range_class'] = np.select(conditions, choices, default=1)
        
        # 9. ì í”„ ì—¬ë¶€
        df['past_30min_max'] = df[self.target_col].rolling(30).max()
        df['is_jump'] = ((df['past_30min_max'].shift(10) < 280) & 
                        (df[self.target_col] >= 300)).astype(int)
        
        # 10. ìƒìŠ¹/í•˜ë½ íŒ¨í„´ (ìˆ«ìë¡œ ë³€í™˜)
        df['change_20min'] = df[self.target_col] - df[self.target_col].shift(20)
        
        trend_conditions = [
            df['change_20min'] < -20,
            (df['change_20min'] >= -20) & (df['change_20min'] < 20),
            (df['change_20min'] >= 20) & (df['change_20min'] < 50),
            df['change_20min'] >= 50
        ]
        trend_choices = [0, 1, 2, 3]  # 0:down, 1:stable, 2:gradual_up, 3:rapid_up
        df['trend_pattern'] = np.select(trend_conditions, trend_choices, default=1)
        
        # 11. ìƒìŠ¹ë¥ /í•˜ë½ë¥  (%)
        df['change_rate_10min'] = ((df[self.target_col] - df[self.target_col].shift(10)) / 
                                   (df[self.target_col].shift(10) + 1)) * 100
        df['change_rate_20min'] = ((df[self.target_col] - df[self.target_col].shift(20)) / 
                                   (df[self.target_col].shift(20) + 1)) * 100
        df['change_rate_30min'] = ((df[self.target_col] - df[self.target_col].shift(30)) / 
                                   (df[self.target_col].shift(30) + 1)) * 100
        
        # 12. ë³€ë™ì„±
        df['volatility_10min'] = df[self.target_col].rolling(10).std()
        df['volatility_20min'] = df[self.target_col].rolling(20).std()
        df['volatility_30min'] = df[self.target_col].rolling(30).std()
        
        # 13. ê·¹ë‹¨ê°’ ê·¼ì ‘ë„
        df['distance_to_300'] = 300 - df[self.target_col]
        df['near_extreme'] = (df[self.target_col] > 280).astype(int)
        
        # 14. ìµœê·¼ í†µê³„
        df['recent_5min_mean'] = df[self.target_col].rolling(5).mean()
        df['recent_5min_max'] = df[self.target_col].rolling(5).max()
        df['recent_10min_mean'] = df[self.target_col].rolling(10).mean()
        
        # 15. 277 êµ¬ê°„ íŠ¹ë³„ ì§€í‘œ
        df['in_jump_zone'] = ((df[self.target_col] >= 275) & (df[self.target_col] <= 279)).astype(int)
        
        # NaN ì²˜ë¦¬
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].fillna(method='ffill').fillna(0)
        
        print(f"âœ… ì´ {len(df.columns)}ê°œ íŠ¹ì§• ìƒì„± ì™„ë£Œ")
        return df
    
    def create_sequences(self, df, seq_len=30, pred_len=10):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
        print(f"\n[3ë‹¨ê³„] ì‹œí€€ìŠ¤ ìƒì„± ({seq_len}ë¶„ â†’ {pred_len}ë¶„ í›„)")
        
        # íŠ¹ì§• ì»¬ëŸ¼ ì„ íƒ
        feature_cols = [col for col in df.columns 
                       if col not in ['datetime', 'range_class', 'is_jump', 'trend_pattern']]
        
        sequences = []
        
        for i in tqdm(range(seq_len, len(df) - pred_len), desc="ì‹œí€€ìŠ¤ ìƒì„±"):
            seq_data = {
                'X': df[feature_cols].iloc[i-seq_len:i].values,
                'datetime_start': df['datetime'].iloc[i-seq_len],
                'datetime_end': df['datetime'].iloc[i-1],
                'datetime_current': df['datetime'].iloc[i-1],
                'datetime_target': df['datetime'].iloc[i+pred_len-1],
                'seq_max': df[self.target_col].iloc[i-seq_len:i].max(),
                'seq_min': df[self.target_col].iloc[i-seq_len:i].min(),
                'target_value': df[self.target_col].iloc[i+pred_len-1],
                'is_jump_actual': df['is_jump'].iloc[i+pred_len-1],
                'range_class_actual': df['range_class'].iloc[i+pred_len-1],
                'trend_pattern_actual': df['trend_pattern'].iloc[i+pred_len-1]
            }
            sequences.append(seq_data)
        
        print(f"âœ… {len(sequences)}ê°œ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ")
        return sequences, feature_cols

# ==============================================================================
# ğŸ¤– PatchTST ëª¨ë¸ ì •ì˜ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼)
# ==============================================================================

class PatchTSTModel(keras.Model):
    """ì•ˆì • êµ¬ê°„ ì „ë¬¸ PatchTST"""
    
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(128, activation='relu')
        
        # Transformer ë¸”ë¡
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm1 = layers.LayerNormalization()
        self.norm2 = layers.LayerNormalization()
        
        # Feed Forward
        self.ffn = keras.Sequential([
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(128)
        ])
        
        # ì¶œë ¥ì¸µ
        self.flatten = layers.Flatten()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dropout = layers.Dropout(0.3)
        self.dense2 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(1)
    
    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        
        # íŒ¨ì¹˜í™”
        x = tf.reshape(x, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        
        # Transformer
        attn = self.attention(x, x, training=training)
        x = self.norm1(x + attn)
        
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        
        # ì¶œë ¥
        x = self.flatten(x)
        x = self.dense1(x)
        x = self.dropout(x, training=training)
        x = self.dense2(x)
        output = self.output_layer(x)
        
        return tf.squeeze(output, axis=-1)

class PatchTSTPINN(keras.Model):
    """ê·¹ë‹¨ê°’ ì „ë¬¸ PatchTST + PINN"""
    
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # PatchTST ë¶€ë¶„
        self.patch_embedding = layers.Dense(128, activation='relu')
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm = layers.LayerNormalization()
        
        self.flatten = layers.Flatten()
        self.temporal_dense = layers.Dense(64, activation='relu')
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬
        self.physics_net = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.BatchNormalization(),
            layers.Dense(32, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(16, activation='relu')
        ])
        
        # ìœµí•©
        self.fusion = keras.Sequential([
            layers.Dense(128, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(64, activation='relu'),
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(1)
        ])
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŠ¸
        self.extreme_boost = layers.Dense(1, activation='sigmoid')
    
    def call(self, inputs, training=False):
        x_seq, x_physics = inputs
        batch_size = tf.shape(x_seq)[0]
        
        # PatchTST ì²˜ë¦¬
        x = tf.reshape(x_seq, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        
        attn = self.attention(x, x, training=training)
        x = self.norm(x + attn)
        
        x = self.flatten(x)
        x_temporal = self.temporal_dense(x)
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬
        x_physics = self.physics_net(x_physics)
        
        # ìœµí•©
        x_combined = tf.concat([x_temporal, x_physics], axis=-1)
        output = self.fusion(x_combined)
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ…
        boost_factor = self.extreme_boost(x_physics)
        output = output * (1 + boost_factor * 0.1)
        
        return tf.squeeze(output, axis=-1)

# ==============================================================================
# ğŸ¤– ëª¨ë¸ ì‹œìŠ¤í…œ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼)
# ==============================================================================

class JumpDetectionSystem:
    """ì í”„ ê°ì§€ ì‹œìŠ¤í…œ - ëª¨ë“  ëª¨ë¸ í¬í•¨"""
    
    def __init__(self):
        # ëª¨ë“  ëª¨ë¸ ì´ˆê¸°í™”
        self.model_jump = None  # XGBoost
        self.model_extra_rules = None  # ExtraTrees
        self.model_range = None  # RandomForest
        self.model_trend = None  # RandomForest
        self.model_value = None  # ExtraTreesRegressor
        self.model_patchtst = None
        self.model_patchtst_pinn = None
        
        # ìŠ¤ì¼€ì¼ëŸ¬
        self.scaler_X = None
        self.scaler_y = None
        self.scaler_physics = None
        
        self.feature_indices = {}
    
    def prepare_features(self, X_seq):
        """ì‹œí€€ìŠ¤ë¥¼ íŠ¹ì§•ìœ¼ë¡œ ë³€í™˜"""
        # ë§ˆì§€ë§‰ ì‹œì  íŠ¹ì§•
        last_features = X_seq[:, -1, :]
        
        # í†µê³„ íŠ¹ì§•
        mean_features = np.mean(X_seq, axis=1)
        std_features = np.std(X_seq, axis=1)
        max_features = np.max(X_seq, axis=1)
        min_features = np.min(X_seq, axis=1)
        
        # ì¶”ì„¸ íŠ¹ì§•
        trend_features = X_seq[:, -1, :] - X_seq[:, 0, :]
        
        # ëª¨ë“  íŠ¹ì§• ê²°í•©
        features = np.hstack([
            last_features,
            mean_features,
            std_features,
            max_features,
            min_features,
            trend_features
        ])
        
        return features
    
    def create_physics_features(self, X_seq, feature_cols):
        """ë¬¼ë¦¬ íŠ¹ì§• ìƒì„± (PatchTST-PINNìš©)"""
        # íŠ¹ì§• ì¸ë±ìŠ¤ ì°¾ê¸°
        idx_storage = None
        idx_bridge = None
        idx_flow_balance = None
        
        for i, col in enumerate(feature_cols):
            if 'M16A_3F_STORAGE_UTIL' in col:
                idx_storage = i
            elif col == 'BRIDGE_TIME':
                idx_bridge = i
            elif 'flow_balance' in col:
                idx_flow_balance = i
        
        physics_features = []
        
        for seq in X_seq:
            features = []
            
            # ë§ˆì§€ë§‰ ì‹œì  ê°’ë“¤
            if idx_storage is not None:
                features.append(seq[-1, idx_storage])
            else:
                features.append(0)
                
            if idx_bridge is not None:
                features.append(seq[-1, idx_bridge])
            else:
                features.append(3.5)
                
            if idx_flow_balance is not None:
                features.append(seq[-1, idx_flow_balance])
            else:
                features.append(0)
            
            # ì¶”ì„¸
            features.append(np.mean(seq[-10:, 0]) - np.mean(seq[:10, 0]))  # target ì¶”ì„¸
            features.append(np.std(seq[:, 0]))  # ë³€ë™ì„±
            
            physics_features.append(features)
        
        return np.array(physics_features)
    
    def get_feature_indices(self, feature_cols):
        """ì¤‘ìš” íŠ¹ì§•ë“¤ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°"""
        indices = {}
        for i, col in enumerate(feature_cols):
            if 'M16A_3F_STORAGE_UTIL' in col:
                indices['storage_util'] = i
            elif col == 'BRIDGE_TIME':
                indices['bridge_time'] = i
            elif 'flow_balance' in col:
                indices['flow_balance'] = i
            elif 'consecutive_250+' in col:
                indices['consecutive_250'] = i
            elif 'cmd_sync_count' in col:
                indices['cmd_sync'] = i
            elif 'trend_20min' in col:
                indices['trend_20min'] = i
            elif 'flow_ratio' in col:
                indices['flow_ratio'] = i
            elif 'acceleration' in col:
                indices['acceleration'] = i
            elif 'cmd_max' in col:
                indices['cmd_max'] = i
            elif 'bridge_high' in col:
                indices['bridge_high'] = i
            elif 'consecutive_300_prob' in col:
                indices['prob_extreme'] = i
        
        self.feature_indices = indices
        return indices
    
    def apply_rule_based_boost(self, X, predictions, prob_scores=None):
        """ê·œì¹™ ê¸°ë°˜ ë¶€ìŠ¤íŒ… - 80% ë‹¬ì„±ìš©"""
        boosted_predictions = predictions.copy()
        
        if not self.feature_indices:
            return boosted_predictions
        
        idx = self.feature_indices
        
        # Phase 1: ê°•í•œ ì‹ í˜¸ (55%)
        if 'storage_util' in idx:
            strong_signal = X[:, idx['storage_util']] > 20
            boosted_predictions[strong_signal] = 1
        
        # Phase 2: ì¤‘ê°„ ì‹ í˜¸ (70%)
        medium_conditions = []
        
        if 'storage_util' in idx:
            medium_conditions.append(X[:, idx['storage_util']] > 10)
        if 'flow_balance' in idx:
            medium_conditions.append(X[:, idx['flow_balance']] > 50)
        if 'bridge_time' in idx:
            medium_conditions.append(X[:, idx['bridge_time']] > 4.0)
        if 'consecutive_250' in idx:
            medium_conditions.append(X[:, idx['consecutive_250']] >= 5)
        if 'cmd_sync' in idx:
            medium_conditions.append(X[:, idx['cmd_sync']] >= 3)
        if 'trend_20min' in idx:
            medium_conditions.append(X[:, idx['trend_20min']] > 30)
        
        if len(medium_conditions) >= 2:
            medium_signal = np.sum(medium_conditions, axis=0) >= 2
            if 'storage_util' in idx:
                storage_medium = X[:, idx['storage_util']] > 10
                final_medium = medium_signal & storage_medium & (boosted_predictions == 0)
                boosted_predictions[final_medium] = 1
        
        # Phase 3: ì•½í•œ ì‹ í˜¸ (80%)
        weak_conditions = []
        
        if 'bridge_time' in idx:
            weak_conditions.append(X[:, idx['bridge_time']] > 3.85)
        if 'flow_ratio' in idx:
            weak_conditions.append(X[:, idx['flow_ratio']] > 1.5)
        if 'acceleration' in idx:
            weak_conditions.append(X[:, idx['acceleration']] > 20)
        if 'cmd_max' in idx:
            weak_conditions.append(X[:, idx['cmd_max']] > 238)
        if 'consecutive_250' in idx:
            weak_conditions.append(X[:, idx['consecutive_250']] >= 3)
        
        if len(weak_conditions) >= 3:
            weak_signal = np.sum(weak_conditions, axis=0) >= 3
            final_weak = weak_signal & (boosted_predictions == 0)
            
            if prob_scores is not None:
                final_weak = final_weak & (prob_scores > 0.3)
            
            boosted_predictions[final_weak] = 1
        
        return boosted_predictions

# ==============================================================================
# ğŸ“Š í‰ê°€ í´ë˜ìŠ¤
# ==============================================================================

class CompleteModelEvaluator:
    """í•™ìŠµ ì½”ë“œì˜ ëª¨ë“  ëª¨ë¸ì„ ì •í™•íˆ ë¡œë“œí•˜ì—¬ í‰ê°€"""
    
    def __init__(self, model_dir='./checkpoints_jump80/models'):
        self.model_dir = model_dir
        self.system = JumpDetectionSystem()
        self.models_loaded = {}
    
    def load_all_models(self, n_features):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        print("\n[4ë‹¨ê³„] í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ")
        print("-"*60)
        
        # 1. XGBoost ì í”„ ëª¨ë¸
        try:
            self.system.model_jump = joblib.load(os.path.join(self.model_dir, 'model_jump.pkl'))
            self.models_loaded['xgboost'] = True
            print("âœ… XGBoost ì í”„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ XGBoost ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.models_loaded['xgboost'] = False
        
        # 2. ExtraTrees + ê·œì¹™ ëª¨ë¸
        try:
            self.system.model_extra_rules = joblib.load(os.path.join(self.model_dir, 'model_extra_rules.pkl'))
            self.models_loaded['extra_trees'] = True
            print("âœ… ExtraTrees ì í”„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ExtraTrees ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.models_loaded['extra_trees'] = False
        
        # 3. RandomForest 3êµ¬ê°„ ë¶„ë¥˜ ëª¨ë¸
        try:
            self.system.model_range = joblib.load(os.path.join(self.model_dir, 'model_range.pkl'))
            self.models_loaded['range'] = True
            print("âœ… RandomForest 3êµ¬ê°„ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ 3êµ¬ê°„ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.models_loaded['range'] = False
        
        # 4. RandomForest íŠ¸ë Œë“œ ëª¨ë¸
        try:
            self.system.model_trend = joblib.load(os.path.join(self.model_dir, 'model_trend.pkl'))
            self.models_loaded['trend'] = True
            print("âœ… RandomForest íŠ¸ë Œë“œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ íŠ¸ë Œë“œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.models_loaded['trend'] = False
        
        # 5. ExtraTreesRegressor ê°’ ì˜ˆì¸¡ ëª¨ë¸
        try:
            self.system.model_value = joblib.load(os.path.join(self.model_dir, 'model_value.pkl'))
            self.models_loaded['value'] = True
            print("âœ… ExtraTreesRegressor ê°’ ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ê°’ ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.models_loaded['value'] = False
        
        # 6. ìŠ¤ì¼€ì¼ëŸ¬ë“¤
        try:
            self.system.scaler_X = joblib.load(os.path.join(self.model_dir, 'scaler_X.pkl'))
            self.system.scaler_y = joblib.load(os.path.join(self.model_dir, 'scaler_y.pkl'))
            self.system.scaler_physics = joblib.load(os.path.join(self.model_dir, 'scaler_physics.pkl'))
            print("âœ… ëª¨ë“  ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # 7. PatchTST ëª¨ë¸ë“¤
        try:
            # PatchTST ëª¨ë¸ êµ¬ì¡° ì¬ìƒì„±
            config = {'seq_len': 30, 'n_features': n_features, 'patch_len': 6}
            
            # PatchTST ì•ˆì •í˜•
            self.system.model_patchtst = PatchTSTModel(config)
            dummy_input = np.zeros((1, 30, n_features))
            self.system.model_patchtst(dummy_input)  # ë¹Œë“œ
            self.system.model_patchtst.load_weights(os.path.join(self.model_dir, 'model_patchtst.weights.h5'))
            self.models_loaded['patchtst'] = True
            print("âœ… PatchTST ì•ˆì •í˜• ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            # PatchTST-PINN ê·¹ë‹¨í˜•
            self.system.model_patchtst_pinn = PatchTSTPINN(config)
            dummy_physics = np.zeros((1, 5))
            self.system.model_patchtst_pinn([dummy_input, dummy_physics])  # ë¹Œë“œ
            self.system.model_patchtst_pinn.load_weights(os.path.join(self.model_dir, 'model_patchtst_pinn.weights.h5'))
            self.models_loaded['patchtst_pinn'] = True
            print("âœ… PatchTST-PINN ê·¹ë‹¨í˜• ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ PatchTST ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.models_loaded['patchtst'] = False
            self.models_loaded['patchtst_pinn'] = False
        
        print(f"\nğŸ“Š ë¡œë“œ ì™„ë£Œ ëª¨ë¸ ìˆ˜: {sum(self.models_loaded.values())}/{len(self.models_loaded)}")
    
    def evaluate_sequences(self, sequences, feature_cols):
        """ëª¨ë“  ëª¨ë¸ë¡œ ì‹œí€€ìŠ¤ í‰ê°€"""
        print("\n[5ë‹¨ê³„] ëª¨ë¸ë³„ ì˜ˆì¸¡ ìˆ˜í–‰")
        print("-"*60)
        
        results = []
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¤€ë¹„
        all_X = np.array([seq['X'] for seq in sequences])
        all_features = self.system.prepare_features(all_X)
        all_physics = self.system.create_physics_features(all_X, feature_cols)
        
        # íŠ¹ì§• ì¸ë±ìŠ¤ ì„¤ì •
        self.system.get_feature_indices(feature_cols)
        
        # ë””ë²„ê¹… ì •ë³´
        print(f"\nğŸ“Š í‰ê°€ ì •ë³´:")
        print(f"  - ì „ì²´ ì‹œí€€ìŠ¤ ìˆ˜: {len(sequences)}")
        print(f"  - íŠ¹ì§• shape: {all_features.shape}")
        print(f"  - ë¬¼ë¦¬ íŠ¹ì§• shape: {all_physics.shape}")
        
        # ì‹¤ì œ ì í”„ ì¼€ì´ìŠ¤ í™•ì¸
        actual_jumps = sum(1 for seq in sequences if seq['is_jump_actual'])
        print(f"  - ì‹¤ì œ ì í”„ ì¼€ì´ìŠ¤: {actual_jumps} ({actual_jumps/len(sequences)*100:.2f}%)")
        
        # ì˜ˆì¸¡ ì €ì¥
        predictions = {}
        
        # 1. XGBoost ì í”„ ì˜ˆì¸¡
        if self.models_loaded.get('xgboost', False):
            print("\nğŸ”· XGBoost ì˜ˆì¸¡ ì¤‘...")
            predictions['xgboost'] = self.system.model_jump.predict(all_features)
            predictions['xgboost_rules'] = self.system.apply_rule_based_boost(all_features, predictions['xgboost'])
            print(f"  - XGBoost ì˜ˆì¸¡ ì í”„: {sum(predictions['xgboost'])}")
            print(f"  - XGBoost+ê·œì¹™ ì˜ˆì¸¡ ì í”„: {sum(predictions['xgboost_rules'])}")
        
        # 2. ExtraTrees ì í”„ ì˜ˆì¸¡
        if self.models_loaded.get('extra_trees', False):
            print("\nğŸ”¶ ExtraTrees ì˜ˆì¸¡ ì¤‘...")
            predictions['extra_trees'] = self.system.model_extra_rules.predict(all_features)
            predictions['extra_trees_rules'] = self.system.apply_rule_based_boost(all_features, predictions['extra_trees'])
            print(f"  - ExtraTrees ì˜ˆì¸¡ ì í”„: {sum(predictions['extra_trees'])}")
            print(f"  - ExtraTrees+ê·œì¹™ ì˜ˆì¸¡ ì í”„: {sum(predictions['extra_trees_rules'])}")
        
        # 3. ê°’ ì˜ˆì¸¡ ê¸°ë°˜ ì í”„ (300+ ì—¬ë¶€)
        if self.models_loaded.get('value', False):
            print("\nğŸ’° ê°’ ì˜ˆì¸¡ ê¸°ë°˜ ì í”„ íŒë‹¨...")
            value_predictions = self.system.model_value.predict(all_features)
            predictions['value_based_jump'] = (value_predictions >= 300).astype(int)
            print(f"  - ê°’ ì˜ˆì¸¡ ê¸°ë°˜ ì í”„: {sum(predictions['value_based_jump'])}")
        
        # 4. í•˜ì´ë¸Œë¦¬ë“œ (ExtraTrees + PatchTST)
        if self.models_loaded.get('extra_trees', False) and self.models_loaded.get('patchtst_pinn', False):
            print("\nğŸ”µ í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡ ì¤‘...")
            hybrid_preds = []
            
            # ExtraTrees í™•ë¥ 
            if hasattr(self.system.model_extra_rules, 'predict_proba'):
                et_proba = self.system.model_extra_rules.predict_proba(all_features)[:, 1]
            else:
                et_proba = self.system.model_extra_rules.predict(all_features).astype(float)
            
            # í•˜ì´ë¸Œë¦¬ë“œ ë¡œì§
            for i in tqdm(range(len(sequences)), desc="í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡"):
                if et_proba[i] > 0.8:
                    hybrid_preds.append(1)
                elif et_proba[i] < 0.2:
                    hybrid_preds.append(0)
                else:
                    # PatchTST-PINN ì‚¬ìš©
                    x_seq = all_X[i:i+1]
                    x_physics = all_physics[i:i+1]
                    
                    x_seq_scaled = self.system.scaler_X.transform(
                        x_seq.reshape(-1, x_seq.shape[-1])
                    ).reshape(x_seq.shape)
                    x_physics_scaled = self.system.scaler_physics.transform(x_physics)
                    
                    pred_value = self.system.model_patchtst_pinn.predict(
                        [x_seq_scaled, x_physics_scaled], verbose=0
                    )[0]
                    
                    # ì—­ë³€í™˜
                    pred_value = self.system.scaler_y.inverse_transform([[pred_value]])[0][0]
                    
                    hybrid_preds.append(1 if pred_value > 300 else 0)
            
            predictions['hybrid'] = np.array(hybrid_preds)
            print(f"  - í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡ ì í”„: {sum(predictions['hybrid'])}")
        
        # 5. 3êµ¬ê°„ ë¶„ë¥˜ ì˜ˆì¸¡
        if self.models_loaded.get('range', False):
            print("\nğŸ“Š 3êµ¬ê°„ ë¶„ë¥˜ ì˜ˆì¸¡...")
            range_predictions = self.system.model_range.predict(all_features)
            predictions['range_class'] = range_predictions
        
        # 6. íŠ¸ë Œë“œ ì˜ˆì¸¡
        if self.models_loaded.get('trend', False):
            print("\nğŸ“ˆ íŠ¸ë Œë“œ ì˜ˆì¸¡...")
            trend_predictions = self.system.model_trend.predict(all_features)
            predictions['trend_pattern'] = trend_predictions
        
        # 7. ExtraTreesRegressor ê°’ ì˜ˆì¸¡
        if self.models_loaded.get('value', False):
            print("\nğŸ’° ExtraTreesRegressor ê°’ ì˜ˆì¸¡...")
            value_predictions = self.system.model_value.predict(all_features)
            predictions['value_prediction'] = value_predictions
            print(f"  - ì˜ˆì¸¡ê°’ ë²”ìœ„: {value_predictions.min():.1f} ~ {value_predictions.max():.1f}")
            print(f"  - í‰ê·  ì˜ˆì¸¡ê°’: {value_predictions.mean():.1f}")
            print(f"  - 300+ ì˜ˆì¸¡ ê°œìˆ˜: {sum(value_predictions >= 300)}")
        
        # 8. PatchTST ì•ˆì •í˜• ê°’ ì˜ˆì¸¡
        if self.models_loaded.get('patchtst', False) and self.system.scaler_X is not None:
            print("\nğŸ”· PatchTST ì•ˆì •í˜• ê°’ ì˜ˆì¸¡...")
            patchtst_preds = []
            
            # ë°°ì¹˜ ì²˜ë¦¬
            X_scaled = self.system.scaler_X.transform(
                all_X.reshape(-1, all_X.shape[-1])
            ).reshape(all_X.shape)
            
            # ë°°ì¹˜ ì˜ˆì¸¡
            predictions_scaled = self.system.model_patchtst.predict(X_scaled, batch_size=32, verbose=0)
            
            # ì—­ë³€í™˜
            predictions_values = self.system.scaler_y.inverse_transform(
                predictions_scaled.reshape(-1, 1)
            ).flatten()
            
            predictions['patchtst_value'] = predictions_values
            print(f"  - PatchTST ì˜ˆì¸¡ê°’ ë²”ìœ„: {predictions_values.min():.1f} ~ {predictions_values.max():.1f}")
            print(f"  - PatchTST í‰ê·  ì˜ˆì¸¡ê°’: {predictions_values.mean():.1f}")
            print(f"  - PatchTST 300+ ì˜ˆì¸¡ ê°œìˆ˜: {sum(predictions_values >= 300)}")
        
        # 9. PatchTST-PINN ê·¹ë‹¨í˜• ê°’ ì˜ˆì¸¡
        if self.models_loaded.get('patchtst_pinn', False) and self.system.scaler_X is not None:
            print("\nğŸ”¶ PatchTST-PINN ê·¹ë‹¨í˜• ê°’ ì˜ˆì¸¡...")
            
            # ìŠ¤ì¼€ì¼ë§
            X_scaled = self.system.scaler_X.transform(
                all_X.reshape(-1, all_X.shape[-1])
            ).reshape(all_X.shape)
            X_physics_scaled = self.system.scaler_physics.transform(all_physics)
            
            # ë°°ì¹˜ ì˜ˆì¸¡
            predictions_scaled = self.system.model_patchtst_pinn.predict(
                [X_scaled, X_physics_scaled], 
                batch_size=32, 
                verbose=0
            )
            
            # ì—­ë³€í™˜
            predictions_values = self.system.scaler_y.inverse_transform(
                predictions_scaled.reshape(-1, 1)
            ).flatten()
            
            predictions['patchtst_pinn_value'] = predictions_values
            print(f"  - PatchTST-PINN ì˜ˆì¸¡ê°’ ë²”ìœ„: {predictions_values.min():.1f} ~ {predictions_values.max():.1f}")
            print(f"  - PatchTST-PINN í‰ê·  ì˜ˆì¸¡ê°’: {predictions_values.mean():.1f}")
            print(f"  - PatchTST-PINN 300+ ì˜ˆì¸¡ ê°œìˆ˜: {sum(predictions_values >= 300)}")
        
        # ê²°ê³¼ ì •ë¦¬
        for i, seq in enumerate(sequences):
            result = {
                'ë‚ ì§œ': seq['datetime_current'].strftime('%Y-%m-%d %H:%M'),
                'ì˜ˆì¸¡ë‚ ì§œ': seq['datetime_target'].strftime('%Y-%m-%d %H:%M'),
                'ì¸¡ì •ì‹œí€€ìŠ¤MAX': seq['seq_max'],
                'ì¸¡ì •ì‹œí€€ìŠ¤MIN': seq['seq_min'],
                'ì‹œí€€ìŠ¤ì‹œì‘ì‹œê°„': seq['datetime_start'].strftime('%Y-%m-%d %H:%M'),
                'ì‹œí€€ìŠ¤ì™„ë£Œì‹œê°„': seq['datetime_end'].strftime('%Y-%m-%d %H:%M'),
                'ì‹¤ì œê°’': seq['target_value'],
                'ì‹¤ì œì í”„ì¼€ì´ìŠ¤300+': seq['is_jump_actual']
            }
            
            # ì í”„ ì˜ˆì¸¡ (0/1)
            for model_name in ['xgboost', 'xgboost_rules', 'extra_trees', 'extra_trees_rules', 
                              'value_based_jump', 'hybrid']:
                if model_name in predictions:
                    result[f'ì í”„ì˜ˆì¸¡_{model_name}'] = predictions[model_name][i]
            
            # ê°’ ì˜ˆì¸¡ (ì‹¤ì œ ìˆ˜ì¹˜)
            for model_name in ['value_prediction', 'patchtst_value', 'patchtst_pinn_value']:
                if model_name in predictions:
                    result[f'ê°’ì˜ˆì¸¡_{model_name}'] = predictions[model_name][i]
            
            # ë¶„ë¥˜ ì˜ˆì¸¡
            if 'range_class' in predictions:
                result['range_class_ì˜ˆì¸¡'] = predictions['range_class'][i]
            
            if 'trend_pattern' in predictions:
                result['trend_pattern_ì˜ˆì¸¡'] = predictions['trend_pattern'][i]
            
            results.append(result)
        
        return pd.DataFrame(results)

# ==============================================================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰
# ==============================================================================

def main():
    """í‰ê°€ ë©”ì¸ í•¨ìˆ˜"""
    
    # ë°ì´í„° ì²˜ë¦¬
    processor = HubRoomDataProcessor()
    df = processor.load_evaluation_data()
    df = processor.create_all_features(df)
    sequences, feature_cols = processor.create_sequences(df)
    
    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = CompleteModelEvaluator()
    
    # ëª¨ë¸ ë¡œë“œ
    n_features = sequences[0]['X'].shape[1]
    evaluator.load_all_models(n_features)
    
    # í‰ê°€ ìˆ˜í–‰
    results_df = evaluator.evaluate_sequences(sequences, feature_cols)
    
    # ê²°ê³¼ ì €ì¥
    output_file = 'hubroom_evaluation_results_august_2025_complete.csv'
    results_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
    
    # ì„±ëŠ¥ ìš”ì•½
    print("\n" + "="*80)
    print("ğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½")
    print("="*80)
    
    actual_jumps = results_df['ì‹¤ì œì í”„ì¼€ì´ìŠ¤300+'].sum()
    total_samples = len(results_df)
    
    print(f"\nì „ì²´ ìƒ˜í”Œ: {total_samples}")
    print(f"ì‹¤ì œ ì í”„ ì¼€ì´ìŠ¤: {actual_jumps} ({actual_jumps/total_samples*100:.2f}%)")
    
    print("\n[1] ì í”„ ê°ì§€ ì„±ëŠ¥ (ì´ì§„ ë¶„ë¥˜)")
    print("-"*60)
    
    for col in results_df.columns:
        if col.startswith('ì í”„ì˜ˆì¸¡_'):
            model_name = col.replace('ì í”„ì˜ˆì¸¡_', '')
            predicted_jumps = results_df[col].sum()
            tp = ((results_df['ì‹¤ì œì í”„ì¼€ì´ìŠ¤300+'] == 1) & (results_df[col] == 1)).sum()
            recall = tp / actual_jumps if actual_jumps > 0 else 0
            correct = (results_df['ì‹¤ì œì í”„ì¼€ì´ìŠ¤300+'] == results_df[col]).sum()
            accuracy = correct / total_samples
            
            print(f"\n{model_name}:")
            print(f"  - ì í”„ ê°ì§€ìœ¨: {recall*100:.1f}% ({tp}/{actual_jumps})")
            print(f"  - ì „ì²´ ì •í™•ë„: {accuracy*100:.1f}%")
            print(f"  - ì˜ˆì¸¡ ì í”„ ìˆ˜: {predicted_jumps}")
    
    print("\n[2] ê°’ ì˜ˆì¸¡ ì„±ëŠ¥ (íšŒê·€)")
    print("-"*60)
    
    for col in results_df.columns:
        if col.startswith('ê°’ì˜ˆì¸¡_'):
            model_name = col.replace('ê°’ì˜ˆì¸¡_', '')
            pred_values = results_df[col]
            actual_values = results_df['ì‹¤ì œê°’']
            
            # MAE
            mae = np.abs(pred_values - actual_values).mean()
            
            # RMSE
            rmse = np.sqrt(((pred_values - actual_values) ** 2).mean())
            
            # 300+ ì˜ˆì¸¡ ì •í™•ë„
            pred_300plus = (pred_values >= 300).astype(int)
            actual_300plus = (actual_values >= 300).astype(int)
            accuracy_300 = (pred_300plus == actual_300plus).sum() / len(pred_300plus) * 100
            
            print(f"\n{model_name}:")
            print(f"  - MAE: {mae:.2f}")
            print(f"  - RMSE: {rmse:.2f}")
            print(f"  - ì˜ˆì¸¡ê°’ ë²”ìœ„: {pred_values.min():.1f} ~ {pred_values.max():.1f}")
            print(f"  - ì‹¤ì œê°’ ë²”ìœ„: {actual_values.min():.1f} ~ {actual_values.max():.1f}")
            print(f"  - 300+ ë¶„ë¥˜ ì •í™•ë„: {accuracy_300:.1f}%")
    
    # ì˜ˆì¸¡ ìƒ˜í”Œ ì¶œë ¥
    print("\n[3] ì˜ˆì¸¡ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ)")
    print("-"*60)
    
    sample_cols = ['ë‚ ì§œ', 'ì‹¤ì œê°’']
    for col in results_df.columns:
        if col.startswith('ê°’ì˜ˆì¸¡_'):
            sample_cols.append(col)
    
    if len(sample_cols) > 2:
        print(results_df[sample_cols].head(10).to_string(index=False))
    
    # ì í”„ ì¼€ì´ìŠ¤ ì˜ˆì¸¡ ë¹„êµ
    jump_cases = results_df[results_df['ì‹¤ì œì í”„ì¼€ì´ìŠ¤300+'] == 1]
    if len(jump_cases) > 0:
        print(f"\n[4] ì í”„ ì¼€ì´ìŠ¤ ì˜ˆì¸¡ ë¹„êµ ({len(jump_cases)}ê°œ)")
        print("-"*60)
        
        jump_cols = ['ë‚ ì§œ', 'ì¸¡ì •ì‹œí€€ìŠ¤MAX', 'ì‹¤ì œê°’']
        for col in results_df.columns:
            if col.startswith('ê°’ì˜ˆì¸¡_'):
                jump_cols.append(col)
        
        if len(jump_cols) > 3:
            print(jump_cases[jump_cols].head(10).to_string(index=False))
    
    print("\n" + "="*80)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("="*80)

if __name__ == "__main__":
    main()