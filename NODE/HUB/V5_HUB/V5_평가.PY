#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ğŸ¯ HUBROOM V4 Ultimate í‰ê°€ ì‹œìŠ¤í…œ
================================================================================
ëª©í‘œ:
- ê³¼ê±° 20ë¶„ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡
- Model 1: PatchTST (ì•ˆì •í˜•)
- Model 2: PatchTST+PINN (ê·¹ë‹¨í˜•)
- í‰ê°€ ë°ì´í„°: 2025ë…„ 8ì›” ë°ì´í„°
- CSV ê²°ê³¼ ì¶œë ¥
================================================================================
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import RobustScaler, StandardScaler
import joblib
import os
import pickle
import warnings
from datetime import datetime, timedelta
from tqdm import tqdm
import h5py

warnings.filterwarnings('ignore')

print("="*80)
print("ğŸ¯ HUBROOM V4 Ultimate í‰ê°€ ì‹œìŠ¤í…œ")
print("ğŸ“Š 20ë¶„ ì‹œí€€ìŠ¤ â†’ 10ë¶„ í›„ ì˜ˆì¸¡ í‰ê°€")
print("ğŸ“… í‰ê°€ ë°ì´í„°: 2025ë…„ 8ì›”")
print("="*80)

# ==============================================================================
# ğŸ“Š ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤
# ==============================================================================

class HubRoomDataProcessor:
    """V4 Ultimate ë°ì´í„° ì²˜ë¦¬"""
    
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        
        # V4 í•„ìˆ˜ 21ê°œ ì»¬ëŸ¼
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB',
            'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2',
            'M14B_7F_TO_HUB_JOB2',
            'M16B_10F_TO_HUB_JOB'
        ]
        
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB',
            'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB',
            'M16A_3F_TO_M14B_7F_JOB',
            'M16A_3F_TO_3F_MLUD_JOB'
        ]
        
        self.cmd_cols = [
            'M16A_3F_CMD',
            'M16A_6F_TO_HUB_CMD',
            'M16A_2F_TO_HUB_CMD',
            'M14A_3F_TO_HUB_CMD',
            'M14B_7F_TO_HUB_CMD'
        ]
        
        self.capa_cols = [
            'M16A_6F_LFT_MAXCAPA',
            'M16A_2F_LFT_MAXCAPA'
        ]
        
        self.other_cols = [
            'M16A_3F_STORAGE_UTIL',
            'M14_TO_M16_OFS_CUR',
            'M16_TO_M14_OFS_CUR'
        ]
        
        self.scaler_X = None
        self.scaler_y = None
        self.scaler_physics = None
    
    def load_and_merge_data(self, data_path, bridge_path=None):
        """ë°ì´í„° ë¡œë“œ ë° BRIDGE_TIME ë³‘í•©"""
        print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ë©”ì¸ ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(data_path)
        print(f"âœ… í‰ê°€ ë°ì´í„° ë¡œë“œ: {df.shape}")
        
        # ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬
        time_col = df.columns[0]
        if 'datetime' not in df.columns:
            df['datetime'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M')
        
        # BRIDGE_TIME ë°ì´í„° ë³‘í•© (ìˆë‹¤ë©´)
        if bridge_path and os.path.exists(bridge_path):
            print("ğŸ“‚ BRIDGE_TIME ë°ì´í„° ë³‘í•© ì¤‘...")
            bridge_df = pd.read_csv(bridge_path)
            
            if 'IDC_VAL' in bridge_df.columns:
                bridge_df['BRIDGE_TIME'] = bridge_df['IDC_VAL']
                bridge_df['datetime'] = pd.to_datetime(bridge_df['CRT_TM'])
                
                # ì‹œê°„ëŒ€ ì •ë³´ ì œê±°
                if hasattr(bridge_df['datetime'].dtype, 'tz'):
                    bridge_df['datetime'] = bridge_df['datetime'].dt.tz_localize(None)
                if hasattr(df['datetime'].dtype, 'tz'):
                    df['datetime'] = df['datetime'].dt.tz_localize(None)
                
                # ë¶„ ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
                bridge_df['datetime'] = bridge_df['datetime'].dt.floor('min')
                df['datetime'] = df['datetime'].dt.floor('min')
                
                # ë³‘í•©
                df = pd.merge(df, bridge_df[['datetime', 'BRIDGE_TIME']], 
                             on='datetime', how='left')
                
                # ë³´ê°„
                df['BRIDGE_TIME'] = df['BRIDGE_TIME'].interpolate(method='linear', limit_direction='both')
        
        # BRIDGE_TIMEì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
        if 'BRIDGE_TIME' not in df.columns:
            df['BRIDGE_TIME'] = 3.5
        
        df['BRIDGE_TIME'] = df['BRIDGE_TIME'].fillna(3.5)
        
        return df
    
    def create_features(self, df):
        """íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§"""
        print("\nğŸ”§ íŠ¹ì§• ìƒì„± ì¤‘...")
        
        # 1. ìœ ì…/ìœ ì¶œ ë°¸ëŸ°ìŠ¤
        df['total_inflow'] = df[self.inflow_cols].sum(axis=1)
        df['total_outflow'] = df[self.outflow_cols].sum(axis=1)
        df['flow_balance'] = df['total_inflow'] - df['total_outflow']
        df['flow_ratio'] = df['total_inflow'] / (df['total_outflow'] + 1)
        
        # 2. ì´ë™ í‰ê· 
        for window in [5, 10, 20]:
            df[f'target_ma{window}'] = df[self.target_col].rolling(window).mean()
            df[f'target_std{window}'] = df[self.target_col].rolling(window).std()
        
        # 3. ì¶”ì„¸ íŠ¹ì§•
        df['trend_20min'] = df[self.target_col] - df[self.target_col].shift(20)
        df['trend_10min'] = df[self.target_col] - df[self.target_col].shift(10)
        df['acceleration'] = df['trend_10min'] - df['trend_10min'].shift(10)
        
        # 4. CMD ê´€ë ¨
        df['cmd_sync_score'] = (df[self.cmd_cols] > 235).sum(axis=1)
        df['cmd_max'] = df[self.cmd_cols].max(axis=1)
        
        # 5. ê·¹ë‹¨ê°’ ê´€ë ¨ íŠ¹ì§•
        df['near_extreme'] = (df[self.target_col] > 280).astype(int)
        df['consecutive_high'] = (df[self.target_col] > 270).rolling(10).sum()
        
        # 6. ë¸Œë¦¿ì§€íƒ€ì„ ê´€ë ¨
        df['bridge_high'] = (df['BRIDGE_TIME'] > 4.0).astype(int)
        df['storage_x_bridge'] = df['M16A_3F_STORAGE_UTIL'] * df['BRIDGE_TIME']
        
        # NaN ì²˜ë¦¬
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(method='ffill').fillna(0)
        
        return df
    
    def prepare_physics_features(self, df):
        """ë¬¼ë¦¬ ê¸°ë°˜ íŠ¹ì§• ì¤€ë¹„"""
        physics_features = pd.DataFrame()
        
        # 1. ìœ ì…/ìœ ì¶œ ì´ëŸ‰
        physics_features['total_inflow'] = df[self.inflow_cols].sum(axis=1)
        physics_features['total_outflow'] = df[self.outflow_cols].sum(axis=1)
        
        # 2. ë°¸ëŸ°ìŠ¤
        physics_features['flow_balance'] = physics_features['total_inflow'] - physics_features['total_outflow']
        physics_features['flow_ratio'] = physics_features['total_inflow'] / (physics_features['total_outflow'] + 1)
        
        # 3. ìš©ëŸ‰ ì œí•œ
        physics_features['capa_mean'] = df[self.capa_cols].mean(axis=1)
        physics_features['storage_util'] = df['M16A_3F_STORAGE_UTIL']
        
        # 4. OFS
        physics_features['ofs_balance'] = df['M14_TO_M16_OFS_CUR'] - df['M16_TO_M14_OFS_CUR']
        
        # 5. CMD ë™ê¸°í™”
        physics_features['cmd_sync'] = df['cmd_sync_score']
        
        # 6. ë¸Œë¦¿ì§€íƒ€ì„
        physics_features['bridge_time'] = df['BRIDGE_TIME']
        
        return physics_features
    
    def create_sequences_for_evaluation(self, df, seq_len=20, pred_len=10):
        """í‰ê°€ìš© ì‹œí€€ìŠ¤ ìƒì„±"""
        print(f"\nğŸ“Š ì‹œí€€ìŠ¤ ìƒì„± ì¤‘ ({seq_len}ë¶„ â†’ {pred_len}ë¶„ í›„)...")
        
        # íŠ¹ì§• ì»¬ëŸ¼ ì„ íƒ
        feature_cols = [self.target_col] + self.inflow_cols + self.outflow_cols + \
                      self.cmd_cols + self.capa_cols + self.other_cols + ['BRIDGE_TIME']
        
        # íŒŒìƒ íŠ¹ì§•ë“¤ë„ ì¶”ê°€
        derived_features = [
            'total_inflow', 'total_outflow', 'flow_balance', 'flow_ratio',
            'target_ma5', 'target_ma10', 'target_ma20',
            'target_std5', 'target_std10', 'target_std20',
            'trend_20min', 'trend_10min', 'acceleration',
            'cmd_sync_score', 'cmd_max', 'near_extreme', 'consecutive_high',
            'bridge_high', 'storage_x_bridge'
        ]
        
        # ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸ í›„ ì¶”ê°€
        for col in derived_features:
            if col in df.columns and col not in feature_cols:
                feature_cols.append(col)
        
        # ë¬¼ë¦¬ íŠ¹ì§• ì¤€ë¹„
        physics_df = self.prepare_physics_features(df)
        
        sequences = []
        physics_sequences = []
        targets = []
        timestamps = []
        sequence_info = []
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        for i in tqdm(range(seq_len, len(df) - pred_len), desc="ì‹œí€€ìŠ¤ ìƒì„±"):
            # ì…ë ¥ ì‹œí€€ìŠ¤ (ê³¼ê±° 20ë¶„)
            seq = df[feature_cols].iloc[i-seq_len:i].values
            physics_seq = physics_df.iloc[i-seq_len:i].values
            
            # íƒ€ê²Ÿ (10ë¶„ í›„)
            target = df[self.target_col].iloc[i + pred_len - 1]
            
            # ì‹œê°„ ì •ë³´
            current_time = df['datetime'].iloc[i-1]
            predict_time = df['datetime'].iloc[i + pred_len - 1]
            seq_start_time = df['datetime'].iloc[i-seq_len]
            seq_end_time = df['datetime'].iloc[i-1]
            
            # ì‹œí€€ìŠ¤ í†µê³„
            seq_values = df[self.target_col].iloc[i-seq_len:i].values
            seq_max = np.max(seq_values)
            seq_min = np.min(seq_values)
            
            sequences.append(seq)
            physics_sequences.append(physics_seq)
            targets.append(target)
            timestamps.append(current_time)
            
            sequence_info.append({
                'current_time': current_time,
                'predict_time': predict_time,
                'seq_start_time': seq_start_time,
                'seq_end_time': seq_end_time,
                'seq_max': seq_max,
                'seq_min': seq_min,
                'target_value': target
            })
        
        print(f"âœ… ì´ {len(sequences)}ê°œ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        return (np.array(sequences), np.array(physics_sequences), 
                np.array(targets), timestamps, sequence_info)

# ==============================================================================
# ğŸ¤– ëª¨ë¸ ì •ì˜ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼)
# ==============================================================================

class PatchTST(keras.Model):
    """PatchTST Base Model"""
    def __init__(self, config):
        super().__init__()
        
        self.patch_len = config['patch_len']
        self.stride = config['stride']
        self.d_model = config['d_model']
        self.n_heads = config['n_heads']
        self.d_ff = config['d_ff']
        self.n_layers = config['n_layers']
        self.seq_len = config['seq_len']
        self.pred_len = config['pred_len']
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(self.d_model)
        self.positional_encoding = self._get_positional_encoding()
        
        # Transformer ì¸ì½”ë”
        self.encoder_layers = [
            layers.MultiHeadAttention(
                num_heads=self.n_heads,
                key_dim=self.d_model // self.n_heads,
                dropout=0.1
            ) for _ in range(self.n_layers)
        ]
        
        self.ffn_layers = [
            keras.Sequential([
                layers.Dense(self.d_ff, activation='relu'),
                layers.Dropout(0.1),
                layers.Dense(self.d_model),
                layers.Dropout(0.1)
            ]) for _ in range(self.n_layers)
        ]
        
        self.layer_norms = [[layers.LayerNormalization(epsilon=1e-6) 
                            for _ in range(2)] 
                           for _ in range(self.n_layers)]
        
        # ì¶œë ¥ì¸µ
        self.flatten = layers.Flatten()
        self.output_projection = layers.Dense(self.pred_len)
    
    def _get_positional_encoding(self):
        max_patches = self.seq_len // self.stride
        position = np.arange(max_patches)[:, np.newaxis]
        div_term = np.exp(np.arange(0, self.d_model, 2) * 
                         -(np.log(10000.0) / self.d_model))
        
        pos_encoding = np.zeros((max_patches, self.d_model))
        pos_encoding[:, 0::2] = np.sin(position * div_term)
        pos_encoding[:, 1::2] = np.cos(position * div_term)
        
        return tf.constant(pos_encoding, dtype=tf.float32)
    
    def create_patches(self, x):
        batch_size = tf.shape(x)[0]
        n_vars = tf.shape(x)[2]
        
        patches = []
        for i in range(0, self.seq_len - self.patch_len + 1, self.stride):
            patch = x[:, i:i+self.patch_len, :]
            patches.append(patch)
        
        patches = tf.stack(patches, axis=1)
        patches = tf.reshape(patches, [batch_size, -1, self.patch_len * n_vars])
        
        return patches
    
    def call(self, inputs, training=False):
        x = inputs
        
        # íŒ¨ì¹˜ ìƒì„±
        patches = self.create_patches(x)
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x = self.patch_embedding(patches)
        
        # ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        seq_len = tf.shape(x)[1]
        x = x + self.positional_encoding[:seq_len]
        
        # Transformer ì¸ì½”ë”
        for i in range(self.n_layers):
            # Multi-head attention
            attn_output = self.encoder_layers[i](x, x, training=training)
            x = self.layer_norms[i][0](x + attn_output)
            
            # Feed forward
            ffn_output = self.ffn_layers[i](x, training=training)
            x = self.layer_norms[i][1](x + ffn_output)
        
        # ì¶œë ¥ ìƒì„±
        x = self.flatten(x)
        output = self.output_projection(x)
        
        return output

class PatchTSTPINN(keras.Model):
    """PatchTST + Physics-Informed Neural Network"""
    def __init__(self, config):
        super().__init__()
        
        self.patchtst = PatchTST(config)
        
        # ë¬¼ë¦¬ ë„¤íŠ¸ì›Œí¬
        self.physics_net = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(8, activation='relu')
        ])
        
        # í†µí•© ë„¤íŠ¸ì›Œí¬
        self.fusion_net = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(16, activation='relu'),
            layers.Dense(config['pred_len'])
        ])
    
    def call(self, inputs, training=False):
        x_seq, x_physics = inputs
        
        # PatchTST ì˜ˆì¸¡
        patchtst_out = self.patchtst(x_seq, training=training)
        
        # ë¬¼ë¦¬ íŠ¹ì§• ì²˜ë¦¬
        physics_out = self.physics_net(x_physics[:, -1, :], training=training)
        
        # í†µí•©
        combined = tf.concat([patchtst_out, physics_out], axis=1)
        output = self.fusion_net(combined, training=training)
        
        return output

# ==============================================================================
# ğŸ§® ëª¨ë¸ ì„ íƒ ë¡œì§
# ==============================================================================

class ModelSelector:
    """V4 Ultimate ëª¨ë¸ ì„ íƒ ë¡œì§"""
    
    @staticmethod
    def select_model(sequence_data, past_max, recent_5min_mean, acceleration):
        """
        ëª¨ë¸ ì„ íƒ ë¡œì§
        Returns: 'model1' or 'model2'
        """
        # ê°•í•œ í•˜ë½ ì¶”ì„¸ â†’ Model 1
        if acceleration < -50:
            return 'model1', "ê°•í•œ í•˜ë½ ì¶”ì„¸"
        
        # 277 ì í”„ ê°ì§€ êµ¬ê°„ â†’ Model 2
        if 275 <= past_max <= 279:
            return 'model2', "277 ì í”„ ìœ„í—˜ êµ¬ê°„"
        
        # ê·¹ë‹¨ê°’ ê·¼ì ‘ â†’ Model 2
        if recent_5min_mean > 285:
            return 'model2', "ê·¹ë‹¨ê°’ ê·¼ì ‘"
        
        # ê°€ì† ì‹ í˜¸ â†’ Model 2
        if acceleration > 5:
            return 'model2', "ê°€ì† ì‹ í˜¸ ê°ì§€"
        
        # ì•ˆì • êµ¬ê°„ â†’ Model 1
        if past_max < 250:
            return 'model1', "ì•ˆì • êµ¬ê°„"
        
        # ê¸°ë³¸ê°’
        return 'model1', "ê¸°ë³¸ ì„ íƒ"
    
    @staticmethod
    def apply_fine_tuning(model_name, prediction, past_max, recent_mean):
        """V4 íŒŒì¸íŠœë‹ ë¡œì§"""
        if model_name == 'model1':
            # ì•ˆì •í˜• ì¡°ì •
            if past_max < 270:
                return prediction * 0.98
            elif prediction > 300 and past_max < 280:
                return 295  # False Positive ë°©ì§€
        else:
            # ê·¹ë‹¨í˜• ì¡°ì •
            if recent_mean > 310:
                return prediction * 1.07
            elif 275 <= past_max <= 279 and recent_mean >= 275:
                return 308  # 277 ì í”„ ì˜ˆì¸¡
        
        return prediction

# ==============================================================================
# ğŸ“Š í‰ê°€ ë° ê²°ê³¼ ì €ì¥
# ==============================================================================

def evaluate_models():
    """ëª¨ë¸ í‰ê°€ ë©”ì¸ í•¨ìˆ˜"""
    
    print("\nğŸš€ V4 Ultimate í‰ê°€ ì‹œì‘...")
    
    # 1. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint_dir = './checkpoints_ultimate'
    if not os.path.exists(checkpoint_dir):
        print("âŒ í•™ìŠµëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í•™ìŠµì„ ì§„í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    # 2. ë°ì´í„° ì²˜ë¦¬
    processor = HubRoomDataProcessor()
    
    # í‰ê°€ ë°ì´í„° ë¡œë“œ
    eval_data_path = 'data/20250801_to_20250831.csv'
    bridge_data_path = 'data/BRIDGE_TIME_202508.csv'
    
    if not os.path.exists(eval_data_path):
        print(f"âŒ í‰ê°€ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {eval_data_path}")
        return
    
    df = processor.load_and_merge_data(eval_data_path, bridge_data_path)
    df = processor.create_features(df)
    
    # 3. ì‹œí€€ìŠ¤ ìƒì„±
    X_seq, X_physics, y_true, timestamps, sequence_info = processor.create_sequences_for_evaluation(df)
    
    print(f"\nğŸ“Š í‰ê°€ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
    print(f"  - ì‹œí€€ìŠ¤ ìˆ˜: {len(X_seq)}")
    print(f"  - ì‹œí€€ìŠ¤ í˜•íƒœ: {X_seq.shape}")
    print(f"  - ë¬¼ë¦¬ íŠ¹ì§• í˜•íƒœ: {X_physics.shape}")
    
    # 4. ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    scaler_path = os.path.join(checkpoint_dir, 'scalers')
    processor.scaler_X = joblib.load(os.path.join(scaler_path, 'scaler_X.pkl'))
    processor.scaler_y = joblib.load(os.path.join(scaler_path, 'scaler_y.pkl'))
    processor.scaler_physics = joblib.load(os.path.join(scaler_path, 'scaler_physics.pkl'))
    
    # ë°ì´í„° ì •ê·œí™”
    X_seq_scaled = processor.scaler_X.transform(X_seq.reshape(-1, X_seq.shape[-1])).reshape(X_seq.shape)
    X_physics_scaled = processor.scaler_physics.transform(X_physics.reshape(-1, X_physics.shape[-1])).reshape(X_physics.shape)
    
    # 5. ëª¨ë¸ ë¡œë“œ
    print("\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    # ëª¨ë¸ ì„¤ì •
    config = {
        'seq_len': 20,
        'pred_len': 10,
        'patch_len': 5,
        'stride': 2,
        'd_model': 128,
        'n_heads': 8,
        'd_ff': 512,
        'n_layers': 3
    }
    
    # Model 1 ë¡œë“œ
    model1 = PatchTST(config)
    model1.build([(None, 20, X_seq.shape[-1])])
    model1.load_weights(os.path.join(checkpoint_dir, 'model1_stable.h5'))
    
    # Model 2 ë¡œë“œ
    model2 = PatchTSTPINN(config)
    model2.build([(None, 20, X_seq.shape[-1]), (None, 20, 9)])
    model2.load_weights(os.path.join(checkpoint_dir, 'model2_extreme.h5'))
    
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # 6. ì˜ˆì¸¡ ìˆ˜í–‰
    print("\nğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
    
    results = []
    model_selector = ModelSelector()
    
    for i in tqdm(range(len(X_seq)), desc="ì˜ˆì¸¡ ì§„í–‰"):
        # ì‹œí€€ìŠ¤ ì •ë³´
        seq_info = sequence_info[i]
        seq_values = df[processor.target_col].iloc[i:i+20].values
        
        # í†µê³„ ê³„ì‚°
        past_max = np.max(seq_values)
        recent_5min_mean = np.mean(seq_values[-5:])
        acceleration = (np.mean(seq_values[-5:]) - np.mean(seq_values[-10:-5])) - \
                      (np.mean(seq_values[-10:-5]) - np.mean(seq_values[-15:-10]))
        
        # ëª¨ë¸ ì„ íƒ
        selected_model, reason = model_selector.select_model(
            seq_values, past_max, recent_5min_mean, acceleration
        )
        
        # ì˜ˆì¸¡
        if selected_model == 'model1':
            pred_scaled = model1.predict(X_seq_scaled[i:i+1], verbose=0)
            pred = processor.scaler_y.inverse_transform(pred_scaled.reshape(-1, 1))[0, 0]
        else:
            pred_scaled = model2.predict([X_seq_scaled[i:i+1], X_physics_scaled[i:i+1]], verbose=0)
            pred = processor.scaler_y.inverse_transform(pred_scaled.reshape(-1, 1))[0, 0]
        
        # íŒŒì¸íŠœë‹
        pred_adjusted = model_selector.apply_fine_tuning(
            selected_model, pred, past_max, recent_5min_mean
        )
        
        # ì í”„ ì¼€ì´ìŠ¤ í™•ì¸
        is_jump = (past_max < 280) and (seq_info['target_value'] >= 300)
        
        # ê²°ê³¼ ì €ì¥
        results.append({
            'ë‚ ì§œ': seq_info['current_time'].strftime('%Y-%m-%d %H:%M'),
            'ì˜ˆì¸¡ë‚ ì§œ': seq_info['predict_time'].strftime('%Y-%m-%d %H:%M'),
            'ì¸¡ì •ì‹œí€€ìŠ¤MAX': round(seq_info['seq_max'], 2),
            'ì¸¡ì •ì‹œí€€ìŠ¤MIN': round(seq_info['seq_min'], 2),
            'ì‹œí€€ìŠ¤ì‹œì‘ì‹œê°„': seq_info['seq_start_time'].strftime('%Y-%m-%d %H:%M'),
            'ì‹œí€€ìŠ¤ì™„ë£Œì‹œê°„': seq_info['seq_end_time'].strftime('%Y-%m-%d %H:%M'),
            'ì‹¤ì œê°’': round(seq_info['target_value'], 2),
            'ì˜ˆì¸¡ê°’': round(pred_adjusted, 2),
            'ì˜¤ì°¨': round(abs(seq_info['target_value'] - pred_adjusted), 2),
            'ì„ íƒëª¨ë¸': selected_model,
            'ì„ íƒì´ìœ ': reason,
            'ì í”„ì¼€ì´ìŠ¤300+': 'O' if is_jump else 'X'
        })
    
    # 7. ê²°ê³¼ ë¶„ì„
    results_df = pd.DataFrame(results)
    
    print("\nğŸ“Š í‰ê°€ ê²°ê³¼ ë¶„ì„")
    print("="*60)
    
    # ì „ì²´ ì„±ëŠ¥
    mae = results_df['ì˜¤ì°¨'].mean()
    rmse = np.sqrt((results_df['ì˜¤ì°¨'] ** 2).mean())
    
    print(f"\nğŸ“ˆ ì „ì²´ ì„±ëŠ¥:")
    print(f"  - MAE: {mae:.2f}")
    print(f"  - RMSE: {rmse:.2f}")
    print(f"  - ì´ ì˜ˆì¸¡ ìˆ˜: {len(results_df)}")
    
    # ê·¹ë‹¨ê°’ ê°ì§€ ì„±ëŠ¥
    extreme_mask = results_df['ì‹¤ì œê°’'] >= 300
    if extreme_mask.sum() > 0:
        extreme_detected = (results_df[extreme_mask]['ì˜ˆì¸¡ê°’'] >= 300).sum()
        extreme_recall = extreme_detected / extreme_mask.sum() * 100
        print(f"\nğŸ¯ ê·¹ë‹¨ê°’(300+) ê°ì§€:")
        print(f"  - ì‹¤ì œ 300+ ì¼€ì´ìŠ¤: {extreme_mask.sum()}ê°œ")
        print(f"  - ê°ì§€ëœ ì¼€ì´ìŠ¤: {extreme_detected}ê°œ")
        print(f"  - ê°ì§€ìœ¨: {extreme_recall:.1f}%")
    
    # ì í”„ ì¼€ì´ìŠ¤ ì„±ëŠ¥
    jump_cases = results_df[results_df['ì í”„ì¼€ì´ìŠ¤300+'] == 'O']
    if len(jump_cases) > 0:
        jump_detected = (jump_cases['ì˜ˆì¸¡ê°’'] >= 290).sum()
        jump_recall = jump_detected / len(jump_cases) * 100
        print(f"\nğŸš€ ì í”„ ì¼€ì´ìŠ¤ (ê³¼ê±°<280 â†’ ì‹¤ì œ300+):")
        print(f"  - ì´ ì í”„ ì¼€ì´ìŠ¤: {len(jump_cases)}ê°œ")
        print(f"  - ê°ì§€ëœ ì¼€ì´ìŠ¤: {jump_detected}ê°œ")
        print(f"  - ì í”„ ê°ì§€ìœ¨: {jump_recall:.1f}%")
    
    # False Positive
    stable_mask = results_df['ì‹¤ì œê°’'] < 300
    if stable_mask.sum() > 0:
        fp_count = (results_df[stable_mask]['ì˜ˆì¸¡ê°’'] >= 300).sum()
        fp_rate = fp_count / stable_mask.sum() * 100
        print(f"\nâš ï¸ False Positive:")
        print(f"  - ì‹¤ì œ <300 ì¼€ì´ìŠ¤: {stable_mask.sum()}ê°œ")
        print(f"  - ì˜ëª» ì˜ˆì¸¡ëœ ì¼€ì´ìŠ¤: {fp_count}ê°œ")
        print(f"  - FP Rate: {fp_rate:.1f}%")
    
    # ëª¨ë¸ ì„ íƒ í†µê³„
    print(f"\nğŸ¤– ëª¨ë¸ ì„ íƒ í†µê³„:")
    model_stats = results_df['ì„ íƒëª¨ë¸'].value_counts()
    for model, count in model_stats.items():
        print(f"  - {model}: {count}ê°œ ({count/len(results_df)*100:.1f}%)")
    
    # 8. CSV íŒŒì¼ ì €ì¥
    output_path = 'hubroom_v4_evaluation_results.csv'
    results_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
    
    # ìƒ˜í”Œ ì¶œë ¥
    print(f"\nğŸ“‹ ìƒ˜í”Œ ê²°ê³¼ (ì²˜ìŒ 10ê°œ):")
    print(results_df[['ë‚ ì§œ', 'ì˜ˆì¸¡ë‚ ì§œ', 'ì‹¤ì œê°’', 'ì˜ˆì¸¡ê°’', 'ì˜¤ì°¨', 'ì í”„ì¼€ì´ìŠ¤300+']].head(10))
    
    print("\n" + "="*80)
    print("âœ… V4 Ultimate í‰ê°€ ì™„ë£Œ!")
    print("="*80)

if __name__ == "__main__":
    evaluate_models()