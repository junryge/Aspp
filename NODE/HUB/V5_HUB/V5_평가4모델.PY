#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ğŸ¯ HUBROOM ì™„ì „ í‰ê°€ ì‹œìŠ¤í…œ - 4ê°œ ëª¨ë¸ í†µí•© í™œìš©
================================================================================
ëª©í‘œ:
- 4ê°œ ëª¨ë¸ ëª¨ë‘ ì œëŒ€ë¡œ í‰ê°€ ë° í™œìš©:
  * model_jump: ì í”„ ê°ì§€ (280ë¯¸ë§Œâ†’300+ ê¸‰ìƒìŠ¹)
  * model_range: 3êµ¬ê°„ ë¶„ë¥˜ (ì €ìœ„í—˜/ì¤‘ìœ„í—˜/ê³ ìœ„í—˜)  
  * model_trend: ìƒìŠ¹/í•˜ë½ íŒ¨í„´ (í•˜ë½/ì•ˆì •/ì ì§„ìƒìŠ¹/ê¸‰ìƒìŠ¹)
  * model_value: ê°’ ì˜ˆì¸¡
- ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ + í†µí•© ì„±ëŠ¥ í‰ê°€
- ìƒì„¸í•œ CSV ê²°ê³¼ ì¶œë ¥
================================================================================
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, ExtraTreesRegressor
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, r2_score, accuracy_score
from xgboost import XGBClassifier
import joblib
import os
import pickle
from datetime import datetime, timedelta
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("ğŸ¯ HUBROOM ì™„ì „ í‰ê°€ ì‹œìŠ¤í…œ - 4ê°œ ëª¨ë¸ í†µí•©")
print("ğŸ“Š 30ë¶„ ì‹œí€€ìŠ¤ â†’ 10ë¶„ í›„ ì˜ˆì¸¡")
print("ğŸ¤– model_jump + model_range + model_trend + model_value")
print("="*80)

# ==============================================================================
# ğŸ“Š ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤ (ë™ì¼)
# ==============================================================================

class HubRoomDataProcessor:
    """ì™„ì „í•œ ë°ì´í„° ì²˜ë¦¬ - ëª¨ë“  íŠ¹ì§• í¬í•¨"""
    
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        
        # 21ê°œ í•„ìˆ˜ ì»¬ëŸ¼
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB',
            'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2',
            'M14B_7F_TO_HUB_JOB2',
            'M16B_10F_TO_HUB_JOB'
        ]
        
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB',
            'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB',
            'M16A_3F_TO_M14B_7F_JOB',
            'M16A_3F_TO_3F_MLUD_JOB'
        ]
        
        self.cmd_cols = [
            'M16A_3F_CMD',
            'M16A_6F_TO_HUB_CMD',
            'M16A_2F_TO_HUB_CMD',
            'M14A_3F_TO_HUB_CMD',
            'M14B_7F_TO_HUB_CMD'
        ]
        
        self.capa_cols = [
            'M16A_6F_LFT_MAXCAPA',
            'M16A_2F_LFT_MAXCAPA'
        ]
        
        self.other_cols = [
            'M16A_3F_STORAGE_UTIL',
            'M14_TO_M16_OFS_CUR',
            'M16_TO_M14_OFS_CUR'
        ]
        
        # í™•ë¥  ë§µ
        self.probability_map = {
            0: 0.003, 1: 0.15, 2: 0.25, 3: 0.31, 4: 0.43, 5: 0.43,
            6: 0.35, 7: 0.42, 8: 0.53, 9: 0.49, 10: 0.42,
            11: 0.47, 12: 0.52, 13: 0.60, 14: 0.54, 15: 0.66,
            16: 0.62, 17: 0.71, 18: 0.79, 19: 0.83, 20: 0.987,
            21: 0.99, 22: 0.99, 23: 0.99, 24: 0.99, 25: 0.99,
            26: 0.99, 27: 0.99, 28: 0.99, 29: 0.99, 30: 0.99
        }
    
    def load_and_merge_data(self, data_path):
        """ë°ì´í„° ë¡œë“œ ë° BRIDGE_TIME ë³‘í•©"""
        print("\n[1ë‹¨ê³„] ë°ì´í„° ë¡œë“œ")
        
        # ë©”ì¸ ë°ì´í„°
        df = pd.read_csv(data_path)
        print(f"âœ… í‰ê°€ ë°ì´í„°: {df.shape}")
        
        # ì‹œê°„ ì²˜ë¦¬
        time_col = df.columns[0]
        df['datetime'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M')
        
        # BRIDGE_TIME ì²˜ë¦¬ (ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
        if 'BRIDGE_TIME' not in df.columns:
            df['BRIDGE_TIME'] = 3.5
            print("âš ï¸ BRIDGE_TIME ì—†ìŒ - ê¸°ë³¸ê°’ 3.5 ì‚¬ìš©")
        else:
            df['BRIDGE_TIME'] = df['BRIDGE_TIME'].fillna(3.5)
            print("âœ… BRIDGE_TIME ì²˜ë¦¬ ì™„ë£Œ")
        
        return df
    
    def create_all_features(self, df):
        """ì™„ì „í•œ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§"""
        print("\n[2ë‹¨ê³„] íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§")
        
        # ê¸°ë³¸ íŠ¹ì§•ë“¤
        df['flow_balance'] = df[self.inflow_cols].sum(axis=1) - df[self.outflow_cols].sum(axis=1)
        df['flow_ratio'] = df[self.inflow_cols].sum(axis=1) / (df[self.outflow_cols].sum(axis=1) + 1)
        df['trend_20min'] = df[self.target_col].diff(20)
        df['trend_10min'] = df[self.target_col].diff(10)
        df['acceleration'] = df['trend_10min'] - df['trend_10min'].shift(10)
        df['consecutive_250+'] = (df[self.target_col] > 250).rolling(10).sum()
        df['consecutive_270+'] = (df[self.target_col] > 270).rolling(10).sum()
        df['cmd_sync_count'] = (df[self.cmd_cols] > 235).sum(axis=1)
        df['cmd_max'] = df[self.cmd_cols].max(axis=1)
        df['bridge_diff'] = df['BRIDGE_TIME'].diff(5)
        df['bridge_high'] = (df['BRIDGE_TIME'] > 4.0).astype(int)
        df['storage_x_bridge'] = df['M16A_3F_STORAGE_UTIL'] * df['BRIDGE_TIME']
        
        # ì—°ì† 300+ í™•ë¥ 
        consecutive_300_counts = []
        consecutive_300_probs = []
        
        for i in tqdm(range(len(df)), desc="300+ íŒ¨í„´ ê³„ì‚°"):
            if i < 30:
                count = 0
                prob = 0.003
            else:
                window = df[self.target_col].iloc[i-30:i].values
                count = sum(1 for v in window if v >= 300)
                prob = self.probability_map.get(count, 0.5)
            
            consecutive_300_counts.append(count)
            consecutive_300_probs.append(prob)
        
        df['consecutive_300_count'] = consecutive_300_counts
        df['consecutive_300_prob'] = consecutive_300_probs
        
        # ë¶„ë¥˜ ë¼ë²¨ë“¤
        conditions = [df[self.target_col] < 150, (df[self.target_col] >= 150) & (df[self.target_col] < 300), df[self.target_col] >= 300]
        df['range_class'] = np.select(conditions, [0, 1, 2], default=1)
        
        df['past_30min_max'] = df[self.target_col].rolling(30).max()
        df['is_jump'] = ((df['past_30min_max'].shift(10) < 280) & (df[self.target_col] >= 300)).astype(int)
        
        df['change_20min'] = df[self.target_col] - df[self.target_col].shift(20)
        trend_conditions = [df['change_20min'] < -20, (df['change_20min'] >= -20) & (df['change_20min'] < 20), (df['change_20min'] >= 20) & (df['change_20min'] < 50), df['change_20min'] >= 50]
        df['trend_pattern'] = np.select(trend_conditions, [0, 1, 2, 3], default=1)
        
        # ì¶”ê°€ íŠ¹ì§•ë“¤
        df['change_rate_10min'] = ((df[self.target_col] - df[self.target_col].shift(10)) / (df[self.target_col].shift(10) + 1)) * 100
        df['change_rate_20min'] = ((df[self.target_col] - df[self.target_col].shift(20)) / (df[self.target_col].shift(20) + 1)) * 100
        df['change_rate_30min'] = ((df[self.target_col] - df[self.target_col].shift(30)) / (df[self.target_col].shift(30) + 1)) * 100
        df['volatility_10min'] = df[self.target_col].rolling(10).std()
        df['volatility_20min'] = df[self.target_col].rolling(20).std()
        df['volatility_30min'] = df[self.target_col].rolling(30).std()
        df['distance_to_300'] = 300 - df[self.target_col]
        df['near_extreme'] = (df[self.target_col] > 280).astype(int)
        df['recent_5min_mean'] = df[self.target_col].rolling(5).mean()
        df['recent_5min_max'] = df[self.target_col].rolling(5).max()
        df['recent_10min_mean'] = df[self.target_col].rolling(10).mean()
        df['in_jump_zone'] = ((df[self.target_col] >= 275) & (df[self.target_col] <= 279)).astype(int)
        
        # NaN ì²˜ë¦¬
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].fillna(method='ffill').fillna(0)
        
        print(f"âœ… ì´ {len(df.columns)}ê°œ íŠ¹ì§• ìƒì„± ì™„ë£Œ")
        return df
    
    def create_sequences_for_evaluation(self, df, seq_len=30, pred_len=10):
        """í‰ê°€ìš© ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
        print(f"\n[3ë‹¨ê³„] ì‹œí€€ìŠ¤ ìƒì„± ({seq_len}ë¶„ â†’ {pred_len}ë¶„ í›„)")
        
        # íŠ¹ì§• ì»¬ëŸ¼ ì„ íƒ
        feature_cols = [col for col in df.columns 
                       if col not in ['datetime', 'range_class', 'is_jump', 'trend_pattern']]
        
        X = []
        y_info = []
        
        for i in tqdm(range(seq_len, len(df) - pred_len), desc="ì‹œí€€ìŠ¤ ìƒì„±"):
            # ì…ë ¥: ê³¼ê±° 30ë¶„
            X.append(df[feature_cols].iloc[i-seq_len:i].values)
            
            # íƒ€ê²Ÿ ì •ë³´ë“¤
            target_idx = i + pred_len - 1
            
            # ì‹œê°„ ì •ë³´
            current_time = df['datetime'].iloc[i-1]
            predict_time = df['datetime'].iloc[target_idx]
            seq_start_time = df['datetime'].iloc[i-seq_len]
            seq_end_time = df['datetime'].iloc[i-1]
            
            # ì‹œí€€ìŠ¤ í†µê³„
            seq_values = df[self.target_col].iloc[i-seq_len:i].values
            seq_max = np.max(seq_values)
            seq_min = np.min(seq_values)
            
            y_info.append({
                'current_time': current_time,
                'predict_time': predict_time,
                'seq_start_time': seq_start_time,
                'seq_end_time': seq_end_time,
                'seq_max': seq_max,
                'seq_min': seq_min,
                'target_value': df[self.target_col].iloc[target_idx],
                'is_jump': df['is_jump'].iloc[target_idx],
                'range_class': df['range_class'].iloc[target_idx],
                'trend_pattern': df['trend_pattern'].iloc[target_idx]
            })
        
        print(f"âœ… {len(X)}ê°œ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        return np.array(X), y_info, df

# ==============================================================================
# ğŸ¤– 4ê°œ ëª¨ë¸ í†µí•© í‰ê°€ ì‹œìŠ¤í…œ
# ==============================================================================

class ComprehensiveEvaluationSystem:
    """4ê°œ ëª¨ë¸ ì™„ì „ í™œìš© í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # 4ê°œ ëª¨ë¸
        self.model_jump = None    # ì í”„ ê°ì§€
        self.model_range = None   # 3êµ¬ê°„ ë¶„ë¥˜
        self.model_trend = None   # ìƒìŠ¹/í•˜ë½ íŒ¨í„´
        self.model_value = None   # ê°’ ì˜ˆì¸¡
        
        self.feature_indices = {}
    
    def prepare_features(self, X_seq):
        """ì‹œí€€ìŠ¤ë¥¼ íŠ¹ì§•ìœ¼ë¡œ ë³€í™˜"""
        # ë§ˆì§€ë§‰ ì‹œì , í†µê³„, ì¶”ì„¸ íŠ¹ì§•
        last_features = X_seq[:, -1, :]
        mean_features = np.mean(X_seq, axis=1)
        std_features = np.std(X_seq, axis=1)
        max_features = np.max(X_seq, axis=1)
        min_features = np.min(X_seq, axis=1)
        trend_features = X_seq[:, -1, :] - X_seq[:, 0, :]
        
        features = np.hstack([
            last_features, mean_features, std_features,
            max_features, min_features, trend_features
        ])
        
        return features
    
    def load_models(self, model_dir):
        """4ê°œ ëª¨ë¸ ëª¨ë‘ ë¡œë“œ"""
        print("\nğŸ¤– 4ê°œ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        models_dir = os.path.join(model_dir, 'models')
        
        model_files = {
            'model_jump': 'model_jump.pkl',
            'model_range': 'model_range.pkl', 
            'model_trend': 'model_trend.pkl',
            'model_value': 'model_value.pkl'
        }
        
        loaded_count = 0
        for model_name, filename in model_files.items():
            filepath = os.path.join(models_dir, filename)
            if os.path.exists(filepath):
                setattr(self, model_name, joblib.load(filepath))
                print(f"  âœ… {model_name} ë¡œë“œ ì™„ë£Œ")
                loaded_count += 1
            else:
                print(f"  âŒ {model_name} íŒŒì¼ ì—†ìŒ: {filepath}")
        
        if loaded_count < 4:
            print(f"âš ï¸ {loaded_count}/4 ëª¨ë¸ë§Œ ë¡œë“œë¨")
            return False
        
        print("âœ… 4ê°œ ëª¨ë¸ ëª¨ë‘ ë¡œë“œ ì™„ë£Œ")
        return True
    
    def predict_all_models_sequential(self, X_features):
        """ì˜¬ë°”ë¥¸ ìˆœì°¨ì  ì˜ˆì¸¡: 3ê°œ ëª¨ë¸ â†’ model_value"""
        print("\nğŸ”® ìˆœì°¨ì  ì˜ˆì¸¡ ìˆ˜í–‰...")
        
        predictions = {}
        
        # 1ë‹¨ê³„: 3ê°œ ë¶„ë¥˜ ëª¨ë¸ ë¨¼ì € ì˜ˆì¸¡
        if self.model_jump:
            print("  - 1ë‹¨ê³„: ì í”„ ê°ì§€ ëª¨ë¸...")
            predictions['jump_pred'] = self.model_jump.predict(X_features)
            predictions['jump_proba'] = self.model_jump.predict_proba(X_features)[:, 1]
            print(f"    ì í”„ ì˜ˆì¸¡: {np.sum(predictions['jump_pred'])}ê°œ")
        
        if self.model_range:
            print("  - 1ë‹¨ê³„: 3êµ¬ê°„ ë¶„ë¥˜ ëª¨ë¸...")
            predictions['range_pred'] = self.model_range.predict(X_features)
            predictions['range_proba'] = self.model_range.predict_proba(X_features)
            range_dist = np.bincount(predictions['range_pred'], minlength=3)
            print(f"    êµ¬ê°„ ë¶„í¬: ì €ìœ„í—˜ {range_dist[0]}ê°œ, ì¤‘ìœ„í—˜ {range_dist[1]}ê°œ, ê³ ìœ„í—˜ {range_dist[2]}ê°œ")
        
        if self.model_trend:
            print("  - 1ë‹¨ê³„: ìƒìŠ¹/í•˜ë½ íŒ¨í„´ ëª¨ë¸...")
            predictions['trend_pred'] = self.model_trend.predict(X_features)
            predictions['trend_proba'] = self.model_trend.predict_proba(X_features)
            trend_dist = np.bincount(predictions['trend_pred'], minlength=4)
            print(f"    íŒ¨í„´ ë¶„í¬: í•˜ë½ {trend_dist[0]}ê°œ, ì•ˆì • {trend_dist[1]}ê°œ, ì ì§„ìƒìŠ¹ {trend_dist[2]}ê°œ, ê¸‰ìƒìŠ¹ {trend_dist[3]}ê°œ")
        
        # 2ë‹¨ê³„: 3ê°œ ê²°ê³¼ë¥¼ ì¶”ê°€ íŠ¹ì§•ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ê°’ ì˜ˆì¸¡
        if self.model_value and all(k in predictions for k in ['jump_pred', 'range_pred', 'trend_pred']):
            print("  - 2ë‹¨ê³„: í™•ì¥ íŠ¹ì§•ìœ¼ë¡œ ê°’ ì˜ˆì¸¡...")
            
            # ì›ë˜ íŠ¹ì§• + 3ê°œ ì˜ˆì¸¡ ê²°ê³¼ ê²°í•©
            extended_features = np.column_stack([
                X_features,
                predictions['jump_pred'].reshape(-1, 1),
                predictions['range_pred'].reshape(-1, 1), 
                predictions['trend_pred'].reshape(-1, 1)
            ])
            
            print(f"    í™•ì¥ íŠ¹ì§• í¬ê¸°: {X_features.shape} â†’ {extended_features.shape}")
            
            try:
                predictions['value_pred'] = self.model_value.predict(extended_features)
                value_stats = predictions['value_pred']
                print(f"    ê°’ ì˜ˆì¸¡: í‰ê·  {np.mean(value_stats):.1f}, ìµœì†Œ {np.min(value_stats):.1f}, ìµœëŒ€ {np.max(value_stats):.1f}")
            except Exception as e:
                print(f"    âŒ ê°’ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                print(f"    âš ï¸ model_valueê°€ í™•ì¥ íŠ¹ì§•ìœ¼ë¡œ í•™ìŠµë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                # í´ë°±: ì›ë˜ íŠ¹ì§•ë§Œ ì‚¬ìš©
                predictions['value_pred'] = self.model_value.predict(X_features)
                print(f"    ğŸ“Œ í´ë°±: ì›ë˜ íŠ¹ì§•ë§Œ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡")
        
        elif self.model_value:
            print("  - âš ï¸ 3ê°œ ëª¨ë¸ ê²°ê³¼ ì—†ìŒ, ë…ë¦½ì  ê°’ ì˜ˆì¸¡")
            predictions['value_pred'] = self.model_value.predict(X_features)
        
        return predictions
    
    def apply_ensemble_logic(self, predictions, X_seq):
        """4ê°œ ëª¨ë¸ì˜ ì•™ìƒë¸” ë¡œì§"""
        print("\nğŸ”— 4ê°œ ëª¨ë¸ ì•™ìƒë¸” ì ìš©...")
        
        final_predictions = predictions.copy()
        
        # 1. ì í”„ ê°ì§€ ê°œì„  (3ê°œ ëª¨ë¸ ì •ë³´ í™œìš©)
        if all(k in predictions for k in ['jump_pred', 'range_pred', 'trend_pred', 'value_pred']):
            
            enhanced_jump = predictions['jump_pred'].copy()
            enhancement_count = 0
            
            for i in range(len(enhanced_jump)):
                # ì¡°ê±´ 1: ê°’ ì˜ˆì¸¡ì´ 300+ & êµ¬ê°„ ì˜ˆì¸¡ì´ ê³ ìœ„í—˜ & ì í”„ í™•ë¥  > 0.1
                value_high = predictions['value_pred'][i] >= 300
                range_high = predictions['range_pred'][i] == 2  # ê³ ìœ„í—˜
                jump_potential = predictions['jump_proba'][i] > 0.1
                
                if value_high and range_high and jump_potential:
                    enhanced_jump[i] = 1
                    enhancement_count += 1
                
                # ì¡°ê±´ 2: ê¸‰ìƒìŠ¹ íŒ¨í„´ & ê°’ ì˜ˆì¸¡ > 280 & ì‹œí€€ìŠ¤ ìµœëŒ€ê°’ < 280
                rapid_up = predictions['trend_pred'][i] == 3  # ê¸‰ìƒìŠ¹
                value_moderate = predictions['value_pred'][i] > 280
                seq_max = np.max(X_seq[i, :, 0])  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ íƒ€ê²Ÿì´ë¼ ê°€ì •
                seq_safe = seq_max < 280
                
                if rapid_up and value_moderate and seq_safe:
                    enhanced_jump[i] = 1
                    enhancement_count += 1
            
            final_predictions['jump_enhanced'] = enhanced_jump
            print(f"  ì í”„ ê°ì§€ ê°œì„ : {enhancement_count}ê°œ ì¶”ê°€")
        
        # 2. ê°’ ì˜ˆì¸¡ ì¡°ì • (ì í”„ ì˜ˆì¸¡ ë°˜ì˜)
        if 'jump_enhanced' in final_predictions and 'value_pred' in predictions:
            adjusted_value = predictions['value_pred'].copy()
            jump_adjustment_count = 0
            
            for i in range(len(adjusted_value)):
                if final_predictions['jump_enhanced'][i] == 1:
                    # ì í”„ ì˜ˆì¸¡ ì‹œ ê°’ì— ë³´ì • ì ìš©
                    if adjusted_value[i] < 300:
                        adjusted_value[i] = max(300, adjusted_value[i] + 30)
                        jump_adjustment_count += 1
                    else:
                        adjusted_value[i] = adjusted_value[i] + 20
                        jump_adjustment_count += 1
            
            final_predictions['value_adjusted'] = adjusted_value
            print(f"  ê°’ ì˜ˆì¸¡ ì¡°ì •: {jump_adjustment_count}ê°œ ë³´ì •")
        
        # 3. ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        confidence_scores = []
        
        for i in range(len(predictions['value_pred'])):
            confidence = 0.0
            
            # ì í”„ í™•ë¥  ê¸°ì—¬ë„
            if 'jump_proba' in predictions:
                confidence += predictions['jump_proba'][i] * 0.3
            
            # êµ¬ê°„ ë¶„ë¥˜ í™•ì‹ ë„
            if 'range_proba' in predictions:
                max_range_prob = np.max(predictions['range_proba'][i])
                confidence += max_range_prob * 0.3
            
            # íŒ¨í„´ ë¶„ë¥˜ í™•ì‹ ë„
            if 'trend_proba' in predictions:
                max_trend_prob = np.max(predictions['trend_proba'][i])
                confidence += max_trend_prob * 0.2
            
            # ê°’ ì˜ˆì¸¡ ì•ˆì •ì„± (ë³€ë™ì„± ì—­ìˆ˜)
            if i >= 5:
                recent_values = predictions['value_pred'][max(0, i-5):i+1]
                stability = 1.0 / (1.0 + np.std(recent_values))
                confidence += stability * 0.2
            else:
                confidence += 0.1
            
            confidence_scores.append(min(1.0, confidence))
        
        final_predictions['confidence'] = np.array(confidence_scores)
        print(f"  ì‹ ë¢°ë„ ê³„ì‚°: í‰ê·  {np.mean(confidence_scores):.3f}")
        
        return final_predictions

# ==============================================================================
# ğŸ“Š ì¢…í•© ì„±ëŠ¥ í‰ê°€
# ==============================================================================

def evaluate_comprehensive_performance():
    """4ê°œ ëª¨ë¸ ì¢…í•© ì„±ëŠ¥ í‰ê°€"""
    
    print("\nğŸš€ HUBROOM 4ê°œ ëª¨ë¸ ì¢…í•© í‰ê°€ ì‹œì‘...")
    
    # 1. í™˜ê²½ ì„¤ì •
    checkpoint_dir = './checkpoints_jump80'
    eval_data_path = 'data/HUBROOM_PIVOT_DATA.csv'  # í‰ê°€ ë°ì´í„° ê²½ë¡œ ìˆ˜ì •
    
    if not os.path.exists(eval_data_path):
        print(f"âŒ í‰ê°€ ë°ì´í„° ì—†ìŒ: {eval_data_path}")
        print("data/HUBROOM_PIVOT_DATA.csv íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # 2. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    processor = HubRoomDataProcessor()
    system = ComprehensiveEvaluationSystem()
    
    # 3. ëª¨ë¸ ë¡œë“œ
    if not system.load_models(checkpoint_dir):
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.")
        return
    
    # 4. ë°ì´í„° ì²˜ë¦¬
    df = processor.load_and_merge_data(eval_data_path)
    df = processor.create_all_features(df)
    X_seq, y_info, df = processor.create_sequences_for_evaluation(df)
    
    print(f"\nğŸ“Š í‰ê°€ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
    print(f"  - ì‹œí€€ìŠ¤ ìˆ˜: {len(X_seq)}")
    print(f"  - ì‹œí€€ìŠ¤ í˜•íƒœ: {X_seq.shape}")
    
    # 5. íŠ¹ì§• ì¤€ë¹„
    X_features = system.prepare_features(X_seq)
    print(f"âœ… íŠ¹ì§• ì¤€ë¹„ ì™„ë£Œ: {X_features.shape}")
    
    # 6. ìˆœì°¨ì  ì˜ˆì¸¡ (3ê°œ ëª¨ë¸ â†’ model_value)
    predictions = system.predict_all_models_sequential(X_features)
    
    # 7. ì•™ìƒë¸” ì ìš©
    final_predictions = system.apply_ensemble_logic(predictions, X_seq)
    
    # 8. ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
    print("\n" + "="*80)
    print("ğŸ“Š ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
    print("="*80)
    
    # ì‹¤ì œ ë¼ë²¨ ì¤€ë¹„
    y_true_jump = [info['is_jump'] for info in y_info]
    y_true_range = [info['range_class'] for info in y_info]
    y_true_trend = [info['trend_pattern'] for info in y_info]
    y_true_value = [info['target_value'] for info in y_info]
    
    # 1) ì í”„ ê°ì§€ ì„±ëŠ¥
    if 'jump_pred' in predictions:
        jump_accuracy = accuracy_score(y_true_jump, predictions['jump_pred'])
        jump_enhanced_accuracy = accuracy_score(y_true_jump, final_predictions['jump_enhanced'])
        
        print(f"\nğŸš€ ì í”„ ê°ì§€ ëª¨ë¸:")
        print(f"  - ê¸°ë³¸ ì •í™•ë„: {jump_accuracy:.3f}")
        print(f"  - ì•™ìƒë¸” ì •í™•ë„: {jump_enhanced_accuracy:.3f}")
        
        # ì í”„ ì¼€ì´ìŠ¤ ê°ì§€ìœ¨
        actual_jumps = np.sum(y_true_jump)
        detected_jumps = np.sum(np.array(y_true_jump) & np.array(final_predictions['jump_enhanced']))
        if actual_jumps > 0:
            jump_recall = detected_jumps / actual_jumps
            print(f"  - ì í”„ ê°ì§€ìœ¨: {jump_recall:.3f} ({detected_jumps}/{actual_jumps})")
    
    # 2) 3êµ¬ê°„ ë¶„ë¥˜ ì„±ëŠ¥
    if 'range_pred' in predictions:
        range_accuracy = accuracy_score(y_true_range, predictions['range_pred'])
        print(f"\nğŸ“Š 3êµ¬ê°„ ë¶„ë¥˜ ëª¨ë¸:")
        print(f"  - ì •í™•ë„: {range_accuracy:.3f}")
        
        # êµ¬ê°„ë³„ ì„±ëŠ¥
        for i, class_name in enumerate(['ì €ìœ„í—˜(<150)', 'ì¤‘ìœ„í—˜(150-299)', 'ê³ ìœ„í—˜(300+)']):
            class_mask = np.array(y_true_range) == i
            if np.sum(class_mask) > 0:
                class_accuracy = np.sum((np.array(y_true_range) == i) & (np.array(predictions['range_pred']) == i)) / np.sum(class_mask)
                print(f"    - {class_name}: {class_accuracy:.3f}")
    
    # 3) ìƒìŠ¹/í•˜ë½ íŒ¨í„´ ì„±ëŠ¥
    if 'trend_pred' in predictions:
        trend_accuracy = accuracy_score(y_true_trend, predictions['trend_pred'])
        print(f"\nğŸ“ˆ ìƒìŠ¹/í•˜ë½ íŒ¨í„´ ëª¨ë¸:")
        print(f"  - ì •í™•ë„: {trend_accuracy:.3f}")
        
        # íŒ¨í„´ë³„ ì„±ëŠ¥
        for i, pattern_name in enumerate(['í•˜ë½', 'ì•ˆì •', 'ì ì§„ìƒìŠ¹', 'ê¸‰ìƒìŠ¹']):
            pattern_mask = np.array(y_true_trend) == i
            if np.sum(pattern_mask) > 0:
                pattern_accuracy = np.sum((np.array(y_true_trend) == i) & (np.array(predictions['trend_pred']) == i)) / np.sum(pattern_mask)
                print(f"    - {pattern_name}: {pattern_accuracy:.3f}")
    
    # 4) ê°’ ì˜ˆì¸¡ ì„±ëŠ¥
    if 'value_pred' in predictions:
        basic_mae = np.mean(np.abs(np.array(y_true_value) - predictions['value_pred']))
        basic_rmse = np.sqrt(np.mean((np.array(y_true_value) - predictions['value_pred']) ** 2))
        
        if 'value_adjusted' in final_predictions:
            adjusted_mae = np.mean(np.abs(np.array(y_true_value) - final_predictions['value_adjusted']))
            adjusted_rmse = np.sqrt(np.mean((np.array(y_true_value) - final_predictions['value_adjusted']) ** 2))
        else:
            adjusted_mae = basic_mae
            adjusted_rmse = basic_rmse
            final_predictions['value_adjusted'] = predictions['value_pred']
        
        print(f"\nğŸ’° ê°’ ì˜ˆì¸¡ ëª¨ë¸:")
        print(f"  - ê¸°ë³¸ MAE: {basic_mae:.2f}")
        print(f"  - ê¸°ë³¸ RMSE: {basic_rmse:.2f}")
        print(f"  - ì¡°ì • MAE: {adjusted_mae:.2f}")
        print(f"  - ì¡°ì • RMSE: {adjusted_rmse:.2f}")
    
    # 9. ì¢…í•© ê²°ê³¼ ìƒì„±
    print("\nğŸ“‹ ì¢…í•© ê²°ê³¼ ìƒì„± ì¤‘...")
    results = []
    
    for i in tqdm(range(len(X_seq)), desc="ê²°ê³¼ ì •ë¦¬"):
        info = y_info[i]
        
        # ì í”„ ì¼€ì´ìŠ¤ ì—¬ë¶€
        is_jump_case = (info['seq_max'] < 280) and (info['target_value'] >= 300)
        
        result = {
            'ë‚ ì§œ': info['current_time'].strftime('%Y-%m-%d %H:%M'),
            'ì˜ˆì¸¡ë‚ ì§œ': info['predict_time'].strftime('%Y-%m-%d %H:%M'),
            'ì‹œí€€ìŠ¤MAX': round(info['seq_max'], 2),
            'ì‹œí€€ìŠ¤MIN': round(info['seq_min'], 2),
            'ì‹¤ì œê°’': round(info['target_value'], 2),
            
            # ê°’ ì˜ˆì¸¡: ìˆœì°¨ì  êµ¬ì¡° ë°˜ì˜
            'ìˆœì°¨ì˜ˆì¸¡ê°’': round(predictions.get('value_pred', [0])[i], 2),  # 3ê°œ ëª¨ë¸ ê²°ê³¼ í™œìš©
            'ìˆœì°¨ì˜¤ì°¨': round(abs(info['target_value'] - predictions.get('value_pred', [0])[i]), 2),
            
            # ì í”„ ì˜ˆì¸¡
            'ì í”„ì˜ˆì¸¡': 'O' if predictions.get('jump_pred', [0])[i] == 1 else 'X',
            'ì í”„í™•ë¥ ': round(predictions.get('jump_proba', [0])[i] * 100, 1),
            
            # êµ¬ê°„ ë¶„ë¥˜
            'ì‹¤ì œêµ¬ê°„': ['ì €ìœ„í—˜', 'ì¤‘ìœ„í—˜', 'ê³ ìœ„í—˜'][info['range_class']],
            'ì˜ˆì¸¡êµ¬ê°„': ['ì €ìœ„í—˜', 'ì¤‘ìœ„í—˜', 'ê³ ìœ„í—˜'][predictions.get('range_pred', [1])[i]],
            'êµ¬ê°„ì •í™•': 'O' if info['range_class'] == predictions.get('range_pred', [1])[i] else 'X',
            
            # íŒ¨í„´ ë¶„ë¥˜
            'ì‹¤ì œíŒ¨í„´': ['í•˜ë½', 'ì•ˆì •', 'ì ì§„ìƒìŠ¹', 'ê¸‰ìƒìŠ¹'][info['trend_pattern']],
            'ì˜ˆì¸¡íŒ¨í„´': ['í•˜ë½', 'ì•ˆì •', 'ì ì§„ìƒìŠ¹', 'ê¸‰ìƒìŠ¹'][predictions.get('trend_pred', [1])[i]],
            'íŒ¨í„´ì •í™•': 'O' if info['trend_pattern'] == predictions.get('trend_pred', [1])[i] else 'X',
            
            # 3ê°œ ëª¨ë¸ ê¸°ì—¬ë„ í‘œì‹œ
            'ì í”„ê¸°ì—¬': predictions.get('jump_pred', [0])[i],
            'êµ¬ê°„ê¸°ì—¬': predictions.get('range_pred', [1])[i], 
            'íŒ¨í„´ê¸°ì—¬': predictions.get('trend_pred', [1])[i],
            
            # ë©”íƒ€ ì •ë³´
            'ì í”„ì¼€ì´ìŠ¤': 'O' if is_jump_case else 'X',
            'ëª¨ë¸ì‹ ë¢°ë„': round(predictions.get('jump_proba', [0.5])[i], 3)
        }
        
        results.append(result)
    
    # 10. ìµœì¢… ì„±ëŠ¥ ìš”ì•½
    results_df = pd.DataFrame(results)
    
    print("\n" + "="*80)
    print("ğŸ“ˆ ìˆœì°¨ì  ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½")
    print("="*80)
    
    # ìˆœì°¨ ì˜ˆì¸¡ ì„±ëŠ¥
    sequential_mae = results_df['ìˆœì°¨ì˜¤ì°¨'].mean()
    
    print(f"\nğŸ’° ê°’ ì˜ˆì¸¡ ì„±ëŠ¥ (3ê°œ ëª¨ë¸ â†’ model_value):")
    print(f"  - ìˆœì°¨ ì˜ˆì¸¡ MAE: {sequential_mae:.2f}")
    
    # ì í”„ ê°ì§€ ì„±ëŠ¥
    actual_jump_cases = results_df[results_df['ì í”„ì¼€ì´ìŠ¤'] == 'O']
    if len(actual_jump_cases) > 0:
        jump_detected = (actual_jump_cases['ì í”„ì˜ˆì¸¡'] == 'O').sum()
        
        print(f"\nğŸš€ ì í”„ ê°ì§€ ì„±ëŠ¥:")
        print(f"  - ì´ ì í”„ ì¼€ì´ìŠ¤: {len(actual_jump_cases)}ê°œ")
        print(f"  - ê°ì§€ëœ ì¼€ì´ìŠ¤: {jump_detected}ê°œ ({jump_detected/len(actual_jump_cases)*100:.1f}%)")
    
    # ë¶„ë¥˜ ì •í™•ë„
    range_accuracy = (results_df['êµ¬ê°„ì •í™•'] == 'O').mean()
    trend_accuracy = (results_df['íŒ¨í„´ì •í™•'] == 'O').mean()
    
    print(f"\nğŸ“Š ë¶„ë¥˜ ì„±ëŠ¥:")
    print(f"  - 3êµ¬ê°„ ë¶„ë¥˜ ì •í™•ë„: {range_accuracy:.1%}")
    print(f"  - íŒ¨í„´ ë¶„ë¥˜ ì •í™•ë„: {trend_accuracy:.1%}")
    
    # 3ê°œ ëª¨ë¸ ê¸°ì—¬ë„ ë¶„ì„
    print(f"\nğŸ”— ëª¨ë¸ ê¸°ì—¬ë„ ë¶„ì„:")
    jump_contrib = results_df['ì í”„ê¸°ì—¬'].mean()
    range_contrib = results_df['êµ¬ê°„ê¸°ì—¬'].mean()
    trend_contrib = results_df['íŒ¨í„´ê¸°ì—¬'].mean()
    
    print(f"  - ì í”„ ëª¨ë¸ í‰ê·  ê¸°ì—¬: {jump_contrib:.3f}")
    print(f"  - êµ¬ê°„ ëª¨ë¸ í‰ê·  ê¸°ì—¬: {range_contrib:.3f}")
    print(f"  - íŒ¨í„´ ëª¨ë¸ í‰ê·  ê¸°ì—¬: {trend_contrib:.3f}")
    
    # ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì„±ëŠ¥
    extreme_mask = results_df['ì‹¤ì œê°’'] >= 300
    if extreme_mask.sum() > 0:
        extreme_detected = (results_df[extreme_mask]['ìˆœì°¨ì˜ˆì¸¡ê°’'] >= 300).sum()
        extreme_recall = extreme_detected / extreme_mask.sum() * 100
        print(f"\nğŸ¯ ê·¹ë‹¨ê°’(300+) ê°ì§€:")
        print(f"  - ì‹¤ì œ 300+ ì¼€ì´ìŠ¤: {extreme_mask.sum()}ê°œ")
        print(f"  - ìˆœì°¨ ì˜ˆì¸¡ìœ¼ë¡œ ê°ì§€: {extreme_detected}ê°œ")
        print(f"  - ê°ì§€ìœ¨: {extreme_recall:.1f}%")
    
    # 11. CSV ì €ì¥
    output_path = 'hubroom_comprehensive_evaluation.csv'
    results_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    
    # 12. ìƒ˜í”Œ ì¶œë ¥
    print(f"\nğŸ“‹ ê²°ê³¼ ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):")
    sample_cols = ['ë‚ ì§œ', 'ì‹¤ì œê°’', 'ì¡°ì •ì˜ˆì¸¡ê°’', 'ì¡°ì •ì˜¤ì°¨', 'ì í”„ì˜ˆì¸¡_ì•™ìƒë¸”', 'ì˜ˆì¸¡êµ¬ê°„', 'ì˜ˆì¸¡íŒ¨í„´']
    print(results_df[sample_cols].head().to_string(index=False))
    
    print("\n" + "="*80)
    print("âœ… HUBROOM 4ê°œ ëª¨ë¸ ì¢…í•© í‰ê°€ ì™„ë£Œ!")
    print("ğŸ¯ ëª¨ë“  ëª¨ë¸ì´ í†µí•©ì ìœ¼ë¡œ í™œìš©ë˜ì–´ ìµœì ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.")
    print("="*80)

if __name__ == "__main__":
    evaluate_comprehensive_performance()