#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
üéØ HUBROOM ÏôÑÏ†Ñ ÌèâÍ∞Ä ÏãúÏä§ÌÖú - 4Í∞ú Î™®Îç∏ ÌÜµÌï© ÌôúÏö©
================================================================================
Î™©Ìëú:
- 4Í∞ú Î™®Îç∏ Î™®Îëê Ï†úÎåÄÎ°ú ÌèâÍ∞Ä Î∞è ÌôúÏö©:
  * model_jump: Ï†êÌîÑ Í∞êÏßÄ (280ÎØ∏Îßå‚Üí300+ Í∏âÏÉÅÏäπ)
  * model_range: 3Íµ¨Í∞Ñ Î∂ÑÎ•ò (Ï†ÄÏúÑÌóò/Ï§ëÏúÑÌóò/Í≥†ÏúÑÌóò)  
  * model_trend: ÏÉÅÏäπ/ÌïòÎùΩ Ìå®ÌÑ¥ (ÌïòÎùΩ/ÏïàÏ†ï/Ï†êÏßÑÏÉÅÏäπ/Í∏âÏÉÅÏäπ)
  * model_value: Í∞í ÏòàÏ∏°
- Í∞úÎ≥Ñ Î™®Îç∏ ÏÑ±Îä• + ÌÜµÌï© ÏÑ±Îä• ÌèâÍ∞Ä
- ÏÉÅÏÑ∏Ìïú CSV Í≤∞Í≥º Ï∂úÎ†•
================================================================================
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, ExtraTreesRegressor
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, r2_score, accuracy_score
from xgboost import XGBClassifier
import joblib
import os
import pickle
from datetime import datetime, timedelta
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("üéØ HUBROOM ÏôÑÏ†Ñ ÌèâÍ∞Ä ÏãúÏä§ÌÖú - 4Í∞ú Î™®Îç∏ ÌÜµÌï©")
print("üìä 30Î∂Ñ ÏãúÌÄÄÏä§ ‚Üí 10Î∂Ñ ÌõÑ ÏòàÏ∏°")
print("ü§ñ model_jump + model_range + model_trend + model_value")
print("="*80)

# ==============================================================================
# üìä Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÌÅ¥ÎûòÏä§ (ÎèôÏùº)
# ==============================================================================

class HubRoomDataProcessor:
    """ÏôÑÏ†ÑÌïú Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ - Î™®Îì† ÌäπÏßï Ìè¨Ìï®"""
    
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        
        # 21Í∞ú ÌïÑÏàò Ïª¨Îüº
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB',
            'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2',
            'M14B_7F_TO_HUB_JOB2',
            'M16B_10F_TO_HUB_JOB'
        ]
        
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB',
            'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB',
            'M16A_3F_TO_M14B_7F_JOB',
            'M16A_3F_TO_3F_MLUD_JOB'
        ]
        
        self.cmd_cols = [
            'M16A_3F_CMD',
            'M16A_6F_TO_HUB_CMD',
            'M16A_2F_TO_HUB_CMD',
            'M14A_3F_TO_HUB_CMD',
            'M14B_7F_TO_HUB_CMD'
        ]
        
        self.capa_cols = [
            'M16A_6F_LFT_MAXCAPA',
            'M16A_2F_LFT_MAXCAPA'
        ]
        
        self.other_cols = [
            'M16A_3F_STORAGE_UTIL',
            'M14_TO_M16_OFS_CUR',
            'M16_TO_M14_OFS_CUR'
        ]
        
        # ÌôïÎ•† Îßµ
        self.probability_map = {
            0: 0.003, 1: 0.15, 2: 0.25, 3: 0.31, 4: 0.43, 5: 0.43,
            6: 0.35, 7: 0.42, 8: 0.53, 9: 0.49, 10: 0.42,
            11: 0.47, 12: 0.52, 13: 0.60, 14: 0.54, 15: 0.66,
            16: 0.62, 17: 0.71, 18: 0.79, 19: 0.83, 20: 0.987,
            21: 0.99, 22: 0.99, 23: 0.99, 24: 0.99, 25: 0.99,
            26: 0.99, 27: 0.99, 28: 0.99, 29: 0.99, 30: 0.99
        }
    
    def load_and_merge_data(self, data_path):
        """Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è BRIDGE_TIME Î≥ëÌï©"""
        print("\n[1Îã®Í≥Ñ] Îç∞Ïù¥ÌÑ∞ Î°úÎìú")
        
        # Î©îÏù∏ Îç∞Ïù¥ÌÑ∞
        df = pd.read_csv(data_path)
        print(f"‚úÖ ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞: {df.shape}")
        
        # ÏãúÍ∞Ñ Ï≤òÎ¶¨
        time_col = df.columns[0]
        df['datetime'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M')
        
        # BRIDGE_TIME Ï≤òÎ¶¨ (ÏûàÏúºÎ©¥ ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ Í∏∞Î≥∏Í∞í)
        if 'BRIDGE_TIME' not in df.columns:
            df['BRIDGE_TIME'] = 3.5
            print("‚ö†Ô∏è BRIDGE_TIME ÏóÜÏùå - Í∏∞Î≥∏Í∞í 3.5 ÏÇ¨Ïö©")
        else:
            df['BRIDGE_TIME'] = df['BRIDGE_TIME'].fillna(3.5)
            print("‚úÖ BRIDGE_TIME Ï≤òÎ¶¨ ÏôÑÎ£å")
        
        return df
    
    def create_all_features(self, df):
        """ÏôÑÏ†ÑÌïú ÌäπÏßï ÏóîÏßÄÎãàÏñ¥ÎßÅ"""
        print("\n[2Îã®Í≥Ñ] ÌäπÏßï ÏóîÏßÄÎãàÏñ¥ÎßÅ")
        
        # Í∏∞Î≥∏ ÌäπÏßïÎì§
        df['flow_balance'] = df[self.inflow_cols].sum(axis=1) - df[self.outflow_cols].sum(axis=1)
        df['flow_ratio'] = df[self.inflow_cols].sum(axis=1) / (df[self.outflow_cols].sum(axis=1) + 1)
        df['trend_20min'] = df[self.target_col].diff(20)
        df['trend_10min'] = df[self.target_col].diff(10)
        df['acceleration'] = df['trend_10min'] - df['trend_10min'].shift(10)
        df['consecutive_250+'] = (df[self.target_col] > 250).rolling(10).sum()
        df['consecutive_270+'] = (df[self.target_col] > 270).rolling(10).sum()
        df['cmd_sync_count'] = (df[self.cmd_cols] > 235).sum(axis=1)
        df['cmd_max'] = df[self.cmd_cols].max(axis=1)
        df['bridge_diff'] = df['BRIDGE_TIME'].diff(5)
        df['bridge_high'] = (df['BRIDGE_TIME'] > 4.0).astype(int)
        df['storage_x_bridge'] = df['M16A_3F_STORAGE_UTIL'] * df['BRIDGE_TIME']
        
        # Ïó∞ÏÜç 300+ ÌôïÎ•†
        consecutive_300_counts = []
        consecutive_300_probs = []
        
        for i in tqdm(range(len(df)), desc="300+ Ìå®ÌÑ¥ Í≥ÑÏÇ∞"):
            if i < 30:
                count = 0
                prob = 0.003
            else:
                window = df[self.target_col].iloc[i-30:i].values
                count = sum(1 for v in window if v >= 300)
                prob = self.probability_map.get(count, 0.5)
            
            consecutive_300_counts.append(count)
            consecutive_300_probs.append(prob)
        
        df['consecutive_300_count'] = consecutive_300_counts
        df['consecutive_300_prob'] = consecutive_300_probs
        
        # Î∂ÑÎ•ò ÎùºÎ≤®Îì§
        conditions = [df[self.target_col] < 150, (df[self.target_col] >= 150) & (df[self.target_col] < 300), df[self.target_col] >= 300]
        df['range_class'] = np.select(conditions, [0, 1, 2], default=1)
        
        df['past_30min_max'] = df[self.target_col].rolling(30).max()
        df['is_jump'] = ((df['past_30min_max'].shift(10) < 280) & (df[self.target_col] >= 300)).astype(int)
        
        df['change_20min'] = df[self.target_col] - df[self.target_col].shift(20)
        trend_conditions = [df['change_20min'] < -20, (df['change_20min'] >= -20) & (df['change_20min'] < 20), (df['change_20min'] >= 20) & (df['change_20min'] < 50), df['change_20min'] >= 50]
        df['trend_pattern'] = np.select(trend_conditions, [0, 1, 2, 3], default=1)
        
        # Ï∂îÍ∞Ä ÌäπÏßïÎì§
        df['change_rate_10min'] = ((df[self.target_col] - df[self.target_col].shift(10)) / (df[self.target_col].shift(10) + 1)) * 100
        df['change_rate_20min'] = ((df[self.target_col] - df[self.target_col].shift(20)) / (df[self.target_col].shift(20) + 1)) * 100
        df['change_rate_30min'] = ((df[self.target_col] - df[self.target_col].shift(30)) / (df[self.target_col].shift(30) + 1)) * 100
        df['volatility_10min'] = df[self.target_col].rolling(10).std()
        df['volatility_20min'] = df[self.target_col].rolling(20).std()
        df['volatility_30min'] = df[self.target_col].rolling(30).std()
        df['distance_to_300'] = 300 - df[self.target_col]
        df['near_extreme'] = (df[self.target_col] > 280).astype(int)
        df['recent_5min_mean'] = df[self.target_col].rolling(5).mean()
        df['recent_5min_max'] = df[self.target_col].rolling(5).max()
        df['recent_10min_mean'] = df[self.target_col].rolling(10).mean()
        df['in_jump_zone'] = ((df[self.target_col] >= 275) & (df[self.target_col] <= 279)).astype(int)
        
        # NaN Ï≤òÎ¶¨
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].fillna(method='ffill').fillna(0)
        
        print(f"‚úÖ Ï¥ù {len(df.columns)}Í∞ú ÌäπÏßï ÏÉùÏÑ± ÏôÑÎ£å")
        return df
    
    def create_sequences_for_evaluation(self, df, seq_len=30, pred_len=10):
        """ÌèâÍ∞ÄÏö© ÏãúÌÄÄÏä§ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±"""
        print(f"\n[3Îã®Í≥Ñ] ÏãúÌÄÄÏä§ ÏÉùÏÑ± ({seq_len}Î∂Ñ ‚Üí {pred_len}Î∂Ñ ÌõÑ)")
        
        # ÌäπÏßï Ïª¨Îüº ÏÑ†ÌÉù
        feature_cols = [col for col in df.columns 
                       if col not in ['datetime', 'range_class', 'is_jump', 'trend_pattern']]
        
        X = []
        y_info = []
        
        for i in tqdm(range(seq_len, len(df) - pred_len), desc="ÏãúÌÄÄÏä§ ÏÉùÏÑ±"):
            # ÏûÖÎ†•: Í≥ºÍ±∞ 30Î∂Ñ
            X.append(df[feature_cols].iloc[i-seq_len:i].values)
            
            # ÌÉÄÍ≤ü Ï†ïÎ≥¥Îì§
            target_idx = i + pred_len - 1
            
            # ÏãúÍ∞Ñ Ï†ïÎ≥¥
            current_time = df['datetime'].iloc[i-1]
            predict_time = df['datetime'].iloc[target_idx]
            seq_start_time = df['datetime'].iloc[i-seq_len]
            seq_end_time = df['datetime'].iloc[i-1]
            
            # ÏãúÌÄÄÏä§ ÌÜµÍ≥Ñ
            seq_values = df[self.target_col].iloc[i-seq_len:i].values
            seq_max = np.max(seq_values)
            seq_min = np.min(seq_values)
            
            y_info.append({
                'current_time': current_time,
                'predict_time': predict_time,
                'seq_start_time': seq_start_time,
                'seq_end_time': seq_end_time,
                'seq_max': seq_max,
                'seq_min': seq_min,
                'target_value': df[self.target_col].iloc[target_idx],
                'is_jump': df['is_jump'].iloc[target_idx],
                'range_class': df['range_class'].iloc[target_idx],
                'trend_pattern': df['trend_pattern'].iloc[target_idx]
            })
        
        print(f"‚úÖ {len(X)}Í∞ú ÏãúÌÄÄÏä§ ÏÉùÏÑ± ÏôÑÎ£å")
        
        return np.array(X), y_info, df

# ==============================================================================
# ü§ñ 4Í∞ú Î™®Îç∏ ÌÜµÌï© ÌèâÍ∞Ä ÏãúÏä§ÌÖú
# ==============================================================================

class ComprehensiveEvaluationSystem:
    """4Í∞ú Î™®Îç∏ ÏôÑÏ†Ñ ÌôúÏö© ÌèâÍ∞Ä ÏãúÏä§ÌÖú"""
    
    def __init__(self):
        # 4Í∞ú Î™®Îç∏
        self.model_jump = None    # Ï†êÌîÑ Í∞êÏßÄ
        self.model_range = None   # 3Íµ¨Í∞Ñ Î∂ÑÎ•ò
        self.model_trend = None   # ÏÉÅÏäπ/ÌïòÎùΩ Ìå®ÌÑ¥
        self.model_value = None   # Í∞í ÏòàÏ∏°
        
        self.feature_indices = {}
    
    def prepare_features(self, X_seq):
        """ÏãúÌÄÄÏä§Î•º ÌäπÏßïÏúºÎ°ú Î≥ÄÌôò"""
        # ÎßàÏßÄÎßâ ÏãúÏ†ê, ÌÜµÍ≥Ñ, Ï∂îÏÑ∏ ÌäπÏßï
        last_features = X_seq[:, -1, :]
        mean_features = np.mean(X_seq, axis=1)
        std_features = np.std(X_seq, axis=1)
        max_features = np.max(X_seq, axis=1)
        min_features = np.min(X_seq, axis=1)
        trend_features = X_seq[:, -1, :] - X_seq[:, 0, :]
        
        features = np.hstack([
            last_features, mean_features, std_features,
            max_features, min_features, trend_features
        ])
        
        return features
    
    def load_models(self, model_dir):
        """4Í∞ú Î™®Îç∏ Î™®Îëê Î°úÎìú"""
        print("\nü§ñ 4Í∞ú Î™®Îç∏ Î°úÎìú Ï§ë...")
        
        models_dir = os.path.join(model_dir, 'models')
        
        model_files = {
            'model_jump': 'model_jump.pkl',
            'model_range': 'model_range.pkl', 
            'model_trend': 'model_trend.pkl',
            'model_value': 'model_value.pkl'
        }
        
        loaded_count = 0
        for model_name, filename in model_files.items():
            filepath = os.path.join(models_dir, filename)
            if os.path.exists(filepath):
                setattr(self, model_name, joblib.load(filepath))
                print(f"  ‚úÖ {model_name} Î°úÎìú ÏôÑÎ£å")
                loaded_count += 1
            else:
                print(f"  ‚ùå {model_name} ÌååÏùº ÏóÜÏùå: {filepath}")
        
        if loaded_count < 4:
            print(f"‚ö†Ô∏è {loaded_count}/4 Î™®Îç∏Îßå Î°úÎìúÎê®")
            return False
        
        print("‚úÖ 4Í∞ú Î™®Îç∏ Î™®Îëê Î°úÎìú ÏôÑÎ£å")
        return True
    
    def predict_all_models_sequential(self, X_features):
        """Ïò¨Î∞îÎ•∏ ÏàúÏ∞®Ï†Å ÏòàÏ∏°: 3Í∞ú Î™®Îç∏ ‚Üí model_value"""
        print("\nüîÆ ÏàúÏ∞®Ï†Å ÏòàÏ∏° ÏàòÌñâ...")
        
        predictions = {}
        
        # 1Îã®Í≥Ñ: 3Í∞ú Î∂ÑÎ•ò Î™®Îç∏ Î®ºÏ†Ä ÏòàÏ∏°
        if self.model_jump:
            print("  - 1Îã®Í≥Ñ: Ï†êÌîÑ Í∞êÏßÄ Î™®Îç∏...")
            predictions['jump_pred'] = self.model_jump.predict(X_features)
            predictions['jump_proba'] = self.model_jump.predict_proba(X_features)[:, 1]
            print(f"    Ï†êÌîÑ ÏòàÏ∏°: {np.sum(predictions['jump_pred'])}Í∞ú")
        
        if self.model_range:
            print("  - 1Îã®Í≥Ñ: 3Íµ¨Í∞Ñ Î∂ÑÎ•ò Î™®Îç∏...")
            predictions['range_pred'] = self.model_range.predict(X_features)
            predictions['range_proba'] = self.model_range.predict_proba(X_features)
            range_dist = np.bincount(predictions['range_pred'], minlength=3)
            print(f"    Íµ¨Í∞Ñ Î∂ÑÌè¨: Ï†ÄÏúÑÌóò {range_dist[0]}Í∞ú, Ï§ëÏúÑÌóò {range_dist[1]}Í∞ú, Í≥†ÏúÑÌóò {range_dist[2]}Í∞ú")
        
        if self.model_trend:
            print("  - 1Îã®Í≥Ñ: ÏÉÅÏäπ/ÌïòÎùΩ Ìå®ÌÑ¥ Î™®Îç∏...")
            predictions['trend_pred'] = self.model_trend.predict(X_features)
            predictions['trend_proba'] = self.model_trend.predict_proba(X_features)
            trend_dist = np.bincount(predictions['trend_pred'], minlength=4)
            print(f"    Ìå®ÌÑ¥ Î∂ÑÌè¨: ÌïòÎùΩ {trend_dist[0]}Í∞ú, ÏïàÏ†ï {trend_dist[1]}Í∞ú, Ï†êÏßÑÏÉÅÏäπ {trend_dist[2]}Í∞ú, Í∏âÏÉÅÏäπ {trend_dist[3]}Í∞ú")
        
        # 2Îã®Í≥Ñ: 3Í∞ú Í≤∞Í≥ºÎ•º Ï∂îÍ∞Ä ÌäπÏßïÏúºÎ°ú ÏÇ¨Ïö©ÌïòÏó¨ Í∞í ÏòàÏ∏°
        if self.model_value and all(k in predictions for k in ['jump_pred', 'range_pred', 'trend_pred']):
            print("  - 2Îã®Í≥Ñ: ÌôïÏû• ÌäπÏßïÏúºÎ°ú Í∞í ÏòàÏ∏°...")
            
            # ÏõêÎûò ÌäπÏßï + 3Í∞ú ÏòàÏ∏° Í≤∞Í≥º Í≤∞Ìï©
            extended_features = np.column_stack([
                X_features,
                predictions['jump_pred'].reshape(-1, 1),
                predictions['range_pred'].reshape(-1, 1), 
                predictions['trend_pred'].reshape(-1, 1)
            ])
            
            print(f"    ÌôïÏû• ÌäπÏßï ÌÅ¨Í∏∞: {X_features.shape} ‚Üí {extended_features.shape}")
            
            try:
                predictions['value_pred'] = self.model_value.predict(extended_features)
                value_stats = predictions['value_pred']
                print(f"    Í∞í ÏòàÏ∏°: ÌèâÍ∑† {np.mean(value_stats):.1f}, ÏµúÏÜå {np.min(value_stats):.1f}, ÏµúÎåÄ {np.max(value_stats):.1f}")
            except Exception as e:
                print(f"    ‚ùå Í∞í ÏòàÏ∏° Ïã§Ìå®: {e}")
                print(f"    ‚ö†Ô∏è model_valueÍ∞Ä ÌôïÏû• ÌäπÏßïÏúºÎ°ú ÌïôÏäµÎêòÏßÄ ÏïäÏïòÏùÑ Ïàò ÏûàÏäµÎãàÎã§.")
                # Ìè¥Î∞±: ÏõêÎûò ÌäπÏßïÎßå ÏÇ¨Ïö©
                predictions['value_pred'] = self.model_value.predict(X_features)
                print(f"    üìå Ìè¥Î∞±: ÏõêÎûò ÌäπÏßïÎßå ÏÇ¨Ïö©ÌïòÏó¨ ÏòàÏ∏°")
        
        elif self.model_value:
            print("  - ‚ö†Ô∏è 3Í∞ú Î™®Îç∏ Í≤∞Í≥º ÏóÜÏùå, ÎèÖÎ¶ΩÏ†Å Í∞í ÏòàÏ∏°")
            predictions['value_pred'] = self.model_value.predict(X_features)
        
        return predictions
    
    def apply_ensemble_logic(self, predictions, X_seq):
        """4Í∞ú Î™®Îç∏Ïùò ÏïôÏÉÅÎ∏î Î°úÏßÅ"""
        print("\nüîó 4Í∞ú Î™®Îç∏ ÏïôÏÉÅÎ∏î Ï†ÅÏö©...")
        
        final_predictions = predictions.copy()
        
        # 1. Ï†êÌîÑ Í∞êÏßÄ Í∞úÏÑ† (3Í∞ú Î™®Îç∏ Ï†ïÎ≥¥ ÌôúÏö©)
        if all(k in predictions for k in ['jump_pred', 'range_pred', 'trend_pred', 'value_pred']):
            
            enhanced_jump = predictions['jump_pred'].copy()
            enhancement_count = 0
            
            for i in range(len(enhanced_jump)):
                # Ï°∞Í±¥ 1: Í∞í ÏòàÏ∏°Ïù¥ 300+ & Íµ¨Í∞Ñ ÏòàÏ∏°Ïù¥ Í≥†ÏúÑÌóò & Ï†êÌîÑ ÌôïÎ•† > 0.1
                value_high = predictions['value_pred'][i] >= 300
                range_high = predictions['range_pred'][i] == 2  # Í≥†ÏúÑÌóò
                jump_potential = predictions['jump_proba'][i] > 0.1
                
                if value_high and range_high and jump_potential:
                    enhanced_jump[i] = 1
                    enhancement_count += 1
                
                # Ï°∞Í±¥ 2: Í∏âÏÉÅÏäπ Ìå®ÌÑ¥ & Í∞í ÏòàÏ∏° > 280 & ÏãúÌÄÄÏä§ ÏµúÎåÄÍ∞í < 280
                rapid_up = predictions['trend_pred'][i] == 3  # Í∏âÏÉÅÏäπ
                value_moderate = predictions['value_pred'][i] > 280
                seq_max = np.max(X_seq[i, :, 0])  # Ï≤´ Î≤àÏß∏ Ïª¨ÎüºÏù¥ ÌÉÄÍ≤üÏù¥Îùº Í∞ÄÏ†ï
                seq_safe = seq_max < 280
                
                if rapid_up and value_moderate and seq_safe:
                    enhanced_jump[i] = 1
                    enhancement_count += 1
            
            final_predictions['jump_enhanced'] = enhanced_jump
            print(f"  Ï†êÌîÑ Í∞êÏßÄ Í∞úÏÑ†: {enhancement_count}Í∞ú Ï∂îÍ∞Ä")
        
        # 2. Í∞í ÏòàÏ∏° Ï°∞Ï†ï (Ï†êÌîÑ ÏòàÏ∏° Î∞òÏòÅ)
        if 'jump_enhanced' in final_predictions and 'value_pred' in predictions:
            adjusted_value = predictions['value_pred'].copy()
            jump_adjustment_count = 0
            
            for i in range(len(adjusted_value)):
                if final_predictions['jump_enhanced'][i] == 1:
                    # Ï†êÌîÑ ÏòàÏ∏° Ïãú Í∞íÏóê Î≥¥Ï†ï Ï†ÅÏö©
                    if adjusted_value[i] < 300:
                        adjusted_value[i] = max(300, adjusted_value[i] + 30)
                        jump_adjustment_count += 1
                    else:
                        adjusted_value[i] = adjusted_value[i] + 20
                        jump_adjustment_count += 1
            
            final_predictions['value_adjusted'] = adjusted_value
            print(f"  Í∞í ÏòàÏ∏° Ï°∞Ï†ï: {jump_adjustment_count}Í∞ú Î≥¥Ï†ï")
        
        # 3. Ïã†Î¢∞ÎèÑ Ï†êÏàò Í≥ÑÏÇ∞
        confidence_scores = []
        
        for i in range(len(predictions['value_pred'])):
            confidence = 0.0
            
            # Ï†êÌîÑ ÌôïÎ•† Í∏∞Ïó¨ÎèÑ
            if 'jump_proba' in predictions:
                confidence += predictions['jump_proba'][i] * 0.3
            
            # Íµ¨Í∞Ñ Î∂ÑÎ•ò ÌôïÏã†ÎèÑ
            if 'range_proba' in predictions:
                max_range_prob = np.max(predictions['range_proba'][i])
                confidence += max_range_prob * 0.3
            
            # Ìå®ÌÑ¥ Î∂ÑÎ•ò ÌôïÏã†ÎèÑ
            if 'trend_proba' in predictions:
                max_trend_prob = np.max(predictions['trend_proba'][i])
                confidence += max_trend_prob * 0.2
            
            # Í∞í ÏòàÏ∏° ÏïàÏ†ïÏÑ± (Î≥ÄÎèôÏÑ± Ïó≠Ïàò)
            if i >= 5:
                recent_values = predictions['value_pred'][max(0, i-5):i+1]
                stability = 1.0 / (1.0 + np.std(recent_values))
                confidence += stability * 0.2
            else:
                confidence += 0.1
            
            confidence_scores.append(min(1.0, confidence))
        
        final_predictions['confidence'] = np.array(confidence_scores)
        print(f"  Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞: ÌèâÍ∑† {np.mean(confidence_scores):.3f}")
        
        return final_predictions

# ==============================================================================
# üìä Ï¢ÖÌï© ÏÑ±Îä• ÌèâÍ∞Ä
# ==============================================================================

def evaluate_comprehensive_performance():
    """4Í∞ú Î™®Îç∏ Ï¢ÖÌï© ÏÑ±Îä• ÌèâÍ∞Ä"""
    
    print("\nüöÄ HUBROOM 4Í∞ú Î™®Îç∏ Ï¢ÖÌï© ÌèâÍ∞Ä ÏãúÏûë...")
    
    # 1. ÌôòÍ≤Ω ÏÑ§Ï†ï
    checkpoint_dir = './checkpoints_jump80'
    eval_data_path = 'data/HUBROOM_PIVOT_DATA.csv'  # ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú ÏàòÏ†ï
    
    if not os.path.exists(eval_data_path):
        print(f"‚ùå ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå: {eval_data_path}")
        print("data/HUBROOM_PIVOT_DATA.csv ÌååÏùºÏùÑ ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.")
        return
    
    # 2. ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî
    processor = HubRoomDataProcessor()
    system = ComprehensiveEvaluationSystem()
    
    # 3. Î™®Îç∏ Î°úÎìú
    if not system.load_models(checkpoint_dir):
        print("‚ùå Î™®Îç∏ Î°úÎìú Ïã§Ìå®. Î®ºÏ†Ä ÌïôÏäµÏùÑ ÏôÑÎ£åÌï¥Ï£ºÏÑ∏Ïöî.")
        return
    
    # 4. Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
    df = processor.load_and_merge_data(eval_data_path)
    df = processor.create_all_features(df)
    X_seq, y_info, df = processor.create_sequences_for_evaluation(df)
    
    print(f"\nüìä ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ ÏôÑÎ£å")
    print(f"  - ÏãúÌÄÄÏä§ Ïàò: {len(X_seq)}")
    print(f"  - ÏãúÌÄÄÏä§ ÌòïÌÉú: {X_seq.shape}")
    
    # 5. ÌäπÏßï Ï§ÄÎπÑ
    X_features = system.prepare_features(X_seq)
    print(f"‚úÖ ÌäπÏßï Ï§ÄÎπÑ ÏôÑÎ£å: {X_features.shape}")
    
    # 6. ÏàúÏ∞®Ï†Å ÏòàÏ∏° (3Í∞ú Î™®Îç∏ ‚Üí model_value)
    predictions = system.predict_all_models_sequential(X_features)
    
    # 7. ÏïôÏÉÅÎ∏î Ï†ÅÏö©
    final_predictions = system.apply_ensemble_logic(predictions, X_seq)
    
    # 8. Í∞úÎ≥Ñ Î™®Îç∏ ÏÑ±Îä• ÌèâÍ∞Ä
    print("\n" + "="*80)
    print("üìä Í∞úÎ≥Ñ Î™®Îç∏ ÏÑ±Îä• ÌèâÍ∞Ä")
    print("="*80)
    
    # Ïã§Ï†ú ÎùºÎ≤® Ï§ÄÎπÑ
    y_true_jump = [info['is_jump'] for info in y_info]
    y_true_range = [info['range_class'] for info in y_info]
    y_true_trend = [info['trend_pattern'] for info in y_info]
    y_true_value = [info['target_value'] for info in y_info]
    
    # 1) Ï†êÌîÑ Í∞êÏßÄ ÏÑ±Îä•
    if 'jump_pred' in predictions:
        jump_accuracy = accuracy_score(y_true_jump, predictions['jump_pred'])
        jump_enhanced_accuracy = accuracy_score(y_true_jump, final_predictions['jump_enhanced'])
        
        print(f"\nüöÄ Ï†êÌîÑ Í∞êÏßÄ Î™®Îç∏:")
        print(f"  - Í∏∞Î≥∏ Ï†ïÌôïÎèÑ: {jump_accuracy:.3f}")
        print(f"  - ÏïôÏÉÅÎ∏î Ï†ïÌôïÎèÑ: {jump_enhanced_accuracy:.3f}")
        
        # Ï†êÌîÑ ÏºÄÏù¥Ïä§ Í∞êÏßÄÏú®
        actual_jumps = np.sum(y_true_jump)
        detected_jumps = np.sum(np.array(y_true_jump) & np.array(final_predictions['jump_enhanced']))
        if actual_jumps > 0:
            jump_recall = detected_jumps / actual_jumps
            print(f"  - Ï†êÌîÑ Í∞êÏßÄÏú®: {jump_recall:.3f} ({detected_jumps}/{actual_jumps})")
    
    # 2) 3Íµ¨Í∞Ñ Î∂ÑÎ•ò ÏÑ±Îä•
    if 'range_pred' in predictions:
        range_accuracy = accuracy_score(y_true_range, predictions['range_pred'])
        print(f"\nüìä 3Íµ¨Í∞Ñ Î∂ÑÎ•ò Î™®Îç∏:")
        print(f"  - Ï†ïÌôïÎèÑ: {range_accuracy:.3f}")
        
        # Íµ¨Í∞ÑÎ≥Ñ ÏÑ±Îä•
        for i, class_name in enumerate(['Ï†ÄÏúÑÌóò(<150)', 'Ï§ëÏúÑÌóò(150-299)', 'Í≥†ÏúÑÌóò(300+)']):
            class_mask = np.array(y_true_range) == i
            if np.sum(class_mask) > 0:
                class_accuracy = np.sum((np.array(y_true_range) == i) & (np.array(predictions['range_pred']) == i)) / np.sum(class_mask)
                print(f"    - {class_name}: {class_accuracy:.3f}")
    
    # 3) ÏÉÅÏäπ/ÌïòÎùΩ Ìå®ÌÑ¥ ÏÑ±Îä•
    if 'trend_pred' in predictions:
        trend_accuracy = accuracy_score(y_true_trend, predictions['trend_pred'])
        print(f"\nüìà ÏÉÅÏäπ/ÌïòÎùΩ Ìå®ÌÑ¥ Î™®Îç∏:")
        print(f"  - Ï†ïÌôïÎèÑ: {trend_accuracy:.3f}")
        
        # Ìå®ÌÑ¥Î≥Ñ ÏÑ±Îä•
        for i, pattern_name in enumerate(['ÌïòÎùΩ', 'ÏïàÏ†ï', 'Ï†êÏßÑÏÉÅÏäπ', 'Í∏âÏÉÅÏäπ']):
            pattern_mask = np.array(y_true_trend) == i
            if np.sum(pattern_mask) > 0:
                pattern_accuracy = np.sum((np.array(y_true_trend) == i) & (np.array(predictions['trend_pred']) == i)) / np.sum(pattern_mask)
                print(f"    - {pattern_name}: {pattern_accuracy:.3f}")
    
    # 4) Í∞í ÏòàÏ∏° ÏÑ±Îä•
    if 'value_pred' in predictions:
        basic_mae = np.mean(np.abs(np.array(y_true_value) - predictions['value_pred']))
        basic_rmse = np.sqrt(np.mean((np.array(y_true_value) - predictions['value_pred']) ** 2))
        
        if 'value_adjusted' in final_predictions:
            adjusted_mae = np.mean(np.abs(np.array(y_true_value) - final_predictions['value_adjusted']))
            adjusted_rmse = np.sqrt(np.mean((np.array(y_true_value) - final_predictions['value_adjusted']) ** 2))
        else:
            adjusted_mae = basic_mae
            adjusted_rmse = basic_rmse
            final_predictions['value_adjusted'] = predictions['value_pred']
        
        print(f"\nüí∞ Í∞í ÏòàÏ∏° Î™®Îç∏:")
        print(f"  - Í∏∞Î≥∏ MAE: {basic_mae:.2f}")
        print(f"  - Í∏∞Î≥∏ RMSE: {basic_rmse:.2f}")
        print(f"  - Ï°∞Ï†ï MAE: {adjusted_mae:.2f}")
        print(f"  - Ï°∞Ï†ï RMSE: {adjusted_rmse:.2f}")
    
    # 9. Ï¢ÖÌï© Í≤∞Í≥º ÏÉùÏÑ±
    print("\nüìã Ï¢ÖÌï© Í≤∞Í≥º ÏÉùÏÑ± Ï§ë...")
    results = []
    
    for i in tqdm(range(len(X_seq)), desc="Í≤∞Í≥º Ï†ïÎ¶¨"):
        info = y_info[i]
        
        # Ï†êÌîÑ ÏºÄÏù¥Ïä§ Ïó¨Î∂Ä
        is_jump_case = (info['seq_max'] < 280) and (info['target_value'] >= 300)
        
        result = {
            'ÎÇ†Ïßú': info['current_time'].strftime('%Y-%m-%d %H:%M'),
            'ÏòàÏ∏°ÎÇ†Ïßú': info['predict_time'].strftime('%Y-%m-%d %H:%M'),
            'ÏãúÌÄÄÏä§MAX': round(info['seq_max'], 2),
            'ÏãúÌÄÄÏä§MIN': round(info['seq_min'], 2),
            'Ïã§Ï†úÍ∞í': round(info['target_value'], 2),
            
            # Í∞í ÏòàÏ∏°: ÏàúÏ∞®Ï†Å Íµ¨Ï°∞ Î∞òÏòÅ
            'ÏàúÏ∞®ÏòàÏ∏°Í∞í': round(predictions.get('value_pred', [0])[i], 2),  # 3Í∞ú Î™®Îç∏ Í≤∞Í≥º ÌôúÏö©
            'ÏàúÏ∞®Ïò§Ï∞®': round(abs(info['target_value'] - predictions.get('value_pred', [0])[i]), 2),
            
            # Ï†êÌîÑ ÏòàÏ∏°
            'Ï†êÌîÑÏòàÏ∏°': 'O' if predictions.get('jump_pred', [0])[i] == 1 else 'X',
            'Ï†êÌîÑÌôïÎ•†': round(predictions.get('jump_proba', [0])[i] * 100, 1),
            
            # Íµ¨Í∞Ñ Î∂ÑÎ•ò
            'Ïã§Ï†úÍµ¨Í∞Ñ': ['Ï†ÄÏúÑÌóò', 'Ï§ëÏúÑÌóò', 'Í≥†ÏúÑÌóò'][info['range_class']],
            'ÏòàÏ∏°Íµ¨Í∞Ñ': ['Ï†ÄÏúÑÌóò', 'Ï§ëÏúÑÌóò', 'Í≥†ÏúÑÌóò'][predictions.get('range_pred', [1])[i]],
            'Íµ¨Í∞ÑÏ†ïÌôï': 'O' if info['range_class'] == predictions.get('range_pred', [1])[i] else 'X',
            
            # Ìå®ÌÑ¥ Î∂ÑÎ•ò
            'Ïã§Ï†úÌå®ÌÑ¥': ['ÌïòÎùΩ', 'ÏïàÏ†ï', 'Ï†êÏßÑÏÉÅÏäπ', 'Í∏âÏÉÅÏäπ'][info['trend_pattern']],
            'ÏòàÏ∏°Ìå®ÌÑ¥': ['ÌïòÎùΩ', 'ÏïàÏ†ï', 'Ï†êÏßÑÏÉÅÏäπ', 'Í∏âÏÉÅÏäπ'][predictions.get('trend_pred', [1])[i]],
            'Ìå®ÌÑ¥Ï†ïÌôï': 'O' if info['trend_pattern'] == predictions.get('trend_pred', [1])[i] else 'X',
            
            # 3Í∞ú Î™®Îç∏ Í∏∞Ïó¨ÎèÑ ÌëúÏãú
            'Ï†êÌîÑÍ∏∞Ïó¨': predictions.get('jump_pred', [0])[i],
            'Íµ¨Í∞ÑÍ∏∞Ïó¨': predictions.get('range_pred', [1])[i], 
            'Ìå®ÌÑ¥Í∏∞Ïó¨': predictions.get('trend_pred', [1])[i],
            
            # Î©îÌÉÄ Ï†ïÎ≥¥
            'Ï†êÌîÑÏºÄÏù¥Ïä§': 'O' if is_jump_case else 'X',
            'Î™®Îç∏Ïã†Î¢∞ÎèÑ': round(predictions.get('jump_proba', [0.5])[i], 3)
        }
        
        results.append(result)
    
    # 10. ÏµúÏ¢Ö ÏÑ±Îä• ÏöîÏïΩ
    results_df = pd.DataFrame(results)
    
    print("\n" + "="*80)
    print("üìà ÏàúÏ∞®Ï†Å Î™®Îç∏ ÏÑ±Îä• ÏöîÏïΩ")
    print("="*80)
    
    # ÏàúÏ∞® ÏòàÏ∏° ÏÑ±Îä•
    sequential_mae = results_df['ÏàúÏ∞®Ïò§Ï∞®'].mean()
    
    print(f"\nüí∞ Í∞í ÏòàÏ∏° ÏÑ±Îä• (3Í∞ú Î™®Îç∏ ‚Üí model_value):")
    print(f"  - ÏàúÏ∞® ÏòàÏ∏° MAE: {sequential_mae:.2f}")
    
    # Ï†êÌîÑ Í∞êÏßÄ ÏÑ±Îä•
    actual_jump_cases = results_df[results_df['Ï†êÌîÑÏºÄÏù¥Ïä§'] == 'O']
    if len(actual_jump_cases) > 0:
        jump_detected = (actual_jump_cases['Ï†êÌîÑÏòàÏ∏°'] == 'O').sum()
        
        print(f"\nüöÄ Ï†êÌîÑ Í∞êÏßÄ ÏÑ±Îä•:")
        print(f"  - Ï¥ù Ï†êÌîÑ ÏºÄÏù¥Ïä§: {len(actual_jump_cases)}Í∞ú")
        print(f"  - Í∞êÏßÄÎêú ÏºÄÏù¥Ïä§: {jump_detected}Í∞ú ({jump_detected/len(actual_jump_cases)*100:.1f}%)")
    
    # Î∂ÑÎ•ò Ï†ïÌôïÎèÑ
    range_accuracy = (results_df['Íµ¨Í∞ÑÏ†ïÌôï'] == 'O').mean()
    trend_accuracy = (results_df['Ìå®ÌÑ¥Ï†ïÌôï'] == 'O').mean()
    
    print(f"\nüìä Î∂ÑÎ•ò ÏÑ±Îä•:")
    print(f"  - 3Íµ¨Í∞Ñ Î∂ÑÎ•ò Ï†ïÌôïÎèÑ: {range_accuracy:.1%}")
    print(f"  - Ìå®ÌÑ¥ Î∂ÑÎ•ò Ï†ïÌôïÎèÑ: {trend_accuracy:.1%}")
    
    # 3Í∞ú Î™®Îç∏ Í∏∞Ïó¨ÎèÑ Î∂ÑÏÑù
    print(f"\nüîó Î™®Îç∏ Í∏∞Ïó¨ÎèÑ Î∂ÑÏÑù:")
    jump_contrib = results_df['Ï†êÌîÑÍ∏∞Ïó¨'].mean()
    range_contrib = results_df['Íµ¨Í∞ÑÍ∏∞Ïó¨'].mean()
    trend_contrib = results_df['Ìå®ÌÑ¥Í∏∞Ïó¨'].mean()
    
    print(f"  - Ï†êÌîÑ Î™®Îç∏ ÌèâÍ∑† Í∏∞Ïó¨: {jump_contrib:.3f}")
    print(f"  - Íµ¨Í∞Ñ Î™®Îç∏ ÌèâÍ∑† Í∏∞Ïó¨: {range_contrib:.3f}")
    print(f"  - Ìå®ÌÑ¥ Î™®Îç∏ ÌèâÍ∑† Í∏∞Ïó¨: {trend_contrib:.3f}")
    
    # Í∑πÎã®Í∞í ÏòàÏ∏° ÏÑ±Îä•
    extreme_mask = results_df['Ïã§Ï†úÍ∞í'] >= 300
    if extreme_mask.sum() > 0:
        extreme_detected = (results_df[extreme_mask]['ÏàúÏ∞®ÏòàÏ∏°Í∞í'] >= 300).sum()
        extreme_recall = extreme_detected / extreme_mask.sum() * 100
        print(f"\nüéØ Í∑πÎã®Í∞í(300+) Í∞êÏßÄ:")
        print(f"  - Ïã§Ï†ú 300+ ÏºÄÏù¥Ïä§: {extreme_mask.sum()}Í∞ú")
        print(f"  - ÏàúÏ∞® ÏòàÏ∏°ÏúºÎ°ú Í∞êÏßÄ: {extreme_detected}Í∞ú")
        print(f"  - Í∞êÏßÄÏú®: {extreme_recall:.1f}%")
    
    # 11. CSV Ï†ÄÏû•
    output_path = 'hubroom_comprehensive_evaluation.csv'
    results_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nüíæ Í≤∞Í≥º Ï†ÄÏû•: {output_path}")
    
    # 12. ÏÉòÌîå Ï∂úÎ†•
    print(f"\nüìã Í≤∞Í≥º ÏÉòÌîå (Ï≤òÏùå 5Í∞ú):")
    sample_cols = ['ÎÇ†Ïßú', 'Ïã§Ï†úÍ∞í', 'Ï°∞Ï†ïÏòàÏ∏°Í∞í', 'Ï°∞Ï†ïÏò§Ï∞®', 'Ï†êÌîÑÏòàÏ∏°_ÏïôÏÉÅÎ∏î', 'ÏòàÏ∏°Íµ¨Í∞Ñ', 'ÏòàÏ∏°Ìå®ÌÑ¥']
    print(results_df[sample_cols].head().to_string(index=False))
    
    print("\n" + "="*80)
    print("‚úÖ HUBROOM 4Í∞ú Î™®Îç∏ Ï¢ÖÌï© ÌèâÍ∞Ä ÏôÑÎ£å!")
    print("üéØ Î™®Îì† Î™®Îç∏Ïù¥ ÌÜµÌï©Ï†ÅÏúºÎ°ú ÌôúÏö©ÎêòÏñ¥ ÏµúÏ†ÅÏùò ÏÑ±Îä•ÏùÑ Îã¨ÏÑ±ÌñàÏäµÎãàÎã§.")
    print("="*80)

if __name__ == "__main__":
    evaluate_comprehensive_performance()