#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ğŸ¯ HUBROOM ì í”„ ê°ì§€ ì‹œìŠ¤í…œ í‰ê°€ ì½”ë“œ + í ë³€í™”ëŸ‰ ë¶„ì„ ì¶”ê°€
================================================================================
ëª©í‘œ:
- ê³¼ê±° 30ë¶„ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡ í‰ê°€
- í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©:
  * ExtraTreesClassifier (ìŠ¤í¬ë¦¬ë‹)
  * XGBClassifier (ì í”„ ê°ì§€)
  * RandomForestClassifier (êµ¬ê°„, íŒ¨í„´)
  * ExtraTreesRegressor (ê°’ ì˜ˆì¸¡)
- í‰ê°€ ë°ì´í„°: 2025ë…„ 8ì›”
- CSV ê²°ê³¼ ì¶œë ¥ (ìš”ì²­ëœ í˜•ì‹)
- í ë³€í™”ëŸ‰ ë¶„ì„ ì¶”ê°€ (5ë¶„, 10ë¶„, 20ë¶„, 30ë¶„ í›„)
================================================================================
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, ExtraTreesRegressor
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, r2_score
#from sklearn.metrics import classification_report, confusion_matrix
from xgboost import XGBClassifier
import joblib
import os
import pickle
from datetime import datetime, timedelta
from tqdm import tqdm
import warnings
try:
    import openpyxl
except ImportError:
    print("âš ï¸ openpyxlì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openpyxlì„ ì‹¤í–‰í•˜ì„¸ìš”.")
warnings.filterwarnings('ignore')

print("="*80)
print("ğŸ¯ HUBROOM ì í”„ ê°ì§€ ì‹œìŠ¤í…œ í‰ê°€")
print("ğŸ“Š 30ë¶„ ì‹œí€€ìŠ¤ â†’ 10ë¶„ í›„ ì˜ˆì¸¡")
print("ğŸ“… í‰ê°€ ë°ì´í„°: 2025ë…„ 8ì›”")
print("="*80)

# ==============================================================================
# ğŸ“Š ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼)
# ==============================================================================

class HubRoomDataProcessor:
    """ì™„ì „í•œ ë°ì´í„° ì²˜ë¦¬ - ëª¨ë“  íŠ¹ì§• í¬í•¨"""
    
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        
        # 21ê°œ í•„ìˆ˜ ì»¬ëŸ¼
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB',
            'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2',
            'M14B_7F_TO_HUB_JOB2',
            'M16B_10F_TO_HUB_JOB'
        ]
        
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB',
            'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB',
            'M16A_3F_TO_M14B_7F_JOB',
            'M16A_3F_TO_3F_MLUD_JOB'
        ]
        
        self.cmd_cols = [
            'M16A_3F_CMD',
            'M16A_6F_TO_HUB_CMD',
            'M16A_2F_TO_HUB_CMD',
            'M14A_3F_TO_HUB_CMD',
            'M14B_7F_TO_HUB_CMD'
        ]
        
        self.capa_cols = [
            'M16A_6F_LFT_MAXCAPA',
            'M16A_2F_LFT_MAXCAPA'
        ]
        
        self.other_cols = [
            'M16A_3F_STORAGE_UTIL',
            'M14_TO_M16_OFS_CUR',
            'M16_TO_M14_OFS_CUR'
        ]
        
        # í™•ë¥  ë§µ - ë§¤ìš° ì¤‘ìš”!
        self.probability_map = {
            0: 0.003, 1: 0.15, 2: 0.25, 3: 0.31, 4: 0.43, 5: 0.43,
            6: 0.35, 7: 0.42, 8: 0.53, 9: 0.49, 10: 0.42,
            11: 0.47, 12: 0.52, 13: 0.60, 14: 0.54, 15: 0.66,
            16: 0.62, 17: 0.71, 18: 0.79, 19: 0.83, 20: 0.987,
            21: 0.99, 22: 0.99, 23: 0.99, 24: 0.99, 25: 0.99,
            26: 0.99, 27: 0.99, 28: 0.99, 29: 0.99, 30: 0.99
        }
    
    def load_and_merge_data(self, data_path):
        """ë°ì´í„° ë¡œë“œ ë° BRIDGE_TIME ë³‘í•©"""
        print("\n[1ë‹¨ê³„] ë°ì´í„° ë¡œë“œ")
        
        # ë©”ì¸ ë°ì´í„°
        df = pd.read_csv(data_path)
        print(f"âœ… í‰ê°€ ë°ì´í„°: {df.shape}")
        
        # ì‹œê°„ ì²˜ë¦¬
        time_col = df.columns[0]
        df['datetime'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M')
        
        # BRIDGE_TIME ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        bridge_path = data_path.replace('.csv', '_BRIDGE.csv')
        if os.path.exists(bridge_path):
            bridge_df = pd.read_csv(bridge_path)
            print(f"âœ… BRIDGE_TIME ë°ì´í„°: {bridge_df.shape}")
            
            if 'IDC_VAL' in bridge_df.columns:
                bridge_df['BRIDGE_TIME'] = bridge_df['IDC_VAL']
                bridge_df['datetime'] = pd.to_datetime(bridge_df['CRT_TM'])
                
                # ì‹œê°„ëŒ€ ì •ë³´ ì œê±°
                if hasattr(bridge_df['datetime'].dtype, 'tz'):
                    bridge_df['datetime'] = bridge_df['datetime'].dt.tz_localize(None)
                if hasattr(df['datetime'].dtype, 'tz'):
                    df['datetime'] = df['datetime'].dt.tz_localize(None)
                
                # ë¶„ ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
                bridge_df['datetime'] = bridge_df['datetime'].dt.floor('min')
                df['datetime'] = df['datetime'].dt.floor('min')
                
                # ë³‘í•©
                df = pd.merge(df, bridge_df[['datetime', 'BRIDGE_TIME']], 
                             on='datetime', how='left')
                
                # BRIDGE_TIME ë³´ê°„
                df['BRIDGE_TIME'] = df['BRIDGE_TIME'].interpolate(method='linear', limit_direction='both')
        
        # BRIDGE_TIMEì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
        if 'BRIDGE_TIME' not in df.columns:
            df['BRIDGE_TIME'] = 3.5
            
        df['BRIDGE_TIME'] = df['BRIDGE_TIME'].fillna(3.5)
        
        return df
    
    def create_all_features(self, df):
        """ì™„ì „í•œ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ - ëˆ„ë½ ì—†ìŒ!"""
        print("\n[2ë‹¨ê³„] íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§")
        
        # 1. ìœ ì…/ìœ ì¶œ ë°¸ëŸ°ìŠ¤
        df['flow_balance'] = df[self.inflow_cols].sum(axis=1) - df[self.outflow_cols].sum(axis=1)
        df['flow_ratio'] = df[self.inflow_cols].sum(axis=1) / (df[self.outflow_cols].sum(axis=1) + 1)
        
        # 2. ì¶”ì„¸ íŠ¹ì§•
        df['trend_20min'] = df[self.target_col].diff(20)
        df['trend_10min'] = df[self.target_col].diff(10)
        df['acceleration'] = df['trend_10min'] - df['trend_10min'].shift(10)
        
        # 3. ì—°ì† íŒ¨í„´
        df['consecutive_250+'] = (df[self.target_col] > 250).rolling(10).sum()
        df['consecutive_270+'] = (df[self.target_col] > 270).rolling(10).sum()
        
        # 4. CMD ë™ê¸°í™”
        df['cmd_sync_count'] = (df[self.cmd_cols] > 235).sum(axis=1)
        df['cmd_max'] = df[self.cmd_cols].max(axis=1)
        
        # 5. ë¸Œë¦¿ì§€íƒ€ì„ ë³€í™”
        df['bridge_diff'] = df['BRIDGE_TIME'].diff(5)
        df['bridge_high'] = (df['BRIDGE_TIME'] > 4.0).astype(int)
        
        # 6. storage x bridge ìƒí˜¸ì‘ìš©
        df['storage_x_bridge'] = df['M16A_3F_STORAGE_UTIL'] * df['BRIDGE_TIME']
        
        # 7. ì—°ì† 300+ ì¹´ìš´íŠ¸ì™€ í™•ë¥ 
        consecutive_300_counts = []
        consecutive_300_probs = []
        
        for i in tqdm(range(len(df)), desc="300+ íŒ¨í„´ ê³„ì‚°"):
            if i < 30:
                count = 0
                prob = 0.003
            else:
                window = df[self.target_col].iloc[i-30:i].values
                count = sum(1 for v in window if v >= 300)
                prob = self.probability_map.get(count, 0.5)
            
            consecutive_300_counts.append(count)
            consecutive_300_probs.append(prob)
        
        df['consecutive_300_count'] = consecutive_300_counts
        df['consecutive_300_prob'] = consecutive_300_probs
        
        # 8. 3êµ¬ê°„ ë¶„ë¥˜ (ìˆ«ìë¡œ ì§ì ‘ ë³€í™˜)
        conditions = [
            df[self.target_col] < 150,
            (df[self.target_col] >= 150) & (df[self.target_col] < 300),
            df[self.target_col] >= 300
        ]
        choices = [0, 1, 2]
        df['range_class'] = np.select(conditions, choices, default=1)
        
        # 9. ì í”„ ì—¬ë¶€
        df['past_30min_max'] = df[self.target_col].rolling(30).max()
        df['is_jump'] = ((df['past_30min_max'].shift(10) < 280) & 
                        (df[self.target_col] >= 300)).astype(int)
        
        # 10. ìƒìŠ¹/í•˜ë½ íŒ¨í„´ (ìˆ«ìë¡œ ë³€í™˜)
        df['change_20min'] = df[self.target_col] - df[self.target_col].shift(20)
        
        trend_conditions = [
            df['change_20min'] < -20,
            (df['change_20min'] >= -20) & (df['change_20min'] < 20),
            (df['change_20min'] >= 20) & (df['change_20min'] < 50),
            df['change_20min'] >= 50
        ]
        trend_choices = [0, 1, 2, 3]  # 0:down, 1:stable, 2:gradual_up, 3:rapid_up
        df['trend_pattern'] = np.select(trend_conditions, trend_choices, default=1)
        
        # 11. ìƒìŠ¹ë¥ /í•˜ë½ë¥  (%)
        df['change_rate_10min'] = ((df[self.target_col] - df[self.target_col].shift(10)) / 
                                   (df[self.target_col].shift(10) + 1)) * 100
        df['change_rate_20min'] = ((df[self.target_col] - df[self.target_col].shift(20)) / 
                                   (df[self.target_col].shift(20) + 1)) * 100
        df['change_rate_30min'] = ((df[self.target_col] - df[self.target_col].shift(30)) / 
                                   (df[self.target_col].shift(30) + 1)) * 100
        
        # 12. ë³€ë™ì„±
        df['volatility_10min'] = df[self.target_col].rolling(10).std()
        df['volatility_20min'] = df[self.target_col].rolling(20).std()
        df['volatility_30min'] = df[self.target_col].rolling(30).std()
        
        # 13. ê·¹ë‹¨ê°’ ê·¼ì ‘ë„
        df['distance_to_300'] = 300 - df[self.target_col]
        df['near_extreme'] = (df[self.target_col] > 280).astype(int)
        
        # 14. ìµœê·¼ í†µê³„
        df['recent_5min_mean'] = df[self.target_col].rolling(5).mean()
        df['recent_5min_max'] = df[self.target_col].rolling(5).max()
        df['recent_10min_mean'] = df[self.target_col].rolling(10).mean()
        
        # 15. 277 êµ¬ê°„ íŠ¹ë³„ ì§€í‘œ
        df['in_jump_zone'] = ((df[self.target_col] >= 275) & (df[self.target_col] <= 279)).astype(int)
        
        # NaN ì²˜ë¦¬
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].fillna(method='ffill').fillna(0)
        
        print(f"âœ… ì´ {len(df.columns)}ê°œ íŠ¹ì§• ìƒì„± ì™„ë£Œ")
        return df
    
    def create_sequences_for_evaluation(self, df, seq_len=30, pred_len=10):
        """í‰ê°€ìš© ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
        print(f"\n[3ë‹¨ê³„] ì‹œí€€ìŠ¤ ìƒì„± ({seq_len}ë¶„ â†’ {pred_len}ë¶„ í›„)")
        
        # íŠ¹ì§• ì»¬ëŸ¼ ì„ íƒ
        feature_cols = [col for col in df.columns 
                       if col not in ['datetime', 'range_class', 'is_jump', 'trend_pattern']]
        
        X = []
        y_info = []
        
        for i in tqdm(range(seq_len, len(df) - pred_len), desc="ì‹œí€€ìŠ¤ ìƒì„±"):
            # ì…ë ¥: ê³¼ê±° 30ë¶„
            X.append(df[feature_cols].iloc[i-seq_len:i].values)
            
            # íƒ€ê²Ÿ ì •ë³´ë“¤
            target_idx = i + pred_len - 1
            
            # ì‹œê°„ ì •ë³´
            current_time = df['datetime'].iloc[i-1]
            predict_time = df['datetime'].iloc[target_idx]
            seq_start_time = df['datetime'].iloc[i-seq_len]
            seq_end_time = df['datetime'].iloc[i-1]
            
            # ì‹œí€€ìŠ¤ í†µê³„
            seq_values = df[self.target_col].iloc[i-seq_len:i].values
            seq_max = np.max(seq_values)
            seq_min = np.min(seq_values)
            
            y_info.append({
                'current_time': current_time,
                'predict_time': predict_time,
                'seq_start_time': seq_start_time,
                'seq_end_time': seq_end_time,
                'seq_max': seq_max,
                'seq_min': seq_min,
                'target_value': df[self.target_col].iloc[target_idx],
                'is_jump': df['is_jump'].iloc[target_idx],
                'range_class': df['range_class'].iloc[target_idx],
                'trend_pattern': df['trend_pattern'].iloc[target_idx]
            })
        
        print(f"âœ… {len(X)}ê°œ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        return np.array(X), y_info, df

# ==============================================================================
# ğŸ¤– ëª¨ë¸ ì‹œìŠ¤í…œ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼)
# ==============================================================================

class JumpDetectionSystem:
    """ì í”„ ê°ì§€ 80% ë‹¬ì„± ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # ëª¨ë¸ë“¤
        self.model_screening = None
        self.model_jump = None
        self.model_range = None
        self.model_trend = None
        self.model_value = None
        
        self.feature_indices = {}
    
    def prepare_features(self, X_seq):
        """ì‹œí€€ìŠ¤ë¥¼ íŠ¹ì§•ìœ¼ë¡œ ë³€í™˜"""
        # ë§ˆì§€ë§‰ ì‹œì  íŠ¹ì§•
        last_features = X_seq[:, -1, :]
        
        # í†µê³„ íŠ¹ì§•
        mean_features = np.mean(X_seq, axis=1)
        std_features = np.std(X_seq, axis=1)
        max_features = np.max(X_seq, axis=1)
        min_features = np.min(X_seq, axis=1)
        
        # ì¶”ì„¸ íŠ¹ì§•
        trend_features = X_seq[:, -1, :] - X_seq[:, 0, :]
        
        # ëª¨ë“  íŠ¹ì§• ê²°í•©
        features = np.hstack([
            last_features,    # 0 ~ n-1
            mean_features,    # n ~ 2n-1
            std_features,     # 2n ~ 3n-1
            max_features,     # 3n ~ 4n-1
            min_features,     # 4n ~ 5n-1
            trend_features    # 5n ~ 6n-1
        ])
        
        return features
    
    def get_expanded_feature_indices(self, df):
        """í™•ì¥ëœ íŠ¹ì§• ì¸ë±ìŠ¤ ê³„ì‚° (6ë°° í™•ì¥ ê³ ë ¤)"""
        feature_cols = [col for col in df.columns 
                       if col not in ['datetime', 'range_class', 'is_jump', 'trend_pattern']]
        
        n_base_features = len(feature_cols)
        expanded_indices = {}
        
        # ì›ë³¸ íŠ¹ì§•ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
        base_indices = {}
        for i, col in enumerate(feature_cols):
            if 'STORAGE_UTIL' in col:
                base_indices['storage_util'] = i
            elif col == 'BRIDGE_TIME':  # ì •í™•í•œ ë§¤ì¹­
                base_indices['bridge_time'] = i
            elif 'flow_balance' in col:
                base_indices['flow_balance'] = i
            elif 'consecutive_250+' in col:
                base_indices['consecutive_250'] = i
            elif 'cmd_sync_count' in col:
                base_indices['cmd_sync'] = i
            elif 'trend_20min' in col:
                base_indices['trend_20min'] = i
            elif 'acceleration' in col:
                base_indices['acceleration'] = i
            elif 'consecutive_300_prob' in col:
                base_indices['prob_extreme'] = i
            elif 'in_jump_zone' in col:
                base_indices['in_jump_zone'] = i
            elif 'recent_5min_max' in col:
                base_indices['recent_5min_max'] = i
        
        # í™•ì¥ëœ ì¸ë±ìŠ¤ ê³„ì‚°
        for key, base_idx in base_indices.items():
            expanded_indices[f'{key}_last'] = base_idx
            expanded_indices[f'{key}_mean'] = base_idx + n_base_features
            expanded_indices[f'{key}_std'] = base_idx + 2 * n_base_features
            expanded_indices[f'{key}_max'] = base_idx + 3 * n_base_features
            expanded_indices[f'{key}_min'] = base_idx + 4 * n_base_features
            expanded_indices[f'{key}_trend'] = base_idx + 5 * n_base_features
        
        # ë””ë²„ê¹…
        print(f"\nğŸ“ í™•ì¥ëœ íŠ¹ì§• ì¸ë±ìŠ¤:")
        print(f"  - ê¸°ë³¸ íŠ¹ì§• ìˆ˜: {n_base_features}")
        print(f"  - í™•ì¥ íŠ¹ì§• ìˆ˜: {n_base_features * 6}")
        print(f"  - storage_util_last: {expanded_indices.get('storage_util_last', 'N/A')}")
        print(f"  - storage_util_max: {expanded_indices.get('storage_util_max', 'N/A')}")
        print(f"  - bridge_time_last: {expanded_indices.get('bridge_time_last', 'N/A')}")
        
        self.feature_indices = expanded_indices
        return expanded_indices
    
    def apply_emergency_boost(self, X_seq, X_features, predictions, value_pred):
        """ê¸´ê¸‰ ë¶€ìŠ¤íŒ… - í‰ê°€ ê²°ê³¼ë¥¼ ë³´ê³  ì¶”ê°€í•œ ê°•ë ¥í•œ ê·œì¹™"""
        boosted = predictions.copy()
        boost_count = 0
        
        for i in range(len(X_seq)):
            # ê³¼ê±° 30ë¶„ ìµœëŒ€ê°’
            seq_max = np.max(X_seq[i, :, 0])  # target_colì´ ì²« ë²ˆì§¸ë¼ê³  ê°€ì •
            
            # ê·œì¹™ 1: ê°’ ì˜ˆì¸¡ì´ 290 ì´ìƒì´ê³  ê³¼ê±° ìµœëŒ€ê°’ì´ 280 ë¯¸ë§Œ
            if value_pred[i] >= 290 and seq_max < 280:
                boosted[i] = 1
                boost_count += 1
            
            # ê·œì¹™ 2: ìµœê·¼ 5ë¶„ í‰ê· ì´ 275 ì´ìƒì´ê³  ê³¼ê±° ìµœëŒ€ê°’ì´ 279 ì´í•˜
            recent_5min = np.mean(X_seq[i, -5:, 0])
            if recent_5min >= 275 and seq_max <= 279:
                boosted[i] = 1
                boost_count += 1
            
            # ê·œì¹™ 3: ê°€ì†ë„ê°€ í¬ê³  í˜„ì¬ê°’ì´ 270 ì´ìƒ
            if len(X_seq[i]) >= 20:
                accel = (np.mean(X_seq[i, -5:, 0]) - np.mean(X_seq[i, -10:-5, 0])) - \
                       (np.mean(X_seq[i, -10:-5, 0]) - np.mean(X_seq[i, -15:-10, 0]))
                current = X_seq[i, -1, 0]
                
                if accel > 15 and current >= 270:
                    boosted[i] = 1
                    boost_count += 1
        
        print(f"  ğŸš¨ ê¸´ê¸‰ ë¶€ìŠ¤íŒ…: {boost_count}ê°œ ì¶”ê°€")
        return boosted
    
    def apply_rule_based_boost_v3(self, X, predictions, prob_scores=None):
        """ê°œì„ ëœ ê·œì¹™ ê¸°ë°˜ ë¶€ìŠ¤íŒ… - í™•ì¥ëœ ì¸ë±ìŠ¤ ì‚¬ìš©"""
        boosted_predictions = predictions.copy()
        
        if not self.feature_indices:
            print("âš ï¸ íŠ¹ì§• ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return boosted_predictions
        
        idx = self.feature_indices
        boost_count = 0
        
        # Phase 1: storage_util ê¸°ë°˜ (ì—¬ëŸ¬ í†µê³„ ì‚¬ìš©)
        conditions = []
        
        if 'storage_util_last' in idx:
            conditions.append(X[:, idx['storage_util_last']] > 15)
        if 'storage_util_max' in idx:
            conditions.append(X[:, idx['storage_util_max']] > 20)
        if 'storage_util_mean' in idx:
            conditions.append(X[:, idx['storage_util_mean']] > 10)
        
        if conditions:
            strong_signal = np.any(conditions, axis=0)
            new_boost = strong_signal & (boosted_predictions == 0)
            boosted_predictions[strong_signal] = 1
            phase1_count = np.sum(new_boost)
            print(f"  Phase 1 (storage): {phase1_count}ê°œ ì¶”ê°€")
            boost_count += phase1_count
        
        # Phase 2: bridge_time ê¸°ë°˜
        conditions = []
        
        if 'bridge_time_last' in idx:
            conditions.append(X[:, idx['bridge_time_last']] > 3.8)
        if 'bridge_time_max' in idx:
            conditions.append(X[:, idx['bridge_time_max']] > 4.0)
        if 'bridge_time_mean' in idx:
            conditions.append(X[:, idx['bridge_time_mean']] > 3.7)
        
        if conditions:
            bridge_signal = np.any(conditions, axis=0)
            new_boost = bridge_signal & (boosted_predictions == 0)
            boosted_predictions[bridge_signal] = 1
            phase2_count = np.sum(new_boost)
            print(f"  Phase 2 (bridge): {phase2_count}ê°œ ì¶”ê°€")
            boost_count += phase2_count
        
        # Phase 3: ë³µí•© ì¡°ê±´
        complex_conditions = []
        
        if 'flow_balance_mean' in idx:
            complex_conditions.append(X[:, idx['flow_balance_mean']] > 30)
        if 'acceleration_last' in idx:
            complex_conditions.append(X[:, idx['acceleration_last']] > 10)
        if 'consecutive_250_max' in idx:
            complex_conditions.append(X[:, idx['consecutive_250_max']] >= 5)
        if 'trend_20min_last' in idx:
            complex_conditions.append(X[:, idx['trend_20min_last']] > 20)
        
        if len(complex_conditions) >= 2:
            complex_signal = np.sum(complex_conditions, axis=0) >= 2
            new_boost = complex_signal & (boosted_predictions == 0)
            boosted_predictions[complex_signal] = 1
            phase3_count = np.sum(new_boost)
            print(f"  Phase 3 (ë³µí•©): {phase3_count}ê°œ ì¶”ê°€")
            boost_count += phase3_count
        
        # Phase 4: í™•ë¥  ê¸°ë°˜
        if prob_scores is not None:
            # í™•ë¥ ì´ 0.1 ì´ìƒì´ë©´ ë¶€ìŠ¤íŠ¸ (ë” ë‚®ì¶¤)
            prob_boost = (prob_scores > 0.1) & (boosted_predictions == 0)
            boosted_predictions[prob_boost] = 1
            phase4_count = np.sum(prob_boost)
            print(f"  Phase 4 (í™•ë¥ >0.1): {phase4_count}ê°œ ì¶”ê°€")
            boost_count += phase4_count
        
        # Phase 5: ì í”„ì¡´ ê°ì§€
        if 'in_jump_zone_last' in idx:
            jump_zone = X[:, idx['in_jump_zone_last']] > 0
            new_boost = jump_zone & (boosted_predictions == 0)
            boosted_predictions[jump_zone] = 1
            phase5_count = np.sum(new_boost)
            print(f"  Phase 5 (277ì¡´): {phase5_count}ê°œ ì¶”ê°€")
            boost_count += phase5_count
        
        # Phase 6: recent_5min_maxê°€ 275 ì´ìƒ
        if 'recent_5min_max_last' in idx:
            recent_high = X[:, idx['recent_5min_max_last']] >= 275
            new_boost = recent_high & (boosted_predictions == 0)
            boosted_predictions[recent_high] = 1
            phase6_count = np.sum(new_boost)
            print(f"  Phase 6 (ìµœê·¼5ë¶„maxâ‰¥275): {phase6_count}ê°œ ì¶”ê°€")
            boost_count += phase6_count
        
        print(f"  ğŸ“Š ì´ ë¶€ìŠ¤íŒ…: {boost_count}ê°œ")
        
        return boosted_predictions

# ==============================================================================
# ğŸ“Š í ë³€í™”ëŸ‰ ë¶„ì„ í•¨ìˆ˜ (ì¶”ê°€)
# ==============================================================================

def analyze_queue_changes(df, target_col='CURRENT_M16A_3F_JOB_2'):
    """í ë³€í™”ëŸ‰ ë¶„ì„ - 5ë¶„, 10ë¶„, 20ë¶„, 30ë¶„ í›„ ë³€í™”ëŸ‰ ë¶„ì„"""
    
    print("\nğŸ” í ë³€í™”ëŸ‰ ìƒì„¸ ë¶„ì„ ì‹œì‘...")
    
    # ì—‘ì…€ ì €ì¥ì„ ìœ„í•œ ê²°ê³¼ ì €ì¥ ë”•ì…”ë„ˆë¦¬
    excel_results = {}
    
    # 1. 300 ë¯¸ë§Œ ì•ˆì • êµ¬ê°„ ë¶„ì„
    print("\n[1] 300 ë¯¸ë§Œ ì•ˆì • êµ¬ê°„ ë¶„ì„")
    stable_count = 0
    
    for i in range(len(df) - 30):
        # í˜„ì¬ ì‹œì ë¶€í„° 10ë¶„ê¹Œì§€ ëª¨ë‘ 300 ë¯¸ë§Œì¸ì§€ ì²´í¬
        if i + 10 < len(df):
            current_and_future = df[target_col].iloc[i:i+11].values
            if np.all(current_and_future < 300):
                stable_count += 1
    
    print(f"  - í˜„ì¬ ë° í–¥í›„ 10ë¶„ê°„ ëª¨ë‘ 300 ë¯¸ë§Œì¸ ì¼€ì´ìŠ¤: {stable_count}ê°œ")
    print(f"  - ì „ì²´ ëŒ€ë¹„ ë¹„ìœ¨: {stable_count/(len(df)-30)*100:.2f}%")
    
    # ì•ˆì • êµ¬ê°„ ë¶„ì„ ê²°ê³¼ ì €ì¥
    excel_results['ì•ˆì •êµ¬ê°„ë¶„ì„'] = pd.DataFrame([{
        'ë¶„ì„í•­ëª©': '300ë¯¸ë§Œ ì•ˆì •êµ¬ê°„',
        'ì¼€ì´ìŠ¤ìˆ˜': stable_count,
        'ì „ì²´ëŒ€ë¹„ë¹„ìœ¨(%)': round(stable_count/(len(df)-30)*100, 2)
    }])
    
    # 2. ì‹œê°„ëŒ€ë³„ ë³€í™”ëŸ‰ ë¶„ì„
    time_horizons = [5, 10, 20, 30]
    
    # êµ¬ê°„ ì •ì˜
    ranges = [
        (200, 250, "200-250"),
        (250, 300, "250-300"),
        (300, 350, "300-350")
    ]
    
    # ë³€í™”ëŸ‰ êµ¬ê°„ ì •ì˜
    change_bins = [
        (0, 20, "0~20"),
        (21, 40, "21~40"),
        (41, 60, "41~60"),
        (61, 80, "61~80"),
        (81, 100, "81~100"),
        (101, float('inf'), "100+")
    ]
    
    # ê° ì‹œê°„ëŒ€ë³„ ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    all_analysis_results = []
    
    # ê° ì‹œê°„ëŒ€ë³„ ë¶„ì„
    for horizon in time_horizons:
        print(f"\n[2-{horizon}] {horizon}ë¶„ í›„ ë³€í™”ëŸ‰ ë¶„ì„")
        
        # ê° êµ¬ê°„ë³„ ë¶„ì„
        for range_min, range_max, range_name in ranges:
            print(f"\n  ğŸ“Š {range_name} êµ¬ê°„:")
            
            # í˜„ì¬ ê°’ì´ í•´ë‹¹ êµ¬ê°„ì— ìˆëŠ” ì¼€ì´ìŠ¤ë“¤ ì°¾ê¸°
            range_mask = (df[target_col] >= range_min) & (df[target_col] < range_max)
            range_indices = df.index[range_mask].tolist()
            
            # ë³€í™”ëŸ‰ ê³„ì‚°
            changes_dict = {bin_name: [] for _, _, bin_name in change_bins}
            
            for idx in range_indices:
                if idx + horizon < len(df):
                    current_val = df[target_col].iloc[idx]
                    future_val = df[target_col].iloc[idx + horizon]
                    change = abs(future_val - current_val)
                    
                    # ì–´ëŠ êµ¬ê°„ì— ì†í•˜ëŠ”ì§€ í™•ì¸
                    for bin_min, bin_max, bin_name in change_bins:
                        if bin_min <= change < bin_max:
                            changes_dict[bin_name].append({
                                'index': idx,
                                'current': current_val,
                                'future': future_val,
                                'change': change,
                                'direction': 'up' if future_val > current_val else 'down'
                            })
                            break
            
            # ê²°ê³¼ ì¶œë ¥ ë° ì—‘ì…€ìš© ë°ì´í„° ì €ì¥
            total_cases = sum(len(v) for v in changes_dict.values())
            if total_cases > 0:
                for bin_name in ["0~20", "21~40", "41~60", "61~80", "81~100"]:
                    count = len(changes_dict.get(bin_name, []))
                    pct = count / total_cases * 100 if total_cases > 0 else 0
                    up_count = sum(1 for item in changes_dict.get(bin_name, []) if item['direction'] == 'up')
                    down_count = count - up_count
                    print(f"    {bin_name:8}: {count:5}ê°œ ({pct:5.1f}%) [ìƒìŠ¹:{up_count:4}ê°œ, í•˜ë½:{down_count:4}ê°œ]")
                    
                    # ì—‘ì…€ ì €ì¥ìš© ë°ì´í„°
                    all_analysis_results.append({
                        'ì˜ˆì¸¡ì‹œê°„(ë¶„)': horizon,
                        'í˜„ì¬ê°’êµ¬ê°„': range_name,
                        'ë³€í™”ëŸ‰êµ¬ê°„': bin_name,
                        'ì¼€ì´ìŠ¤ìˆ˜': count,
                        'ë¹„ì¤‘(%)': round(pct, 1),
                        'ìƒìŠ¹ì¼€ì´ìŠ¤': up_count,
                        'í•˜ë½ì¼€ì´ìŠ¤': down_count
                    })
            else:
                print(f"    í•´ë‹¹ êµ¬ê°„ ë°ì´í„° ì—†ìŒ")
    
    # ì‹œê°„ëŒ€ë³„ ë³€í™”ëŸ‰ ë¶„ì„ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    excel_results['ë³€í™”ëŸ‰ë¶„ì„'] = pd.DataFrame(all_analysis_results)
    
    # 3. ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
    output_file = 'queue_change_analysis.xlsx'
    
    try:
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # ì•ˆì •êµ¬ê°„ ë¶„ì„ ì‹œíŠ¸
            excel_results['ì•ˆì •êµ¬ê°„ë¶„ì„'].to_excel(writer, sheet_name='ì•ˆì •êµ¬ê°„ë¶„ì„', index=False)
            
            # ë³€í™”ëŸ‰ ë¶„ì„ ì‹œíŠ¸
            excel_results['ë³€í™”ëŸ‰ë¶„ì„'].to_excel(writer, sheet_name='ë³€í™”ëŸ‰ë¶„ì„', index=False)
            
            # ì›Œí¬ë¶ê³¼ ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸°
            workbook = writer.book
            
            # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
            for sheet_name in workbook.sheetnames:
                worksheet = workbook[sheet_name]
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
        
        print(f"\nğŸ’¾ í ë³€í™”ëŸ‰ ë¶„ì„ ê²°ê³¼ê°€ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")
    except Exception as e:
        print(f"\nâš ï¸ ì—‘ì…€ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # CSVë¡œ ëŒ€ì²´ ì €ì¥
        excel_results['ë³€í™”ëŸ‰ë¶„ì„'].to_csv('queue_change_analysis.csv', index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ CSV íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: queue_change_analysis.csv")
    
    print("\nâœ… í ë³€í™”ëŸ‰ ë¶„ì„ ì™„ë£Œ")

# ==============================================================================
# ğŸ“Š í‰ê°€ í•¨ìˆ˜
# ==============================================================================

def evaluate_models():
    """ëª¨ë¸ í‰ê°€ ë©”ì¸ í•¨ìˆ˜"""
    
    print("\nğŸš€ ì í”„ ê°ì§€ ì‹œìŠ¤í…œ í‰ê°€ ì‹œì‘...")
    
    # 1. ì²´í¬í¬ì¸íŠ¸ í™•ì¸
    checkpoint_dir = './checkpoints_jump80'
    models_dir = os.path.join(checkpoint_dir, 'models')
    
    if not os.path.exists(models_dir):
        print("âŒ í•™ìŠµëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í•™ìŠµì„ ì§„í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    # 2. ë°ì´í„° ì²˜ë¦¬
    processor = HubRoomDataProcessor()
    system = JumpDetectionSystem()
    
    # í‰ê°€ ë°ì´í„° ë¡œë“œ
    eval_data_path = 'data/20250801_to_20250831.csv'
    
    if not os.path.exists(eval_data_path):
        print(f"âŒ í‰ê°€ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {eval_data_path}")
        return
    
    # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    df = processor.load_and_merge_data(eval_data_path)
    df = processor.create_all_features(df)
    
    # 3. ì‹œí€€ìŠ¤ ìƒì„±
    X_seq, y_info, df = processor.create_sequences_for_evaluation(df)
    
    print(f"\nğŸ“Š í‰ê°€ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
    print(f"  - ì‹œí€€ìŠ¤ ìˆ˜: {len(X_seq)}")
    print(f"  - ì‹œí€€ìŠ¤ í˜•íƒœ: {X_seq.shape}")
    
    # 4. ëª¨ë¸ ë¡œë“œ
    print("\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    try:
        system.model_jump = joblib.load(os.path.join(models_dir, 'model_jump.pkl'))
        system.model_range = joblib.load(os.path.join(models_dir, 'model_range.pkl'))
        system.model_trend = joblib.load(os.path.join(models_dir, 'model_trend.pkl'))
        system.model_value = joblib.load(os.path.join(models_dir, 'model_value.pkl'))
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # íŠ¹ì§• ì¸ë±ìŠ¤ ì„¤ì • (í™•ì¥ëœ ì¸ë±ìŠ¤ ì‚¬ìš©)
    system.get_expanded_feature_indices(df)
    
    # 5. íŠ¹ì§• ì¤€ë¹„
    X_features = system.prepare_features(X_seq)
    print(f"âœ… íŠ¹ì§• ì¤€ë¹„ ì™„ë£Œ: {X_features.shape}")
    
    # 6. ì˜ˆì¸¡ ìˆ˜í–‰
    print("\nğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
    
    # ì í”„ ì˜ˆì¸¡
    print("  - ì í”„ ê°ì§€ ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...")
    jump_pred = system.model_jump.predict(X_features)
    jump_pred_proba = system.model_jump.predict_proba(X_features)[:, 1]
    
    # ì í”„ ì˜ˆì¸¡ í†µê³„
    print(f"    ì´ˆê¸° ì í”„ ì˜ˆì¸¡: {np.sum(jump_pred)}ê°œ ê°ì§€")
    print(f"    ì í”„ í™•ë¥  ë¶„í¬: ìµœì†Œ={np.min(jump_pred_proba):.3f}, ìµœëŒ€={np.max(jump_pred_proba):.3f}, í‰ê· ={np.mean(jump_pred_proba):.3f}")
    
    # ê°’ ì˜ˆì¸¡ (ë¶€ìŠ¤íŒ…ì— í•„ìš”)
    print("  - ê°’ ì˜ˆì¸¡ ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...")
    value_pred = system.model_value.predict(X_features)
    
    print("\n  ğŸ“Š ê·œì¹™ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ì ìš©")
    jump_pred_boosted = system.apply_rule_based_boost_v3(X_features, jump_pred, jump_pred_proba)
    print(f"    ë¶€ìŠ¤íŒ… í›„ ì í”„ ì˜ˆì¸¡: {np.sum(jump_pred_boosted)}ê°œ ê°ì§€")
    
    # ê¸´ê¸‰ ë¶€ìŠ¤íŒ… ì¶”ê°€
    print("\n  ğŸš¨ ê¸´ê¸‰ ë¶€ìŠ¤íŒ… ì ìš©")
    jump_pred_final = system.apply_emergency_boost(X_seq, X_features, jump_pred_boosted, value_pred)
    print(f"    ìµœì¢… ì í”„ ì˜ˆì¸¡: {np.sum(jump_pred_final)}ê°œ ê°ì§€")
    
    # êµ¬ê°„ ì˜ˆì¸¡
    print("\n  - 3êµ¬ê°„ ë¶„ë¥˜ ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...")
    range_pred = system.model_range.predict(X_features)
    
    # íŒ¨í„´ ì˜ˆì¸¡
    print("  - ìƒìŠ¹/í•˜ë½ íŒ¨í„´ ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...")
    trend_pred = system.model_trend.predict(X_features)
    
    # 7. ê²°ê³¼ ì •ë¦¬
    print("\nğŸ“‹ ê²°ê³¼ ìƒì„± ì¤‘...")
    results = []
    
    # ì‹¤ì œ ì í”„ ì¼€ì´ìŠ¤ ì¹´ìš´íŠ¸
    actual_jump_count = 0
    detected_jump_count = 0
    
    for i in tqdm(range(len(X_seq)), desc="ê²°ê³¼ ìƒì„±"):
        info = y_info[i]
        
        # ì í”„ ì¼€ì´ìŠ¤ í™•ì¸ (ê³¼ê±° ìµœëŒ€ê°’ < 280 & ì‹¤ì œê°’ >= 300)
        is_jump_case = (info['seq_max'] < 280) and (info['target_value'] >= 300)
        if is_jump_case:
            actual_jump_count += 1
        
        predicted_jump = jump_pred_final[i] == 1
        if is_jump_case and predicted_jump:
            detected_jump_count += 1
        
        # ì˜ˆì¸¡ê°’ ì¡°ì • - ì í”„ ì˜ˆì¸¡ëœ ê²½ìš° +40
        adjusted_value_pred = value_pred[i]
        if predicted_jump:
            adjusted_value_pred = value_pred[i] + 40
        
        # í˜„ì¬ ì‹œì  ì¸ë±ìŠ¤ ì°¾ê¸°
        current_idx = 30 + i - 1  # 30ë¶„ ì‹œí€€ìŠ¤ + ië²ˆì§¸ - 1
        
        # í˜„ì¬ê°’
        current_value = df[processor.target_col].iloc[current_idx] if current_idx < len(df) else 0
        
        # í˜„ì¬ê°’ êµ¬ê°„ ë¶„ë¥˜
        if current_value < 200:
            current_range = "0-200"
        elif current_value < 250:
            current_range = "200-250"
        elif current_value < 300:
            current_range = "250-300"
        elif current_value < 350:
            current_range = "300-350"
        else:
            current_range = "350+"
        
        # 5ë¶„ í›„ ì‹¤ì œê°’ê³¼ ë³€í™”ëŸ‰
        actual_5min = 0
        change_5min = 0
        change_5min_range = ""
        if current_idx + 5 < len(df):
            actual_5min = df[processor.target_col].iloc[current_idx + 5]
            change_5min = actual_5min - current_value
            # ë³€í™”ëŸ‰ êµ¬ê°„
            abs_change = abs(change_5min)
            if abs_change <= 20:
                change_5min_range = "0~20"
            elif abs_change <= 40:
                change_5min_range = "21~40"
            elif abs_change <= 60:
                change_5min_range = "41~60"
            elif abs_change <= 80:
                change_5min_range = "61~80"
            elif abs_change <= 100:
                change_5min_range = "81~100"
            else:
                change_5min_range = "100+"
        
        # 10ë¶„ í›„ëŠ” ì´ë¯¸ target_valueë¡œ ìˆìŒ
        actual_10min = info['target_value']
        change_10min = actual_10min - current_value
        # 10ë¶„ ë³€í™”ëŸ‰ êµ¬ê°„
        abs_change_10 = abs(change_10min)
        if abs_change_10 <= 20:
            change_10min_range = "0~20"
        elif abs_change_10 <= 40:
            change_10min_range = "21~40"
        elif abs_change_10 <= 60:
            change_10min_range = "41~60"
        elif abs_change_10 <= 80:
            change_10min_range = "61~80"
        elif abs_change_10 <= 100:
            change_10min_range = "81~100"
        else:
            change_10min_range = "100+"
        
        # 20ë¶„ í›„ ì‹¤ì œê°’ê³¼ ë³€í™”ëŸ‰
        actual_20min = 0
        change_20min = 0
        change_20min_range = ""
        if current_idx + 20 < len(df):
            actual_20min = df[processor.target_col].iloc[current_idx + 20]
            change_20min = actual_20min - current_value
            # ë³€í™”ëŸ‰ êµ¬ê°„
            abs_change = abs(change_20min)
            if abs_change <= 20:
                change_20min_range = "0~20"
            elif abs_change <= 40:
                change_20min_range = "21~40"
            elif abs_change <= 60:
                change_20min_range = "41~60"
            elif abs_change <= 80:
                change_20min_range = "61~80"
            elif abs_change <= 100:
                change_20min_range = "81~100"
            else:
                change_20min_range = "100+"
        
        # 30ë¶„ í›„ ì‹¤ì œê°’ê³¼ ë³€í™”ëŸ‰
        actual_30min = 0
        change_30min = 0
        change_30min_range = ""
        if current_idx + 30 < len(df):
            actual_30min = df[processor.target_col].iloc[current_idx + 30]
            change_30min = actual_30min - current_value
            # ë³€í™”ëŸ‰ êµ¬ê°„
            abs_change = abs(change_30min)
            if abs_change <= 20:
                change_30min_range = "0~20"
            elif abs_change <= 40:
                change_30min_range = "21~40"
            elif abs_change <= 60:
                change_30min_range = "41~60"
            elif abs_change <= 80:
                change_30min_range = "61~80"
            elif abs_change <= 100:
                change_30min_range = "81~100"
            else:
                change_30min_range = "100+"
        
        # ì—°ì† 300+ í™•ì¸ (í˜„ì¬ë¶€í„° 30ë¶„ê°„)
        stable_under_300 = "X"
        if current_idx + 30 < len(df):
            future_30min = df[processor.target_col].iloc[current_idx:current_idx+31].values
            if np.all(future_30min < 300):
                stable_under_300 = "O"
        
        results.append({
            'ë‚ ì§œ': info['current_time'].strftime('%Y-%m-%d %H:%M'),
            'ì˜ˆì¸¡ë‚ ì§œ': info['predict_time'].strftime('%Y-%m-%d %H:%M'),
            'ì¸¡ì •ì‹œí€€ìŠ¤MAX': round(info['seq_max'], 2),
            'ì¸¡ì •ì‹œí€€ìŠ¤MIN': round(info['seq_min'], 2),
            'ì‹œí€€ìŠ¤ì‹œì‘ì‹œê°„': info['seq_start_time'].strftime('%Y-%m-%d %H:%M'),
            'ì‹œí€€ìŠ¤ì™„ë£Œì‹œê°„': info['seq_end_time'].strftime('%Y-%m-%d %H:%M'),
            'í˜„ì¬ê°’': round(current_value, 2),
            'í˜„ì¬ê°’êµ¬ê°„': current_range,
            'ì‹¤ì œê°’_10ë¶„í›„': round(info['target_value'], 2),
            'ì˜ˆì¸¡ê°’': round(adjusted_value_pred, 2),
            'ì˜¤ì°¨': round(abs(info['target_value'] - adjusted_value_pred), 2),
            'ì í”„ì˜ˆì¸¡': 'O' if predicted_jump else 'X',
            'ì í”„í™•ë¥ ': round(jump_pred_proba[i] * 100, 1),
            'êµ¬ê°„ì˜ˆì¸¡': ['50-150', '150-299', '300+'][range_pred[i]],
            'íŒ¨í„´ì˜ˆì¸¡': ['í•˜ë½', 'ì•ˆì •', 'ì ì§„ìƒìŠ¹', 'ê¸‰ìƒìŠ¹'][trend_pred[i]],
            'ì í”„ì¼€ì´ìŠ¤300+': 'O' if is_jump_case else 'X',
            # í ë³€í™”ëŸ‰ ë¶„ì„ ì¶”ê°€ ì»¬ëŸ¼ë“¤
            'ì‹¤ì œê°’_5ë¶„í›„': round(actual_5min, 2),
            'ì‹¤ì œê°’_20ë¶„í›„': round(actual_20min, 2),
            'ì‹¤ì œê°’_30ë¶„í›„': round(actual_30min, 2),
            'ë³€í™”ëŸ‰_5ë¶„': round(change_5min, 2),
            'ë³€í™”ëŸ‰_10ë¶„': round(change_10min, 2),
            'ë³€í™”ëŸ‰_20ë¶„': round(change_20min, 2),
            'ë³€í™”ëŸ‰_30ë¶„': round(change_30min, 2),
            'ë³€í™”ë°©í–¥_5ë¶„': 'ìƒìŠ¹' if change_5min > 0 else ('í•˜ë½' if change_5min < 0 else 'ìœ ì§€'),
            'ë³€í™”ë°©í–¥_10ë¶„': 'ìƒìŠ¹' if change_10min > 0 else ('í•˜ë½' if change_10min < 0 else 'ìœ ì§€'),
            'ë³€í™”ë°©í–¥_20ë¶„': 'ìƒìŠ¹' if change_20min > 0 else ('í•˜ë½' if change_20min < 0 else 'ìœ ì§€'),
            'ë³€í™”ë°©í–¥_30ë¶„': 'ìƒìŠ¹' if change_30min > 0 else ('í•˜ë½' if change_30min < 0 else 'ìœ ì§€'),
            'ë³€í™”êµ¬ê°„_5ë¶„': change_5min_range,
            'ë³€í™”êµ¬ê°„_10ë¶„': change_10min_range,
            'ë³€í™”êµ¬ê°„_20ë¶„': change_20min_range,
            'ë³€í™”êµ¬ê°„_30ë¶„': change_30min_range,
            '30ë¶„ê°„_300ë¯¸ë§Œìœ ì§€': stable_under_300
        })
    
    print(f"\nğŸ“Š ì í”„ ì¼€ì´ìŠ¤ ê°ì§€ í˜„í™©:")
    print(f"  - ì‹¤ì œ ì í”„ ì¼€ì´ìŠ¤: {actual_jump_count}ê°œ")
    print(f"  - ê°ì§€ëœ ì í”„ ì¼€ì´ìŠ¤: {detected_jump_count}ê°œ")
    if actual_jump_count > 0:
        print(f"  - ê°ì§€ìœ¨: {detected_jump_count/actual_jump_count*100:.1f}%")
    
    # 8. ê²°ê³¼ ë¶„ì„
    results_df = pd.DataFrame(results)
    
    print("\n" + "="*80)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ë¶„ì„")
    print("="*80)
    
    # ì „ì²´ ì„±ëŠ¥
    mae = results_df['ì˜¤ì°¨'].mean()
    rmse = np.sqrt((results_df['ì˜¤ì°¨'] ** 2).mean())
    
    print(f"\nğŸ“ˆ ì „ì²´ ì„±ëŠ¥:")
    print(f"  - MAE: {mae:.2f}")
    print(f"  - RMSE: {rmse:.2f}")
    print(f"  - ì´ ì˜ˆì¸¡ ìˆ˜: {len(results_df)}")
    
    # ê·¹ë‹¨ê°’ ê°ì§€ ì„±ëŠ¥
    extreme_mask = results_df['ì‹¤ì œê°’'] >= 300
    if extreme_mask.sum() > 0:
        extreme_detected = (results_df[extreme_mask]['ì˜ˆì¸¡ê°’'] >= 300).sum()
        extreme_recall = extreme_detected / extreme_mask.sum() * 100
        print(f"\nğŸ¯ ê·¹ë‹¨ê°’(300+) ê°ì§€:")
        print(f"  - ì‹¤ì œ 300+ ì¼€ì´ìŠ¤: {extreme_mask.sum()}ê°œ")
        print(f"  - ê°ì§€ëœ ì¼€ì´ìŠ¤: {extreme_detected}ê°œ")
        print(f"  - ê°ì§€ìœ¨: {extreme_recall:.1f}%")
    
    # ì í”„ ì¼€ì´ìŠ¤ ì„±ëŠ¥
    jump_cases = results_df[results_df['ì í”„ì¼€ì´ìŠ¤300+'] == 'O']
    if len(jump_cases) > 0:
        jump_detected = (jump_cases['ì í”„ì˜ˆì¸¡'] == 'O').sum()
        jump_recall = jump_detected / len(jump_cases) * 100
        print(f"\nğŸš€ ì í”„ ì¼€ì´ìŠ¤ (ê³¼ê±°<280 â†’ ì‹¤ì œ300+):")
        print(f"  - ì´ ì í”„ ì¼€ì´ìŠ¤: {len(jump_cases)}ê°œ")
        print(f"  - ê°ì§€ëœ ì¼€ì´ìŠ¤: {jump_detected}ê°œ")
        print(f"  - ì í”„ ê°ì§€ìœ¨: {jump_recall:.1f}%")
    
    # False Positive
    stable_mask = results_df['ì‹¤ì œê°’'] < 300
    if stable_mask.sum() > 0:
        fp_count = (results_df[stable_mask]['ì˜ˆì¸¡ê°’'] >= 300).sum()
        fp_rate = fp_count / stable_mask.sum() * 100
        print(f"\nâš ï¸ False Positive:")
        print(f"  - ì‹¤ì œ <300 ì¼€ì´ìŠ¤: {stable_mask.sum()}ê°œ")
        print(f"  - ì˜ëª» ì˜ˆì¸¡ëœ ì¼€ì´ìŠ¤: {fp_count}ê°œ")
        print(f"  - FP Rate: {fp_rate:.1f}%")
    
    # 3êµ¬ê°„ ë¶„ë¥˜ ì •í™•ë„
    actual_ranges = []
    for info in y_info:
        if info['target_value'] < 150:
            actual_ranges.append(0)
        elif info['target_value'] < 300:
            actual_ranges.append(1)
        else:
            actual_ranges.append(2)
    
    range_accuracy = (range_pred == actual_ranges).mean() * 100
    print(f"\nğŸ“Š 3êµ¬ê°„ ë¶„ë¥˜ ì •í™•ë„: {range_accuracy:.1f}%")
    
    # íŒ¨í„´ ì˜ˆì¸¡ ë¶„ì„
    pattern_counts = results_df['íŒ¨í„´ì˜ˆì¸¡'].value_counts()
    print(f"\nğŸ“ˆ íŒ¨í„´ ì˜ˆì¸¡ ë¶„í¬:")
    for pattern, count in pattern_counts.items():
        print(f"  - {pattern}: {count}ê°œ ({count/len(results_df)*100:.1f}%)")
    
    # CSV íŒŒì¼ ì €ì¥ (í ë³€í™”ëŸ‰ ë¶„ì„ í¬í•¨)
    output_path = 'RESULT_2025_with_queue_analysis.csv'
    results_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ CSV ê²°ê³¼ ì €ì¥ ì™„ë£Œ (í ë³€í™”ëŸ‰ ë¶„ì„ í¬í•¨): {output_path}")
    
    # ì—‘ì…€ íŒŒì¼ë¡œë„ ì €ì¥
    try:
        excel_output = 'RESULT_2025_with_queue_analysis.xlsx'
        with pd.ExcelWriter(excel_output, engine='openpyxl') as writer:
            results_df.to_excel(writer, sheet_name='í‰ê°€ê²°ê³¼_ë³€í™”ëŸ‰ë¶„ì„', index=False)
            
            # ì›Œí¬ë¶ê³¼ ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸°
            workbook = writer.book
            worksheet = workbook['í‰ê°€ê²°ê³¼_ë³€í™”ëŸ‰ë¶„ì„']
            
            # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
            for column in worksheet.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 30)
                worksheet.column_dimensions[column_letter].width = adjusted_width
        
        print(f"ğŸ’¾ ì—‘ì…€ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {excel_output}")
    except Exception as e:
        print(f"âš ï¸ ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # í ë³€í™”ëŸ‰ í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š í ë³€í™”ëŸ‰ í†µê³„ ë¶„ì„:")
    
    # êµ¬ê°„ë³„ í‰ê·  ë³€í™”ëŸ‰
    for range_name in ['200-250', '250-300', '300-350']:
        range_data = results_df[results_df['í˜„ì¬ê°’êµ¬ê°„'] == range_name]
        if len(range_data) > 0:
            print(f"\n  {range_name} êµ¬ê°„:")
            print(f"    - ì¼€ì´ìŠ¤ ìˆ˜: {len(range_data)}ê°œ")
            print(f"    - 10ë¶„ í›„ í‰ê·  ë³€í™”ëŸ‰: {range_data['ë³€í™”ëŸ‰_10ë¶„'].mean():.1f}")
            print(f"    - 10ë¶„ í›„ ìƒìŠ¹ ë¹„ìœ¨: {(range_data['ë³€í™”ë°©í–¥_10ë¶„'] == 'ìƒìŠ¹').sum() / len(range_data) * 100:.1f}%")
            print(f"    - 300+ ë„ë‹¬ ë¹„ìœ¨: {(range_data['ì‹¤ì œê°’_10ë¶„í›„'] >= 300).sum() / len(range_data) * 100:.1f}%")
    
    # ==============================================================================
    # ğŸ” í ë³€í™”ëŸ‰ ë¶„ì„ ì¶”ê°€
    # ==============================================================================
    print("\n" + "="*80)
    print("ğŸ“Š í ë³€í™”ëŸ‰ ë¶„ì„ (ì¶”ê°€ ë¶„ì„)")
    print("="*80)
    
    # í ë³€í™”ëŸ‰ ë¶„ì„ í•¨ìˆ˜ í˜¸ì¶œ
    analyze_queue_changes(df, processor.target_col)
    
    print("\n" + "="*80)
    print("âœ… ì í”„ ê°ì§€ ì‹œìŠ¤í…œ í‰ê°€ ì™„ë£Œ!")
    print("="*80)

if __name__ == "__main__":
    evaluate_models()