import numpy as np
import pandas as pd
import xgboost as xgb
import pickle
import warnings
import gc
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_absolute_error, classification_report

warnings.filterwarnings('ignore')

def train_v8_2stage():
    """
    ğŸ¯ V8 2ë‹¨ê³„ ë¶„ë¦¬ ëª¨ë¸ í•™ìŠµ
    
    [1ë‹¨ê³„] ê¸‰ë³€ íƒì§€ ëª¨ë¸ (ì´ì§„ ë¶„ë¥˜)
            â†’ ê¸‰ë³€(Â±50 ì´ìƒ) vs ì¼ë°˜
            â†’ ê¸‰ë³€ì— 50ë°° ê°€ì¤‘ì¹˜
    
    [2ë‹¨ê³„] 
        ê¸‰ë³€ Yes â†’ ê¸‰ë“±/ê¸‰ë½ ì „ìš© ëª¨ë¸
        ê¸‰ë³€ No  â†’ ì¼ë°˜ 3ë‹¨ê³„ ëª¨ë¸ (ì†Œí­í•˜ë½/ì •ì²´/ì†Œí­ìƒìŠ¹)
    
    ì €ì¥: xgboost_2stage_V8.pkl
    """
    print("="*80)
    print("ğŸ¯ V8 2ë‹¨ê³„ ë¶„ë¦¬ ëª¨ë¸ í•™ìŠµ")
    print("="*80)
    
    FEATURE_COLS_V8 = {
        'storage': ['M16A_3F_STORAGE_UTIL'],
        'fs_storage': ['CD_M163FSTORAGEUSE', 'CD_M163FSTORAGETOTAL', 'CD_M163FSTORAGEUTIL'],
        'hub': ['HUBROOMTOTAL'],
        'cmd': ['M16A_3F_CMD', 'M16A_6F_TO_HUB_CMD'],
        'inflow': ['M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2', 'M14A_3F_TO_HUB_JOB2'],
        'outflow': ['M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB', 'M16A_3F_TO_M14A_3F_JOB'],
        'maxcapa': ['M16A_6F_LFT_MAXCAPA', 'M16A_2F_LFT_MAXCAPA'],
    }
    
    TARGET_COL = 'CURRENT_M16A_3F_JOB_2'
    
    # ë°ì´í„° ë¡œë“œ
    print("\n[STEP 1] ë°ì´í„° ë¡œë“œ")
    print("-"*40)
    
    try:
        df = pd.read_csv('train_data.csv', on_bad_lines='skip', encoding='utf-8', low_memory=False)
    except:
        try:
            df = pd.read_csv('train_data.csv', on_bad_lines='skip', encoding='cp949', low_memory=False)
        except:
            df = pd.read_csv('train_data.csv', on_bad_lines='skip', encoding='euc-kr', low_memory=False)
    
    df[TARGET_COL] = pd.to_numeric(df[TARGET_COL], errors='coerce')
    df = df.dropna(subset=[TARGET_COL])
    print(f"í•™ìŠµ ë°ì´í„°: {len(df)}ê°œ í–‰")
    
    available_cols = set(df.columns)
    v8_cols = [col for cols in FEATURE_COLS_V8.values() for col in cols]
    print(f"ğŸ“‹ V8 ì»¬ëŸ¼: {sum(1 for c in v8_cols if c in available_cols)}/14ê°œ")
    
    # ë°°ì—´ ë³€í™˜
    print("\n[STEP 2] ë°ì´í„° ë³€í™˜")
    print("-"*40)
    
    target_arr = df[TARGET_COL].values.astype(np.float32)
    col_arrays = {col: pd.to_numeric(df[col], errors='coerce').fillna(0).values.astype(np.float32) 
                  for col in v8_cols if col in available_cols}
    print(f"  ë³€í™˜ ì™„ë£Œ: {len(col_arrays)}ê°œ ì»¬ëŸ¼")
    del df
    gc.collect()
    
    # Feature ìƒì„±
    print("\n[STEP 3] Feature ìƒì„±")
    print("-"*40)
    
    n_samples = len(target_arr) - 40
    x_range = np.arange(30, dtype=np.float32)
    
    features_list = []
    labels_sudden = []      # ê¸‰ë³€ ì—¬ë¶€ (0: ì¼ë°˜, 1: ê¸‰ë³€)
    labels_direction = []   # ë°©í–¥ (0: ê¸‰ë½, 1: ê¸‰ë“±)
    labels_normal = []      # ì¼ë°˜ ë¶„ë¥˜ (0: ì†Œí­í•˜ë½, 1: ì •ì²´, 2: ì†Œí­ìƒìŠ¹)
    labels_change = []      # ì‹¤ì œ ë³€í™”ëŸ‰
    current_values = []
    slopes = []
    
    for idx, i in enumerate(range(30, len(target_arr) - 10)):
        if idx % 10000 == 0:
            print(f"  ì§„í–‰: {idx}/{n_samples} ({idx/n_samples*100:.1f}%)")
            gc.collect()
        
        seq_target = target_arr[i-30:i]
        current_value = seq_target[-1]
        actual_change = target_arr[i+9] - current_value
        slope = np.polyfit(x_range, seq_target, 1)[0]
        
        # ê¸‰ë³€ ì—¬ë¶€ (Â±50 ì´ìƒ)
        is_sudden = 1 if abs(actual_change) >= 50 else 0
        
        # ë°©í–¥ (ê¸‰ë³€ì¼ ë•Œë§Œ ì˜ë¯¸ ìˆìŒ)
        direction = 1 if actual_change >= 0 else 0  # 1: ê¸‰ë“±, 0: ê¸‰ë½
        
        # ì¼ë°˜ ë¶„ë¥˜ (ê¸‰ë³€ ì•„ë‹ ë•Œ)
        if actual_change <= -20:
            normal_class = 0  # ì†Œí­í•˜ë½
        elif actual_change <= 20:
            normal_class = 1  # ì •ì²´
        else:
            normal_class = 2  # ì†Œí­ìƒìŠ¹
        
        # Feature
        features = {
            'target_mean': np.mean(seq_target),
            'target_std': np.std(seq_target),
            'target_max': np.max(seq_target),
            'target_min': np.min(seq_target),
            'target_last_value': current_value,
            'target_slope': slope,
            # ê¸‰ë³€ ê°ì§€ìš© ì¶”ê°€ Feature
            'target_range': np.max(seq_target) - np.min(seq_target),
            'target_momentum': seq_target[-1] - seq_target[-5] if len(seq_target) >= 5 else 0,
            'target_accel': (seq_target[-1] - seq_target[-5]) - (seq_target[-5] - seq_target[-10]) if len(seq_target) >= 10 else 0,
            'target_volatility': np.std(np.diff(seq_target)),
        }
        
        for group_name, cols in FEATURE_COLS_V8.items():
            for col in cols:
                if col not in col_arrays: continue
                col_seq = col_arrays[col][i-30:i]
                if group_name == 'maxcapa':
                    features[f'{col}_last_value'] = col_seq[-1]
                elif group_name in ['cmd', 'storage', 'fs_storage', 'hub']:
                    features[f'{col}_mean'] = np.mean(col_seq)
                    features[f'{col}_std'] = np.std(col_seq)
                    features[f'{col}_max'] = np.max(col_seq)
                    features[f'{col}_min'] = np.min(col_seq)
                    features[f'{col}_last_value'] = col_seq[-1]
                    features[f'{col}_slope'] = np.polyfit(x_range, col_seq, 1)[0]
                else:
                    features[f'{col}_mean'] = np.mean(col_seq)
                    features[f'{col}_last_value'] = col_seq[-1]
                    features[f'{col}_slope'] = np.polyfit(x_range, col_seq, 1)[0]
        
        if 'CD_M163FSTORAGEUTIL' in col_arrays:
            util_last = col_arrays['CD_M163FSTORAGEUTIL'][i-1]
            features['storage_util_high'] = 1 if util_last >= 7 else 0
            features['storage_util_critical'] = 1 if util_last >= 10 else 0
        
        if 'HUBROOMTOTAL' in col_arrays:
            hub_last = col_arrays['HUBROOMTOTAL'][i-1]
            features['hub_critical'] = 1 if hub_last < 590 else 0
            features['hub_high'] = 1 if hub_last < 610 else 0
        
        inflow = sum(col_arrays[c][i-1] for c in FEATURE_COLS_V8['inflow'] if c in col_arrays)
        outflow = sum(col_arrays[c][i-1] for c in FEATURE_COLS_V8['outflow'] if c in col_arrays)
        features['net_flow'] = inflow - outflow
        
        features_list.append(features)
        labels_sudden.append(is_sudden)
        labels_direction.append(direction)
        labels_normal.append(normal_class)
        labels_change.append(actual_change)
        current_values.append(current_value)
        slopes.append(slope)
    
    X_all = pd.DataFrame(features_list)
    y_sudden = np.array(labels_sudden)
    y_direction = np.array(labels_direction)
    y_normal = np.array(labels_normal)
    y_change = np.array(labels_change)
    current_vals = np.array(current_values)
    slope_arr = np.array(slopes)
    
    del features_list, col_arrays
    gc.collect()
    
    print(f"\nâœ… Feature: {len(X_all)}ê°œ ìƒ˜í”Œ, {len(X_all.columns)}ê°œ Feature")
    
    # ë¶„í¬ í™•ì¸
    sudden_count = y_sudden.sum()
    print(f"\nğŸ“Š ê¸‰ë³€ ë¶„í¬:")
    print(f"  ê¸‰ë³€: {sudden_count}ê°œ ({sudden_count/len(y_sudden)*100:.1f}%)")
    print(f"  ì¼ë°˜: {len(y_sudden) - sudden_count}ê°œ ({(len(y_sudden)-sudden_count)/len(y_sudden)*100:.1f}%)")
    
    # ========== [1ë‹¨ê³„] ê¸‰ë³€ íƒì§€ ëª¨ë¸ ==========
    print("\n" + "="*60)
    print("[1ë‹¨ê³„] ê¸‰ë³€ íƒì§€ ëª¨ë¸ í•™ìŠµ (ì´ì§„ ë¶„ë¥˜)")
    print("="*60)
    
    # ê°€ì¤‘ì¹˜: ê¸‰ë³€ 50ë°°
    sudden_weights = np.ones(len(y_sudden))
    sudden_weights[y_sudden == 1] = 50.0
    
    print(f"  ê°€ì¤‘ì¹˜: ê¸‰ë³€ 50ë°°")
    
    X_tr, X_val, y_tr, y_val, w_tr, _ = train_test_split(
        X_all, y_sudden, sudden_weights, test_size=0.2, random_state=42, stratify=y_sudden
    )
    
    sudden_model = xgb.XGBClassifier(
        n_estimators=300, max_depth=8, learning_rate=0.03,
        subsample=0.85, colsample_bytree=0.85,
        random_state=42, tree_method='hist', n_jobs=-1,
        scale_pos_weight=1  # sample_weightë¡œ ì²˜ë¦¬
    )
    sudden_model.fit(X_tr, y_tr, sample_weight=w_tr, verbose=False)
    
    y_pred_sudden = sudden_model.predict(X_val)
    acc = accuracy_score(y_val, y_pred_sudden)
    print(f"\n  âœ… ê¸‰ë³€ íƒì§€ ì •í™•ë„: {acc:.1%}")
    
    # ê¸‰ë³€ Recall í™•ì¸
    sudden_mask = y_val == 1
    if sudden_mask.sum() > 0:
        recall = (y_pred_sudden[sudden_mask] == 1).mean()
        print(f"  âœ… ê¸‰ë³€ Recall: {recall:.1%} ({(y_pred_sudden[sudden_mask] == 1).sum()}/{sudden_mask.sum()}ê°œ)")
    
    # ì¼ë°˜ ì¤‘ ì˜¤íƒ
    normal_mask = y_val == 0
    if normal_mask.sum() > 0:
        false_alarm = (y_pred_sudden[normal_mask] == 1).mean()
        print(f"  âš ï¸ ì˜¤íƒë¥  (ì¼ë°˜â†’ê¸‰ë³€): {false_alarm:.1%}")
    
    # ========== [2ë‹¨ê³„-A] ê¸‰ë“± ì „ìš© ëª¨ë¸ ==========
    print("\n" + "="*60)
    print("[2ë‹¨ê³„-A] ê¸‰ë“± ì „ìš© ëª¨ë¸ í•™ìŠµ")
    print("="*60)
    
    # ê¸‰ë“± ë°ì´í„°ë§Œ (change >= 50)
    surge_mask = y_change >= 50
    X_surge = X_all[surge_mask]
    y_surge = y_change[surge_mask]
    
    print(f"  ê¸‰ë“± ë°ì´í„°: {len(X_surge)}ê°œ")
    
    if len(X_surge) >= 30:
        X_tr_s, X_val_s, y_tr_s, y_val_s = train_test_split(
            X_surge, y_surge, test_size=0.2, random_state=42
        )
        
        surge_model = xgb.XGBRegressor(
            n_estimators=300, max_depth=8, learning_rate=0.03,
            subsample=0.85, colsample_bytree=0.85,
            random_state=42, tree_method='hist', n_jobs=-1
        )
        surge_model.fit(X_tr_s, y_tr_s, verbose=False)
        
        mae = mean_absolute_error(y_val_s, surge_model.predict(X_val_s))
        print(f"  âœ… ê¸‰ë“±ëª¨ë¸ MAE: {mae:.2f}")
    else:
        surge_model = None
        print(f"  âš ï¸ ë°ì´í„° ë¶€ì¡±, ê¸°ë³¸ê°’ ì‚¬ìš©")
    
    # ========== [2ë‹¨ê³„-B] ê¸‰ë½ ì „ìš© ëª¨ë¸ ==========
    print("\n" + "="*60)
    print("[2ë‹¨ê³„-B] ê¸‰ë½ ì „ìš© ëª¨ë¸ í•™ìŠµ")
    print("="*60)
    
    # ê¸‰ë½ ë°ì´í„°ë§Œ (change <= -50)
    drop_mask = y_change <= -50
    X_drop = X_all[drop_mask]
    y_drop = y_change[drop_mask]
    
    print(f"  ê¸‰ë½ ë°ì´í„°: {len(X_drop)}ê°œ")
    
    if len(X_drop) >= 30:
        X_tr_d, X_val_d, y_tr_d, y_val_d = train_test_split(
            X_drop, y_drop, test_size=0.2, random_state=42
        )
        
        drop_model = xgb.XGBRegressor(
            n_estimators=300, max_depth=8, learning_rate=0.03,
            subsample=0.85, colsample_bytree=0.85,
            random_state=42, tree_method='hist', n_jobs=-1
        )
        drop_model.fit(X_tr_d, y_tr_d, verbose=False)
        
        mae = mean_absolute_error(y_val_d, drop_model.predict(X_val_d))
        print(f"  âœ… ê¸‰ë½ëª¨ë¸ MAE: {mae:.2f}")
    else:
        drop_model = None
        print(f"  âš ï¸ ë°ì´í„° ë¶€ì¡±, ê¸°ë³¸ê°’ ì‚¬ìš©")
    
    # ========== [2ë‹¨ê³„-C] ì¼ë°˜ 3ë‹¨ê³„ ëª¨ë¸ ==========
    print("\n" + "="*60)
    print("[2ë‹¨ê³„-C] ì¼ë°˜ 3ë‹¨ê³„ ëª¨ë¸ í•™ìŠµ (ì†Œí­í•˜ë½/ì •ì²´/ì†Œí­ìƒìŠ¹)")
    print("="*60)
    
    # ê¸‰ë³€ ì•„ë‹Œ ë°ì´í„°ë§Œ
    normal_data_mask = y_sudden == 0
    X_normal = X_all[normal_data_mask]
    y_normal_cls = y_normal[normal_data_mask]
    y_normal_change = y_change[normal_data_mask]
    
    print(f"  ì¼ë°˜ ë°ì´í„°: {len(X_normal)}ê°œ")
    
    normal_class_names = {0: 'ì†Œí­í•˜ë½', 1: 'ì •ì²´', 2: 'ì†Œí­ìƒìŠ¹'}
    
    print(f"\n  ğŸ“Š ì¼ë°˜ í´ë˜ìŠ¤ ë¶„í¬:")
    for cls in range(3):
        count = (y_normal_cls == cls).sum()
        print(f"    {normal_class_names[cls]}: {count}ê°œ ({count/len(y_normal_cls)*100:.1f}%)")
    
    # ì¼ë°˜ ë¶„ë¥˜ê¸°
    normal_weights = np.ones(len(y_normal_cls))
    normal_weights[y_normal_cls == 0] = 3.0  # ì†Œí­í•˜ë½
    normal_weights[y_normal_cls == 2] = 3.0  # ì†Œí­ìƒìŠ¹
    
    X_tr_n, X_val_n, y_tr_n, y_val_n, w_tr_n, _ = train_test_split(
        X_normal, y_normal_cls, normal_weights, test_size=0.2, random_state=42, stratify=y_normal_cls
    )
    
    normal_clf = xgb.XGBClassifier(
        n_estimators=300, max_depth=8, learning_rate=0.03,
        subsample=0.85, colsample_bytree=0.85,
        random_state=42, tree_method='hist', n_jobs=-1,
        objective='multi:softprob', num_class=3
    )
    normal_clf.fit(X_tr_n, y_tr_n, sample_weight=w_tr_n, verbose=False)
    
    acc = accuracy_score(y_val_n, normal_clf.predict(X_val_n))
    print(f"\n  âœ… ì¼ë°˜ ë¶„ë¥˜ ì •í™•ë„: {acc:.1%}")
    
    # ì¼ë°˜ ìˆ˜ì¹˜ ëª¨ë¸ 3ê°œ
    normal_regressors = {}
    for cls in range(3):
        cls_mask = y_normal_cls == cls
        X_cls = X_normal[cls_mask]
        y_cls = y_normal_change[cls_mask]
        
        print(f"\n  [{normal_class_names[cls]}] {len(X_cls)}ê°œ")
        
        if len(X_cls) >= 50:
            X_tr_c, X_val_c, y_tr_c, y_val_c = train_test_split(
                X_cls, y_cls, test_size=0.2, random_state=42
            )
            
            reg_model = xgb.XGBRegressor(
                n_estimators=300, max_depth=8, learning_rate=0.03,
                subsample=0.85, colsample_bytree=0.85,
                random_state=42, tree_method='hist', n_jobs=-1
            )
            reg_model.fit(X_tr_c, y_tr_c, verbose=False)
            
            mae = mean_absolute_error(y_val_c, reg_model.predict(X_val_c))
            print(f"    âœ… MAE: {mae:.2f}")
            normal_regressors[cls] = reg_model
        else:
            normal_regressors[cls] = None
    
    # ========== ì €ì¥ ==========
    print("\n" + "="*60)
    print("ëª¨ë¸ ì €ì¥")
    print("="*60)
    
    model_dict = {
        # 1ë‹¨ê³„: ê¸‰ë³€ íƒì§€
        'sudden_detector': sudden_model,
        # 2ë‹¨ê³„-A: ê¸‰ë“± ì „ìš©
        'surge_model': surge_model,
        # 2ë‹¨ê³„-B: ê¸‰ë½ ì „ìš©
        'drop_model': drop_model,
        # 2ë‹¨ê³„-C: ì¼ë°˜ ë¶„ë¥˜ + ìˆ˜ì¹˜
        'normal_classifier': normal_clf,
        'normal_regressors': normal_regressors,
        # ë©”íƒ€
        'feature_names': X_all.columns.tolist(),
        'normal_class_names': normal_class_names,
        'version': 'V8_2stage_separate'
    }
    
    with open('xgboost_2stage_V8.pkl', 'wb') as f:
        pickle.dump(model_dict, f)
    
    print(f"âœ… ì €ì¥: xgboost_2stage_V8.pkl")
    print("\nğŸ‰ V8 2ë‹¨ê³„ ë¶„ë¦¬ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

if __name__ == '__main__':
    train_v8_2stage()