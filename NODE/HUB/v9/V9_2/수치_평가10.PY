import numpy as np
import pandas as pd
import pickle
from datetime import datetime, timedelta

def evaluate_v9_regression_balanced():
    """
    ğŸ¯ V9-ìˆ˜ì¹˜í˜• ë°¸ëŸ°ìŠ¤: MIN/MAX/AVG êµ¬ê°„ ì˜ˆì¸¡ í‰ê°€
    - ìµœì¢…_ì˜ˆì¸¡: ë³µí•© ë¡œì§ìœ¼ë¡œ MIN/MAX/AVG ì¤‘ ì„ íƒ
    """
    print("="*80)
    print("ğŸ¯ V9-ìˆ˜ì¹˜í˜• ë°¸ëŸ°ìŠ¤ í‰ê°€ (ìµœì¢…_ì˜ˆì¸¡ í¬í•¨)")
    print("   âš–ï¸ ë³µí•© ë¡œì§: ì˜ˆì¸¡ê°’ + ì¶”ì„¸ + HUB ì¢…í•© íŒë‹¨")
    print("="*80)
    
    # ========== V8 ê¸°ì¡´ 14ê°œ ==========
    FEATURE_COLS_V8 = {
        'storage': ['M16A_3F_STORAGE_UTIL'],
        'fs_storage': ['CD_M163FSTORAGEUSE', 'CD_M163FSTORAGETOTAL', 'CD_M163FSTORAGEUTIL'],
        'hub': ['HUBROOMTOTAL'],
        'cmd': ['M16A_3F_CMD', 'M16A_6F_TO_HUB_CMD'],
        'inflow': ['M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2', 'M14A_3F_TO_HUB_JOB2'],
        'outflow': ['M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB', 'M16A_3F_TO_M14A_3F_JOB'],
        'maxcapa': ['M16A_6F_LFT_MAXCAPA', 'M16A_2F_LFT_MAXCAPA'],
    }
    
    # ========== V9 ì‹ ê·œ 20ê°œ ==========
    V9_NEW_COLS = [
        'M16A_LFTTOALL',
        'M16HUB_M16TOM14_CREATED',
        'M16A_ALLTONORTHCNV',
        'M16A_NORTHCNVTOALL',
        'M16A_M16ATOM14A',
        'M14_TOTALCNVCURRENTQCNT',
        'M16HUB_M16TOM14B_CREATED',
        'M16HUB_M14TOM16_CREATED',
        'M16A_CURRENTQCNT',
        'M16A_ALLTOSOUTHCNV',
        'M14_ALLTOSOUTHCNV',
        'M14_ALLTONORTHCNV',
        'M14_CURRENTQCREATED',
        'M14_RETURNTOM16',
        'M14B_LFTTOALL',
        'M14B_M14BTOM16A',
        'M16B_CURRENTQCREATED',
        'M16_SENDTOM14',
        'M16HUB_M14TOM16_MESCNT',
        'M16HUB_MESCURRENTQCNT',
    ]
    
    TARGET_COL = 'CURRENT_M16A_3F_JOB_2'
    
    def determine_final_prediction(pred_min, pred_max, pred_avg, 
                                    pred_min_change, pred_max_change, pred_avg_change):
        """
        ìµœì¢… ì˜ˆì¸¡ê°’ ê²°ì • (ë°ì´í„° ê²€ì¦ ê¸°ë°˜)
        
        ê²€ì¦ ê²°ê³¼:
        - ê¸‰ë“± ì‹œ: MAX ì˜¤ì°¨ 19.7 (ìµœì )
        - ê¸‰ë½ ì‹œ: MIN ì˜¤ì°¨ 15.0 (ìµœì )
        - í‰ìƒì‹œ: AVG ì˜¤ì°¨ 11.6 (ìµœì )
        
        ê¸°ì¤€:
        - ì˜ˆì¸¡_MINë³€í™”ëŸ‰ <= -40 â†’ ê¸‰ë½ ì§•í›„ â†’ MIN ì‚¬ìš©
        - ì˜ˆì¸¡_MAXë³€í™”ëŸ‰ >= 40 â†’ ê¸‰ë“± ì§•í›„ â†’ MAX ì‚¬ìš©
        - ê·¸ ì™¸ â†’ í‰ìƒì‹œ â†’ AVG ì‚¬ìš©
        
        Returns:
            (ìµœì¢…_ì˜ˆì¸¡ê°’, ìµœì¢…_ë³€í™”ëŸ‰, íŒë‹¨ê·¼ê±°, ì‚¬ìš©ëª¨ë¸)
        """
        # ê¸‰ë½ ì§•í›„: MINë³€í™”ëŸ‰ <= -40
        is_drop_signal = pred_min_change <= -40
        
        # ê¸‰ë“± ì§•í›„: MAXë³€í™”ëŸ‰ >= 40
        is_surge_signal = pred_max_change >= 40
        
        # ë‘˜ ë‹¤ ìˆìœ¼ë©´ ë” ê°•í•œ ìª½ ì„ íƒ
        if is_drop_signal and is_surge_signal:
            # ì ˆëŒ€ê°’ ë¹„êµ
            if abs(pred_min_change) > abs(pred_max_change):
                return pred_min, pred_min_change, f"ê¸‰ë½ìš°ì„¸(MIN:{pred_min_change:.0f},MAX:{pred_max_change:.0f})", "MIN"
            else:
                return pred_max, pred_max_change, f"ê¸‰ë“±ìš°ì„¸(MIN:{pred_min_change:.0f},MAX:{pred_max_change:.0f})", "MAX"
        
        # ê¸‰ë½ ì§•í›„ë§Œ
        elif is_drop_signal:
            return pred_min, pred_min_change, f"ê¸‰ë½ì§•í›„(MINë³€í™”ëŸ‰:{pred_min_change:.0f})", "MIN"
        
        # ê¸‰ë“± ì§•í›„ë§Œ
        elif is_surge_signal:
            return pred_max, pred_max_change, f"ê¸‰ë“±ì§•í›„(MAXë³€í™”ëŸ‰:{pred_max_change:.0f})", "MAX"
        
        # í‰ìƒì‹œ
        else:
            return pred_avg, pred_avg_change, "í‰ìƒì‹œ", "AVG"
    
    # ========== ëª¨ë¸ ë¡œë“œ ==========
    try:
        with open('xgboost_ìˆ˜ì¹˜í˜•_V9_ë°¸ëŸ°ìŠ¤.pkl', 'rb') as f:
            model_dict = pickle.load(f)
        models = model_dict['models']
        print("âœ… ëª¨ë¸ ë¡œë“œ: xgboost_ìˆ˜ì¹˜í˜•_V9_ë°¸ëŸ°ìŠ¤.pkl")
        
        if 'quantile_settings' in model_dict:
            qs = model_dict['quantile_settings']
            print(f"   Quantile ì„¤ì •: MIN={qs['MIN']}, MAX={qs['MAX']}, AVG={qs['AVG']}")
    except Exception as e:
        print(f"âŒ ë°¸ëŸ°ìŠ¤ ëª¨ë¸ ì—†ìŒ: {e}")
        print("   â†’ ê¸°ì¡´ ëª¨ë¸ ì‹œë„...")
        try:
            with open('xgboost_ìˆ˜ì¹˜í˜•_V9.pkl', 'rb') as f:
                model_dict = pickle.load(f)
            models = model_dict['models']
            print("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: xgboost_ìˆ˜ì¹˜í˜•_V9.pkl")
        except:
            print("âŒ ëª¨ë¸ ì—†ìŒ")
            return None
    
    # ========== ë°ì´í„° ë¡œë“œ ==========
    try:
        df = pd.read_csv('test_data.csv', on_bad_lines='skip', encoding='utf-8', low_memory=False)
    except:
        try:
            df = pd.read_csv('test_data.csv', on_bad_lines='skip', encoding='cp949', low_memory=False)
        except:
            df = pd.read_csv('test_data.csv', on_bad_lines='skip', encoding='euc-kr', low_memory=False)
    
    df[TARGET_COL] = pd.to_numeric(df[TARGET_COL], errors='coerce')
    df = df.dropna(subset=[TARGET_COL])
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ í–‰")
    
    available_cols = set(df.columns)
    
    # ì‹œê°„ ì²˜ë¦¬
    if 'STAT_DT' in df.columns:
        try:
            df['STAT_DT'] = pd.to_datetime(df['STAT_DT'].astype(str), format='%Y%m%d%H%M')
        except:
            base_time = datetime(2024, 1, 1, 0, 0)
            df['STAT_DT'] = [base_time + timedelta(minutes=i) for i in range(len(df))]
    
    results = []
    
    print("\ní‰ê°€ ì‹œì‘...")
    
    total_samples = len(df) - 40
    
    for idx, i in enumerate(range(30, len(df) - 10)):
        if idx % 500 == 0:
            print(f"  ì§„í–‰: {idx}/{total_samples} ({idx/total_samples*100:.1f}%)")
        
        seq_data = df.iloc[i-30:i].copy()
        seq_target = seq_data[TARGET_COL].values
        
        current_time = seq_data['STAT_DT'].iloc[-1]
        current_value = seq_target[-1]
        prediction_time = current_time + timedelta(minutes=10)
        
        # ì‹¤ì œ 10ë¶„ í›„
        future_10min = df[TARGET_COL].iloc[i:i+10].values
        actual_value = df[TARGET_COL].iloc[i+9]
        actual_min = np.min(future_10min)
        actual_max = np.max(future_10min)
        actual_change = actual_value - current_value
        
        # ì¶”ì„¸ ê³„ì‚°
        target_slope = np.polyfit(np.arange(30), seq_target, 1)[0]
        
        # HUB ê°’
        hub_value = 999
        if 'HUBROOMTOTAL' in available_cols:
            hub_value = seq_data['HUBROOMTOTAL'].iloc[-1]
        
        # ========== Feature ìƒì„± ==========
        features = {
            'target_mean': np.mean(seq_target),
            'target_std': np.std(seq_target),
            'target_max': np.max(seq_target),
            'target_min': np.min(seq_target),
            'target_last_value': seq_target[-1],
            'target_slope': target_slope,
        }
        
        # V8 ê¸°ì¡´ ì»¬ëŸ¼
        for group_name, cols in FEATURE_COLS_V8.items():
            for col in cols:
                if col not in available_cols:
                    continue
                
                col_seq = seq_data[col].values
                
                if group_name == 'maxcapa':
                    features[f'{col}_last_value'] = col_seq[-1]
                elif group_name in ['cmd', 'storage', 'fs_storage', 'hub']:
                    features[f'{col}_mean'] = np.mean(col_seq)
                    features[f'{col}_std'] = np.std(col_seq)
                    features[f'{col}_max'] = np.max(col_seq)
                    features[f'{col}_min'] = np.min(col_seq)
                    features[f'{col}_last_value'] = col_seq[-1]
                    features[f'{col}_slope'] = np.polyfit(np.arange(30), col_seq, 1)[0]
                else:
                    features[f'{col}_mean'] = np.mean(col_seq)
                    features[f'{col}_last_value'] = col_seq[-1]
                    features[f'{col}_slope'] = np.polyfit(np.arange(30), col_seq, 1)[0]
        
        # V9 ì‹ ê·œ ì»¬ëŸ¼
        for col in V9_NEW_COLS:
            if col not in available_cols:
                continue
            
            col_seq = seq_data[col].values
            features[f'{col}_mean'] = np.mean(col_seq)
            features[f'{col}_std'] = np.std(col_seq)
            features[f'{col}_max'] = np.max(col_seq)
            features[f'{col}_min'] = np.min(col_seq)
            features[f'{col}_last_value'] = col_seq[-1]
            features[f'{col}_slope'] = np.polyfit(np.arange(30), col_seq, 1)[0]
        
        # íŠ¹ìˆ˜ Feature
        if all(col in available_cols for col in ['CD_M163FSTORAGEUSE', 'CD_M163FSTORAGETOTAL', 'CD_M163FSTORAGEUTIL']):
            use_seq = seq_data['CD_M163FSTORAGEUSE'].values
            total_seq = seq_data['CD_M163FSTORAGETOTAL'].values
            util_seq = seq_data['CD_M163FSTORAGEUTIL'].values
            features['storage_use_rate'] = (use_seq[-1] - use_seq[0]) / 30
            features['storage_remaining'] = total_seq[-1] - use_seq[-1]
            features['storage_util_last'] = util_seq[-1]
            features['storage_util_high'] = 1 if util_seq[-1] >= 7 else 0
            features['storage_util_critical'] = 1 if util_seq[-1] >= 10 else 0
        
        if 'HUBROOMTOTAL' in available_cols:
            hub_seq = seq_data['HUBROOMTOTAL'].values
            hub_last = hub_seq[-1]
            features['hub_critical'] = 1 if hub_last < 590 else 0
            features['hub_high'] = 1 if hub_last < 610 else 0
            features['hub_warning'] = 1 if hub_last < 620 else 0
            features['hub_decrease_rate'] = (hub_seq[0] - hub_last) / 30
        
        # ë³µí•© Feature
        inflow_sum = sum(df[col].iloc[i-1] for col in FEATURE_COLS_V8['inflow'] if col in available_cols)
        outflow_sum = sum(df[col].iloc[i-1] for col in FEATURE_COLS_V8['outflow'] if col in available_cols)
        features['net_flow'] = inflow_sum - outflow_sum
        
        cmd_sum = sum(df[col].iloc[i-1] for col in FEATURE_COLS_V8['cmd'] if col in available_cols)
        features['total_cmd'] = cmd_sum
        features['total_cmd_low'] = 1 if cmd_sum < 220 else 0
        features['total_cmd_very_low'] = 1 if cmd_sum < 200 else 0
        
        features['surge_risk_score'] = (
            features.get('hub_high', 0) * 3 +
            features.get('storage_util_critical', 0) * 2 +
            features.get('total_cmd_low', 0) * 1 +
            features.get('storage_util_high', 0) * 1
        )
        
        # V9 Boolean
        if 'M16A_LFTTOALL' in available_cols:
            lft = df['M16A_LFTTOALL'].iloc[i-1]
            features['lfttoall_high'] = 1 if lft >= 100 else 0
            features['lfttoall_critical'] = 1 if lft >= 150 else 0
        
        if 'M16HUB_M16TOM14_CREATED' in available_cols:
            m16tom14 = df['M16HUB_M16TOM14_CREATED'].iloc[i-1]
            features['m16tom14_high'] = 1 if m16tom14 >= 50 else 0
            features['m16tom14_critical'] = 1 if m16tom14 >= 80 else 0
        
        if 'M16HUB_M14TOM16_CREATED' in available_cols:
            m14tom16 = df['M16HUB_M14TOM16_CREATED'].iloc[i-1]
            features['m14tom16_high'] = 1 if m14tom16 >= 50 else 0
            features['m14tom16_critical'] = 1 if m14tom16 >= 80 else 0
        
        X_pred = pd.DataFrame([features])
        
        # ========== ì˜ˆì¸¡ ==========
        pred_change_min = models[0].predict(X_pred)[0]
        pred_change_max = models[1].predict(X_pred)[0]
        pred_change_avg = models[2].predict(X_pred)[0]
        
        pred_min = current_value + pred_change_min
        pred_max = current_value + pred_change_max
        pred_avg = current_value + pred_change_avg
        
        # ========== ìµœì¢… ì˜ˆì¸¡ ê²°ì • ==========
        final_pred, final_change, reason, used_model = determine_final_prediction(
            pred_min, pred_max, pred_avg,
            pred_change_min, pred_change_max, pred_change_avg
        )
        
        # ë²”ìœ„ ë‚´ í¬í•¨ ì—¬ë¶€
        in_range = (pred_min <= actual_max and pred_max >= actual_min)
        
        # ë²”ìœ„í­
        pred_width = pred_change_max - pred_change_min
        actual_width = actual_max - actual_min
        
        # ê¸‰ë“±/ê¸‰ë½ íŒì •
        is_surge = actual_change >= 50
        is_drop = actual_change <= -50
        
        # ê°ì§€ ì—¬ë¶€
        surge_detected = pred_change_max >= 40 if is_surge else None
        drop_detected = pred_change_min <= -40 if is_drop else None
        
        # ìµœì¢…ì˜ˆì¸¡ ì •í™•ë„ (ì˜¤ì°¨ ë²”ìœ„ ë‚´)
        final_error = abs(actual_value - final_pred)
        final_accurate_10 = final_error <= 10
        final_accurate_20 = final_error <= 20
        final_accurate_30 = final_error <= 30
        
        results.append({
            'í˜„ì¬ì‹œê°„': current_time.strftime('%Y-%m-%d %H:%M'),
            'ì˜ˆì¸¡ì‹œì ': prediction_time.strftime('%Y-%m-%d %H:%M'),
            'í˜„ì¬ê°’': round(current_value, 2),
            'ì‹¤ì œê°’': round(actual_value, 2),
            'ì‹¤ì œ_ë³€í™”ëŸ‰': round(actual_change, 2),
            'ì˜ˆì¸¡_MIN': round(pred_min, 2),
            'ì˜ˆì¸¡_MAX': round(pred_max, 2),
            'ì˜ˆì¸¡_AVG': round(pred_avg, 2),
            'ìµœì¢…_ì˜ˆì¸¡': round(final_pred, 2),
            'ìµœì¢…_ë³€í™”ëŸ‰': round(final_change, 2),
            'ì‚¬ìš©ëª¨ë¸': used_model,
            'íŒë‹¨ê·¼ê±°': reason,
            'ìµœì¢…_ì˜¤ì°¨': round(final_error, 2),
            'ì˜¤ì°¨â‰¤10': 'âœ…' if final_accurate_10 else '',
            'ì˜¤ì°¨â‰¤20': 'âœ…' if final_accurate_20 else '',
            'ì˜¤ì°¨â‰¤30': 'âœ…' if final_accurate_30 else '',
            'ë²”ìœ„ì ì¤‘': 'âœ…' if in_range else 'âŒ',
            'ì˜ˆì¸¡ë²”ìœ„í­': round(pred_width, 2),
            'ê¸‰ë“±ì—¬ë¶€': 'ğŸ“ˆ' if is_surge else '',
            'ê¸‰ë½ì—¬ë¶€': 'ğŸ“‰' if is_drop else '',
            'ê¸‰ë“±ê°ì§€': 'âœ…' if surge_detected == True else ('âŒ' if surge_detected == False else ''),
            'ê¸‰ë½ê°ì§€': 'âœ…' if drop_detected == True else ('âŒ' if drop_detected == False else ''),
            'ì¶”ì„¸': round(target_slope, 4),
            'HUBROOMTOTAL': round(hub_value, 0),
        })
    
    results_df = pd.DataFrame(results)
    
    output_file = 'ìˆ˜ì¹˜í˜•_V9_ìµœì¢…ì˜ˆì¸¡_í‰ê°€ê²°ê³¼.csv'
    results_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_file}")
    
    # ========== í†µê³„ ==========
    print("\n" + "="*80)
    print("ğŸ“Š í‰ê°€ í†µê³„ (V9-ìˆ˜ì¹˜í˜• ìµœì¢…ì˜ˆì¸¡)")
    print("="*80)
    
    print(f"ì´ ì˜ˆì¸¡: {len(results_df)}ê°œ")
    
    # ========== ìµœì¢…ì˜ˆì¸¡ ì •í™•ë„ (í•µì‹¬!) ==========
    print(f"\n" + "="*60)
    print(f"ğŸ¯ ìµœì¢…_ì˜ˆì¸¡ ì •í™•ë„ (í•µì‹¬ ì§€í‘œ)")
    print("="*60)
    
    acc_10 = (results_df['ì˜¤ì°¨â‰¤10'] == 'âœ…').sum()
    acc_20 = (results_df['ì˜¤ì°¨â‰¤20'] == 'âœ…').sum()
    acc_30 = (results_df['ì˜¤ì°¨â‰¤30'] == 'âœ…').sum()
    
    print(f"  ì˜¤ì°¨ â‰¤ 10: {acc_10}/{len(results_df)}ê°œ ({acc_10/len(results_df)*100:.1f}%)")
    print(f"  ì˜¤ì°¨ â‰¤ 20: {acc_20}/{len(results_df)}ê°œ ({acc_20/len(results_df)*100:.1f}%)")
    print(f"  ì˜¤ì°¨ â‰¤ 30: {acc_30}/{len(results_df)}ê°œ ({acc_30/len(results_df)*100:.1f}%)")
    
    avg_error = results_df['ìµœì¢…_ì˜¤ì°¨'].mean()
    median_error = results_df['ìµœì¢…_ì˜¤ì°¨'].median()
    print(f"\n  í‰ê·  ì˜¤ì°¨: {avg_error:.2f}")
    print(f"  ì¤‘ì•™ê°’ ì˜¤ì°¨: {median_error:.2f}")
    
    # ì‚¬ìš© ëª¨ë¸ ë¶„í¬
    print(f"\nğŸ“Š ì‚¬ìš© ëª¨ë¸ ë¶„í¬:")
    model_counts = results_df['ì‚¬ìš©ëª¨ë¸'].value_counts()
    for model, count in model_counts.items():
        print(f"  {model}: {count}ê°œ ({count/len(results_df)*100:.1f}%)")
    
    # ëª¨ë¸ë³„ ì •í™•ë„
    print(f"\nğŸ“Š ëª¨ë¸ë³„ ì •í™•ë„ (ì˜¤ì°¨ â‰¤ 20):")
    for model in ['MIN', 'MAX', 'AVG']:
        subset = results_df[results_df['ì‚¬ìš©ëª¨ë¸'] == model]
        if len(subset) > 0:
            acc = (subset['ì˜¤ì°¨â‰¤20'] == 'âœ…').sum() / len(subset) * 100
            avg_err = subset['ìµœì¢…_ì˜¤ì°¨'].mean()
            print(f"  {model}: {acc:.1f}% (í‰ê· ì˜¤ì°¨: {avg_err:.1f})")
    
    # ë²”ìœ„ ì ì¤‘ë¥ 
    in_range_count = (results_df['ë²”ìœ„ì ì¤‘'] == 'âœ…').sum()
    print(f"\nğŸ¯ ë²”ìœ„ ì ì¤‘ë¥ : {in_range_count}/{len(results_df)}ê°œ ({in_range_count/len(results_df)*100:.1f}%)")
    
    # ========== ê¸‰ë“±/ê¸‰ë½ ê°ì§€ ==========
    print(f"\n" + "="*60)
    print(f"ğŸ“ˆğŸ“‰ ê¸‰ë“±/ê¸‰ë½ ê°ì§€ ì„±ëŠ¥")
    print("="*60)
    
    # ê¸‰ë“± ê°ì§€
    surge_samples = results_df[results_df['ê¸‰ë“±ì—¬ë¶€'] == 'ğŸ“ˆ']
    if len(surge_samples) > 0:
        surge_detected = (surge_samples['ê¸‰ë“±ê°ì§€'] == 'âœ…').sum()
        print(f"ğŸ“ˆ ê¸‰ë“± (+50 ì´ìƒ): {surge_detected}/{len(surge_samples)}ê°œ ê°ì§€ ({surge_detected/len(surge_samples)*100:.1f}%)")
    
    # ê¸‰ë½ ê°ì§€
    drop_samples = results_df[results_df['ê¸‰ë½ì—¬ë¶€'] == 'ğŸ“‰']
    if len(drop_samples) > 0:
        drop_detected = (drop_samples['ê¸‰ë½ê°ì§€'] == 'âœ…').sum()
        print(f"ğŸ“‰ ê¸‰ë½ (-50 ì´í•˜): {drop_detected}/{len(drop_samples)}ê°œ ê°ì§€ ({drop_detected/len(drop_samples)*100:.1f}%)")
    
    # ========== íŒë‹¨ê·¼ê±°ë³„ ë¶„ì„ ==========
    print(f"\nğŸ“Š íŒë‹¨ê·¼ê±°ë³„ ë¶„í¬:")
    reason_counts = results_df['íŒë‹¨ê·¼ê±°'].value_counts().head(10)
    for reason, count in reason_counts.items():
        print(f"  {reason}: {count}ê°œ")
    
    # ========== ìƒ˜í”Œ ì¶œë ¥ ==========
    print(f"\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„° (ê¸‰ë“±/ê¸‰ë½ ê° 5ê°œ):")
    
    print(f"\n[ê¸‰ë“± ìƒ˜í”Œ]")
    surge_sample = results_df[results_df['ê¸‰ë“±ì—¬ë¶€'] == 'ğŸ“ˆ'].head(5)
    if len(surge_sample) > 0:
        display_cols = ['í˜„ì¬ì‹œê°„', 'ì‹¤ì œ_ë³€í™”ëŸ‰', 'ìµœì¢…_ì˜ˆì¸¡', 'ìµœì¢…_ë³€í™”ëŸ‰', 'ì‚¬ìš©ëª¨ë¸', 'íŒë‹¨ê·¼ê±°', 'ìµœì¢…_ì˜¤ì°¨']
        print(surge_sample[display_cols].to_string(index=False))
    
    print(f"\n[ê¸‰ë½ ìƒ˜í”Œ]")
    drop_sample = results_df[results_df['ê¸‰ë½ì—¬ë¶€'] == 'ğŸ“‰'].head(5)
    if len(drop_sample) > 0:
        display_cols = ['í˜„ì¬ì‹œê°„', 'ì‹¤ì œ_ë³€í™”ëŸ‰', 'ìµœì¢…_ì˜ˆì¸¡', 'ìµœì¢…_ë³€í™”ëŸ‰', 'ì‚¬ìš©ëª¨ë¸', 'íŒë‹¨ê·¼ê±°', 'ìµœì¢…_ì˜¤ì°¨']
        print(drop_sample[display_cols].to_string(index=False))
    
    return results_df

if __name__ == '__main__':
    print("ğŸš€ V9-ìˆ˜ì¹˜í˜• ìµœì¢…ì˜ˆì¸¡ í‰ê°€ ì‹œì‘...\n")
    results = evaluate_v9_regression_balanced()
    
    if results is not None:
        print(f"\nâœ… í‰ê°€ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼: ìˆ˜ì¹˜í˜•_V9_ìµœì¢…ì˜ˆì¸¡_í‰ê°€ê²°ê³¼.csv")