#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ğŸ“Š ê°œì„ ëœ í‰ê°€ ì‹œìŠ¤í…œ - ë§ˆì§€ë§‰ 5ë¶„ ì¶”ì„¸ ì§‘ì¤‘ ë¶„ì„
================================================================================
30ë¶„ ì‹œí€€ìŠ¤ì—ì„œ ë§ˆì§€ë§‰ 5ë¶„ì˜ ìƒìŠ¹/í•˜ë½ ì¶”ì„¸ë¥¼ ì •ë°€ ë¶„ì„í•˜ì—¬ ì í”„ ê°ì§€
================================================================================
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from datetime import datetime, timedelta
import joblib
import h5py
import os
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

# ==============================================================================
# í‰ê°€ í´ë˜ìŠ¤ - 30ë¶„ ì‹œí€€ìŠ¤ ë²„ì „
# ==============================================================================

class V4UltimateEvaluator30min:
    """V4 Ultimate 30ë¶„ ì‹œí€€ìŠ¤ ëª¨ë¸ í‰ê°€ê¸° - 5ë¶„ ì¶”ì„¸ ë¶„ì„ ë²„ì „"""
    
    def __init__(self, model_dir='./checkpoints_ultimate_30min'):
        self.model_dir = model_dir
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        self.seq_len = 30
        
        # V4 í•„ìˆ˜ ì»¬ëŸ¼
        self.v4_essential_cols = [
            'CURRENT_M16A_3F_JOB_2',
            'M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2', 
            'M14A_3F_TO_HUB_JOB2', 'M14B_7F_TO_HUB_JOB2', 'M16B_10F_TO_HUB_JOB',
            'M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB', 'M16A_3F_TO_M14B_7F_JOB', 'M16A_3F_TO_3F_MLUD_JOB',
            'M16A_3F_CMD', 'M16A_6F_TO_HUB_CMD', 'M16A_2F_TO_HUB_CMD',
            'M14A_3F_TO_HUB_CMD', 'M14B_7F_TO_HUB_CMD',
            'M16A_6F_LFT_MAXCAPA', 'M16A_2F_LFT_MAXCAPA',
            'M16A_3F_STORAGE_UTIL',
            'M14_TO_M16_OFS_CUR', 'M16_TO_M14_OFS_CUR'
        ]
        
        self.v4_cols = self.v4_essential_cols.copy()
        self.load_models()
    
    def load_models(self):
        """ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        print("ğŸ”§ 30ë¶„ ì‹œí€€ìŠ¤ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        scaler_dir = os.path.join(self.model_dir, 'scalers')
        if os.path.exists(scaler_dir):
            self.scaler_X = joblib.load(os.path.join(scaler_dir, 'scaler_X.pkl'))
            self.scaler_y = joblib.load(os.path.join(scaler_dir, 'scaler_y.pkl'))
            self.scaler_physics = joblib.load(os.path.join(scaler_dir, 'scaler_physics.pkl'))
            
            scaled_data_path = os.path.join(self.model_dir, 'scaled_data.h5')
            if os.path.exists(scaled_data_path):
                with h5py.File(scaled_data_path, 'r') as f:
                    self.n_features = f.attrs['n_features']
            else:
                print("âš ï¸ scaled_data.h5 ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                self.n_features = len(self.v4_cols) + 2
        else:
            print("âš ï¸ ìŠ¤ì¼€ì¼ëŸ¬ ì—†ìŒ - ë”ë¯¸ ëª¨ë“œë¡œ ì‹¤í–‰")
            self.scaler_X = None
            self.scaler_y = None
            self.scaler_physics = None
            self.n_features = len(self.v4_cols) + 2
        
        self.config = {
            'seq_len': 30,
            'n_features': self.n_features,
            'patch_len': 6
        }
        
        print(f"âœ… 30ë¶„ ì‹œí€€ìŠ¤ ëª¨ë¸ ì„¤ì • ì™„ë£Œ (features: {self.n_features})")
    
    def load_september_data(self, filepath='data/202509.csv'):
        """9ì›” ë°ì´í„° ë¡œë“œ"""
        print(f"\nğŸ“Š {filepath} ë¡œë“œ ì¤‘...")
        
        df = pd.read_csv(filepath)
        print(f"  ì›ë³¸ shape: {df.shape}")
        
        time_col = df.columns[0]
        df['datetime'] = pd.to_datetime(df[time_col].astype(str), format='%Y%m%d%H%M', errors='coerce')
        
        if 'BRIDGE_TIME' in df.columns:
            print("  âœ… BRIDGE_TIME ì»¬ëŸ¼ ë°œê²¬")
            if 'BRIDGE_TIME' not in self.v4_cols:
                self.v4_cols.append('BRIDGE_TIME')
        else:
            print("  â„¹ï¸ BRIDGE_TIME ì—†ìŒ - ê¸°ë³¸ê°’ 3.5 ì‚¬ìš©")
            df['BRIDGE_TIME'] = 3.5
            if 'BRIDGE_TIME' not in self.v4_cols:
                self.v4_cols.append('BRIDGE_TIME')
        
        available_cols = ['datetime']
        missing_cols = []
        
        for col in self.v4_cols:
            if col in df.columns:
                available_cols.append(col)
            else:
                missing_cols.append(col)
                if col == self.target_col:
                    print(f"âŒ íƒ€ê²Ÿ ì»¬ëŸ¼ {self.target_col} ì—†ìŒ!")
                    raise ValueError(f"íƒ€ê²Ÿ ì»¬ëŸ¼ {self.target_col}ì´ ì—†ìŠµë‹ˆë‹¤!")
                else:
                    df[col] = 0
        
        df = df[available_cols]
        
        if missing_cols:
            print(f"âš ï¸ ëˆ„ë½ ì»¬ëŸ¼ {len(missing_cols)}ê°œ: {missing_cols[:3]}...")
        
        df = df.fillna(method='ffill').fillna(0)
        df = self.add_consecutive_patterns(df)
        
        print(f"âœ… ìµœì¢… shape: {df.shape}")
        print(f"  ì‚¬ìš© ì»¬ëŸ¼: {len(available_cols)-1}ê°œ")
        return df
    
    def add_consecutive_patterns(self, df):
        """ì—°ì† 300+ íŒ¨í„´ ì¶”ê°€"""
        consecutive_counts = []
        consecutive_probs = []
        
        probability_map = {
            0: 0.003, 5: 0.43, 10: 0.42, 15: 0.66, 20: 0.987,
            25: 0.99, 30: 0.99
        }
        
        for i in range(len(df)):
            if i < self.seq_len:
                count = 0
                prob = 0
            else:
                window = df[self.target_col].iloc[i-self.seq_len:i].values
                count = sum(1 for v in window if v >= 300)
                prob = probability_map.get(count, probability_map.get(min(probability_map.keys(), key=lambda x: abs(x-count))))
            
            consecutive_counts.append(count)
            consecutive_probs.append(prob)
        
        df['consecutive_300_count'] = consecutive_counts
        df['consecutive_300_prob'] = consecutive_probs
        
        return df
    
    def create_evaluation_sequences(self, df):
        """í‰ê°€ìš© ì‹œí€€ìŠ¤ ìƒì„± (30ë¶„)"""
        print(f"\nğŸ”„ í‰ê°€ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘ ({self.seq_len}ë¶„)...")
        
        sequences = []
        pred_len = 10
        
        for i in range(len(df) - self.seq_len - pred_len):
            input_data = df.iloc[i:i+self.seq_len]
            actual_data = df.iloc[i+self.seq_len+pred_len-1]
            physics_features = self.create_physics_features(df, i+self.seq_len-1)
            
            sequence = {
                'index': i,
                'input_start_time': input_data['datetime'].iloc[0],
                'input_end_time': input_data['datetime'].iloc[-1],
                'current_time': input_data['datetime'].iloc[-1],
                'actual_time': actual_data['datetime'],
                'input_data': input_data[self.v4_cols + ['consecutive_300_count', 'consecutive_300_prob']].values,
                'actual_value': actual_data[self.target_col],
                'past_30min_values': input_data[self.target_col].values.tolist(),
                'physics_features': physics_features
            }
            
            sequences.append(sequence)
        
        print(f"âœ… ì´ {len(sequences)}ê°œ ì‹œí€€ìŠ¤ ìƒì„± (30ë¶„)")
        return sequences
    
    def create_physics_features(self, df, idx):
        """ë¬¼ë¦¬ íŠ¹ì§• ìƒì„± (11ì°¨ì›)"""
        physics = []
        
        physics.append(df[self.target_col].iloc[idx])
        physics.append(100)
        physics.append(90)
        physics.append(df.get('BRIDGE_TIME', pd.Series([3.5])).iloc[idx])
        physics.append(df.get('consecutive_300_count', pd.Series([0])).iloc[idx])
        physics.append(df.get('consecutive_300_prob', pd.Series([0.5])).iloc[idx])
        physics.append(df.get('M16A_3F_STORAGE_UTIL', pd.Series([0])).iloc[idx])
        physics.append(100)
        
        if idx >= 4:
            recent_avg = df[self.target_col].iloc[idx-4:idx+1].mean()
        else:
            recent_avg = df[self.target_col].iloc[idx]
        physics.append(recent_avg)
        
        if idx >= 30:
            first_10_avg = df[self.target_col].iloc[idx-29:idx-19].mean()
            last_10_avg = df[self.target_col].iloc[idx-9:idx+1].mean()
            long_trend = last_10_avg - first_10_avg
            
            first_10_std = df[self.target_col].iloc[idx-29:idx-19].std()
            last_10_std = df[self.target_col].iloc[idx-9:idx+1].std()
            volatility_change = last_10_std / max(first_10_std, 1)
        else:
            long_trend = 0
            volatility_change = 1
            
        physics.append(long_trend)
        physics.append(volatility_change)
        
        return np.array(physics)
    
    def predict_sequence(self, sequence):
        """ë§ˆì§€ë§‰ 5ë¶„ ì¶”ì„¸ ì§‘ì¤‘ ë¶„ì„ ë²„ì „"""
        
        past_values = sequence['past_30min_values']
        physics = sequence['physics_features']
        
        # ê¸°ë³¸ ë°ì´í„°
        recent_3 = past_values[-3:]
        recent_5 = past_values[-5:]
        recent_10 = past_values[-10:]
        first_10 = past_values[:10]
        last_10 = past_values[-10:]
        
        max_val = max(past_values)
        max_recent5 = max(recent_5)
        mean_recent5 = np.mean(recent_5)
        
        # ========================================
        # ğŸ” ë§ˆì§€ë§‰ 5ë¶„ ìƒì„¸ ì¶”ì„¸ ë¶„ì„
        # ========================================
        
        # 1. 5ë¶„ê°„ ê°œë³„ ë³€í™” ë¶„ì„
        changes = []
        for i in range(1, len(recent_5)):
            changes.append(recent_5[i] - recent_5[i-1])
        
        # 2. ìƒìŠ¹/í•˜ë½ íŒ¨í„´
        rising_steps = sum(1 for c in changes if c > 0)
        falling_steps = sum(1 for c in changes if c < 0)
        
        # 3. ë³€í™” ê°•ë„
        total_5min_change = recent_5[-1] - recent_5[0]
        avg_change = np.mean(changes) if changes else 0
        max_up = max(changes) if changes else 0
        max_down = min(changes) if changes else 0
        
        # 4. ê°€ì†ë„ ì²´í¬
        if len(changes) >= 2:
            recent_momentum = changes[-1]
            early_momentum = changes[0]
            acceleration = recent_momentum - early_momentum
        else:
            acceleration = 0
        
        # 5. ìµœê³ ì  ìœ„ì¹˜
        max_position = np.argmax(recent_5)
        is_peaking = max_position >= 3
        
        # 6. ê²©ì°¨ ë¶„ì„
        gap = max_recent5 - mean_recent5
        gap_ratio = gap / max(mean_recent5, 1)
        
        # ========================================
        # ğŸ¯ ì í”„ íŒë‹¨ ë¡œì§
        # ========================================
        
        jump_score = 0
        
        # ì ìˆ˜ ê³„ì‚°
        if 240 <= max_recent5 < 280:
            
            # 1. ìœ„ì¹˜ ì ìˆ˜
            if max_recent5 >= 275:
                jump_score += 40
            elif max_recent5 >= 270:
                jump_score += 35
            elif max_recent5 >= 265:
                jump_score += 30
            elif max_recent5 >= 260:
                jump_score += 25
            elif max_recent5 >= 255:
                jump_score += 20
            elif max_recent5 >= 250:
                jump_score += 15
            elif max_recent5 >= 245:
                jump_score += 10
            else:
                jump_score += 5
            
            # 2. ìƒìŠ¹ ì¶”ì„¸ ì ìˆ˜ (í•µì‹¬!)
            if rising_steps >= 3:
                jump_score += 30
            elif rising_steps >= 2:
                jump_score += 20
            elif rising_steps >= 1:
                jump_score += 10
            
            # 3. 5ë¶„ ì´ ë³€í™”ëŸ‰
            if total_5min_change > 5:
                jump_score += 25
            elif total_5min_change > 3:
                jump_score += 15
            elif total_5min_change > 1:
                jump_score += 10
            elif total_5min_change < -3:
                jump_score -= 10
            
            # 4. ê°€ì†ë„ ì ìˆ˜
            if acceleration > 2:
                jump_score += 20
            elif acceleration > 1:
                jump_score += 10
            elif acceleration < -2:
                jump_score -= 10
            
            # 5. ìµœê³ ì  ìœ„ì¹˜ ì ìˆ˜
            if is_peaking:
                jump_score += 15
            
            # 6. ê²©ì°¨ ì ìˆ˜
            if gap > 25:
                jump_score += 25
            elif gap > 20:
                jump_score += 20
            elif gap > 15:
                jump_score += 15
            elif gap > 10:
                jump_score += 10
            
            # 7. ìµœëŒ€ ìƒìŠ¹í­ ì ìˆ˜
            if max_up > 3:
                jump_score += 15
            elif max_up > 2:
                jump_score += 10
            
            # 8. ë¬¼ë¦¬ íŠ¹ì§•
            if len(physics) > 3 and physics[3] > 4:
                jump_score += 10
        
        # ========================================
        # ì˜ˆì¸¡ ê²°ì •
        # ========================================
        
        # ì í”„ í™•ì • (ì ìˆ˜ 70+)
        if jump_score >= 70:
            if max_recent5 >= 275:
                predicted = mean_recent5 * 1.22
            elif max_recent5 >= 270:
                predicted = mean_recent5 * 1.20
            elif max_recent5 >= 265:
                predicted = mean_recent5 * 1.18
            elif max_recent5 >= 260:
                predicted = mean_recent5 * 1.16
            else:
                predicted = mean_recent5 * 1.14
            return predicted, "Model2"
        
        # ì í”„ ê°€ëŠ¥ (ì ìˆ˜ 50-69)
        elif jump_score >= 50:
            if max_recent5 >= 270:
                predicted = mean_recent5 * 1.15
            elif max_recent5 >= 260:
                predicted = mean_recent5 * 1.12
            else:
                predicted = mean_recent5 * 1.10
            return predicted, "Model2"
        
        # ì í”„ ì˜ì‹¬ (ì ìˆ˜ 30-49)
        elif jump_score >= 30:
            predicted = mean_recent5 * 1.08
            return predicted, "Model2"
        
        # ========================================
        # ì¼ë°˜ ì¼€ì´ìŠ¤
        # ========================================
        
        # ì´ë¯¸ ê·¹ë‹¨ê°’
        if mean_recent5 > 300:
            return mean_recent5 * 1.02, "Model2"
        
        # ëª…í™•í•œ í•˜ë½ ì¶”ì„¸
        if total_5min_change < -5 and falling_steps >= 3:
            return mean_recent5 * 0.95, "Model1"
        
        # ì•ˆì • êµ¬ê°„
        if max_val < 240:
            return mean_recent5 * 0.98, "Model1"
        
        # ê¸°ë³¸
        return mean_recent5 * 0.99, "Model1"
    
    def evaluate_all(self, sequences, output_file='202509_5min_trend.csv'):
        """ì „ì²´ í‰ê°€ ìˆ˜í–‰"""
        print("\nğŸ¯ í‰ê°€ ì‹œì‘ (ë§ˆì§€ë§‰ 5ë¶„ ì¶”ì„¸ ë¶„ì„)...")
        
        results = []
        
        for i, seq in enumerate(sequences):
            if i % 100 == 0:
                print(f"  ì§„í–‰: {i}/{len(sequences)}")
            
            predicted, selected_model = self.predict_sequence(seq)
            
            error = abs(seq['actual_value'] - predicted)
            mae_threshold = 30
            ok_ng = "OK" if error < mae_threshold else "NG"
            
            is_extreme = seq['actual_value'] >= 300
            extreme_detected = predicted >= 300
            
            past_max = max(seq['past_30min_values'])
            past_mean = np.mean(seq['past_30min_values'])
            is_jump = past_max < 280 and seq['actual_value'] >= 300
            jump_detected = past_max < 280 and predicted >= 290
            
            # 5ë¶„ ì¶”ì„¸ ê³„ì‚°
            recent_5 = seq['past_30min_values'][-5:]
            trend_5min = recent_5[-1] - recent_5[0]
            rising_count = sum(1 for i in range(1, len(recent_5)) if recent_5[i] > recent_5[i-1])
            
            result = {
                'current_time': seq['current_time'].strftime('%Y-%m-%d %H:%M'),
                'actual_time': seq['actual_time'].strftime('%Y-%m-%d %H:%M'),
                'input_start_time': seq['input_start_time'].strftime('%Y-%m-%d %H:%M'),
                'input_end_time': seq['input_end_time'].strftime('%Y-%m-%d %H:%M'),
                'actual_value': round(seq['actual_value'], 2),
                'predicted': round(predicted, 2),
                'error': round(error, 2),
                'OK_NG': ok_ng,
                'selected_model': selected_model,
                'is_extreme': is_extreme,
                'extreme_detected': extreme_detected,
                'is_jump': is_jump,
                'jump_detected': jump_detected,
                'past_min': round(min(seq['past_30min_values']), 2),
                'past_max': round(past_max, 2),
                'past_mean': round(past_mean, 2),
                'past_std': round(np.std(seq['past_30min_values']), 2),
                'past_300plus_count': sum(1 for v in seq['past_30min_values'] if v >= 300),
                'long_trend': round(seq['physics_features'][9], 2) if len(seq['physics_features']) > 9 else 0,
                'volatility_change': round(seq['physics_features'][10], 2) if len(seq['physics_features']) > 10 else 1,
                '5min_trend': round(trend_5min, 2),
                '5min_rising_count': rising_count
            }
            
            results.append(result)
        
        df_results = pd.DataFrame(results)
        df_results.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_file}")
        
        self.print_statistics(df_results)
        
        return df_results
    
    def print_statistics(self, df_results):
        """í‰ê°€ í†µê³„ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“ˆ í‰ê°€ í†µê³„ (ë§ˆì§€ë§‰ 5ë¶„ ì¶”ì„¸ ë¶„ì„)")
        print("="*80)
        
        total = len(df_results)
        ok_count = (df_results['OK_NG'] == 'OK').sum()
        accuracy = ok_count / total * 100
        
        print(f"\nğŸ“Š ì „ì²´ ì„±ëŠ¥")
        print(f"  ì´ í‰ê°€: {total}ê°œ")
        print(f"  OK: {ok_count}ê°œ ({accuracy:.1f}%)")
        print(f"  NG: {total-ok_count}ê°œ ({100-accuracy:.1f}%)")
        print(f"  í‰ê·  ì˜¤ì°¨: {df_results['error'].mean():.2f}")
        print(f"  ìµœëŒ€ ì˜¤ì°¨: {df_results['error'].max():.2f}")
        
        model_counts = df_results['selected_model'].value_counts()
        print(f"\nğŸ¤– ëª¨ë¸ë³„ ì‚¬ìš©")
        for model, count in model_counts.items():
            model_data = df_results[df_results['selected_model'] == model]
            model_accuracy = (model_data['OK_NG'] == 'OK').sum() / len(model_data) * 100
            print(f"  {model}: {count}íšŒ ({count/total*100:.1f}%) - ì •í™•ë„: {model_accuracy:.1f}%")
        
        extreme_data = df_results[df_results['is_extreme']]
        if len(extreme_data) > 0:
            extreme_detected = extreme_data['extreme_detected'].sum()
            detection_rate = extreme_detected / len(extreme_data) * 100
            print(f"\nğŸ”¥ ê·¹ë‹¨ê°’ ì„±ëŠ¥")
            print(f"  ê·¹ë‹¨ê°’ ê°œìˆ˜: {len(extreme_data)}ê°œ")
            print(f"  ê°ì§€ìœ¨: {detection_rate:.1f}%")
        
        jump_data = df_results[df_results['is_jump']]
        if len(jump_data) > 0:
            jump_detected = jump_data['jump_detected'].sum()
            jump_detection_rate = jump_detected / len(jump_data) * 100
            print(f"\nğŸš€ ì í”„ ì¼€ì´ìŠ¤ ì„±ëŠ¥ (ì •ìƒâ†’ê·¹ë‹¨)")
            print(f"  ì í”„ ì¼€ì´ìŠ¤: {len(jump_data)}ê°œ")
            print(f"  ê°ì§€ìœ¨: {jump_detection_rate:.1f}%")
            
            missed_jumps = jump_data[~jump_data['jump_detected']]
            if len(missed_jumps) > 0:
                print(f"  ë†“ì¹œ ì¼€ì´ìŠ¤: {len(missed_jumps)}ê°œ")
                print(f"\n  ë†“ì¹œ ì¼€ì´ìŠ¤ ìƒì„¸ (ìµœëŒ€ 5ê°œ):")
                for idx, row in missed_jumps.head(5).iterrows():
                    print(f"    {row['current_time']}: max={row['past_max']:.0f}, "
                          f"mean={row['past_mean']:.0f}, 5min_trend={row['5min_trend']:.1f}, "
                          f"rising={row['5min_rising_count']} â†’ "
                          f"ì‹¤ì œ={row['actual_value']:.0f}, ì˜ˆì¸¡={row['predicted']:.0f}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("="*80)
    print("ğŸš€ ê°œì„ ëœ í‰ê°€ - ë§ˆì§€ë§‰ 5ë¶„ ì¶”ì„¸ ë¶„ì„")
    print("ğŸ’¡ 30ë¶„ ì‹œí€€ìŠ¤ì—ì„œ ë§ˆì§€ë§‰ 5ë¶„ì˜ ìƒìŠ¹/í•˜ë½ íŒ¨í„´ ì§‘ì¤‘ ë¶„ì„")
    print("="*80)
    
    evaluator = V4UltimateEvaluator30min()
    df = evaluator.load_september_data('data/20250801_to_20250831.csv')
    sequences = evaluator.create_evaluation_sequences(df)
    results = evaluator.evaluate_all(sequences)
    
    print("\n" + "="*80)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("ğŸ“Š ê²°ê³¼ íŒŒì¼: 202509_5min_trend.csv")
    print("="*80)

if __name__ == "__main__":
    main()