import numpy as np
import pandas as pd
import xgboost as xgb
import pickle
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

warnings.filterwarnings('ignore')
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def train_and_evaluate_complete():
    """
    í•™ìŠµ(aas.csv) â†’ í‰ê°€(BBB.CSV) ì „ì²´ í”„ë¡œì„¸ìŠ¤
    ì‹œí€€ìŠ¤ ì‹œì‘/ì™„ë£Œ ì‹œê°„ í¬í•¨ (ì»¬ëŸ¼ ìˆœì„œ ì¡°ì •)
    """
    print("="*80)
    print("XGBoost 30ë¶„â†’10ë¶„ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€")
    print("="*80)
    
    # ===== 1. í•™ìŠµ ë‹¨ê³„ (aas.csv) =====
    print("\n[STEP 1] aas.csvë¡œ ëª¨ë¸ í•™ìŠµ")
    print("-"*40)
    
    df_train = pd.read_csv('aas.csv', on_bad_lines='skip')
    
    # Feature ìƒì„± í•¨ìˆ˜
    def create_features(df, start_idx=30):
        features_list = []
        labels = []
        seq_max_list = []
        seq_min_list = []
        indices = []
        
        TARGET_COL = 'CURRENT_M16A_3F_JOB_2'
        
        for i in range(start_idx, len(df) - 10):
            seq_target = df[TARGET_COL].iloc[i-30:i].values
            
            features = {
                'target_mean': np.mean(seq_target),
                'target_std': np.std(seq_target),
                'target_last_5_mean': np.mean(seq_target[-5:]),
                'target_max': np.max(seq_target),
                'target_min': np.min(seq_target),
                'target_slope': np.polyfit(np.arange(30), seq_target, 1)[0],
                'target_last_10_mean': np.mean(seq_target[-10:]),
                'target_first_10_mean': np.mean(seq_target[:10])
            }
            
            features_list.append(features)
            labels.append(df[TARGET_COL].iloc[i:i+10].max())
            seq_max_list.append(np.max(seq_target))
            seq_min_list.append(np.min(seq_target))
            indices.append(i)
        
        return pd.DataFrame(features_list), np.array(labels), seq_max_list, seq_min_list, indices
    
    # í•™ìŠµ ë°ì´í„° ìƒì„±
    X_train, y_train, _, _, _ = create_features(df_train)
    
    # ëª¨ë¸ í•™ìŠµ
    model = xgb.XGBRegressor(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        random_state=42
    )
    
    # í•™ìŠµ/ê²€ì¦ ë¶„í• 
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42
    )
    
    model.fit(X_tr, y_tr)
    
    # í•™ìŠµ ë°ì´í„° í‰ê°€
    y_val_pred = model.predict(X_val)
    train_mae = mean_absolute_error(y_val, y_val_pred)
    train_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
    train_r2 = r2_score(y_val, y_val_pred)
    
    print(f"í•™ìŠµ ë°ì´í„° ì„±ëŠ¥:")
    print(f"  MAE:  {train_mae:.4f}")
    print(f"  RMSE: {train_rmse:.4f}")
    print(f"  RÂ²:   {train_r2:.4f}")
    
    # ëª¨ë¸ ì €ì¥
    with open('xgboost_model_30min_10min.pkl', 'wb') as f:
        pickle.dump(model, f)
    print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
    
    # ===== 2. í‰ê°€ ë‹¨ê³„ (BBB.CSV) =====
    print("\n[STEP 2] BBB.CSVë¡œ ëª¨ë¸ í‰ê°€")
    print("-"*40)
    
    df_test = pd.read_csv('BBB.CSV', on_bad_lines='skip')
    
    # BBB.CSV Feature ìƒì„±
    X_test, y_test, seq_max_list, seq_min_list, indices = create_features(df_test)
    
    # ì˜ˆì¸¡
    y_pred = model.predict(X_test)
    
    # í‰ê°€ ì§€í‘œ
    test_mae = mean_absolute_error(y_test, y_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    test_r2 = r2_score(y_test, y_pred)
    
    print(f"BBB.CSV í‰ê°€ ê²°ê³¼:")
    print(f"  MAE:  {test_mae:.4f}")
    print(f"  RMSE: {test_rmse:.4f}")
    print(f"  RÂ²:   {test_r2:.4f}")
    
    # ===== 3. ìƒì„¸ ë¶„ì„ ê²°ê³¼ ìƒì„± =====
    print("\n[STEP 3] ìƒì„¸ ë¶„ì„ ê²°ê³¼ ìƒì„±")
    print("-"*40)
    
    # STAT_DT ì²˜ë¦¬ (ë‚ ì§œ ì‹œê°„ í˜•ì‹)
    TARGET_COL = 'CURRENT_M16A_3F_JOB_2'
    
    # ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
    if 'STAT_DT' in df_test.columns:
        try:
            # YYYYMMDDHHMM í˜•ì‹ ì‹œë„
            df_test['STAT_DT'] = pd.to_datetime(df_test['STAT_DT'], format='%Y%m%d%H%M')
        except:
            try:
                # YYYY-MM-DD HH:MM:SS í˜•ì‹ ì‹œë„
                df_test['STAT_DT'] = pd.to_datetime(df_test['STAT_DT'])
            except:
                # ì‹¤íŒ¨ì‹œ ì¸ë±ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
                base_date = datetime(2024, 1, 1)
                df_test['STAT_DT'] = [base_date + timedelta(minutes=i) for i in range(len(df_test))]
    else:
        # STAT_DTê°€ ì—†ìœ¼ë©´ ê°€ìƒìœ¼ë¡œ ìƒì„±
        base_date = datetime(2024, 1, 1)
        df_test['STAT_DT'] = [base_date + timedelta(minutes=i) for i in range(len(df_test))]
    
    # ê²°ê³¼ DataFrame ìƒì„±
    results = []
    jump_count = 0
    extreme_count = 0
    extreme_detected = 0
    
    for i, idx in enumerate(indices):
        # ì‹œê°„ ê³„ì‚°
        current_time = df_test['STAT_DT'].iloc[idx]  # ì‹œí€€ìŠ¤ ì™„ë£Œ ì‹œì  (í˜„ì¬)
        seq_start_time = df_test['STAT_DT'].iloc[idx-30]  # ì‹œí€€ìŠ¤ ì‹œì‘ ì‹œì 
        prediction_time = current_time + timedelta(minutes=10)  # ì˜ˆì¸¡ ì‹œì 
        
        # ì í”„ ì¼€ì´ìŠ¤ íŒë‹¨ (ì‹œí€€ìŠ¤ MAX < 280 & ì‹¤ì œê°’ >= 300)
        is_jump = (seq_max_list[i] < 280) and (y_test[i] >= 300)
        if is_jump:
            jump_count += 1
        
        # ê·¹ë‹¨ê°’ ì¼€ì´ìŠ¤
        if y_test[i] >= 300:
            extreme_count += 1
            if y_pred[i] >= 290:  # ê·¹ë‹¨ê°’ ê°ì§€ ì„±ê³µ
                extreme_detected += 1
        
        # ì»¬ëŸ¼ ìˆœì„œ ì¡°ì •: í˜„ì¬ì‹œê°„ â†’ ì˜ˆì¸¡ì‹œê°„ â†’ ì‹œí€€ìŠ¤ì‹œì‘ â†’ ì‹œí€€ìŠ¤ì™„ë£Œ â†’ ë‚˜ë¨¸ì§€
        results.append({
            'í˜„ì¬ì‹œê°„': current_time.strftime('%Y-%m-%d %H:%M'),
            'ì˜ˆì¸¡ì‹œê°„(+10ë¶„)': prediction_time.strftime('%Y-%m-%d %H:%M'),
            'ì‹œí€€ìŠ¤ì‹œì‘': seq_start_time.strftime('%Y-%m-%d %H:%M'),
            'ì‹œí€€ìŠ¤ì™„ë£Œ': current_time.strftime('%Y-%m-%d %H:%M'),
            'ì‹¤ì œê°’': round(y_test[i], 2),
            'ì˜ˆì¸¡ê°’': round(y_pred[i], 2),
            'ì˜¤ì°¨': round(abs(y_test[i] - y_pred[i]), 2),
            'ì˜¤ì°¨ìœ¨(%)': round(abs(y_test[i] - y_pred[i]) / y_test[i] * 100, 2),
            'ì‹œí€€ìŠ¤MAX': round(seq_max_list[i], 2),
            'ì‹œí€€ìŠ¤MIN': round(seq_min_list[i], 2),
            'ì‹œí€€ìŠ¤ë²”ìœ„': round(seq_max_list[i] - seq_min_list[i], 2),
            'ì í”„ì¼€ì´ìŠ¤': 'O' if is_jump else '-',
            'ê·¹ë‹¨ê°’(300+)': 'O' if y_test[i] >= 300 else '-',
            'ê·¹ë‹¨ê°’ê°ì§€': 'O' if (y_test[i] >= 300 and y_pred[i] >= 290) else '-'
        })
    
    df_results = pd.DataFrame(results)
    
    # í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ë¶„ì„:")
    print(f"  - ì „ì²´ ì˜ˆì¸¡: {len(df_results)}ê°œ")
    print(f"  - ê·¹ë‹¨ê°’(300+): {extreme_count}ê°œ ({extreme_count/len(df_results)*100:.1f}%)")
    print(f"  - ê·¹ë‹¨ê°’ ê°ì§€: {extreme_detected}/{extreme_count}ê°œ ({extreme_detected/extreme_count*100 if extreme_count > 0 else 0:.1f}%)")
    print(f"  - ì í”„ ì¼€ì´ìŠ¤: {jump_count}ê°œ ({jump_count/len(df_results)*100:.1f}%)")
    
    # ì í”„ ì¼€ì´ìŠ¤ ìƒì„¸ ì¶œë ¥
    if jump_count > 0:
        jump_df = df_results[df_results['ì í”„ì¼€ì´ìŠ¤'] == 'O']
        print(f"\nğŸ”¥ ì í”„ ì¼€ì´ìŠ¤ ìƒì„¸ (ì‹œí€€ìŠ¤MAX<280 â†’ ì‹¤ì œê°’â‰¥300):")
        display_cols = ['í˜„ì¬ì‹œê°„', 'ì‹œí€€ìŠ¤ì‹œì‘', 'ì‹œí€€ìŠ¤ì™„ë£Œ', 'ì‹œí€€ìŠ¤MAX', 'ì‹¤ì œê°’', 'ì˜ˆì¸¡ê°’', 'ì˜¤ì°¨']
        print(jump_df[display_cols].head(5).to_string())
    
    # ê·¹ë‹¨ê°’ ì¼€ì´ìŠ¤ ìƒì„¸ ì¶œë ¥
    if extreme_count > 0:
        extreme_df = df_results[df_results['ê·¹ë‹¨ê°’(300+)'] == 'O']
        print(f"\nâš ï¸ ê·¹ë‹¨ê°’ ì¼€ì´ìŠ¤ ìƒì„¸ (ì‹¤ì œê°’ â‰¥ 300):")
        display_cols = ['í˜„ì¬ì‹œê°„', 'ì‹œí€€ìŠ¤ì™„ë£Œ', 'ì‹¤ì œê°’', 'ì˜ˆì¸¡ê°’', 'ì˜¤ì°¨', 'ê·¹ë‹¨ê°’ê°ì§€']
        print(extreme_df[display_cols].head(5).to_string())
    
    # CSV ì €ì¥
    df_results.to_csv('BBB_evaluation_results.csv', index=False, encoding='utf-8-sig')
    print(f"\nâœ… ìƒì„¸ ê²°ê³¼ ì €ì¥: BBB_evaluation_results.csv")
    
    # ===== 4. ê·¸ë˜í”„ ìƒì„± =====
    print("\n[STEP 4] í‰ê°€ ê·¸ë˜í”„ ìƒì„±")
    print("-"*40)
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    
    # 1. ì˜ˆì¸¡ vs ì‹¤ì œ ì‚°ì ë„
    ax1 = axes[0, 0]
    ax1.scatter(y_test, y_pred, alpha=0.5, s=10)
    ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    ax1.set_xlabel('Actual')
    ax1.set_ylabel('Predicted')
    ax1.set_title(f'Actual vs Predicted\nMAE={test_mae:.2f}, RÂ²={test_r2:.3f}')
    ax1.grid(True, alpha=0.3)
    
    # 2. ì‹œê³„ì—´ ë¹„êµ (ì²˜ìŒ 300ê°œ)
    ax2 = axes[0, 1]
    plot_size = min(300, len(y_test))
    ax2.plot(range(plot_size), y_test[:plot_size], 'b-', label='Actual', alpha=0.7, linewidth=1)
    ax2.plot(range(plot_size), y_pred[:plot_size], 'r--', label='Predicted', alpha=0.7, linewidth=1)
    ax2.axhline(y=300, color='orange', linestyle='--', label='Extreme(300)', alpha=0.5)
    ax2.set_xlabel('Time Index')
    ax2.set_ylabel('Value')
    ax2.set_title('Time Series Comparison (First 300)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. ì˜¤ì°¨ ë¶„í¬
    ax3 = axes[0, 2]
    errors = y_pred - y_test
    ax3.hist(errors, bins=50, edgecolor='black', alpha=0.7, color='skyblue')
    ax3.axvline(x=0, color='r', linestyle='--', linewidth=2)
    ax3.set_xlabel('Prediction Error')
    ax3.set_ylabel('Frequency')
    ax3.set_title(f'Error Distribution\nMean={np.mean(errors):.2f}, Std={np.std(errors):.2f}')
    ax3.grid(True, alpha=0.3)
    
    # 4. ê·¹ë‹¨ê°’ ì„±ëŠ¥
    ax4 = axes[1, 0]
    extreme_mask = y_test >= 300
    if extreme_mask.any():
        ax4.scatter(y_test[~extreme_mask], y_pred[~extreme_mask], 
                   alpha=0.3, s=5, label='Normal', color='blue')
        ax4.scatter(y_test[extreme_mask], y_pred[extreme_mask], 
                   alpha=0.8, s=20, label='Extreme(300+)', color='red')
        ax4.plot([200, 500], [200, 500], 'k--', lw=1)
        ax4.axhline(y=300, color='orange', linestyle='--', alpha=0.5)
        ax4.axvline(x=300, color='orange', linestyle='--', alpha=0.5)
        ax4.set_xlabel('Actual')
        ax4.set_ylabel('Predicted')
        ax4.set_title(f'Extreme Value Performance\nDetected: {extreme_detected}/{extreme_count}')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
    
    # 5. ì í”„ ì¼€ì´ìŠ¤ ë¶„ì„
    ax5 = axes[1, 1]
    if jump_count > 0:
        jump_mask = df_results['ì í”„ì¼€ì´ìŠ¤'] == 'O'
        jump_actual = df_results[jump_mask]['ì‹¤ì œê°’'].values
        jump_pred = df_results[jump_mask]['ì˜ˆì¸¡ê°’'].values
        jump_seq_max = df_results[jump_mask]['ì‹œí€€ìŠ¤MAX'].values
        
        ax5.scatter(jump_seq_max, jump_actual, label='Actual Jump', s=50, alpha=0.8, color='red')
        ax5.scatter(jump_seq_max, jump_pred, label='Predicted', s=30, alpha=0.6, color='blue')
        
        # ì í”„ ê°ì§€ì„ 
        for i in range(len(jump_seq_max)):
            ax5.plot([jump_seq_max[i], jump_seq_max[i]], 
                    [jump_pred[i], jump_actual[i]], 
                    'gray', alpha=0.3, linewidth=0.5)
        
        ax5.axhline(y=300, color='orange', linestyle='--', label='Threshold')
        ax5.axvline(x=280, color='green', linestyle='--', alpha=0.5, label='SeqMax=280')
        ax5.set_xlabel('Sequence MAX')
        ax5.set_ylabel('Value')
        ax5.set_title(f'Jump Cases Analysis\n(SeqMax<280 â†’ Actualâ‰¥300: {jump_count} cases)')
        ax5.legend()
        ax5.grid(True, alpha=0.3)
    else:
        ax5.text(0.5, 0.5, 'No Jump Cases', ha='center', va='center', fontsize=14)
        ax5.set_title('Jump Cases Analysis')
    
    # 6. Feature ì¤‘ìš”ë„
    ax6 = axes[1, 2]
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=True)
    
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(feature_importance)))
    ax6.barh(range(len(feature_importance)), feature_importance['importance'].values, color=colors)
    ax6.set_yticks(range(len(feature_importance)))
    ax6.set_yticklabels(feature_importance['feature'].values)
    ax6.set_xlabel('Importance')
    ax6.set_title('Feature Importance')
    
    # ì¤‘ìš”ë„ ê°’ í‘œì‹œ
    for i, v in enumerate(feature_importance['importance'].values):
        ax6.text(v + 0.001, i, f'{v:.3f}', va='center')
    
    ax6.grid(True, alpha=0.3)
    
    plt.suptitle('BBB.CSV Evaluation Results', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig('BBB_evaluation_graphs.png', dpi=150, bbox_inches='tight')
    print("âœ… ê·¸ë˜í”„ ì €ì¥: BBB_evaluation_graphs.png")
    
    # ===== 5. ìµœì¢… ìš”ì•½ =====
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… í‰ê°€ ìš”ì•½")
    print("="*80)
    print(f"1. ëª¨ë¸ ì„±ëŠ¥:")
    print(f"   - í•™ìŠµ MAE: {train_mae:.2f}")
    print(f"   - í‰ê°€ MAE: {test_mae:.2f}")
    print(f"   - ì„±ëŠ¥ ì°¨ì´: {abs(test_mae - train_mae):.2f}")
    
    print(f"\n2. íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì„±ëŠ¥:")
    print(f"   - ê·¹ë‹¨ê°’(300+): {extreme_count}ê°œ ë°œìƒ")
    print(f"   - ê·¹ë‹¨ê°’ ê°ì§€: {extreme_detected}ê°œ ({extreme_detected/extreme_count*100 if extreme_count > 0 else 0:.1f}%)")
    print(f"   - ì í”„ ì¼€ì´ìŠ¤: {jump_count}ê°œ ë°œìƒ")
    
    print(f"\n3. ì‹œê°„ ì •ë³´:")
    if len(df_results) > 0:
        print(f"   - ë°ì´í„° ì‹œì‘: {df_results['ì‹œí€€ìŠ¤ì‹œì‘'].iloc[0]}")
        print(f"   - ë°ì´í„° ì¢…ë£Œ: {df_results['ì˜ˆì¸¡ì‹œê°„(+10ë¶„)'].iloc[-1]}")
    
    print(f"\n4. ì €ì¥ íŒŒì¼:")
    print(f"   - ëª¨ë¸: xgboost_model_30min_10min.pkl")
    print(f"   - ê²°ê³¼: BBB_evaluation_results.csv (ì»¬ëŸ¼ ìˆœì„œ ì¡°ì •)")
    print(f"   - ê·¸ë˜í”„: BBB_evaluation_graphs.png")
    
    # ì»¬ëŸ¼ ìˆœì„œ í™•ì¸ ì¶œë ¥
    print(f"\n5. CSV ì»¬ëŸ¼ ìˆœì„œ:")
    print(f"   {' | '.join(df_results.columns[:6])}")
    print(f"   {' | '.join(df_results.columns[6:])}")
    
    return model, df_results

# ì‹¤í–‰
if __name__ == '__main__':
    model, results = train_and_evaluate_complete()