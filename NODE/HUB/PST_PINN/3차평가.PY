# -*- coding: utf-8 -*-
"""
HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ëª¨ë¸ í‰ê°€
- í‰ê°€ ë°ì´í„°: 2025ë…„ 8ì›” (20250801_TO_20250831.CSV)
- í•™ìŠµëœ Model 1, Model 2 ì‚¬ìš©
- ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import joblib
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from datetime import datetime
import os
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("ğŸ¯ HUBROOM ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ")
print("ğŸ“… í‰ê°€ ë°ì´í„°: 2025ë…„ 8ì›”")
print("="*80)

# ========================================
# Model 1: PatchTST (ì½”ë“œì—ì„œ ë³µì‚¬)
# ========================================

class PatchTSTModel(keras.Model):
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(128, activation='relu')
        
        # Transformer
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm1 = layers.LayerNormalization()
        self.norm2 = layers.LayerNormalization()
        
        self.ffn = keras.Sequential([
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(128)
        ])
        
        # ì¶œë ¥
        self.flatten = layers.Flatten()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dropout = layers.Dropout(0.3)
        self.dense2 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(1)
        
    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        
        # íŒ¨ì¹˜ ìƒì„±
        x = tf.reshape(x, [batch_size, self.n_patches, self.patch_len * self.n_features])
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x = self.patch_embedding(x)
        
        # Transformer
        attn = self.attention(x, x, training=training)
        x = self.norm1(x + attn)
        
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        
        # ì¶œë ¥
        x = self.flatten(x)
        x = self.dense1(x)
        x = self.dropout(x, training=training)
        x = self.dense2(x)
        output = self.output_layer(x)
        
        return tf.squeeze(output, axis=-1)

# ========================================
# Model 2: PatchTST + PINN (ì½”ë“œì—ì„œ ë³µì‚¬)
# ========================================

class PatchTSTPINN(keras.Model):
    def __init__(self, config):
        super().__init__()
        
        # PatchTST ë¶€ë¶„
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(128, activation='relu')
        
        # Transformer
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm = layers.LayerNormalization()
        
        # ì‹œê³„ì—´ ì²˜ë¦¬
        self.flatten = layers.Flatten()
        self.temporal_dense = layers.Dense(64, activation='relu')
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬ (PINN)
        self.physics_net = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dense(16, activation='relu')
        ])
        
        # ìœµí•© ë° ê·¹ë‹¨ê°’ ë³´ì •
        self.fusion = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(1)
        ])
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ… ë ˆì´ì–´
        self.extreme_boost = layers.Dense(1, activation='sigmoid')
        
    def call(self, inputs, training=False):
        x_seq, x_physics = inputs
        
        batch_size = tf.shape(x_seq)[0]
        
        # PatchTST ì²˜ë¦¬
        x = tf.reshape(x_seq, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        
        attn = self.attention(x, x, training=training)
        x = self.norm(x + attn)
        
        x = self.flatten(x)
        temporal_features = self.temporal_dense(x)
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬
        physics_features = self.physics_net(x_physics)
        
        # ìœµí•©
        combined = tf.concat([temporal_features, physics_features], axis=-1)
        output = self.fusion(combined)
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ…
        boost_factor = self.extreme_boost(combined)
        output = output * (1 + boost_factor * 0.2)
        
        return tf.squeeze(output, axis=-1)

# ========================================
# í‰ê°€ í•¨ìˆ˜
# ========================================

def evaluate_august_data():
    """2025ë…„ 8ì›” ë°ì´í„° í‰ê°€"""
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df = pd.read_csv('data/20250801_TO_20250831.CSV')
    print(f"âœ… 8ì›” ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape}")
    
    # íƒ€ê²Ÿ ì»¬ëŸ¼
    target_col = 'CURRENT_M16A_3F_JOB_2'
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    df['timestamp'] = pd.to_datetime(df.iloc[:, 0], format='%Y%m%d%H%M', errors='coerce')
    df = df.sort_values('timestamp').reset_index(drop=True)
    df = df.fillna(method='ffill').fillna(0)
    
    # ë°ì´í„° ë¶„ì„
    print(f"\nğŸ“Š 8ì›” ë°ì´í„° ë¶„ì„:")
    print(f"  ê¸°ê°„: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
    print(f"  ì „ì²´ ìƒ˜í”Œ: {len(df)}")
    print(f"  íƒ€ê²Ÿ ë²”ìœ„: {df[target_col].min():.0f} ~ {df[target_col].max():.0f}")
    print(f"  í‰ê· : {df[target_col].mean():.1f}, ì¤‘ì•™ê°’: {df[target_col].median():.1f}")
    print(f"\n  ê·¹ë‹¨ê°’ ë¶„í¬:")
    print(f"    300+: {(df[target_col] >= 300).sum()}ê°œ ({(df[target_col] >= 300).sum()/len(df)*100:.2f}%)")
    print(f"    310+: {(df[target_col] >= 310).sum()}ê°œ ({(df[target_col] >= 310).sum()/len(df)*100:.2f}%)")
    print(f"    335+: {(df[target_col] >= 335).sum()}ê°œ ({(df[target_col] >= 335).sum()/len(df)*100:.2f}%)")
    
    # 2. ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    print("\nğŸ”§ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì¤‘...")
    scaler_X = joblib.load('./scalers/scaler_X.pkl')
    scaler_y = joblib.load('./scalers/scaler_y.pkl')
    scaler_physics = joblib.load('./scalers/scaler_physics.pkl')
    print("âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
    
    # 3. ì‹œí€€ìŠ¤ ìƒì„±
    print("\nğŸ”„ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # ë¬¼ë¦¬ ë°ì´í„°ìš© ì»¬ëŸ¼
    inflow_cols = ['M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2',
                   'M14A_3F_TO_HUB_JOB2', 'M14B_7F_TO_HUB_JOB2']
    outflow_cols = ['M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB',
                    'M16A_3F_TO_M14A_3F_JOB', 'M16A_3F_TO_M14B_7F_JOB']
    
    available_inflow = [col for col in inflow_cols if col in df.columns]
    available_outflow = [col for col in outflow_cols if col in df.columns]
    
    seq_len = 20
    pred_len = 10
    
    X, y, X_physics, timestamps = [], [], [], []
    
    for i in range(len(df) - seq_len - pred_len + 1):
        # ì‹œê³„ì—´ ë°ì´í„°
        X.append(df[numeric_cols].iloc[i:i+seq_len].values)
        
        # íƒ€ê²Ÿ
        y_val = df[target_col].iloc[i + seq_len + pred_len - 1]
        y.append(y_val)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„
        timestamps.append(df['timestamp'].iloc[i + seq_len + pred_len - 1])
        
        # ë¬¼ë¦¬ ë°ì´í„°
        physics = [
            df[target_col].iloc[i + seq_len - 1],  # í˜„ì¬ê°’
            df[available_inflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_inflow else 0,
            df[available_outflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_outflow else 0
        ]
        X_physics.append(physics)
    
    X = np.array(X)
    y = np.array(y)
    X_physics = np.array(X_physics)
    
    print(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {len(X)}ê°œ")
    
    # 4. ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    print("\nğŸ“ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì¤‘...")
    n_features = X.shape[2]
    
    X_scaled = scaler_X.transform(X.reshape(-1, n_features)).reshape(len(X), seq_len, n_features)
    y_scaled = scaler_y.transform(y.reshape(-1, 1)).flatten()
    X_physics_scaled = scaler_physics.transform(X_physics)
    
    print("âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
    
    # 5. ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡
    print("\nğŸ¤– ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡ ì¤‘...")
    
    config = {
        'seq_len': 20,
        'n_features': n_features,
        'patch_len': 5
    }
    
    # Model 1 ì˜ˆì¸¡
    print("  Model 1 (PatchTST) ì˜ˆì¸¡ ì¤‘...")
    model1 = PatchTSTModel(config)
    model1.compile(optimizer='adam', loss='mse')
    dummy_input = np.zeros((1, 20, n_features))
    _ = model1(dummy_input)
    model1.load_weights('./checkpoints/model1_v3.h5')
    
    y_pred1_scaled = model1.predict(X_scaled, batch_size=64, verbose=0)
    y_pred1 = scaler_y.inverse_transform(y_pred1_scaled.reshape(-1, 1)).flatten()
    
    # Model 2 ì˜ˆì¸¡
    print("  Model 2 (PatchTST + PINN) ì˜ˆì¸¡ ì¤‘...")
    model2 = PatchTSTPINN(config)
    model2.compile(optimizer='adam', loss='mse')
    dummy_seq = np.zeros((1, 20, n_features))
    dummy_physics = np.zeros((1, 3))
    _ = model2([dummy_seq, dummy_physics])
    model2.load_weights('./checkpoints/model2_v3.h5')
    
    y_pred2_scaled = model2.predict([X_scaled, X_physics_scaled], batch_size=64, verbose=0)
    y_pred2 = scaler_y.inverse_transform(y_pred2_scaled.reshape(-1, 1)).flatten()
    
    # ì•™ìƒë¸” ì˜ˆì¸¡ (ê°€ì¤‘ í‰ê· )
    y_ensemble = 0.4 * y_pred1 + 0.6 * y_pred2  # Model 2ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
    
    print("âœ… ì˜ˆì¸¡ ì™„ë£Œ")
    
    # 6. í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
    print("\nğŸ“ˆ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
    
    results = {
        'Model': [],
        'MAE': [],
        'RMSE': [],
        'R2': [],
        'MAE_Low(<200)': [],
        'MAE_Normal(200-300)': [],
        'MAE_High(300+)': [],
        'MAE_Extreme(310+)': [],
        'Detection_310+': [],
        'Detection_335+': []
    }
    
    # ê° ëª¨ë¸ë³„ í‰ê°€
    for model_name, y_pred in [('Model1_PatchTST', y_pred1), 
                               ('Model2_PINN', y_pred2), 
                               ('Ensemble', y_ensemble)]:
        
        results['Model'].append(model_name)
        results['MAE'].append(mean_absolute_error(y, y_pred))
        results['RMSE'].append(np.sqrt(mean_squared_error(y, y_pred)))
        results['R2'].append(r2_score(y, y_pred))
        
        # êµ¬ê°„ë³„ MAE
        mask_low = y < 200
        mask_normal = (y >= 200) & (y < 300)
        mask_high = y >= 300
        mask_extreme = y >= 310
        
        results['MAE_Low(<200)'].append(mean_absolute_error(y[mask_low], y_pred[mask_low]) if mask_low.sum() > 0 else 0)
        results['MAE_Normal(200-300)'].append(mean_absolute_error(y[mask_normal], y_pred[mask_normal]) if mask_normal.sum() > 0 else 0)
        results['MAE_High(300+)'].append(mean_absolute_error(y[mask_high], y_pred[mask_high]) if mask_high.sum() > 0 else 0)
        results['MAE_Extreme(310+)'].append(mean_absolute_error(y[mask_extreme], y_pred[mask_extreme]) if mask_extreme.sum() > 0 else 0)
        
        # ê·¹ë‹¨ê°’ ê°ì§€ìœ¨
        mask_310 = y >= 310
        mask_335 = y >= 335
        
        if mask_310.sum() > 0:
            detection_310 = ((y_pred >= 305)[mask_310].sum() / mask_310.sum()) * 100
            results['Detection_310+'].append(f"{detection_310:.1f}%")
        else:
            results['Detection_310+'].append("N/A")
            
        if mask_335.sum() > 0:
            detection_335 = ((y_pred >= 330)[mask_335].sum() / mask_335.sum()) * 100
            results['Detection_335+'].append(f"{detection_335:.1f}%")
        else:
            results['Detection_335+'].append("N/A")
    
    # ê²°ê³¼ DataFrame ìƒì„±
    results_df = pd.DataFrame(results)
    
    # 7. ìƒì„¸ ì˜ˆì¸¡ ê²°ê³¼ DataFrame
    detail_df = pd.DataFrame({
        'Timestamp': timestamps,
        'Actual': y,
        'Pred_Model1': y_pred1,
        'Pred_Model2': y_pred2,
        'Pred_Ensemble': y_ensemble,
        'Error_Model1': np.abs(y - y_pred1),
        'Error_Model2': np.abs(y - y_pred2),
        'Error_Ensemble': np.abs(y - y_ensemble),
        'Is_Extreme(310+)': y >= 310,
        'Is_VeryExtreme(335+)': y >= 335
    })
    
    # 8. ê²°ê³¼ ì €ì¥
    print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # ê²°ê³¼ í´ë” ìƒì„±
    os.makedirs('./evaluation_results', exist_ok=True)
    
    # ì„±ëŠ¥ ìš”ì•½ ì €ì¥
    results_df.to_csv('./evaluation_results/model_performance_summary.csv', index=False)
    print("âœ… ì„±ëŠ¥ ìš”ì•½ ì €ì¥: ./evaluation_results/model_performance_summary.csv")
    
    # ìƒì„¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
    detail_df.to_csv('./evaluation_results/detailed_predictions_august.csv', index=False)
    print("âœ… ìƒì„¸ ì˜ˆì¸¡ ì €ì¥: ./evaluation_results/detailed_predictions_august.csv")
    
    # ê·¹ë‹¨ê°’ë§Œ í•„í„°ë§í•œ ê²°ê³¼
    extreme_df = detail_df[detail_df['Is_Extreme(310+)']].copy()
    extreme_df.to_csv('./evaluation_results/extreme_predictions_august.csv', index=False)
    print("âœ… ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì €ì¥: ./evaluation_results/extreme_predictions_august.csv")
    
    # 9. ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("="*80)
    print(results_df.to_string(index=False))
    
    print("\nğŸ¯ ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì„±ëŠ¥ (310+ êµ¬ê°„)")
    print("-"*50)
    extreme_only = detail_df[detail_df['Is_Extreme(310+)']]
    if len(extreme_only) > 0:
        print(f"  ì´ ê·¹ë‹¨ê°’ ìƒ˜í”Œ: {len(extreme_only)}ê°œ")
        print(f"  Model 1 í‰ê·  ì˜¤ì°¨: {extreme_only['Error_Model1'].mean():.2f}")
        print(f"  Model 2 í‰ê·  ì˜¤ì°¨: {extreme_only['Error_Model2'].mean():.2f}")
        print(f"  Ensemble í‰ê·  ì˜¤ì°¨: {extreme_only['Error_Ensemble'].mean():.2f}")
        
        # ì •í™• ì˜ˆì¸¡ (ì˜¤ì°¨ 10 ì´ë‚´)
        accurate_m1 = (extreme_only['Error_Model1'] <= 10).sum()
        accurate_m2 = (extreme_only['Error_Model2'] <= 10).sum()
        accurate_ens = (extreme_only['Error_Ensemble'] <= 10).sum()
        
        print(f"\n  ì˜¤ì°¨ 10 ì´ë‚´ ì˜ˆì¸¡:")
        print(f"    Model 1: {accurate_m1}/{len(extreme_only)} ({accurate_m1/len(extreme_only)*100:.1f}%)")
        print(f"    Model 2: {accurate_m2}/{len(extreme_only)} ({accurate_m2/len(extreme_only)*100:.1f}%)")
        print(f"    Ensemble: {accurate_ens}/{len(extreme_only)} ({accurate_ens/len(extreme_only)*100:.1f}%)")
    
    print("\nâœ… í‰ê°€ ì™„ë£Œ! ê²°ê³¼ëŠ” ./evaluation_results/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("="*80)
    
    return results_df, detail_df

# ì‹¤í–‰
if __name__ == "__main__":
    try:
        results_df, detail_df = evaluate_august_data()
        print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()