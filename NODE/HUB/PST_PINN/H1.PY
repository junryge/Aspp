# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ HUBROOM ë°˜ì†¡ëŸ‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
- ì§„í–‰ë¥  í‘œì‹œ ê¸°ëŠ¥ ì¶”ê°€
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë³µì› ê¸°ëŠ¥
- ì¤‘ë‹¨ í›„ ì¬ê°œ ê°€ëŠ¥
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import math
import warnings
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import pickle
import os
import json
from tqdm import tqdm
import time
import signal
import sys

warnings.filterwarnings('ignore')

# ===========================
# ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
# ===========================

class CheckpointManager:
    """í•™ìŠµ ì¤‘ë‹¨/ì¬ê°œë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬"""
    
    def __init__(self, checkpoint_dir='./checkpoints'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        self.interrupted = False
        
        # Ctrl+C ì¸í„°ëŸ½íŠ¸ í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
    
    def _signal_handler(self, sig, frame):
        print('\n\nâš ï¸ í•™ìŠµ ì¤‘ë‹¨ ê°ì§€! ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì¤‘...')
        self.interrupted = True
    
    def save_checkpoint(self, state, filename):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        filepath = os.path.join(self.checkpoint_dir, filename)
        torch.save(state, filepath)
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {filepath}")
    
    def load_checkpoint(self, filename):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        filepath = os.path.join(self.checkpoint_dir, filename)
        if os.path.exists(filepath):
            print(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {filepath}")
            return torch.load(filepath)
        return None
    
    def save_data_progress(self, data_dict, filename='data_progress.pkl'):
        """ë°ì´í„° ì²˜ë¦¬ ì§„í–‰ ìƒí™© ì €ì¥"""
        filepath = os.path.join(self.checkpoint_dir, filename)
        with open(filepath, 'wb') as f:
            pickle.dump(data_dict, f)
        print(f"ğŸ’¾ ë°ì´í„° ì§„í–‰ ìƒí™© ì €ì¥: {filepath}")
    
    def load_data_progress(self, filename='data_progress.pkl'):
        """ë°ì´í„° ì²˜ë¦¬ ì§„í–‰ ìƒí™© ë¡œë“œ"""
        filepath = os.path.join(self.checkpoint_dir, filename)
        if os.path.exists(filepath):
            print(f"ğŸ“‚ ë°ì´í„° ì§„í–‰ ìƒí™© ë¡œë“œ: {filepath}")
            with open(filepath, 'rb') as f:
                return pickle.load(f)
        return None

# ===========================
# ğŸ“Š ê°œì„ ëœ ë°ì´í„° ì²˜ë¦¬
# ===========================

class ImprovedHUBROOMProcessor:
    """ì§„í–‰ë¥  í‘œì‹œê°€ ì¶”ê°€ëœ ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, file_path='data/HUB_0509_TO_0730_DATA.CSV'):
        self.file_path = file_path
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        self.checkpoint_manager = CheckpointManager()
        
        # ë¬¼ë¦¬ì  ê³„ì‚°ì„ ìœ„í•œ ì»¬ëŸ¼ ê·¸ë£¹
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2', 'M14B_7F_TO_HUB_JOB2'
        ]
        
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB', 'M16A_3F_TO_M14B_7F_JOB'
        ]
    
    def analyze_data(self, df):
        """ë°ì´í„° íŠ¹ì„± ìƒì„¸ ë¶„ì„"""
        print("\n" + "="*60)
        print("ğŸ“ˆ ë°ì´í„° íŠ¹ì„± ë¶„ì„")
        print("="*60)
        
        total_steps = 8
        current_step = 0
        
        # 1. ê¸°ë³¸ ì •ë³´
        current_step += 1
        print(f"\n[{current_step}/{total_steps}] ê¸°ë³¸ ì •ë³´ ë¶„ì„... ({current_step/total_steps*100:.0f}%)")
        print(f"  - ì „ì²´ í–‰ ìˆ˜: {df.shape[0]:,}")
        print(f"  - ì „ì²´ ì»¬ëŸ¼ ìˆ˜: {df.shape[1]}")
        print(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage().sum() / 1024**2:.2f} MB")
        
        # 2. ê²°ì¸¡ì¹˜ ë¶„ì„
        current_step += 1
        print(f"\n[{current_step}/{total_steps}] ê²°ì¸¡ì¹˜ ë¶„ì„... ({current_step/total_steps*100:.0f}%)")
        missing_ratio = (df.isnull().sum() / len(df)) * 100
        missing_cols = missing_ratio[missing_ratio > 0]
        if len(missing_cols) > 0:
            print(f"  - ê²°ì¸¡ì¹˜ ìˆëŠ” ì»¬ëŸ¼: {len(missing_cols)}ê°œ")
            for col, ratio in missing_cols.head(5).items():
                print(f"    â€¢ {col}: {ratio:.2f}%")
        else:
            print("  - âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ")
        
        # 3. íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ì„
        current_step += 1
        print(f"\n[{current_step}/{total_steps}] íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ì„... ({current_step/total_steps*100:.0f}%)")
        if self.target_col in df.columns:
            target = df[self.target_col]
            print(f"  - íƒ€ê²Ÿ: {self.target_col}")
            print(f"  - í‰ê· : {target.mean():.2f}")
            print(f"  - í‘œì¤€í¸ì°¨: {target.std():.2f}")
            print(f"  - ìµœì†Œ/ìµœëŒ€: {target.min():.0f} / {target.max():.0f}")
            print(f"  - ë¶„í¬: Q1={target.quantile(0.25):.0f}, Q2={target.median():.0f}, Q3={target.quantile(0.75):.0f}")
        else:
            print(f"  - âš ï¸ íƒ€ê²Ÿ ì»¬ëŸ¼ '{self.target_col}' ì—†ìŒ")
        
        # 4. ìœ ì…/ìœ ì¶œ ë¶„ì„
        current_step += 1
        print(f"\n[{current_step}/{total_steps}] ìœ ì…/ìœ ì¶œ ê· í˜• ë¶„ì„... ({current_step/total_steps*100:.0f}%)")
        available_inflow = [col for col in self.inflow_cols if col in df.columns]
        available_outflow = [col for col in self.outflow_cols if col in df.columns]
        
        if available_inflow and available_outflow:
            total_inflow = df[available_inflow].sum().sum()
            total_outflow = df[available_outflow].sum().sum()
            balance = total_inflow - total_outflow
            print(f"  - ì´ ìœ ì…: {total_inflow:,.0f}")
            print(f"  - ì´ ìœ ì¶œ: {total_outflow:,.0f}")
            print(f"  - ìˆœ ë³€í™”: {balance:+,.0f} ({balance/total_inflow*100:+.1f}%)")
        
        # 5. ì‹œê°„ íŒ¨í„´ ë¶„ì„
        current_step += 1
        print(f"\n[{current_step}/{total_steps}] ì‹œê°„ íŒ¨í„´ ë¶„ì„... ({current_step/total_steps*100:.0f}%)")
        if 'timestamp' in df.columns or df.columns[0].lower().find('time') >= 0:
            time_col = 'timestamp' if 'timestamp' in df.columns else df.columns[0]
            print(f"  - ì‹œì‘: {df[time_col].iloc[0]}")
            print(f"  - ì¢…ë£Œ: {df[time_col].iloc[-1]}")
            print(f"  - ë°ì´í„° ê°„ê²©: ë¶„ ë‹¨ìœ„ (ì¶”ì •)")
        
        # 6. ìƒê´€ê´€ê³„ ë¶„ì„
        current_step += 1
        print(f"\n[{current_step}/{total_steps}] ì£¼ìš” ìƒê´€ê´€ê³„ ë¶„ì„... ({current_step/total_steps*100:.0f}%)")
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if self.target_col in numeric_cols and len(numeric_cols) > 1:
            correlations = df[numeric_cols].corr()[self.target_col].sort_values(ascending=False)
            print(f"  íƒ€ê²Ÿê³¼ ìƒê´€ê´€ê³„ ë†’ì€ TOP 5:")
            for col, corr in correlations[1:6].items():
                print(f"    â€¢ {col}: {corr:.3f}")
        
        # 7. ì´ìƒì¹˜ íƒì§€
        current_step += 1
        print(f"\n[{current_step}/{total_steps}] ì´ìƒì¹˜ íƒì§€... ({current_step/total_steps*100:.0f}%)")
        for col in numeric_cols[:3]:  # ì²˜ìŒ 3ê°œ ì»¬ëŸ¼ë§Œ
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = ((df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)).sum()
            if outliers > 0:
                print(f"  - {col}: {outliers}ê°œ ({outliers/len(df)*100:.1f}%)")
        
        # 8. ë°ì´í„° í’ˆì§ˆ ì ìˆ˜
        current_step += 1
        print(f"\n[{current_step}/{total_steps}] ë°ì´í„° í’ˆì§ˆ í‰ê°€... ({current_step/total_steps*100:.0f}%)")
        quality_score = 100
        quality_score -= missing_ratio.mean() * 10  # ê²°ì¸¡ì¹˜ í˜ë„í‹°
        quality_score -= min(20, outliers/len(df)*100*5)  # ì´ìƒì¹˜ í˜ë„í‹°
        print(f"  - ë°ì´í„° í’ˆì§ˆ ì ìˆ˜: {quality_score:.1f}/100")
        
        if quality_score >= 80:
            print("  - âœ… ìš°ìˆ˜í•œ ë°ì´í„° í’ˆì§ˆ")
        elif quality_score >= 60:
            print("  - âš ï¸ ë³´í†µ ë°ì´í„° í’ˆì§ˆ (ì „ì²˜ë¦¬ ê¶Œì¥)")
        else:
            print("  - âŒ ë‚®ì€ ë°ì´í„° í’ˆì§ˆ (ì „ì²˜ë¦¬ í•„ìˆ˜)")
        
        print("\n" + "="*60)
        
        return {
            'total_rows': df.shape[0],
            'total_cols': df.shape[1],
            'missing_ratio': missing_ratio.mean(),
            'quality_score': quality_score
        }
    
    def create_sequences_with_progress(self, df, numeric_cols, seq_len=20, pred_len=10, resume_from=0):
        """ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ì‹œí€€ìŠ¤ ìƒì„±"""
        
        print("\nğŸ“¦ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì¤‘...")
        
        X, y, X_physics = [], [], []
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì „ ì§„í–‰ ìƒí™© ë¡œë“œ
        saved_progress = self.checkpoint_manager.load_data_progress()
        if saved_progress and resume_from > 0:
            print(f"â™»ï¸ ì´ì „ ì§„í–‰ ìƒí™© ë³µì›: {resume_from}ê°œ ì™„ë£Œ")
            X = saved_progress['X']
            y = saved_progress['y']
            X_physics = saved_progress['X_physics']
        
        available_inflow = [col for col in self.inflow_cols if col in df.columns]
        available_outflow = [col for col in self.outflow_cols if col in df.columns]
        
        total_sequences = len(df) - seq_len - pred_len + 1
        
        # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
        with tqdm(total=total_sequences, desc="ì‹œí€€ìŠ¤ ìƒì„±", initial=resume_from) as pbar:
            for i in range(resume_from, total_sequences):
                # ì…ë ¥: ê³¼ê±° 20ë¶„
                X_seq = df[numeric_cols].iloc[i:i+seq_len].values
                X.append(X_seq)
                
                # íƒ€ê²Ÿ: 10ë¶„ í›„
                if self.target_col in df.columns:
                    y_val = df[self.target_col].iloc[i+seq_len+pred_len-1]
                else:
                    y_val = df[numeric_cols[0]].iloc[i+seq_len+pred_len-1]
                y.append(y_val)
                
                # ë¬¼ë¦¬ ë°ì´í„°
                physics_data = {
                    'current_hubroom': df[numeric_cols[0]].iloc[i+seq_len-1],
                    'inflow_sum': df[available_inflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_inflow else 0,
                    'outflow_sum': df[available_outflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_outflow else 0,
                }
                X_physics.append(physics_data)
                
                pbar.update(1)
                
                # 100ê°œë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if (i + 1) % 100 == 0:
                    self.checkpoint_manager.save_data_progress({
                        'X': X, 'y': y, 'X_physics': X_physics,
                        'progress': i + 1
                    })
                
                # ì¸í„°ëŸ½íŠ¸ í™•ì¸
                if self.checkpoint_manager.interrupted:
                    print(f"\nğŸ’¾ ì¤‘ë‹¨ ì§€ì  ì €ì¥: {i+1}/{total_sequences}")
                    self.checkpoint_manager.save_data_progress({
                        'X': X, 'y': y, 'X_physics': X_physics,
                        'progress': i + 1
                    })
                    sys.exit(0)
        
        print(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {len(X)}ê°œ")
        
        return np.array(X), np.array(y), X_physics

# ===========================
# ğŸ§  ëª¨ë¸ ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼)
# ===========================

class PatchTST(nn.Module):
    """PatchTST: íŒ¨ì¹˜ ê¸°ë°˜ ì‹œê³„ì—´ Transformer"""
    
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        self.d_model = config['d_model']
        self.n_heads = config['n_heads']
        self.d_ff = config['d_ff']
        self.n_layers = config['n_layers']
        self.dropout = config['dropout']
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = nn.Linear(
            self.patch_len * self.n_features, 
            self.d_model
        )
        
        # ìœ„ì¹˜ ì¸ì½”ë”©
        self.pos_embedding = nn.Parameter(
            torch.randn(1, self.n_patches, self.d_model)
        )
        
        # Transformer ì¸ì½”ë”
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model,
            nhead=self.n_heads,
            dim_feedforward=self.d_ff,
            dropout=self.dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=self.n_layers
        )
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.flatten = nn.Flatten()
        self.output_layer = nn.Sequential(
            nn.Linear(self.d_model * self.n_patches, 128),
            nn.ReLU(),
            nn.Dropout(self.dropout),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
    def create_patches(self, x):
        batch_size = x.shape[0]
        x = x.unfold(dimension=1, size=self.patch_len, step=self.patch_len)
        x = x.permute(0, 1, 3, 2)
        x = x.contiguous().view(batch_size, self.n_patches, -1)
        return x
    
    def forward(self, x):
        x_patches = self.create_patches(x)
        x_embed = self.patch_embedding(x_patches)
        x_embed = x_embed + self.pos_embedding
        x_transformed = self.transformer(x_embed)
        x_flat = self.flatten(x_transformed)
        output = self.output_layer(x_flat)
        return output.squeeze(-1)

# ===========================
# ğŸ“ˆ ê°œì„ ëœ í•™ìŠµ í´ë˜ìŠ¤
# ===========================

class ImprovedTrainer:
    """ì²´í¬í¬ì¸íŠ¸ì™€ ì§„í–‰ë¥  í‘œì‹œê°€ ì¶”ê°€ëœ í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, model, model_name, device='cpu'):
        self.model = model.to(device)
        self.model_name = model_name
        self.device = device
        self.history = {'train_loss': [], 'val_loss': []}
        self.checkpoint_manager = CheckpointManager()
        self.best_val_loss = float('inf')
        
    def train_with_checkpoint(self, train_loader, val_loader, epochs=50, lr=0.001, resume=False):
        """ì²´í¬í¬ì¸íŠ¸ ì§€ì› í•™ìŠµ"""
        
        optimizer = optim.Adam(self.model.parameters(), lr=lr)
        criterion = nn.MSELoss()
        
        start_epoch = 0
        
        # ì²´í¬í¬ì¸íŠ¸ ë³µì›
        if resume:
            checkpoint = self.checkpoint_manager.load_checkpoint(f'{self.model_name}_checkpoint.pth')
            if checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                start_epoch = checkpoint['epoch']
                self.history = checkpoint['history']
                self.best_val_loss = checkpoint['best_val_loss']
                print(f"â™»ï¸ í•™ìŠµ ì¬ê°œ: Epoch {start_epoch}ë¶€í„° ì‹œì‘")
        
        print(f"\nğŸš€ {self.model_name} í•™ìŠµ ì‹œì‘...")
        print("="*60)
        
        for epoch in range(start_epoch, epochs):
            # Training
            self.model.train()
            train_losses = []
            
            train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')
            for batch_x, batch_y, _ in train_pbar:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                optimizer.zero_grad()
                pred = self.model(batch_x)
                loss = criterion(pred, batch_y)
                loss.backward()
                optimizer.step()
                
                train_losses.append(loss.item())
                train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})
            
            # Validation
            self.model.eval()
            val_losses = []
            
            with torch.no_grad():
                val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Valid]')
                for batch_x, batch_y, _ in val_pbar:
                    batch_x = batch_x.to(self.device)
                    batch_y = batch_y.to(self.device)
                    
                    pred = self.model(batch_x)
                    loss = criterion(pred, batch_y)
                    val_losses.append(loss.item())
                    val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})
            
            avg_train_loss = np.mean(train_losses)
            avg_val_loss = np.mean(val_losses)
            
            self.history['train_loss'].append(avg_train_loss)
            self.history['val_loss'].append(avg_val_loss)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"Epoch [{epoch+1}/{epochs}] "
                  f"Train Loss: {avg_train_loss:.4f} | "
                  f"Val Loss: {avg_val_loss:.4f} | "
                  f"Best: {self.best_val_loss:.4f}")
            
            # ìµœê³  ëª¨ë¸ ì €ì¥
            if avg_val_loss < self.best_val_loss:
                self.best_val_loss = avg_val_loss
                self.checkpoint_manager.save_checkpoint({
                    'epoch': epoch + 1,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_val_loss': self.best_val_loss,
                    'history': self.history
                }, f'{self.model_name}_best.pth')
                print(f"  ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±!")
            
            # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸
            if (epoch + 1) % 5 == 0:
                self.checkpoint_manager.save_checkpoint({
                    'epoch': epoch + 1,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_val_loss': self.best_val_loss,
                    'history': self.history
                }, f'{self.model_name}_checkpoint.pth')
            
            # ì¸í„°ëŸ½íŠ¸ í™•ì¸
            if self.checkpoint_manager.interrupted:
                print(f"\nğŸ’¾ í•™ìŠµ ì¤‘ë‹¨, ì²´í¬í¬ì¸íŠ¸ ì €ì¥...")
                self.checkpoint_manager.save_checkpoint({
                    'epoch': epoch + 1,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_val_loss': self.best_val_loss,
                    'history': self.history
                }, f'{self.model_name}_interrupted.pth')
                sys.exit(0)
        
        print("âœ… í•™ìŠµ ì™„ë£Œ!")
        return self.history

# ===========================
# ğŸ¯ ë©”ì¸ ì‹¤í–‰
# ===========================

def main_with_progress():
    """ì§„í–‰ë¥  í‘œì‹œì™€ ì²´í¬í¬ì¸íŠ¸ê°€ ì¶”ê°€ëœ ë©”ì¸ í•¨ìˆ˜"""
    
    print("=" * 80)
    print("ğŸ­ HUBROOM ë°˜ì†¡ëŸ‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (ê°œì„ íŒ)")
    print("ğŸ’¡ íŠ¹ì§•: ì§„í–‰ë¥  í‘œì‹œ, ì¤‘ë‹¨/ì¬ê°œ ê°€ëŠ¥")
    print("=" * 80)
    
    # ë°ì´í„° ì²˜ë¦¬
    processor = ImprovedHUBROOMProcessor()
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ
        print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
        df = pd.read_csv(processor.file_path)
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape[0]:,} í–‰ Ã— {df.shape[1]} ì»¬ëŸ¼")
        
        # 2. ë°ì´í„° íŠ¹ì„± ë¶„ì„
        data_stats = processor.analyze_data(df)
        
        # 3. ì „ì²˜ë¦¬
        print("\nğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        time_col = df.columns[0]
        df['timestamp'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('timestamp').reset_index(drop=True)
        df = df.fillna(method='ffill').fillna(0)
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(numeric_cols)}ê°œ ìˆ«ìí˜• ì»¬ëŸ¼")
        
        # 4. ì‹œí€€ìŠ¤ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ í™•ì¸)
        saved_progress = processor.checkpoint_manager.load_data_progress()
        resume_from = saved_progress['progress'] if saved_progress else 0
        
        X, y, physics_data = processor.create_sequences_with_progress(
            df, numeric_cols, seq_len=20, pred_len=10, resume_from=resume_from
        )
        
        print(f"\nğŸ“Š ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
        print(f"  - X shape: {X.shape}")
        print(f"  - y shape: {y.shape}")
        
        # 5. ë°ì´í„° ë¶„í• 
        print("\nğŸ”€ ë°ì´í„° ë¶„í•  ì¤‘...")
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
        
        print(f"  - Train: {len(X_train):,} samples")
        print(f"  - Valid: {len(X_val):,} samples")
        print(f"  - Test: {len(X_test):,} samples")
        
        # 6. ì •ê·œí™”
        print("\nğŸ“ ë°ì´í„° ì •ê·œí™” ì¤‘...")
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()
        
        n_samples_train, seq_len, n_features = X_train.shape
        X_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, n_features)).reshape(n_samples_train, seq_len, n_features)
        X_val_scaled = scaler_X.transform(X_val.reshape(-1, n_features)).reshape(X_val.shape[0], seq_len, n_features)
        X_test_scaled = scaler_X.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape[0], seq_len, n_features)
        
        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        # 7. ë°ì´í„°ë¡œë” ìƒì„±
        class HUBROOMDataset(Dataset):
            def __init__(self, X, y):
                self.X = torch.FloatTensor(X)
                self.y = torch.FloatTensor(y)
                
            def __len__(self):
                return len(self.X)
            
            def __getitem__(self, idx):
                return self.X[idx], self.y[idx], None
        
        train_dataset = HUBROOMDataset(X_train_scaled, y_train_scaled)
        val_dataset = HUBROOMDataset(X_val_scaled, y_val_scaled)
        test_dataset = HUBROOMDataset(X_test_scaled, y_test_scaled)
        
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
        
        # 8. ëª¨ë¸ í•™ìŠµ
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"\nğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {device}")
        
        config = {
            'seq_len': 20,
            'n_features': n_features,
            'patch_len': 5,
            'd_model': 128,
            'n_heads': 8,
            'd_ff': 256,
            'n_layers': 3,
            'dropout': 0.1
        }
        
        # PatchTST ëª¨ë¸ í•™ìŠµ
        model = PatchTST(config)
        trainer = ImprovedTrainer(model, "PatchTST", device)
        
        # ì´ì „ í•™ìŠµ ì´ì–´ì„œ í•˜ê¸°
        resume_training = input("\nì´ì „ í•™ìŠµì„ ì´ì–´ì„œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y'
        
        history = trainer.train_with_checkpoint(
            train_loader, val_loader, 
            epochs=30, lr=0.001, 
            resume=resume_training
        )
        
        print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨ ê°ì§€")
        print("ğŸ’¾ ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ğŸ’¾ ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main_with_progress()