# -*- coding: utf-8 -*-
"""
HUBROOM ë°˜ì†¡ëŸ‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - TensorFlow 2.15 ë²„ì „
- PyTorch ì½”ë“œë¥¼ TensorFlowë¡œ ì™„ì „ ë³€í™˜
- PatchTST ëª¨ë¸
- PatchTST + PINN í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
- ì§„í–‰ë¥  í‘œì‹œ ë° ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import warnings
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import pickle
import os
import json
from tqdm import tqdm
import time
import signal
import sys

warnings.filterwarnings('ignore')

# TensorFlow ì„¤ì •
print(f"TensorFlow Version: {tf.__version__}")
tf.random.set_seed(42)

# ===========================
# ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
# ===========================

class CheckpointManager:
    """í•™ìŠµ ì¤‘ë‹¨/ì¬ê°œë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬"""
    
    def __init__(self, checkpoint_dir='./checkpoints'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        self.interrupted = False
        
        # Ctrl+C ì¸í„°ëŸ½íŠ¸ í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
    
    def _signal_handler(self, sig, frame):
        print('\n\nâš ï¸ í•™ìŠµ ì¤‘ë‹¨ ê°ì§€! ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì¤‘...')
        self.interrupted = True
    
    def save_checkpoint(self, model, filename):
        """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        filepath = os.path.join(self.checkpoint_dir, filename)
        model.save_weights(filepath)
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {filepath}")
    
    def load_checkpoint(self, model, filename):
        """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        filepath = os.path.join(self.checkpoint_dir, filename)
        if os.path.exists(filepath + '.index'):
            model.load_weights(filepath)
            print(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {filepath}")
            return True
        return False
    
    def save_data_progress(self, data_dict, filename='data_progress.pkl'):
        """ë°ì´í„° ì²˜ë¦¬ ì§„í–‰ ìƒí™© ì €ì¥"""
        filepath = os.path.join(self.checkpoint_dir, filename)
        with open(filepath, 'wb') as f:
            pickle.dump(data_dict, f)
        print(f"ğŸ’¾ ë°ì´í„° ì§„í–‰ ìƒí™© ì €ì¥: {filepath}")
    
    def load_data_progress(self, filename='data_progress.pkl'):
        """ë°ì´í„° ì²˜ë¦¬ ì§„í–‰ ìƒí™© ë¡œë“œ"""
        filepath = os.path.join(self.checkpoint_dir, filename)
        if os.path.exists(filepath):
            print(f"ğŸ“‚ ë°ì´í„° ì§„í–‰ ìƒí™© ë¡œë“œ: {filepath}")
            with open(filepath, 'rb') as f:
                return pickle.load(f)
        return None

# ===========================
# ğŸ“Š ê°œì„ ëœ ë°ì´í„° ì²˜ë¦¬
# ===========================

class ImprovedHUBROOMProcessor:
    """ì§„í–‰ë¥  í‘œì‹œê°€ ì¶”ê°€ëœ ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, file_path='data/HUB_0509_TO_0730_DATA.CSV'):
        self.file_path = file_path
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        self.checkpoint_manager = CheckpointManager()
        
        # ë¬¼ë¦¬ì  ê³„ì‚°ì„ ìœ„í•œ ì»¬ëŸ¼ ê·¸ë£¹
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2', 'M14B_7F_TO_HUB_JOB2'
        ]
        
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB', 'M16A_3F_TO_M14B_7F_JOB'
        ]
    
    def analyze_data(self, df):
        """ë°ì´í„° íŠ¹ì„± ìƒì„¸ ë¶„ì„"""
        print("\n" + "="*60)
        print("ğŸ“ˆ ë°ì´í„° íŠ¹ì„± ë¶„ì„")
        print("="*60)
        
        total_steps = 8
        current_step = 0
        
        # 1. ê¸°ë³¸ ì •ë³´
        current_step += 1
        print(f"\n[{current_step}/{total_steps}] ê¸°ë³¸ ì •ë³´ ë¶„ì„... ({current_step/total_steps*100:.0f}%)")
        print(f"  - ì „ì²´ í–‰ ìˆ˜: {df.shape[0]:,}")
        print(f"  - ì „ì²´ ì»¬ëŸ¼ ìˆ˜: {df.shape[1]}")
        print(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage().sum() / 1024**2:.2f} MB")
        
        # 2. ê²°ì¸¡ì¹˜ ë¶„ì„
        current_step += 1
        print(f"\n[{current_step}/{total_steps}] ê²°ì¸¡ì¹˜ ë¶„ì„... ({current_step/total_steps*100:.0f}%)")
        missing_ratio = (df.isnull().sum() / len(df)) * 100
        missing_cols = missing_ratio[missing_ratio > 0]
        if len(missing_cols) > 0:
            print(f"  - ê²°ì¸¡ì¹˜ ìˆëŠ” ì»¬ëŸ¼: {len(missing_cols)}ê°œ")
            for col, ratio in missing_cols.head(5).items():
                print(f"    â€¢ {col}: {ratio:.2f}%")
        else:
            print("  - âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ")
        
        # 3. íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ì„
        current_step += 1
        print(f"\n[{current_step}/{total_steps}] íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ì„... ({current_step/total_steps*100:.0f}%)")
        if self.target_col in df.columns:
            target = df[self.target_col]
            print(f"  - íƒ€ê²Ÿ: {self.target_col}")
            print(f"  - í‰ê· : {target.mean():.2f}")
            print(f"  - í‘œì¤€í¸ì°¨: {target.std():.2f}")
            print(f"  - ìµœì†Œ/ìµœëŒ€: {target.min():.0f} / {target.max():.0f}")
            print(f"  - ë¶„í¬: Q1={target.quantile(0.25):.0f}, Q2={target.median():.0f}, Q3={target.quantile(0.75):.0f}")
        
        # 4. ìœ ì…/ìœ ì¶œ ë¶„ì„
        current_step += 1
        print(f"\n[{current_step}/{total_steps}] ìœ ì…/ìœ ì¶œ ê· í˜• ë¶„ì„... ({current_step/total_steps*100:.0f}%)")
        available_inflow = [col for col in self.inflow_cols if col in df.columns]
        available_outflow = [col for col in self.outflow_cols if col in df.columns]
        
        if available_inflow and available_outflow:
            total_inflow = df[available_inflow].sum().sum()
            total_outflow = df[available_outflow].sum().sum()
            balance = total_inflow - total_outflow
            print(f"  - ì´ ìœ ì…: {total_inflow:,.0f}")
            print(f"  - ì´ ìœ ì¶œ: {total_outflow:,.0f}")
            print(f"  - ìˆœ ë³€í™”: {balance:+,.0f} ({balance/total_inflow*100:+.1f}%)")
        
        # ë‚˜ë¨¸ì§€ ë¶„ì„ë“¤...
        print("\n" + "="*60)
        
        return {
            'total_rows': df.shape[0],
            'total_cols': df.shape[1],
            'missing_ratio': missing_ratio.mean()
        }
    
    def create_sequences_with_progress(self, df, numeric_cols, seq_len=20, pred_len=10, resume_from=0):
        """ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ì‹œí€€ìŠ¤ ìƒì„±"""
        
        print("\nğŸ“¦ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì¤‘...")
        
        X, y, X_physics = [], [], []
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì „ ì§„í–‰ ìƒí™© ë¡œë“œ
        saved_progress = self.checkpoint_manager.load_data_progress()
        if saved_progress and resume_from > 0:
            print(f"â™»ï¸ ì´ì „ ì§„í–‰ ìƒí™© ë³µì›: {resume_from}ê°œ ì™„ë£Œ")
            X = saved_progress['X']
            y = saved_progress['y']
            X_physics = saved_progress['X_physics']
        
        available_inflow = [col for col in self.inflow_cols if col in df.columns]
        available_outflow = [col for col in self.outflow_cols if col in df.columns]
        
        total_sequences = len(df) - seq_len - pred_len + 1
        
        # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
        with tqdm(total=total_sequences, desc="ì‹œí€€ìŠ¤ ìƒì„±", initial=resume_from) as pbar:
            for i in range(resume_from, total_sequences):
                # ì…ë ¥: ê³¼ê±° 20ë¶„
                X_seq = df[numeric_cols].iloc[i:i+seq_len].values
                X.append(X_seq)
                
                # íƒ€ê²Ÿ: 10ë¶„ í›„
                if self.target_col in df.columns:
                    y_val = df[self.target_col].iloc[i+seq_len+pred_len-1]
                else:
                    y_val = df[numeric_cols[0]].iloc[i+seq_len+pred_len-1]
                y.append(y_val)
                
                # ë¬¼ë¦¬ ë°ì´í„°
                physics_data = {
                    'current_hubroom': df[numeric_cols[0]].iloc[i+seq_len-1],
                    'inflow_sum': df[available_inflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_inflow else 0,
                    'outflow_sum': df[available_outflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_outflow else 0,
                }
                X_physics.append(physics_data)
                
                pbar.update(1)
                
                # 100ê°œë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if (i + 1) % 100 == 0:
                    self.checkpoint_manager.save_data_progress({
                        'X': X, 'y': y, 'X_physics': X_physics,
                        'progress': i + 1
                    })
                
                # ì¸í„°ëŸ½íŠ¸ í™•ì¸
                if self.checkpoint_manager.interrupted:
                    print(f"\nğŸ’¾ ì¤‘ë‹¨ ì§€ì  ì €ì¥: {i+1}/{total_sequences}")
                    self.checkpoint_manager.save_data_progress({
                        'X': X, 'y': y, 'X_physics': X_physics,
                        'progress': i + 1
                    })
                    sys.exit(0)
        
        print(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {len(X)}ê°œ")
        
        return np.array(X), np.array(y), X_physics

# ===========================
# ğŸ§  PatchTST ëª¨ë¸ (TensorFlow)
# ===========================

class PatchTST(keras.Model):
    """PatchTST: íŒ¨ì¹˜ ê¸°ë°˜ ì‹œê³„ì—´ Transformer"""
    
    def __init__(self, config):
        super(PatchTST, self).__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        self.d_model = config['d_model']
        self.n_heads = config['n_heads']
        self.d_ff = config['d_ff']
        self.n_layers = config['n_layers']
        self.dropout = config['dropout']
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(self.d_model)
        
        # ìœ„ì¹˜ ì¸ì½”ë”©
        self.pos_embedding = self.add_weight(
            name='pos_embedding',
            shape=(1, self.n_patches, self.d_model),
            initializer='random_normal',
            trainable=True
        )
        
        # Transformer ì¸ì½”ë” ë ˆì´ì–´ë“¤
        self.encoder_layers = []
        for _ in range(self.n_layers):
            self.encoder_layers.append(
                layers.TransformerEncoderLayer(
                    d_model=self.d_model,
                    num_heads=self.n_heads,
                    dff=self.d_ff,
                    dropout=self.dropout
                )
            )
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.flatten = layers.Flatten()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dropout_layer = layers.Dropout(self.dropout)
        self.dense2 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(1)
    
    def create_patches(self, x):
        """ì…ë ¥ì„ íŒ¨ì¹˜ë¡œ ë¶„í• """
        batch_size = tf.shape(x)[0]
        
        # (batch, seq_len, features) -> (batch, n_patches, patch_len, features)
        x_reshaped = tf.reshape(x, [batch_size, self.n_patches, self.patch_len, self.n_features])
        
        # (batch, n_patches, patch_len * features)
        x_patches = tf.reshape(x_reshaped, [batch_size, self.n_patches, self.patch_len * self.n_features])
        
        return x_patches
    
    def call(self, x, training=False):
        # íŒ¨ì¹˜ ìƒì„±
        x_patches = self.create_patches(x)
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x_embed = self.patch_embedding(x_patches)
        
        # ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        x_embed = x_embed + self.pos_embedding
        
        # Transformer ì¸ì½”ë” í†µê³¼
        for encoder_layer in self.encoder_layers:
            x_embed = encoder_layer(x_embed, training=training)
        
        # ì¶œë ¥ ìƒì„±
        x_flat = self.flatten(x_embed)
        x = self.dense1(x_flat)
        x = self.dropout_layer(x, training=training)
        x = self.dense2(x)
        output = self.output_layer(x)
        
        return tf.squeeze(output, axis=-1)

# ===========================
# ğŸ”¬ PatchTST + PINN í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
# ===========================

class PatchTST_PINN_Hybrid(keras.Model):
    """PatchTSTì™€ Physics-Informed Neural Network ê²°í•© ëª¨ë¸"""
    
    def __init__(self, config):
        super(PatchTST_PINN_Hybrid, self).__init__()
        
        # PatchTST ì»´í¬ë„ŒíŠ¸
        self.patch_tst = PatchTST(config)
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬ ë„¤íŠ¸ì›Œí¬
        self.physics_dense1 = layers.Dense(32, activation='relu')
        self.physics_dense2 = layers.Dense(16, activation='relu')
        
        # ê²°í•© ë ˆì´ì–´
        self.combine_dense1 = layers.Dense(64, activation='relu')
        self.combine_dense2 = layers.Dense(32, activation='relu')
        self.final_output = layers.Dense(1)
        
        self.dropout = layers.Dropout(config['dropout'])
    
    def call(self, inputs, training=False):
        # inputs = [x_seq, x_physics]
        x_seq, x_physics = inputs
        
        # PatchTST ì¶œë ¥
        tst_output = self.patch_tst(x_seq, training=training)
        tst_output = tf.expand_dims(tst_output, axis=-1)
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬
        physics_output = self.physics_dense1(x_physics)
        physics_output = self.dropout(physics_output, training=training)
        physics_output = self.physics_dense2(physics_output)
        
        # ê²°í•©
        combined = tf.concat([tst_output, physics_output], axis=-1)
        x = self.combine_dense1(combined)
        x = self.dropout(x, training=training)
        x = self.combine_dense2(x)
        output = self.final_output(x)
        
        return tf.squeeze(output, axis=-1)

# ===========================
# ğŸ“ˆ ê°œì„ ëœ í•™ìŠµ í´ë˜ìŠ¤
# ===========================

class ImprovedTrainer:
    """ì²´í¬í¬ì¸íŠ¸ì™€ ì§„í–‰ë¥  í‘œì‹œê°€ ì¶”ê°€ëœ í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, model, model_name):
        self.model = model
        self.model_name = model_name
        self.checkpoint_manager = CheckpointManager()
        self.history = None
    
    def create_callbacks(self):
        """ì½œë°± ìƒì„±"""
        callbacks_list = [
            ModelCheckpoint(
                filepath=os.path.join(self.checkpoint_manager.checkpoint_dir, 
                                     f'{self.model_name}_best.h5'),
                monitor='val_loss',
                save_best_only=True,
                save_weights_only=True,
                verbose=1
            ),
            EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-6,
                verbose=1
            )
        ]
        return callbacks_list
    
    def train_with_checkpoint(self, X_train, y_train, X_val, y_val, 
                             epochs=50, batch_size=32, lr=0.001, resume=False):
        """ì²´í¬í¬ì¸íŠ¸ ì§€ì› í•™ìŠµ"""
        
        # ëª¨ë¸ ì»´íŒŒì¼
        self.model.compile(
            optimizer=Adam(learning_rate=lr),
            loss='mse',
            metrics=['mae']
        )
        
        # ì²´í¬í¬ì¸íŠ¸ ë³µì›
        initial_epoch = 0
        if resume:
            if self.checkpoint_manager.load_checkpoint(self.model, f'{self.model_name}_checkpoint'):
                print(f"â™»ï¸ í•™ìŠµ ì¬ê°œ")
                # ì—í¬í¬ ì •ë³´ëŠ” ë³„ë„ íŒŒì¼ì—ì„œ ë¡œë“œ í•„ìš”
                try:
                    with open(os.path.join(self.checkpoint_manager.checkpoint_dir, 
                                          f'{self.model_name}_epoch.txt'), 'r') as f:
                        initial_epoch = int(f.read())
                        print(f"  Epoch {initial_epoch}ë¶€í„° ì‹œì‘")
                except:
                    pass
        
        print(f"\nğŸš€ {self.model_name} í•™ìŠµ ì‹œì‘...")
        print("="*60)
        
        # í•™ìŠµ
        self.history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            initial_epoch=initial_epoch,
            batch_size=batch_size,
            callbacks=self.create_callbacks(),
            verbose=1
        )
        
        # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        self.checkpoint_manager.save_checkpoint(self.model, f'{self.model_name}_final')
        
        print("âœ… í•™ìŠµ ì™„ë£Œ!")
        return self.history
    
    def evaluate(self, X_test, y_test, scaler_y=None):
        """ëª¨ë¸ í‰ê°€"""
        print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        # ì˜ˆì¸¡
        y_pred_scaled = self.model.predict(X_test, verbose=0)
        
        # ì—­ì •ê·œí™”
        if scaler_y:
            y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
            y_true = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()
        else:
            y_pred = y_pred_scaled
            y_true = y_test
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        mae = np.mean(np.abs(y_true - y_pred))
        mse = np.mean((y_true - y_pred) ** 2)
        rmse = np.sqrt(mse)
        
        print(f"  - MAE: {mae:.4f}")
        print(f"  - MSE: {mse:.4f}")
        print(f"  - RMSE: {rmse:.4f}")
        
        return {'mae': mae, 'mse': mse, 'rmse': rmse, 'y_true': y_true, 'y_pred': y_pred}

# ===========================
# ğŸ¯ ë©”ì¸ ì‹¤í–‰
# ===========================

def main_with_progress():
    """ì§„í–‰ë¥  í‘œì‹œì™€ ì²´í¬í¬ì¸íŠ¸ê°€ ì¶”ê°€ëœ ë©”ì¸ í•¨ìˆ˜"""
    
    print("=" * 80)
    print("ğŸ­ HUBROOM ë°˜ì†¡ëŸ‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (TensorFlow 2.15)")
    print("ğŸ’¡ íŠ¹ì§•: ì§„í–‰ë¥  í‘œì‹œ, ì¤‘ë‹¨/ì¬ê°œ ê°€ëŠ¥")
    print("=" * 80)
    
    # ë°ì´í„° ì²˜ë¦¬
    processor = ImprovedHUBROOMProcessor()
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ
        print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
        df = pd.read_csv(processor.file_path)
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape[0]:,} í–‰ Ã— {df.shape[1]} ì»¬ëŸ¼")
        
        # 2. ë°ì´í„° íŠ¹ì„± ë¶„ì„
        data_stats = processor.analyze_data(df)
        
        # 3. ì „ì²˜ë¦¬
        print("\nğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        time_col = df.columns[0]
        df['timestamp'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('timestamp').reset_index(drop=True)
        df = df.fillna(method='ffill').fillna(0)
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(numeric_cols)}ê°œ ìˆ«ìí˜• ì»¬ëŸ¼")
        
        # 4. ì‹œí€€ìŠ¤ ìƒì„±
        saved_progress = processor.checkpoint_manager.load_data_progress()
        resume_from = saved_progress['progress'] if saved_progress else 0
        
        X, y, physics_data = processor.create_sequences_with_progress(
            df, numeric_cols, seq_len=20, pred_len=10, resume_from=resume_from
        )
        
        print(f"\nğŸ“Š ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
        print(f"  - X shape: {X.shape}")
        print(f"  - y shape: {y.shape}")
        
        # 5. ë°ì´í„° ë¶„í• 
        print("\nğŸ”€ ë°ì´í„° ë¶„í•  ì¤‘...")
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
        
        # ë¬¼ë¦¬ ë°ì´í„°ë„ ë¶„í• 
        physics_train = physics_data[:len(X_train)]
        physics_val = physics_data[len(X_train):len(X_train)+len(X_val)]
        physics_test = physics_data[len(X_train)+len(X_val):]
        
        print(f"  - Train: {len(X_train):,} samples")
        print(f"  - Valid: {len(X_val):,} samples")
        print(f"  - Test: {len(X_test):,} samples")
        
        # 6. ì •ê·œí™”
        print("\nğŸ“ ë°ì´í„° ì •ê·œí™” ì¤‘...")
        n_samples_train, seq_len, n_features = X_train.shape
        
        X_train_scaled = processor.scaler_X.fit_transform(X_train.reshape(-1, n_features)).reshape(n_samples_train, seq_len, n_features)
        X_val_scaled = processor.scaler_X.transform(X_val.reshape(-1, n_features)).reshape(X_val.shape[0], seq_len, n_features)
        X_test_scaled = processor.scaler_X.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape[0], seq_len, n_features)
        
        y_train_scaled = processor.scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = processor.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        y_test_scaled = processor.scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        # 7. ëª¨ë¸ ì„ íƒ
        print("\nğŸ¤– ëª¨ë¸ ì„ íƒ")
        print("1. PatchTST")
        print("2. PatchTST + PINN í•˜ì´ë¸Œë¦¬ë“œ")
        model_choice = input("ì„ íƒ (1 ë˜ëŠ” 2): ")
        
        config = {
            'seq_len': 20,
            'n_features': n_features,
            'patch_len': 5,
            'd_model': 128,
            'n_heads': 8,
            'd_ff': 256,
            'n_layers': 3,
            'dropout': 0.1
        }
        
        if model_choice == '2':
            # ë¬¼ë¦¬ ë°ì´í„° ì¤€ë¹„
            X_physics_train = np.array([[p['current_hubroom'], p['inflow_sum'], p['outflow_sum']] 
                                       for p in physics_train])
            X_physics_val = np.array([[p['current_hubroom'], p['inflow_sum'], p['outflow_sum']] 
                                     for p in physics_val])
            X_physics_test = np.array([[p['current_hubroom'], p['inflow_sum'], p['outflow_sum']] 
                                      for p in physics_test])
            
            # ë¬¼ë¦¬ ë°ì´í„° ì •ê·œí™”
            scaler_physics = StandardScaler()
            X_physics_train_scaled = scaler_physics.fit_transform(X_physics_train)
            X_physics_val_scaled = scaler_physics.transform(X_physics_val)
            X_physics_test_scaled = scaler_physics.transform(X_physics_test)
            
            # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
            model = PatchTST_PINN_Hybrid(config)
            trainer = ImprovedTrainer(model, "PatchTST_PINN")
            
            # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
            train_data = [X_train_scaled, X_physics_train_scaled]
            val_data = [X_val_scaled, X_physics_val_scaled]
            test_data = [X_test_scaled, X_physics_test_scaled]
        else:
            # ê¸°ë³¸ PatchTST ëª¨ë¸
            model = PatchTST(config)
            trainer = ImprovedTrainer(model, "PatchTST")
            
            train_data = X_train_scaled
            val_data = X_val_scaled
            test_data = X_test_scaled
        
        # 8. í•™ìŠµ
        resume_training = input("\nì´ì „ í•™ìŠµì„ ì´ì–´ì„œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y'
        
        history = trainer.train_with_checkpoint(
            train_data, y_train_scaled,
            val_data, y_val_scaled,
            epochs=30,
            batch_size=32,
            lr=0.001,
            resume=resume_training
        )
        
        # 9. í‰ê°€
        results = trainer.evaluate(test_data, y_test_scaled, processor.scaler_y)
        
        print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨ ê°ì§€")
        print("ğŸ’¾ ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ğŸ’¾ ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main_with_progress()