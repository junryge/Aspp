# -*- coding: utf-8 -*-
"""
HUBROOM ë°˜ì†¡ëŸ‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - TensorFlow 2.15 ì™„ì „íŒ
- ì§„í–‰ë¥  í‘œì‹œ (%)
- ì¤‘ë‹¨/ì¬ê°œ ê¸°ëŠ¥
- ë°ì´í„° íŠ¹ì„± ìƒì„¸ ë¶„ì„
- Model 1: PatchTST
- Model 2: PatchTST + PINN
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model
from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
from datetime import datetime
import os
import sys
import json
import pickle
from tqdm import tqdm
import signal
import time
import warnings

warnings.filterwarnings('ignore')

# TensorFlow ì„¤ì •
print(f"ğŸ”§ TensorFlow ë²„ì „: {tf.__version__}")
tf.random.set_seed(42)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# ===========================
# ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
# ===========================

class CheckpointManager:
    """í•™ìŠµ ì¤‘ë‹¨/ì¬ê°œë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬"""
    
    def __init__(self, checkpoint_dir='./checkpoints'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        self.interrupted = False
        
        # Ctrl+C ì¸í„°ëŸ½íŠ¸ í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
        
        # ì§„í–‰ ìƒí™© íŒŒì¼
        self.progress_file = os.path.join(checkpoint_dir, 'progress.json')
        self.data_file = os.path.join(checkpoint_dir, 'processed_data.pkl')
    
    def _signal_handler(self, sig, frame):
        print('\n\nâš ï¸ ì¤‘ë‹¨ ê°ì§€! ì§„í–‰ ìƒí™© ì €ì¥ ì¤‘...')
        self.interrupted = True
        
    def save_progress(self, stage, progress, total, details=None):
        """ì§„í–‰ ìƒí™© ì €ì¥"""
        progress_data = {
            'stage': stage,
            'progress': progress,
            'total': total,
            'percentage': (progress/total*100) if total > 0 else 0,
            'timestamp': datetime.now().isoformat(),
            'details': details or {}
        }
        
        with open(self.progress_file, 'w') as f:
            json.dump(progress_data, f, indent=2)
        
        return progress_data
    
    def load_progress(self):
        """ì§„í–‰ ìƒí™© ë¡œë“œ"""
        if os.path.exists(self.progress_file):
            with open(self.progress_file, 'r') as f:
                return json.load(f)
        return None
    
    def save_data(self, data_dict):
        """ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        with open(self.data_file, 'wb') as f:
            pickle.dump(data_dict, f)
        print(f"ğŸ’¾ ë°ì´í„° ì €ì¥: {self.data_file}")
    
    def load_data(self):
        """ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
        if os.path.exists(self.data_file):
            print(f"ğŸ“‚ ì €ì¥ëœ ë°ì´í„° ë¡œë“œ: {self.data_file}")
            with open(self.data_file, 'rb') as f:
                return pickle.load(f)
        return None

# ===========================
# ğŸ“Š ë°ì´í„° ë¶„ì„ ë° ì²˜ë¦¬
# ===========================

class DataAnalyzer:
    """ë°ì´í„° íŠ¹ì„± ìƒì„¸ ë¶„ì„"""
    
    def __init__(self):
        self.analysis_results = {}
        
    def analyze_with_progress(self, df, target_col='CURRENT_M16A_3F_JOB_2'):
        """ì§„í–‰ë¥ ê³¼ í•¨ê»˜ ë°ì´í„° ë¶„ì„"""
        
        print("\n" + "="*70)
        print("ğŸ“ˆ ë°ì´í„° íŠ¹ì„± ìƒì„¸ ë¶„ì„ ì‹œì‘")
        print("="*70)
        
        stages = [
            ("ê¸°ë³¸ ì •ë³´", self._analyze_basic_info),
            ("ê²°ì¸¡ì¹˜ ë¶„ì„", self._analyze_missing),
            ("íƒ€ê²Ÿ ë³€ìˆ˜", self._analyze_target),
            ("ì‹œê°„ íŒ¨í„´", self._analyze_temporal),
            ("í†µê³„ ë¶„í¬", self._analyze_statistics),
            ("ìƒê´€ê´€ê³„", self._analyze_correlation),
            ("ì´ìƒì¹˜ íƒì§€", self._analyze_outliers),
            ("ë°ì´í„° í’ˆì§ˆ", self._analyze_quality)
        ]
        
        total_stages = len(stages)
        
        for i, (stage_name, analyze_func) in enumerate(stages, 1):
            # ì§„í–‰ë¥  í‘œì‹œ
            progress_pct = (i / total_stages) * 100
            print(f"\n[{i}/{total_stages}] {stage_name} ë¶„ì„ ì¤‘... ({progress_pct:.0f}%)")
            print("-" * 50)
            
            # ë¶„ì„ ì‹¤í–‰
            result = analyze_func(df, target_col)
            self.analysis_results[stage_name] = result
            
            # ì§„í–‰ ë°”
            self._show_progress_bar(i, total_stages)
            
            time.sleep(0.1)  # ì‹œê°ì  íš¨ê³¼
        
        print("\n" + "="*70)
        print("âœ… ë°ì´í„° ë¶„ì„ ì™„ë£Œ!")
        print("="*70)
        
        return self.analysis_results
    
    def _show_progress_bar(self, current, total):
        """ì§„í–‰ ë°” í‘œì‹œ"""
        bar_length = 40
        filled = int(bar_length * current / total)
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
        percent = current / total * 100
        print(f"ì „ì²´ ì§„í–‰: [{bar}] {percent:.1f}%")
    
    def _analyze_basic_info(self, df, target_col):
        """ê¸°ë³¸ ì •ë³´ ë¶„ì„"""
        info = {
            'rows': len(df),
            'columns': len(df.columns),
            'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,
            'dtypes': df.dtypes.value_counts().to_dict()
        }
        
        print(f"  â€¢ ë°ì´í„° í¬ê¸°: {info['rows']:,} í–‰ Ã— {info['columns']} ì»¬ëŸ¼")
        print(f"  â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©: {info['memory_mb']:.2f} MB")
        print(f"  â€¢ ë°ì´í„° íƒ€ì…: {info['dtypes']}")
        
        return info
    
    def _analyze_missing(self, df, target_col):
        """ê²°ì¸¡ì¹˜ ë¶„ì„"""
        missing = df.isnull().sum()
        missing_pct = (missing / len(df)) * 100
        
        result = {
            'total_missing': missing.sum(),
            'columns_with_missing': (missing > 0).sum(),
            'missing_percentage': missing_pct.mean()
        }
        
        print(f"  â€¢ ì „ì²´ ê²°ì¸¡ì¹˜: {result['total_missing']:,}ê°œ")
        print(f"  â€¢ ê²°ì¸¡ ì»¬ëŸ¼: {result['columns_with_missing']}ê°œ")
        print(f"  â€¢ í‰ê·  ê²°ì¸¡ë¥ : {result['missing_percentage']:.2f}%")
        
        return result
    
    def _analyze_target(self, df, target_col):
        """íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ì„"""
        if target_col in df.columns:
            target = df[target_col].dropna()
            result = {
                'mean': target.mean(),
                'std': target.std(),
                'min': target.min(),
                'max': target.max(),
                'q25': target.quantile(0.25),
                'q50': target.quantile(0.50),
                'q75': target.quantile(0.75)
            }
            
            print(f"  â€¢ í‰ê· Â±í‘œì¤€í¸ì°¨: {result['mean']:.2f} Â± {result['std']:.2f}")
            print(f"  â€¢ ë²”ìœ„: [{result['min']:.0f}, {result['max']:.0f}]")
            print(f"  â€¢ ì‚¬ë¶„ìœ„ìˆ˜: Q1={result['q25']:.0f}, Q2={result['q50']:.0f}, Q3={result['q75']:.0f}")
        else:
            result = {'error': 'Target column not found'}
            print(f"  â€¢ âš ï¸ íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_col}' ì—†ìŒ")
        
        return result
    
    def _analyze_temporal(self, df, target_col):
        """ì‹œê°„ íŒ¨í„´ ë¶„ì„"""
        time_col = df.columns[0]  # ì²« ì»¬ëŸ¼ì´ ì‹œê°„
        
        result = {
            'start': str(df[time_col].iloc[0]),
            'end': str(df[time_col].iloc[-1]),
            'duration': len(df),
            'interval': 'minute'  # ì¶”ì •
        }
        
        print(f"  â€¢ ê¸°ê°„: {result['start']} ~ {result['end']}")
        print(f"  â€¢ ë°ì´í„° í¬ì¸íŠ¸: {result['duration']:,}ê°œ")
        
        return result
    
    def _analyze_statistics(self, df, target_col):
        """í†µê³„ ë¶„í¬ ë¶„ì„"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        stats = df[numeric_cols].describe()
        result = {
            'numeric_columns': len(numeric_cols),
            'mean_values': stats.loc['mean'].mean(),
            'std_values': stats.loc['std'].mean()
        }
        
        print(f"  â€¢ ìˆ«ìí˜• ì»¬ëŸ¼: {result['numeric_columns']}ê°œ")
        print(f"  â€¢ ì „ì²´ í‰ê· : {result['mean_values']:.2f}")
        
        return result
    
    def _analyze_correlation(self, df, target_col):
        """ìƒê´€ê´€ê³„ ë¶„ì„"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        if target_col in numeric_cols and len(numeric_cols) > 1:
            correlations = df[numeric_cols].corr()[target_col].sort_values(ascending=False)
            top_corr = correlations[1:6]  # íƒ€ê²Ÿ ì œì™¸ ìƒìœ„ 5ê°œ
            
            result = {
                'top_features': top_corr.index.tolist(),
                'correlations': top_corr.values.tolist()
            }
            
            print(f"  â€¢ íƒ€ê²Ÿê³¼ ìƒê´€ê´€ê³„ TOP 3:")
            for feat, corr in zip(result['top_features'][:3], result['correlations'][:3]):
                print(f"    - {feat}: {corr:.3f}")
        else:
            result = {'error': 'Correlation analysis failed'}
        
        return result
    
    def _analyze_outliers(self, df, target_col):
        """ì´ìƒì¹˜ íƒì§€"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns[:5]  # ì²˜ìŒ 5ê°œë§Œ
        
        outlier_counts = {}
        for col in numeric_cols:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = ((df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)).sum()
            outlier_counts[col] = outliers
        
        total_outliers = sum(outlier_counts.values())
        print(f"  â€¢ ì´ ì´ìƒì¹˜: {total_outliers}ê°œ")
        print(f"  â€¢ ì´ìƒì¹˜ ë¹„ìœ¨: {total_outliers/(len(df)*len(numeric_cols))*100:.2f}%")
        
        return {'outlier_counts': outlier_counts, 'total': total_outliers}
    
    def _analyze_quality(self, df, target_col):
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        score = 100.0
        
        # ê²°ì¸¡ì¹˜ ê°ì 
        missing_pct = df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100
        score -= min(30, missing_pct * 3)
        
        # ì¤‘ë³µ í–‰ ê°ì 
        duplicate_pct = df.duplicated().sum() / len(df) * 100
        score -= min(20, duplicate_pct * 2)
        
        # ë°ì´í„° í¬ê¸° ë³´ë„ˆìŠ¤
        if len(df) > 10000:
            score += 5
        
        score = max(0, min(100, score))
        
        grade = "A" if score >= 90 else "B" if score >= 80 else "C" if score >= 70 else "D" if score >= 60 else "F"
        
        print(f"  â€¢ í’ˆì§ˆ ì ìˆ˜: {score:.1f}/100 (ë“±ê¸‰: {grade})")
        
        if score >= 80:
            print("  â€¢ âœ… ìš°ìˆ˜í•œ ë°ì´í„° í’ˆì§ˆ")
        elif score >= 60:
            print("  â€¢ âš ï¸ ë³´í†µ ìˆ˜ì¤€, ì „ì²˜ë¦¬ ê¶Œì¥")
        else:
            print("  â€¢ âŒ ë‚®ì€ í’ˆì§ˆ, ì „ì²˜ë¦¬ í•„ìˆ˜")
        
        return {'score': score, 'grade': grade}

# ===========================
# ğŸ“¦ ë°ì´í„° ì²˜ë¦¬
# ===========================

class HUBROOMProcessor:
    """ë°ì´í„° ì²˜ë¦¬ ë° ì‹œí€€ìŠ¤ ìƒì„±"""
    
    def __init__(self):
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.checkpoint_mgr = CheckpointManager()
        self.analyzer = DataAnalyzer()
        
    def process_with_resume(self, file_path='data/HUB_0509_TO_0730_DATA.CSV'):
        """ì¤‘ë‹¨ ì§€ì ë¶€í„° ì¬ê°œ ê°€ëŠ¥í•œ ë°ì´í„° ì²˜ë¦¬"""
        
        # ì´ì „ ì§„í–‰ ìƒí™© í™•ì¸
        saved_progress = self.checkpoint_mgr.load_progress()
        saved_data = self.checkpoint_mgr.load_data()
        
        if saved_progress and saved_data:
            print(f"\nâ™»ï¸ ì´ì „ ì‘ì—… ë°œê²¬!")
            print(f"  â€¢ ë‹¨ê³„: {saved_progress['stage']}")
            print(f"  â€¢ ì§„í–‰ë¥ : {saved_progress['percentage']:.1f}%")
            
            resume = input("ì´ì–´ì„œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            if resume.lower() == 'y':
                return self._resume_from_checkpoint(saved_data, saved_progress)
        
        # ìƒˆë¡œ ì‹œì‘
        return self._process_from_scratch(file_path)
    
    def _process_from_scratch(self, file_path):
        """ì²˜ìŒë¶€í„° ë°ì´í„° ì²˜ë¦¬"""
        
        print("\n" + "="*70)
        print("ğŸš€ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        print("="*70)
        
        # Stage 1: ë°ì´í„° ë¡œë“œ
        print("\n[Stage 1/5] ë°ì´í„° ë¡œë“œ ì¤‘...")
        df = pd.read_csv(file_path)
        print(f"âœ… ë¡œë“œ ì™„ë£Œ: {len(df):,} í–‰")
        self.checkpoint_mgr.save_progress('load', 1, 1)
        
        # Stage 2: ë°ì´í„° ë¶„ì„
        print("\n[Stage 2/5] ë°ì´í„° ë¶„ì„ ì¤‘...")
        analysis = self.analyzer.analyze_with_progress(df)
        self.checkpoint_mgr.save_progress('analysis', 1, 1, analysis)
        
        # Stage 3: ì „ì²˜ë¦¬
        print("\n[Stage 3/5] ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        df = self._preprocess_data(df)
        self.checkpoint_mgr.save_progress('preprocess', 1, 1)
        
        # Stage 4: ì‹œí€€ìŠ¤ ìƒì„±
        print("\n[Stage 4/5] ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        sequences = self._create_sequences_with_progress(df)
        self.checkpoint_mgr.save_progress('sequences', 1, 1)
        
        # Stage 5: ë°ì´í„° ë¶„í•  ë° ì •ê·œí™”
        print("\n[Stage 5/5] ë°ì´í„° ë¶„í•  ë° ì •ê·œí™” ì¤‘...")
        final_data = self._split_and_scale(sequences)
        self.checkpoint_mgr.save_progress('complete', 1, 1)
        
        # ìµœì¢… ë°ì´í„° ì €ì¥
        self.checkpoint_mgr.save_data(final_data)
        
        print("\n" + "="*70)
        print("âœ… ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ!")
        print("="*70)
        
        return final_data
    
    def _preprocess_data(self, df):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        # ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬
        time_col = df.columns[0]
        df['timestamp'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M', errors='coerce')
        
        # ì •ë ¬ ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df = df.sort_values('timestamp').reset_index(drop=True)
        df = df.fillna(method='ffill').fillna(0)
        
        return df
    
    def _create_sequences_with_progress(self, df, seq_len=20, pred_len=10):
        """ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ì‹œí€€ìŠ¤ ìƒì„±"""
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        total = len(df) - seq_len - pred_len + 1
        
        X, y = [], []
        
        with tqdm(total=total, desc="ì‹œí€€ìŠ¤ ìƒì„±") as pbar:
            for i in range(total):
                # 20ë¶„ ì…ë ¥
                X_seq = df[numeric_cols].iloc[i:i+seq_len].values
                X.append(X_seq)
                
                # 10ë¶„ í›„ íƒ€ê²Ÿ
                y_val = df[numeric_cols[0]].iloc[i+seq_len+pred_len-1]
                y.append(y_val)
                
                pbar.update(1)
                
                # 100ê°œë§ˆë‹¤ ì €ì¥
                if (i + 1) % 100 == 0:
                    self.checkpoint_mgr.save_progress('sequences', i+1, total)
                
                # ì¤‘ë‹¨ í™•ì¸
                if self.checkpoint_mgr.interrupted:
                    print(f"\nğŸ’¾ ì¤‘ë‹¨! ì§„í–‰ ìƒí™© ì €ì¥: {i+1}/{total}")
                    temp_data = {'X': X, 'y': y, 'progress': i+1}
                    self.checkpoint_mgr.save_data(temp_data)
                    sys.exit(0)
        
        return {'X': np.array(X), 'y': np.array(y), 'numeric_cols': numeric_cols}
    
    def _split_and_scale(self, sequences):
        """ë°ì´í„° ë¶„í•  ë° ì •ê·œí™”"""
        X = sequences['X']
        y = sequences['y']
        
        # 70:15:15 ë¶„í• 
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
        
        # ì •ê·œí™”
        n_samples, seq_len, n_features = X_train.shape
        
        X_train_scaled = self.scaler_X.fit_transform(X_train.reshape(-1, n_features)).reshape(n_samples, seq_len, n_features)
        X_val_scaled = self.scaler_X.transform(X_val.reshape(-1, n_features)).reshape(X_val.shape[0], seq_len, n_features)
        X_test_scaled = self.scaler_X.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape[0], seq_len, n_features)
        
        y_train_scaled = self.scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = self.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        y_test_scaled = self.scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (ì¤‘ìš”!)
        self.save_scalers()
        
        return {
            'X_train': X_train_scaled, 'y_train': y_train_scaled,
            'X_val': X_val_scaled, 'y_val': y_val_scaled,
            'X_test': X_test_scaled, 'y_test': y_test_scaled,
            'n_features': n_features,
            'scalers': {'X': self.scaler_X, 'y': self.scaler_y}
        }
    
    def save_scalers(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥"""
        import joblib
        
        scaler_dir = os.path.join(self.checkpoint_mgr.checkpoint_dir, 'scalers')
        os.makedirs(scaler_dir, exist_ok=True)
        
        # X ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        joblib.dump(self.scaler_X, os.path.join(scaler_dir, 'scaler_X.pkl'))
        # y ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        joblib.dump(self.scaler_y, os.path.join(scaler_dir, 'scaler_y.pkl'))
        
        print(f"ğŸ’¾ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ: {scaler_dir}")
    
    def load_scalers(self):
        """ì €ì¥ëœ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        import joblib
        
        scaler_dir = os.path.join(self.checkpoint_mgr.checkpoint_dir, 'scalers')
        
        if os.path.exists(scaler_dir):
            self.scaler_X = joblib.load(os.path.join(scaler_dir, 'scaler_X.pkl'))
            self.scaler_y = joblib.load(os.path.join(scaler_dir, 'scaler_y.pkl'))
            print(f"ğŸ“‚ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ: {scaler_dir}")
            return True
        return False
    
    def _resume_from_checkpoint(self, saved_data, saved_progress):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ"""
        print(f"â™»ï¸ {saved_progress['stage']} ë‹¨ê³„ë¶€í„° ì¬ê°œ...")
        
        # ë‹¨ê³„ë³„ ì¬ê°œ ë¡œì§
        if saved_progress['stage'] == 'sequences':
            # ì‹œí€€ìŠ¤ ìƒì„± ì¤‘ë‹¨ëœ ê²½ìš°
            df = pd.read_csv('data/HUB_0509_TO_0730_DATA.CSV')
            df = self._preprocess_data(df)
            
            # ì´ì–´ì„œ ì‹œí€€ìŠ¤ ìƒì„±
            X_prev = saved_data.get('X', [])
            y_prev = saved_data.get('y', [])
            start_idx = saved_data.get('progress', 0)
            
            print(f"  â€¢ ì´ì „ ì§„í–‰: {start_idx}ê°œ ì™„ë£Œ")
            # ì—¬ê¸°ì„œ ì´ì–´ì„œ ì²˜ë¦¬...
        
        return saved_data

# ===========================
# ğŸ§  ëª¨ë¸ ì •ì˜
# ===========================

def create_patchtst_model(config):
    """PatchTST ëª¨ë¸ (TensorFlow)"""
    
    inputs = layers.Input(shape=(config['seq_len'], config['n_features']))
    
    # íŒ¨ì¹˜ ë¶„í•  (20ë¶„ â†’ 5ë¶„Ã—4)
    patch_len = config['patch_len']
    n_patches = config['seq_len'] // patch_len
    
    # Reshape for patches
    x = layers.Reshape((n_patches, patch_len * config['n_features']))(inputs)
    
    # Patch Embedding
    x = layers.Dense(config['d_model'])(x)
    x = layers.LayerNormalization()(x)
    
    # Positional Encoding
    positions = tf.range(start=0, limit=n_patches, delta=1)
    position_embedding = layers.Embedding(n_patches, config['d_model'])(positions)
    x = x + position_embedding
    
    # Transformer Blocks
    for _ in range(config['n_layers']):
        # Multi-Head Attention
        attn_output = layers.MultiHeadAttention(
            num_heads=config['n_heads'],
            key_dim=config['d_model'] // config['n_heads'],
            dropout=config['dropout']
        )(x, x)
        x = layers.Add()([x, attn_output])
        x = layers.LayerNormalization()(x)
        
        # Feed Forward
        ffn_output = layers.Dense(config['d_ff'], activation='relu')(x)
        ffn_output = layers.Dense(config['d_model'])(ffn_output)
        ffn_output = layers.Dropout(config['dropout'])(ffn_output)
        x = layers.Add()([x, ffn_output])
        x = layers.LayerNormalization()(x)
    
    # Global Pooling
    x = layers.GlobalAveragePooling1D()(x)
    
    # Output
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(config['dropout'])(x)
    x = layers.Dense(64, activation='relu')(x)
    outputs = layers.Dense(1)(x)
    
    model = Model(inputs=inputs, outputs=outputs, name='PatchTST')
    return model

def create_patchtst_pinn_model(config):
    """PatchTST + PINN ëª¨ë¸"""
    
    # ê¸°ë³¸ PatchTST
    base_model = create_patchtst_model(config)
    
    # PINN ë ˆì´ì–´ ì¶”ê°€
    inputs = base_model.input
    base_output = base_model(inputs)
    
    # ë¬¼ë¦¬ ì œì•½ ë ˆì´ì–´
    physics_corrected = layers.Lambda(
        lambda x: tf.clip_by_value(x, 0, config['max_capacity']),
        name='physics_constraint'
    )(base_output)
    
    model = Model(inputs=inputs, outputs=physics_corrected, name='PatchTST_PINN')
    return model

# ===========================
# ğŸ“ˆ ì»¤ìŠ¤í…€ ì½œë°±
# ===========================

class ProgressCallback(Callback):
    """í•™ìŠµ ì§„í–‰ë¥  í‘œì‹œ ì½œë°±"""
    
    def __init__(self, checkpoint_mgr):
        super().__init__()
        self.checkpoint_mgr = checkpoint_mgr
        
    def on_epoch_end(self, epoch, logs=None):
        # ì§„í–‰ ìƒí™© ì €ì¥
        self.checkpoint_mgr.save_progress(
            'training',
            epoch + 1,
            self.params['epochs'],
            {'loss': logs.get('loss'), 'val_loss': logs.get('val_loss')}
        )
        
        # ì§„í–‰ë¥  í‘œì‹œ
        progress = (epoch + 1) / self.params['epochs'] * 100
        print(f"\nì§„í–‰ë¥ : {progress:.1f}% | Loss: {logs.get('loss'):.4f} | Val Loss: {logs.get('val_loss'):.4f}")

# ===========================
# ğŸ¯ ë©”ì¸ ì‹¤í–‰
# ===========================

class HUBROOMPredictor:
    """ì‹¤ì‹œê°„ ì˜ˆì¸¡ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path='checkpoints/best_model.h5', scaler_dir='checkpoints/scalers'):
        """ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        import joblib
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = keras.models.load_model(model_path)
        print(f"âœ… ëª¨ë¸ ë¡œë“œ: {model_path}")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        self.scaler_X = joblib.load(os.path.join(scaler_dir, 'scaler_X.pkl'))
        self.scaler_y = joblib.load(os.path.join(scaler_dir, 'scaler_y.pkl'))
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ: {scaler_dir}")
    
    def predict(self, last_20min_data):
        """ì‹¤ì‹œê°„ 10ë¶„ í›„ ì˜ˆì¸¡"""
        # ì…ë ¥ ë°ì´í„° ì •ê·œí™”
        seq_len, n_features = last_20min_data.shape
        X_scaled = self.scaler_X.transform(last_20min_data.reshape(-1, n_features)).reshape(1, seq_len, n_features)
        
        # ì˜ˆì¸¡
        pred_scaled = self.model.predict(X_scaled, verbose=0)
        
        # ì—­ë³€í™˜
        pred_original = self.scaler_y.inverse_transform(pred_scaled)
        
        return pred_original[0, 0]
    
    def predict_with_confidence(self, last_20min_data, n_predictions=10):
        """ì‹ ë¢°êµ¬ê°„ê³¼ í•¨ê»˜ ì˜ˆì¸¡ (ë“œë¡­ì•„ì›ƒ í™œìš©)"""
        predictions = []
        
        for _ in range(n_predictions):
            pred = self.predict(last_20min_data)
            predictions.append(pred)
        
        predictions = np.array(predictions)
        
        result = {
            'prediction': np.mean(predictions),
            'std': np.std(predictions),
            'lower_bound': np.percentile(predictions, 5),
            'upper_bound': np.percentile(predictions, 95)
        }
        
        return result

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("\n" + "="*70)
    print("ğŸ­ HUBROOM ë°˜ì†¡ëŸ‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
    print("ğŸ“Š TensorFlow 2.15 - ì™„ì „íŒ")
    print("="*70)
    
    # 1. ë°ì´í„° ì²˜ë¦¬
    processor = HUBROOMProcessor()
    data = processor.process_with_resume()
    
    # 2. ëª¨ë¸ ì„¤ì •
    config = {
        'seq_len': 20,
        'n_features': data['n_features'],
        'patch_len': 5,
        'd_model': 128,
        'n_heads': 8,
        'd_ff': 256,
        'n_layers': 3,
        'dropout': 0.1,
        'max_capacity': 200
    }
    
    print(f"\nğŸ“Š ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
    print(f"  â€¢ Train: {data['X_train'].shape}")
    print(f"  â€¢ Valid: {data['X_val'].shape}")
    print(f"  â€¢ Test: {data['X_test'].shape}")
    
    # 3. ëª¨ë¸ ìƒì„±
    print("\nğŸ¤– ëª¨ë¸ ìƒì„± ì¤‘...")
    model1 = create_patchtst_model(config)
    model2 = create_patchtst_pinn_model(config)
    
    model1.compile(
        optimizer=keras.optimizers.Adam(0.001),
        loss='mse',
        metrics=['mae']
    )
    
    model2.compile(
        optimizer=keras.optimizers.Adam(0.001),
        loss='mse',
        metrics=['mae']
    )
    
    print("âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ!")
    print(f"  â€¢ Model 1: PatchTST")
    print(f"  â€¢ Model 2: PatchTST + PINN")
    
    # 4. í•™ìŠµ
    checkpoint_mgr = CheckpointManager()
    
    # ì²´í¬í¬ì¸íŠ¸ ì½œë°±
    callbacks = [
        ProgressCallback(checkpoint_mgr),
        ModelCheckpoint('checkpoints/best_model.h5', save_best_only=True),
        EarlyStopping(patience=10, restore_best_weights=True)
    ]
    
    print("\nğŸš€ Model 1 (PatchTST) í•™ìŠµ ì‹œì‘...")
    history1 = model1.fit(
        data['X_train'], data['y_train'],
        validation_data=(data['X_val'], data['y_val']),
        epochs=30,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )
    
    print("\nğŸš€ Model 2 (PatchTST+PINN) í•™ìŠµ ì‹œì‘...")
    history2 = model2.fit(
        data['X_train'], data['y_train'],
        validation_data=(data['X_val'], data['y_val']),
        epochs=30,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )
    
    # 5. í‰ê°€
    print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
    
    # Model 1 í‰ê°€
    test_loss1, test_mae1 = model1.evaluate(data['X_test'], data['y_test'], verbose=0)
    pred1 = model1.predict(data['X_test'], verbose=0)
    
    # Model 2 í‰ê°€
    test_loss2, test_mae2 = model2.evaluate(data['X_test'], data['y_test'], verbose=0)
    pred2 = model2.predict(data['X_test'], verbose=0)
    
    # ì—­ë³€í™˜
    scaler_y = data['scalers']['y']
    pred1_original = scaler_y.inverse_transform(pred1)
    pred2_original = scaler_y.inverse_transform(pred2)
    y_test_original = scaler_y.inverse_transform(data['y_test'].reshape(-1, 1))
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*70)
    print("ğŸ“ˆ ìµœì¢… ê²°ê³¼")
    print("="*70)
    
    print("\nModel 1 (PatchTST):")
    print(f"  â€¢ Test MAE: {test_mae1:.4f}")
    print(f"  â€¢ Test RMSE: {np.sqrt(test_loss1):.4f}")
    
    print("\nModel 2 (PatchTST + PINN):")
    print(f"  â€¢ Test MAE: {test_mae2:.4f}")
    print(f"  â€¢ Test RMSE: {np.sqrt(test_loss2):.4f}")
    
    # ë¬¼ë¦¬ ì œì•½ ê²€ì¦
    print("\nğŸ”¬ ë¬¼ë¦¬ ì œì•½ ê²€ì¦:")
    print(f"  â€¢ Model 1 ìŒìˆ˜ ì˜ˆì¸¡: {np.sum(pred1_original < 0)}ê°œ")
    print(f"  â€¢ Model 2 ìŒìˆ˜ ì˜ˆì¸¡: {np.sum(pred2_original < 0)}ê°œ")
    print(f"  â€¢ Model 1 ìš©ëŸ‰ ì´ˆê³¼: {np.sum(pred1_original > 200)}ê°œ")
    print(f"  â€¢ Model 2 ìš©ëŸ‰ ì´ˆê³¼: {np.sum(pred2_original > 200)}ê°œ")
    
    print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    
    return model1, model2, history1, history2

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='HUBROOM ì˜ˆì¸¡ ì‹œìŠ¤í…œ')
    parser.add_argument('--mode', choices=['train', 'predict', 'evaluate'], 
                       default='train', help='ì‹¤í–‰ ëª¨ë“œ')
    parser.add_argument('--resume', action='store_true', 
                       help='ì´ì „ í•™ìŠµ ì´ì–´ì„œ ì§„í–‰')
    parser.add_argument('--model', choices=['patchtst', 'pinn', 'both'], 
                       default='both', help='í•™ìŠµí•  ëª¨ë¸')
    
    args = parser.parse_args()
    
    try:
        if args.mode == 'train':
            # í•™ìŠµ ëª¨ë“œ
            model1, model2, history1, history2 = main()
            
        elif args.mode == 'predict':
            # ì˜ˆì¸¡ ëª¨ë“œ
            print("\nğŸ”® ì‹¤ì‹œê°„ ì˜ˆì¸¡ ëª¨ë“œ")
            predictor = HUBROOMPredictor()
            
            # ì˜ˆì‹œ: ë§ˆì§€ë§‰ 20ë¶„ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ì‹¤ì‹œê°„ ë°ì´í„°)
            # last_20min = get_realtime_data()  # ì‹¤ì œ êµ¬í˜„ í•„ìš”
            
            print("âœ… ì˜ˆì¸¡ ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ!")
            
        elif args.mode == 'evaluate':
            # í‰ê°€ ëª¨ë“œ
            print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ëª¨ë“œ")
            # ì €ì¥ëœ ëª¨ë¸ í‰ê°€
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨!")
        print("ğŸ’¾ ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì¤‘ë‹¨ëœ ì§€ì ë¶€í„° ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
        
        # ì¬ê°œ ëª…ë ¹ì–´ ì•ˆë‚´
        print("\nğŸ“Œ ì¬ê°œ ë°©ë²•:")
        print("  python tf_patchtst_complete.py --resume")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ğŸ’¾ ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ë””ë²„ê·¸ ì •ë³´
        import traceback
        print("\në””ë²„ê·¸ ì •ë³´:")
        traceback.print_exc()

# ===========================
# ğŸ“ íŒŒì¼ êµ¬ì¡° ì„¤ëª…
# ===========================
"""
í”„ë¡œì íŠ¸ êµ¬ì¡°:
â”œâ”€â”€ tf_patchtst_complete.py     # ë©”ì¸ ì½”ë“œ
â”œâ”€â”€ data/
â”‚   â””â”€â”€ HUB_0509_TO_0730_DATA.CSV  # ì›ë³¸ ë°ì´í„°
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ progress.json           # ì§„í–‰ ìƒí™©
â”‚   â”œâ”€â”€ processed_data.pkl      # ì²˜ë¦¬ëœ ë°ì´í„°
â”‚   â”œâ”€â”€ best_model.h5          # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
â”‚   â”œâ”€â”€ model_checkpoint/       # í•™ìŠµ ì²´í¬í¬ì¸íŠ¸
â”‚   â””â”€â”€ scalers/               # ìŠ¤ì¼€ì¼ëŸ¬
â”‚       â”œâ”€â”€ scaler_X.pkl       # ì…ë ¥ ìŠ¤ì¼€ì¼ëŸ¬
â”‚       â””â”€â”€ scaler_y.pkl       # ì¶œë ¥ ìŠ¤ì¼€ì¼ëŸ¬
â””â”€â”€ logs/                       # í•™ìŠµ ë¡œê·¸

ì‹¤í–‰ ì˜ˆì‹œ:
1. ì²˜ìŒ í•™ìŠµ: python tf_patchtst_complete.py
2. ì´ì–´ì„œ í•™ìŠµ: python tf_patchtst_complete.py --resume
3. ì˜ˆì¸¡ ëª¨ë“œ: python tf_patchtst_complete.py --mode predict
4. í‰ê°€ ëª¨ë“œ: python tf_patchtst_complete.py --mode evaluate

requirements.txt:
tensorflow==2.15.0
pandas
numpy
scikit-learn
joblib
tqdm
matplotlib
"""