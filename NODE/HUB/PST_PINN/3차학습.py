# -*- coding: utf-8 -*-
"""
HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ V3
- Model 1: PatchTST (ì „ì²´ êµ¬ê°„ ê· í˜•)
- Model 2: PatchTST + PINN (310+ ê·¹ë‹¨ê°’ íŠ¹í™”)
- 310-335 êµ¬ê°„ 10ë°°, 335+ êµ¬ê°„ 15ë°° ì˜¤ë²„ìƒ˜í”Œë§
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback
from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
import os
import pickle
import warnings
from tqdm import tqdm
import joblib

warnings.filterwarnings('ignore')

np.random.seed(42)
tf.random.set_seed(42)
print(f"TensorFlow Version: {tf.__version__}")
print("="*80)
print("ğŸ­ HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ V3")
print("ğŸ¯ ëª©í‘œ: 310+ ê·¹ë‹¨ê°’ ì •í™• ì˜ˆì¸¡")
print("="*80)

# ========================================
# ê·¹ë‹¨ê°’ ëª¨ë‹ˆí„°ë§ ì½œë°±
# ========================================

class ExtremeValueCallback(Callback):
    """310+ ì˜ˆì¸¡ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, X_val, y_val, scaler_y):
        super().__init__()
        self.X_val = X_val[:500] if len(X_val) > 500 else X_val
        self.y_val = y_val[:500] if len(y_val) > 500 else y_val
        self.scaler_y = scaler_y
        
    def on_epoch_end(self, epoch, logs=None):
        if epoch % 10 == 0:
            # ì˜ˆì¸¡
            if isinstance(self.X_val, tuple):
                y_pred_scaled = self.model.predict(self.X_val, verbose=0)
            else:
                y_pred_scaled = self.model.predict(self.X_val, verbose=0)
            
            y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
            y_true = self.scaler_y.inverse_transform(self.y_val.reshape(-1, 1)).flatten()
            
            # êµ¬ê°„ë³„ ì„±ëŠ¥
            print(f"\n[Epoch {epoch}] ê·¹ë‹¨ê°’ ê°ì§€:")
            for threshold in [310, 335]:
                mask = y_true >= threshold
                if mask.sum() > 0:
                    detected = (y_pred >= threshold - 5)[mask].sum()
                    print(f"  {threshold}+: {detected}/{mask.sum()} ({detected/mask.sum()*100:.1f}%)")

# ========================================
# ë°ì´í„° ì²˜ë¦¬ (ê·¹ë‹¨ê°’ íŠ¹í™” V3)
# ========================================

class DataProcessorV3:
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        self.scaler_X = RobustScaler()
        self.scaler_y = MinMaxScaler(feature_range=(0, 1))
        self.scaler_physics = StandardScaler()
        
        # ë¬¼ë¦¬ ë²•ì¹™ìš© ì»¬ëŸ¼
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2', 'M14B_7F_TO_HUB_JOB2'
        ]
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB', 'M16A_3F_TO_M14B_7F_JOB'
        ]
        
    def save_scalers(self):
        os.makedirs('./scalers', exist_ok=True)
        joblib.dump(self.scaler_X, './scalers/scaler_X.pkl')
        joblib.dump(self.scaler_y, './scalers/scaler_y.pkl')
        joblib.dump(self.scaler_physics, './scalers/scaler_physics.pkl')
        print("âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ")
        
    def load_scalers(self):
        try:
            self.scaler_X = joblib.load('./scalers/scaler_X.pkl')
            self.scaler_y = joblib.load('./scalers/scaler_y.pkl')
            self.scaler_physics = joblib.load('./scalers/scaler_physics.pkl')
            print("âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
            return True
        except:
            print("âŒ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨")
            return False
    
    def analyze_data(self, df):
        """ë°ì´í„° ë¶„ì„"""
        target = df[self.target_col]
        print("\nğŸ“Š ë°ì´í„° ë¶„ì„:")
        print(f"  ë²”ìœ„: {target.min():.0f} ~ {target.max():.0f}")
        print(f"  í‰ê· : {target.mean():.1f}")
        print(f"  ì¤‘ì•™ê°’: {target.median():.1f}")
        
        print("\nğŸ¯ 3êµ¬ê°„ ë¶„í¬:")
        print(f"  ì €êµ¬ê°„(<200): {(target < 200).sum():6}ê°œ ({(target < 200).sum()/len(target)*100:5.2f}%)")
        print(f"  ì •ìƒ(200-300): {((target >= 200) & (target < 300)).sum():6}ê°œ ({((target >= 200) & (target < 300)).sum()/len(target)*100:5.2f}%)")
        print(f"  ìœ„í—˜(300+): {(target >= 300).sum():6}ê°œ ({(target >= 300).sum()/len(target)*100:5.2f}%)")
        
        print("\nğŸš¨ ê·¹ë‹¨ê°’ ì„¸ë¶€:")
        print(f"  310+: {(target >= 310).sum():6}ê°œ ({(target >= 310).sum()/len(target)*100:5.2f}%)")
        print(f"  335+: {(target >= 335).sum():6}ê°œ ({(target >= 335).sum()/len(target)*100:5.2f}%)")
    
    def create_sequences_v3(self, df, seq_len=20, pred_len=10):
        """ê·¹ë‹¨ê°’ ì§‘ì¤‘ ì‹œí€€ìŠ¤ ìƒì„± V3"""
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # ë¬¼ë¦¬ ë°ì´í„°ìš© ì»¬ëŸ¼
        available_inflow = [col for col in self.inflow_cols if col in df.columns]
        available_outflow = [col for col in self.outflow_cols if col in df.columns]
        
        X, y, X_physics, weights = [], [], [], []
        total = len(df) - seq_len - pred_len + 1
        
        # êµ¬ê°„ë³„ ì¸ë±ìŠ¤
        indices = {
            'low': [],      # <200
            'normal': [],   # 200-300
            '300': [],      # 300-310
            '310': [],      # 310-335
            '335': []       # 335+
        }
        
        print("\nğŸ“¦ ì‹œí€€ìŠ¤ ë¶„ë¥˜ ì¤‘...")
        for i in tqdm(range(total)):
            target_val = df[self.target_col].iloc[i + seq_len + pred_len - 1]
            
            if target_val < 200:
                indices['low'].append(i)
            elif target_val < 300:
                indices['normal'].append(i)
            elif target_val < 310:
                indices['300'].append(i)
            elif target_val < 335:
                indices['310'].append(i)
            else:
                indices['335'].append(i)
        
        # V3 ì˜¤ë²„ìƒ˜í”Œë§ ì „ëµ
        all_indices = []
        all_indices.extend(indices['low'])         # 1ë°°
        all_indices.extend(indices['normal'])      # 1ë°°
        all_indices.extend(indices['300'] * 3)     # 3ë°°
        all_indices.extend(indices['310'] * 10)    # 10ë°° (í•µì‹¬!)
        all_indices.extend(indices['335'] * 15)    # 15ë°° (ìµœëŒ€!)
        
        print(f"\nğŸ“Š ì˜¤ë²„ìƒ˜í”Œë§ ê²°ê³¼:")
        print(f"  <200: {len(indices['low'])} â†’ {len(indices['low'])}")
        print(f"  200-300: {len(indices['normal'])} â†’ {len(indices['normal'])}")
        print(f"  300-310: {len(indices['300'])} â†’ {len(indices['300'])*3}")
        print(f"  310-335: {len(indices['310'])} â†’ {len(indices['310'])*10} â­")
        print(f"  335+: {len(indices['335'])} â†’ {len(indices['335'])*15} â­â­")
        
        np.random.shuffle(all_indices)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        print(f"\nì‹œí€€ìŠ¤ ìƒì„± ì¤‘... (ì´ {len(all_indices)}ê°œ)")
        for i in tqdm(all_indices):
            # ì‹œê³„ì—´ ë°ì´í„°
            X.append(df[numeric_cols].iloc[i:i+seq_len].values)
            
            # íƒ€ê²Ÿ
            y_val = df[self.target_col].iloc[i + seq_len + pred_len - 1]
            y.append(y_val)
            
            # ë¬¼ë¦¬ ë°ì´í„° (í˜„ì¬ê°’, ìœ ì…í•©, ìœ ì¶œí•©)
            physics = [
                df[self.target_col].iloc[i + seq_len - 1],  # í˜„ì¬ê°’
                df[available_inflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_inflow else 0,
                df[available_outflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_outflow else 0
            ]
            X_physics.append(physics)
            
            # V3 ê°€ì¤‘ì¹˜
            if y_val >= 335:
                weights.append(20.0)  # 335+ ìµœëŒ€ ê°€ì¤‘ì¹˜
            elif y_val >= 310:
                weights.append(15.0)  # 310-335 ê°•í•œ ê°€ì¤‘ì¹˜
            elif y_val >= 300:
                weights.append(5.0)   # 300-310 ì¤‘ê°„ ê°€ì¤‘ì¹˜
            else:
                weights.append(1.0)   # ì •ìƒ êµ¬ê°„
        
        return np.array(X), np.array(y), np.array(X_physics), np.array(weights)

# ========================================
# Model 1: PatchTST (ì „ì²´ ê· í˜•)
# ========================================

class PatchTSTModel(keras.Model):
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(128, activation='relu')
        
        # Transformer
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm1 = layers.LayerNormalization()
        self.norm2 = layers.LayerNormalization()
        
        self.ffn = keras.Sequential([
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(128)
        ])
        
        # ì¶œë ¥
        self.flatten = layers.Flatten()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dropout = layers.Dropout(0.3)
        self.dense2 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(1)
        
    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        
        # íŒ¨ì¹˜ ìƒì„±
        x = tf.reshape(x, [batch_size, self.n_patches, self.patch_len * self.n_features])
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x = self.patch_embedding(x)
        
        # Transformer
        attn = self.attention(x, x, training=training)
        x = self.norm1(x + attn)
        
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        
        # ì¶œë ¥
        x = self.flatten(x)
        x = self.dense1(x)
        x = self.dropout(x, training=training)
        x = self.dense2(x)
        output = self.output_layer(x)
        
        return tf.squeeze(output, axis=-1)

# ========================================
# Model 2: PatchTST + PINN (ê·¹ë‹¨ê°’ íŠ¹í™”)
# ========================================

class PatchTSTPINN(keras.Model):
    def __init__(self, config):
        super().__init__()
        
        # PatchTST ë¶€ë¶„
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(128, activation='relu')
        
        # Transformer
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm = layers.LayerNormalization()
        
        # ì‹œê³„ì—´ ì²˜ë¦¬
        self.flatten = layers.Flatten()
        self.temporal_dense = layers.Dense(64, activation='relu')
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬ (PINN)
        self.physics_net = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dense(16, activation='relu')
        ])
        
        # ìœµí•© ë° ê·¹ë‹¨ê°’ ë³´ì •
        self.fusion = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(1)
        ])
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ… ë ˆì´ì–´
        self.extreme_boost = layers.Dense(1, activation='sigmoid')
        
    def call(self, inputs, training=False):
        x_seq, x_physics = inputs
        
        batch_size = tf.shape(x_seq)[0]
        
        # PatchTST ì²˜ë¦¬
        x = tf.reshape(x_seq, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        
        attn = self.attention(x, x, training=training)
        x = self.norm(x + attn)
        
        x = self.flatten(x)
        temporal_features = self.temporal_dense(x)
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬
        physics_features = self.physics_net(x_physics)
        
        # ìœµí•©
        combined = tf.concat([temporal_features, physics_features], axis=-1)
        output = self.fusion(combined)
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ… (310+ êµ¬ê°„ ê°•í™”)
        boost_factor = self.extreme_boost(combined)
        output = output * (1 + boost_factor * 0.2)  # ìµœëŒ€ 20% ë¶€ìŠ¤íŒ…
        
        return tf.squeeze(output, axis=-1)

# ========================================
# ì†ì‹¤ í•¨ìˆ˜ V3
# ========================================

class ExtremeLossV3(tf.keras.losses.Loss):
    """ê·¹ë‹¨ê°’ íŠ¹í™” ì†ì‹¤í•¨ìˆ˜ V3"""
    
    def __init__(self, extreme_focus=False):
        super().__init__()
        self.extreme_focus = extreme_focus
        
    def call(self, y_true, y_pred):
        y_true = tf.reshape(y_true, [-1])
        y_pred = tf.reshape(y_pred, [-1])
        
        mse = tf.square(y_true - y_pred)
        
        if self.extreme_focus:
            # Model 2ìš© - ê·¹ë‹¨ê°’ ë” ê°•ì¡°
            weight = tf.where(y_true > 0.52, 20.0,    # 335+ (ìŠ¤ì¼€ì¼ í›„)
                     tf.where(y_true > 0.45, 15.0,    # 310+
                     tf.where(y_true > 0.4, 5.0,      # 300+
                     1.0)))
        else:
            # Model 1ìš© - ì „ì²´ ê· í˜•
            weight = tf.where(y_true > 0.52, 10.0,    # 335+
                     tf.where(y_true > 0.45, 8.0,     # 310+
                     tf.where(y_true > 0.4, 3.0,      # 300+
                     1.0)))
        
        return mse * weight

# ========================================
# ë©”ì¸ ì‹¤í–‰
# ========================================

def main():
    # ë°ì´í„° ì²˜ë¦¬
    processor = DataProcessorV3()
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\n[Step 1/5] ë°ì´í„° ë¡œë“œ")
    df = pd.read_csv('data/HUB_0509_TO_0730_DATA.CSV')
    print(f"âœ… ë°ì´í„° ë¡œë“œ: {df.shape}")
    
    # 2. ë°ì´í„° ë¶„ì„
    processor.analyze_data(df)
    
    # 3. ì „ì²˜ë¦¬
    print("\n[Step 2/5] ë°ì´í„° ì „ì²˜ë¦¬")
    df['timestamp'] = pd.to_datetime(df.iloc[:, 0], format='%Y%m%d%H%M', errors='coerce')
    df = df.sort_values('timestamp').reset_index(drop=True)
    df = df.fillna(method='ffill').fillna(0)
    
    # 4. ì‹œí€€ìŠ¤ ìƒì„±
    print("\n[Step 3/5] ì‹œí€€ìŠ¤ ìƒì„±")
    X, y, X_physics, weights = processor.create_sequences_v3(df)
    
    # 5. ë°ì´í„° ë¶„í• 
    print("\n[Step 4/5] ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§")
    indices = np.arange(len(X))
    train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)
    val_idx, test_idx = train_test_split(test_idx, test_size=0.5, random_state=42)
    
    # ë°ì´í„° ë¶„í• 
    X_train, y_train = X[train_idx], y[train_idx]
    X_val, y_val = X[val_idx], y[val_idx]
    X_test, y_test = X[test_idx], y[test_idx]
    
    X_physics_train = X_physics[train_idx]
    X_physics_val = X_physics[val_idx]
    X_physics_test = X_physics[test_idx]
    
    weights_train = weights[train_idx]
    
    # ìŠ¤ì¼€ì¼ë§
    n_features = X.shape[2]
    
    X_train_flat = X_train.reshape(-1, n_features)
    X_train_scaled = processor.scaler_X.fit_transform(X_train_flat)
    X_train_scaled = X_train_scaled.reshape(len(X_train), 20, n_features)
    
    X_val_scaled = processor.scaler_X.transform(X_val.reshape(-1, n_features)).reshape(len(X_val), 20, n_features)
    X_test_scaled = processor.scaler_X.transform(X_test.reshape(-1, n_features)).reshape(len(X_test), 20, n_features)
    
    y_train_scaled = processor.scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_val_scaled = processor.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
    y_test_scaled = processor.scaler_y.transform(y_test.reshape(-1, 1)).flatten()
    
    X_physics_train_scaled = processor.scaler_physics.fit_transform(X_physics_train)
    X_physics_val_scaled = processor.scaler_physics.transform(X_physics_val)
    X_physics_test_scaled = processor.scaler_physics.transform(X_physics_test)
    
    processor.save_scalers()
    
    print(f"\nğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°:")
    print(f"  Train: {len(train_idx)} (310+: {(y_train >= 310).sum()}, 335+: {(y_train >= 335).sum()})")
    print(f"  Valid: {len(val_idx)} (310+: {(y_val >= 310).sum()}, 335+: {(y_val >= 335).sum()})")
    print(f"  Test: {len(test_idx)} (310+: {(y_test >= 310).sum()}, 335+: {(y_test >= 335).sum()})")
    
    # 6. ëª¨ë¸ í•™ìŠµ
    print("\n[Step 5/5] ëª¨ë¸ í•™ìŠµ")
    
    config = {
        'seq_len': 20,
        'n_features': n_features,
        'patch_len': 5
    }
    
    # ì½œë°±
    callbacks_model1 = [
        EarlyStopping(patience=20, restore_best_weights=True),
        ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-6),
        ModelCheckpoint('./checkpoints/model1_v3.h5', save_best_only=True, save_weights_only=True),
        ExtremeValueCallback(X_val_scaled, y_val_scaled, processor.scaler_y)
    ]
    
    callbacks_model2 = [
        EarlyStopping(patience=20, restore_best_weights=True),
        ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-6),
        ModelCheckpoint('./checkpoints/model2_v3.h5', save_best_only=True, save_weights_only=True),
        ExtremeValueCallback((X_val_scaled, X_physics_val_scaled), y_val_scaled, processor.scaler_y)
    ]
    
    # Model 1: PatchTST
    print("\nğŸ¤– Model 1: PatchTST í•™ìŠµ")
    model1 = PatchTSTModel(config)
    model1.compile(
        optimizer=Adam(learning_rate=0.001),
        loss=ExtremeLossV3(extreme_focus=False),
        metrics=['mae']
    )
    
    history1 = model1.fit(
        X_train_scaled, y_train_scaled,
        validation_data=(X_val_scaled, y_val_scaled),
        sample_weight=weights_train,
        epochs=50,
        batch_size=32,
        callbacks=callbacks_model1,
        verbose=1
    )
    
    # Model 2: PatchTST + PINN
    print("\nğŸ¤– Model 2: PatchTST + PINN í•™ìŠµ")
    model2 = PatchTSTPINN(config)
    model2.compile(
        optimizer=Adam(learning_rate=0.0008),
        loss=ExtremeLossV3(extreme_focus=True),
        metrics=['mae']
    )
    
    history2 = model2.fit(
        [X_train_scaled, X_physics_train_scaled], y_train_scaled,
        validation_data=([X_val_scaled, X_physics_val_scaled], y_val_scaled),
        sample_weight=weights_train,
        epochs=60,
        batch_size=32,
        callbacks=callbacks_model2,
        verbose=1
    )
    
    # 7. í‰ê°€
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… í‰ê°€")
    print("="*80)
    
    # Model 1 í‰ê°€
    print("\n[Model 1: PatchTST]")
    y_pred1_scaled = model1.predict(X_test_scaled, verbose=0)
    y_pred1 = processor.scaler_y.inverse_transform(y_pred1_scaled.reshape(-1, 1)).flatten()
    
    mae1 = np.mean(np.abs(y_test - y_pred1))
    print(f"ì „ì²´ MAE: {mae1:.2f}")
    
    # 3êµ¬ê°„ í‰ê°€
    print("\n3êµ¬ê°„ ì„±ëŠ¥:")
    mask_low = y_test < 200
    mask_normal = (y_test >= 200) & (y_test < 300)
    mask_danger = y_test >= 300
    
    if mask_low.sum() > 0:
        print(f"  ì €êµ¬ê°„(<200): MAE={np.mean(np.abs(y_test[mask_low] - y_pred1[mask_low])):.2f}")
    if mask_normal.sum() > 0:
        print(f"  ì •ìƒ(200-300): MAE={np.mean(np.abs(y_test[mask_normal] - y_pred1[mask_normal])):.2f}")
    if mask_danger.sum() > 0:
        print(f"  ìœ„í—˜(300+): MAE={np.mean(np.abs(y_test[mask_danger] - y_pred1[mask_danger])):.2f}")
    
    # ê·¹ë‹¨ê°’ ê°ì§€
    print("\nê·¹ë‹¨ê°’ ê°ì§€:")
    for threshold in [310, 335]:
        mask = y_test >= threshold
        if mask.sum() > 0:
            detected = (y_pred1 >= threshold)[mask].sum()
            print(f"  {threshold}+: {detected}/{mask.sum()} ({detected/mask.sum()*100:.1f}%)")
    
    # Model 2 í‰ê°€
    print("\n[Model 2: PatchTST + PINN]")
    y_pred2_scaled = model2.predict([X_test_scaled, X_physics_test_scaled], verbose=0)
    y_pred2 = processor.scaler_y.inverse_transform(y_pred2_scaled.reshape(-1, 1)).flatten()
    
    mae2 = np.mean(np.abs(y_test - y_pred2))
    print(f"ì „ì²´ MAE: {mae2:.2f}")
    
    # 3êµ¬ê°„ í‰ê°€
    print("\n3êµ¬ê°„ ì„±ëŠ¥:")
    if mask_low.sum() > 0:
        print(f"  ì €êµ¬ê°„(<200): MAE={np.mean(np.abs(y_test[mask_low] - y_pred2[mask_low])):.2f}")
    if mask_normal.sum() > 0:
        print(f"  ì •ìƒ(200-300): MAE={np.mean(np.abs(y_test[mask_normal] - y_pred2[mask_normal])):.2f}")
    if mask_danger.sum() > 0:
        print(f"  ìœ„í—˜(300+): MAE={np.mean(np.abs(y_test[mask_danger] - y_pred2[mask_danger])):.2f}")
    
    # ê·¹ë‹¨ê°’ ê°ì§€
    print("\nê·¹ë‹¨ê°’ ê°ì§€:")
    for threshold in [310, 335]:
        mask = y_test >= threshold
        if mask.sum() > 0:
            detected = (y_pred2 >= threshold)[mask].sum()
            print(f"  {threshold}+: {detected}/{mask.sum()} ({detected/mask.sum()*100:.1f}%)")
    
    print("\nâœ… V3 ì™„ë£Œ!")

if __name__ == "__main__":
    main()