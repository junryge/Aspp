# -*- coding: utf-8 -*-
"""
HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ V3.1
- ì˜¤ë¥˜ ìˆ˜ì •: ExtremeValueCallback ì¸ë±ìŠ¤ ë¬¸ì œ í•´ê²°
- ì¤‘ë‹¨/ì¬ì‹œì‘ ê¸°ëŠ¥ ì¶”ê°€
- Model 1: PatchTST (ì „ì²´ êµ¬ê°„ ê· í˜•)
- Model 2: PatchTST + PINN (310+ ê·¹ë‹¨ê°’ íŠ¹í™”)
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback
from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
import os
import pickle
import warnings
from tqdm import tqdm
import joblib
import signal
import sys

warnings.filterwarnings('ignore')

np.random.seed(42)
tf.random.set_seed(42)
print(f"TensorFlow Version: {tf.__version__}")
print("="*80)
print("ğŸ­ HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ V3.1")
print("ğŸ¯ ëª©í‘œ: 310+ ê·¹ë‹¨ê°’ ì •í™• ì˜ˆì¸¡")
print("âœ… ì¤‘ë‹¨/ì¬ì‹œì‘ ê¸°ëŠ¥ í¬í•¨")
print("="*80)

# ========================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì (ì¤‘ë‹¨/ì¬ì‹œì‘)
# ========================================

class CheckpointManager:
    def __init__(self, checkpoint_dir='./checkpoints'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        self.state_file = os.path.join(checkpoint_dir, 'training_state_v3.pkl')
        self.interrupted = False
        
        # Ctrl+C í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
    
    def _signal_handler(self, sig, frame):
        print('\n\nâš ï¸ ì¤‘ë‹¨ ê°ì§€! ìƒíƒœ ì €ì¥ ì¤‘...')
        self.interrupted = True
        # ì¦‰ì‹œ ì¢…ë£Œí•˜ì§€ ì•Šê³  ë‹¤ìŒ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì €ì¥
        
    def save_state(self, state):
        with open(self.state_file, 'wb') as f:
            pickle.dump(state, f)
        print(f"ğŸ’¾ ìƒíƒœ ì €ì¥ ì™„ë£Œ: Step {state.get('step', 0)}")
        
    def load_state(self):
        if os.path.exists(self.state_file):
            with open(self.state_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def clear_state(self):
        if os.path.exists(self.state_file):
            os.remove(self.state_file)
            print("ğŸ§¹ ì´ì „ ìƒíƒœ ì œê±° ì™„ë£Œ")

# ========================================
# ê·¹ë‹¨ê°’ ëª¨ë‹ˆí„°ë§ ì½œë°± (ì˜¤ë¥˜ ìˆ˜ì •)
# ========================================

class ExtremeValueCallback(Callback):
    """310+ ì˜ˆì¸¡ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ - ì¸ë±ìŠ¤ ì˜¤ë¥˜ ìˆ˜ì •"""
    
    def __init__(self, X_val, y_val, scaler_y):
        super().__init__()
        # ë°ì´í„° í¬ê¸° ì œí•œ
        sample_size = min(500, len(y_val))
        
        if isinstance(X_val, tuple):
            # Model 2ìš© (íŠœí”Œ)
            self.X_val = (X_val[0][:sample_size], X_val[1][:sample_size])
        else:
            # Model 1ìš© (ë‹¨ì¼)
            self.X_val = X_val[:sample_size]
        
        self.y_val = y_val[:sample_size]
        self.scaler_y = scaler_y
        
    def on_epoch_end(self, epoch, logs=None):
        if epoch % 10 == 0:
            try:
                # ì˜ˆì¸¡
                y_pred_scaled = self.model.predict(self.X_val, verbose=0)
                
                # í¬ê¸° í™•ì¸ ë° ì¡°ì •
                y_pred_scaled = y_pred_scaled[:len(self.y_val)]
                
                y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
                y_true = self.scaler_y.inverse_transform(self.y_val.reshape(-1, 1)).flatten()
                
                # êµ¬ê°„ë³„ ì„±ëŠ¥
                print(f"\n[Epoch {epoch}] ê·¹ë‹¨ê°’ ê°ì§€:")
                for threshold in [310, 335]:
                    mask = y_true >= threshold
                    if mask.sum() > 0:
                        # ë§ˆìŠ¤í¬ ì ìš© ì „ í¬ê¸° í™•ì¸
                        detected = (y_pred[mask] >= threshold - 5).sum()
                        print(f"  {threshold}+: {detected}/{mask.sum()} ({detected/mask.sum()*100:.1f}%)")
            except Exception as e:
                print(f"\n[Epoch {epoch}] ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")

# ========================================
# ë°ì´í„° ì²˜ë¦¬ (ê·¹ë‹¨ê°’ íŠ¹í™” V3)
# ========================================

class DataProcessorV3:
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        self.scaler_X = RobustScaler()
        self.scaler_y = MinMaxScaler(feature_range=(0, 1))
        self.scaler_physics = StandardScaler()
        
        # ë¬¼ë¦¬ ë²•ì¹™ìš© ì»¬ëŸ¼
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2', 'M14B_7F_TO_HUB_JOB2'
        ]
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB', 'M16A_3F_TO_M14B_7F_JOB'
        ]
        
    def save_scalers(self):
        os.makedirs('./scalers', exist_ok=True)
        joblib.dump(self.scaler_X, './scalers/scaler_X.pkl')
        joblib.dump(self.scaler_y, './scalers/scaler_y.pkl')
        joblib.dump(self.scaler_physics, './scalers/scaler_physics.pkl')
        print("âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ")
        
    def load_scalers(self):
        try:
            self.scaler_X = joblib.load('./scalers/scaler_X.pkl')
            self.scaler_y = joblib.load('./scalers/scaler_y.pkl')
            self.scaler_physics = joblib.load('./scalers/scaler_physics.pkl')
            print("âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
            return True
        except:
            print("âŒ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨")
            return False
    
    def analyze_data(self, df):
        """ë°ì´í„° ë¶„ì„"""
        target = df[self.target_col]
        print("\nğŸ“Š ë°ì´í„° ë¶„ì„:")
        print(f"  ë²”ìœ„: {target.min():.0f} ~ {target.max():.0f}")
        print(f"  í‰ê· : {target.mean():.1f}")
        print(f"  ì¤‘ì•™ê°’: {target.median():.1f}")
        
        print("\nğŸ¯ 3êµ¬ê°„ ë¶„í¬:")
        print(f"  ì €êµ¬ê°„(<200): {(target < 200).sum():6}ê°œ ({(target < 200).sum()/len(target)*100:5.2f}%)")
        print(f"  ì •ìƒ(200-300): {((target >= 200) & (target < 300)).sum():6}ê°œ ({((target >= 200) & (target < 300)).sum()/len(target)*100:5.2f}%)")
        print(f"  ìœ„í—˜(300+): {(target >= 300).sum():6}ê°œ ({(target >= 300).sum()/len(target)*100:5.2f}%)")
        
        print("\nğŸš¨ ê·¹ë‹¨ê°’ ì„¸ë¶€:")
        print(f"  310+: {(target >= 310).sum():6}ê°œ ({(target >= 310).sum()/len(target)*100:5.2f}%)")
        print(f"  335+: {(target >= 335).sum():6}ê°œ ({(target >= 335).sum()/len(target)*100:5.2f}%)")
    
    def create_sequences_v3(self, df, seq_len=20, pred_len=10):
        """ê·¹ë‹¨ê°’ ì§‘ì¤‘ ì‹œí€€ìŠ¤ ìƒì„± V3"""
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # ë¬¼ë¦¬ ë°ì´í„°ìš© ì»¬ëŸ¼
        available_inflow = [col for col in self.inflow_cols if col in df.columns]
        available_outflow = [col for col in self.outflow_cols if col in df.columns]
        
        X, y, X_physics, weights = [], [], [], []
        total = len(df) - seq_len - pred_len + 1
        
        # êµ¬ê°„ë³„ ì¸ë±ìŠ¤
        indices = {
            'low': [],      # <200
            'normal': [],   # 200-300
            '300': [],      # 300-310
            '310': [],      # 310-335
            '335': []       # 335+
        }
        
        print("\nğŸ“¦ ì‹œí€€ìŠ¤ ë¶„ë¥˜ ì¤‘...")
        for i in tqdm(range(total)):
            target_val = df[self.target_col].iloc[i + seq_len + pred_len - 1]
            
            if target_val < 200:
                indices['low'].append(i)
            elif target_val < 300:
                indices['normal'].append(i)
            elif target_val < 310:
                indices['300'].append(i)
            elif target_val < 335:
                indices['310'].append(i)
            else:
                indices['335'].append(i)
        
        # V3 ì˜¤ë²„ìƒ˜í”Œë§ ì „ëµ
        all_indices = []
        all_indices.extend(indices['low'])         # 1ë°°
        all_indices.extend(indices['normal'])      # 1ë°°
        all_indices.extend(indices['300'] * 3)     # 3ë°°
        all_indices.extend(indices['310'] * 10)    # 10ë°° (í•µì‹¬!)
        all_indices.extend(indices['335'] * 15)    # 15ë°° (ìµœëŒ€!)
        
        print(f"\nğŸ“Š ì˜¤ë²„ìƒ˜í”Œë§ ê²°ê³¼:")
        print(f"  <200: {len(indices['low'])} â†’ {len(indices['low'])}")
        print(f"  200-300: {len(indices['normal'])} â†’ {len(indices['normal'])}")
        print(f"  300-310: {len(indices['300'])} â†’ {len(indices['300'])*3}")
        print(f"  310-335: {len(indices['310'])} â†’ {len(indices['310'])*10} â­")
        print(f"  335+: {len(indices['335'])} â†’ {len(indices['335'])*15} â­â­")
        
        np.random.shuffle(all_indices)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        print(f"\nì‹œí€€ìŠ¤ ìƒì„± ì¤‘... (ì´ {len(all_indices)}ê°œ)")
        for i in tqdm(all_indices):
            # ì‹œê³„ì—´ ë°ì´í„°
            X.append(df[numeric_cols].iloc[i:i+seq_len].values)
            
            # íƒ€ê²Ÿ
            y_val = df[self.target_col].iloc[i + seq_len + pred_len - 1]
            y.append(y_val)
            
            # ë¬¼ë¦¬ ë°ì´í„° (í˜„ì¬ê°’, ìœ ì…í•©, ìœ ì¶œí•©)
            physics = [
                df[self.target_col].iloc[i + seq_len - 1],  # í˜„ì¬ê°’
                df[available_inflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_inflow else 0,
                df[available_outflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_outflow else 0
            ]
            X_physics.append(physics)
            
            # V3 ê°€ì¤‘ì¹˜
            if y_val >= 335:
                weights.append(20.0)  # 335+ ìµœëŒ€ ê°€ì¤‘ì¹˜
            elif y_val >= 310:
                weights.append(15.0)  # 310-335 ê°•í•œ ê°€ì¤‘ì¹˜
            elif y_val >= 300:
                weights.append(5.0)   # 300-310 ì¤‘ê°„ ê°€ì¤‘ì¹˜
            else:
                weights.append(1.0)   # ì •ìƒ êµ¬ê°„
        
        return np.array(X), np.array(y), np.array(X_physics), np.array(weights)

# ========================================
# Model 1: PatchTST (ì „ì²´ ê· í˜•)
# ========================================

class PatchTSTModel(keras.Model):
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(128, activation='relu')
        
        # Transformer
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm1 = layers.LayerNormalization()
        self.norm2 = layers.LayerNormalization()
        
        self.ffn = keras.Sequential([
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(128)
        ])
        
        # ì¶œë ¥
        self.flatten = layers.Flatten()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dropout = layers.Dropout(0.3)
        self.dense2 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(1)
        
    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        
        # íŒ¨ì¹˜ ìƒì„±
        x = tf.reshape(x, [batch_size, self.n_patches, self.patch_len * self.n_features])
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x = self.patch_embedding(x)
        
        # Transformer
        attn = self.attention(x, x, training=training)
        x = self.norm1(x + attn)
        
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        
        # ì¶œë ¥
        x = self.flatten(x)
        x = self.dense1(x)
        x = self.dropout(x, training=training)
        x = self.dense2(x)
        output = self.output_layer(x)
        
        return tf.squeeze(output, axis=-1)

# ========================================
# Model 2: PatchTST + PINN (ê·¹ë‹¨ê°’ íŠ¹í™”)
# ========================================

class PatchTSTPINN(keras.Model):
    def __init__(self, config):
        super().__init__()
        
        # PatchTST ë¶€ë¶„
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(128, activation='relu')
        
        # Transformer
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm = layers.LayerNormalization()
        
        # ì‹œê³„ì—´ ì²˜ë¦¬
        self.flatten = layers.Flatten()
        self.temporal_dense = layers.Dense(64, activation='relu')
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬ (PINN)
        self.physics_net = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dense(16, activation='relu')
        ])
        
        # ìœµí•© ë° ê·¹ë‹¨ê°’ ë³´ì •
        self.fusion = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(1)
        ])
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ… ë ˆì´ì–´
        self.extreme_boost = layers.Dense(1, activation='sigmoid')
        
    def call(self, inputs, training=False):
        x_seq, x_physics = inputs
        
        batch_size = tf.shape(x_seq)[0]
        
        # PatchTST ì²˜ë¦¬
        x = tf.reshape(x_seq, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        
        attn = self.attention(x, x, training=training)
        x = self.norm(x + attn)
        
        x = self.flatten(x)
        temporal_features = self.temporal_dense(x)
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬
        physics_features = self.physics_net(x_physics)
        
        # ìœµí•©
        combined = tf.concat([temporal_features, physics_features], axis=-1)
        output = self.fusion(combined)
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ… (310+ êµ¬ê°„ ê°•í™”)
        boost_factor = self.extreme_boost(combined)
        output = output * (1 + boost_factor * 0.2)  # ìµœëŒ€ 20% ë¶€ìŠ¤íŒ…
        
        return tf.squeeze(output, axis=-1)

# ========================================
# ì†ì‹¤ í•¨ìˆ˜ V3
# ========================================

class ExtremeLossV3(tf.keras.losses.Loss):
    """ê·¹ë‹¨ê°’ íŠ¹í™” ì†ì‹¤í•¨ìˆ˜ V3"""
    
    def __init__(self, extreme_focus=False):
        super().__init__()
        self.extreme_focus = extreme_focus
        
    def call(self, y_true, y_pred):
        y_true = tf.reshape(y_true, [-1])
        y_pred = tf.reshape(y_pred, [-1])
        
        mse = tf.square(y_true - y_pred)
        
        if self.extreme_focus:
            # Model 2ìš© - ê·¹ë‹¨ê°’ ë” ê°•ì¡°
            weight = tf.where(y_true > 0.52, 20.0,    # 335+ (ìŠ¤ì¼€ì¼ í›„)
                     tf.where(y_true > 0.45, 15.0,    # 310+
                     tf.where(y_true > 0.4, 5.0,      # 300+
                     1.0)))
        else:
            # Model 1ìš© - ì „ì²´ ê· í˜•
            weight = tf.where(y_true > 0.52, 10.0,    # 335+
                     tf.where(y_true > 0.45, 8.0,     # 310+
                     tf.where(y_true > 0.4, 3.0,      # 300+
                     1.0)))
        
        return mse * weight

# ========================================
# ë©”ì¸ ì‹¤í–‰ (ì¤‘ë‹¨/ì¬ì‹œì‘ ì§€ì›)
# ========================================

def main():
    # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
    ckpt = CheckpointManager()
    
    # ìƒíƒœ ë¡œë“œ
    state = ckpt.load_state()
    if state:
        print(f"\nâ™»ï¸ ì´ì „ ìƒíƒœ ë°œê²¬! (Step {state.get('step', 1)})")
        
        resume = input("ì´ì–´ì„œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y: ì´ì–´ì„œ, n: ì²˜ìŒë¶€í„°): ").lower()
        
        if resume != 'y':
            ckpt.clear_state()
            state = None
            step = 1
        else:
            step = state.get('step', 1)
            print(f"âœ… Step {step}ë¶€í„° ì¬ê°œí•©ë‹ˆë‹¤.")
    else:
        state = {}
        step = 1
    
    # ë°ì´í„° ì²˜ë¦¬
    processor = DataProcessorV3()
    
    # Step 1: ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    if step == 1:
        print("\n[Step 1/6] ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
        df = pd.read_csv('data/HUB_0509_TO_0730_DATA.CSV')
        print(f"âœ… ë°ì´í„° ë¡œë“œ: {df.shape}")
        
        processor.analyze_data(df)
        
        df['timestamp'] = pd.to_datetime(df.iloc[:, 0], format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('timestamp').reset_index(drop=True)
        df = df.fillna(method='ffill').fillna(0)
        
        state['df_shape'] = df.shape
        state['step'] = 2
        ckpt.save_state(state)
        
        # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•´ df ì €ì¥
        df.to_pickle('./checkpoints/processed_df.pkl')
        print("ğŸ’¾ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
    
    # Step 2: ì‹œí€€ìŠ¤ ìƒì„±
    if step <= 2:
        print("\n[Step 2/6] ì‹œí€€ìŠ¤ ìƒì„±")
        
        if step == 2:
            # Step 2 ì‹¤í–‰
            df = pd.read_pickle('./checkpoints/processed_df.pkl')
            X, y, X_physics, weights = processor.create_sequences_v3(df)
            
            state['X'] = X
            state['y'] = y
            state['X_physics'] = X_physics
            state['weights'] = weights
            state['n_features'] = X.shape[2]
            state['step'] = 3
            ckpt.save_state(state)
            print("âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ")
        else:
            # ì´ì „ Stepì—ì„œ ì €ì¥ëœ ë°ì´í„° ë¡œë“œ
            X = state['X']
            y = state['y']
            X_physics = state['X_physics']
            weights = state['weights']
    
    # Step 3: ë°ì´í„° ë¶„í• 
    if step <= 3:
        if step == 3:
            print("\n[Step 3/6] ë°ì´í„° ë¶„í• ")
            
            X = state['X']
            y = state['y']
            X_physics = state['X_physics']
            weights = state['weights']
            
            indices = np.arange(len(X))
            train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)
            val_idx, test_idx = train_test_split(test_idx, test_size=0.5, random_state=42)
            
            state['train_idx'] = train_idx
            state['val_idx'] = val_idx
            state['test_idx'] = test_idx
            state['step'] = 4
            ckpt.save_state(state)
        else:
            train_idx = state['train_idx']
            val_idx = state['val_idx']
            test_idx = state['test_idx']
    
    # Step 4: ìŠ¤ì¼€ì¼ë§
    if step <= 4:
        if step == 4:
            print("\n[Step 4/6] ë°ì´í„° ìŠ¤ì¼€ì¼ë§")
            
            X = state['X']
            y = state['y']
            X_physics = state['X_physics']
            weights = state['weights']
            n_features = state['n_features']
            
            train_idx = state['train_idx']
            val_idx = state['val_idx']
            test_idx = state['test_idx']
            
            # ë°ì´í„° ë¶„í• 
            X_train, y_train = X[train_idx], y[train_idx]
            X_val, y_val = X[val_idx], y[val_idx]
            X_test, y_test = X[test_idx], y[test_idx]
            
            X_physics_train = X_physics[train_idx]
            X_physics_val = X_physics[val_idx]
            X_physics_test = X_physics[test_idx]
            
            weights_train = weights[train_idx]
            
            # ìŠ¤ì¼€ì¼ë§
            X_train_flat = X_train.reshape(-1, n_features)
            X_train_scaled = processor.scaler_X.fit_transform(X_train_flat)
            X_train_scaled = X_train_scaled.reshape(len(X_train), 20, n_features)
            
            X_val_scaled = processor.scaler_X.transform(X_val.reshape(-1, n_features)).reshape(len(X_val), 20, n_features)
            X_test_scaled = processor.scaler_X.transform(X_test.reshape(-1, n_features)).reshape(len(X_test), 20, n_features)
            
            y_train_scaled = processor.scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
            y_val_scaled = processor.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
            y_test_scaled = processor.scaler_y.transform(y_test.reshape(-1, 1)).flatten()
            
            X_physics_train_scaled = processor.scaler_physics.fit_transform(X_physics_train)
            X_physics_val_scaled = processor.scaler_physics.transform(X_physics_val)
            X_physics_test_scaled = processor.scaler_physics.transform(X_physics_test)
            
            processor.save_scalers()
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥
            os.makedirs('./test_data', exist_ok=True)
            np.save('./test_data/X_test_scaled.npy', X_test_scaled)
            np.save('./test_data/y_test_scaled.npy', y_test_scaled)
            np.save('./test_data/y_test.npy', y_test)
            np.save('./test_data/X_physics_test_scaled.npy', X_physics_test_scaled)
            
            state['X_train_scaled'] = X_train_scaled
            state['y_train_scaled'] = y_train_scaled
            state['X_val_scaled'] = X_val_scaled
            state['y_val_scaled'] = y_val_scaled
            state['X_test_scaled'] = X_test_scaled
            state['y_test_scaled'] = y_test_scaled
            state['X_physics_train_scaled'] = X_physics_train_scaled
            state['X_physics_val_scaled'] = X_physics_val_scaled
            state['X_physics_test_scaled'] = X_physics_test_scaled
            state['weights_train'] = weights_train
            state['y_train'] = y_train
            state['y_val'] = y_val
            state['y_test'] = y_test
            state['step'] = 5
            ckpt.save_state(state)
            
            print(f"\nğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°:")
            print(f"  Train: {len(train_idx)} (310+: {(y_train >= 310).sum()}, 335+: {(y_train >= 335).sum()})")
            print(f"  Valid: {len(val_idx)} (310+: {(y_val >= 310).sum()}, 335+: {(y_val >= 335).sum()})")
            print(f"  Test: {len(test_idx)} (310+: {(y_test >= 310).sum()}, 335+: {(y_test >= 335).sum()})")
    
    # Step 5: ëª¨ë¸ í•™ìŠµ
    if step <= 5:
        print("\n[Step 5/6] ëª¨ë¸ í•™ìŠµ")
        
        # ë°ì´í„° ë¡œë“œ
        X_train_scaled = state['X_train_scaled']
        y_train_scaled = state['y_train_scaled']
        X_val_scaled = state['X_val_scaled']
        y_val_scaled = state['y_val_scaled']
        X_physics_train_scaled = state['X_physics_train_scaled']
        X_physics_val_scaled = state['X_physics_val_scaled']
        weights_train = state['weights_train']
        n_features = state.get('n_features', X_train_scaled.shape[2])
        
        config = {
            'seq_len': 20,
            'n_features': n_features,
            'patch_len': 5
        }
        
        # Model 1 í•™ìŠµ ì²´í¬
        model1_trained = state.get('model1_trained', False)
        
        if not model1_trained:
            print("\nğŸ¤– Model 1: PatchTST í•™ìŠµ")
            model1 = PatchTSTModel(config)
            model1.compile(
                optimizer=Adam(learning_rate=0.001),
                loss=ExtremeLossV3(extreme_focus=False),
                metrics=['mae']
            )
            
            callbacks_model1 = [
                EarlyStopping(patience=20, restore_best_weights=True),
                ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-6),
                ModelCheckpoint('./checkpoints/model1_v3.h5', save_best_only=True, save_weights_only=True),
                ExtremeValueCallback(X_val_scaled, y_val_scaled, processor.scaler_y)
            ]
            
            history1 = model1.fit(
                X_train_scaled, y_train_scaled,
                validation_data=(X_val_scaled, y_val_scaled),
                sample_weight=weights_train,
                epochs=50,
                batch_size=32,
                callbacks=callbacks_model1,
                verbose=1
            )
            
            # ëª¨ë¸ 1 ì™„ë£Œ ìƒíƒœ ì €ì¥
            state['model1_trained'] = True
            ckpt.save_state(state)
            print("âœ… Model 1 í•™ìŠµ ì™„ë£Œ")
        
        # Model 2 í•™ìŠµ
        model2_trained = state.get('model2_trained', False)
        
        if not model2_trained:
            print("\nğŸ¤– Model 2: PatchTST + PINN í•™ìŠµ")
            model2 = PatchTSTPINN(config)
            model2.compile(
                optimizer=Adam(learning_rate=0.0008),
                loss=ExtremeLossV3(extreme_focus=True),
                metrics=['mae']
            )
            
            callbacks_model2 = [
                EarlyStopping(patience=20, restore_best_weights=True),
                ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-6),
                ModelCheckpoint('./checkpoints/model2_v3.h5', save_best_only=True, save_weights_only=True),
                ExtremeValueCallback((X_val_scaled, X_physics_val_scaled), y_val_scaled, processor.scaler_y)
            ]
            
            history2 = model2.fit(
                [X_train_scaled, X_physics_train_scaled], y_train_scaled,
                validation_data=([X_val_scaled, X_physics_val_scaled], y_val_scaled),
                sample_weight=weights_train,
                epochs=60,
                batch_size=32,
                callbacks=callbacks_model2,
                verbose=1
            )
            
            state['model2_trained'] = True
            state['step'] = 6
            ckpt.save_state(state)
            print("âœ… Model 2 í•™ìŠµ ì™„ë£Œ")
    
    # Step 6: í‰ê°€
    if step <= 6:
        print("\n[Step 6/6] ëª¨ë¸ í‰ê°€")
        print("="*80)
        print("ğŸ“Š ìµœì¢… í‰ê°€")
        print("="*80)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        X_test_scaled = state['X_test_scaled']
        y_test = state['y_test']
        X_physics_test_scaled = state['X_physics_test_scaled']
        n_features = state.get('n_features', X_test_scaled.shape[2])
        
        config = {
            'seq_len': 20,
            'n_features': n_features,
            'patch_len': 5
        }
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        if not processor.load_scalers():
            print("âŒ í‰ê°€ ì¤‘ë‹¨: ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨")
            return
        
        # Model 1 í‰ê°€
        print("\n[Model 1: PatchTST]")
        model1 = PatchTSTModel(config)
        model1.compile(optimizer='adam', loss='mse')
        
        # ë”ë¯¸ ë°ì´í„°ë¡œ ë¹Œë“œ
        dummy_input = np.zeros((1, 20, n_features))
        _ = model1(dummy_input)
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model1.load_weights('./checkpoints/model1_v3.h5')
        
        y_pred1_scaled = model1.predict(X_test_scaled, verbose=0)
        y_pred1 = processor.scaler_y.inverse_transform(y_pred1_scaled.reshape(-1, 1)).flatten()
        
        mae1 = np.mean(np.abs(y_test - y_pred1))
        print(f"ì „ì²´ MAE: {mae1:.2f}")
        
        # 3êµ¬ê°„ í‰ê°€
        print("\n3êµ¬ê°„ ì„±ëŠ¥:")
        mask_low = y_test < 200
        mask_normal = (y_test >= 200) & (y_test < 300)
        mask_danger = y_test >= 300
        
        if mask_low.sum() > 0:
            print(f"  ì €êµ¬ê°„(<200): MAE={np.mean(np.abs(y_test[mask_low] - y_pred1[mask_low])):.2f}")
        if mask_normal.sum() > 0:
            print(f"  ì •ìƒ(200-300): MAE={np.mean(np.abs(y_test[mask_normal] - y_pred1[mask_normal])):.2f}")
        if mask_danger.sum() > 0:
            print(f"  ìœ„í—˜(300+): MAE={np.mean(np.abs(y_test[mask_danger] - y_pred1[mask_danger])):.2f}")
        
        # ê·¹ë‹¨ê°’ ê°ì§€
        print("\nê·¹ë‹¨ê°’ ê°ì§€:")
        for threshold in [310, 335]:
            mask = y_test >= threshold
            if mask.sum() > 0:
                detected = (y_pred1 >= threshold)[mask].sum()
                print(f"  {threshold}+: {detected}/{mask.sum()} ({detected/mask.sum()*100:.1f}%)")
        
        # Model 2 í‰ê°€
        print("\n[Model 2: PatchTST + PINN]")
        model2 = PatchTSTPINN(config)
        model2.compile(optimizer='adam', loss='mse')
        
        # ë”ë¯¸ ë°ì´í„°ë¡œ ë¹Œë“œ
        dummy_seq = np.zeros((1, 20, n_features))
        dummy_physics = np.zeros((1, 3))
        _ = model2([dummy_seq, dummy_physics])
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model2.load_weights('./checkpoints/model2_v3.h5')
        
        y_pred2_scaled = model2.predict([X_test_scaled, X_physics_test_scaled], verbose=0)
        y_pred2 = processor.scaler_y.inverse_transform(y_pred2_scaled.reshape(-1, 1)).flatten()
        
        mae2 = np.mean(np.abs(y_test - y_pred2))
        print(f"ì „ì²´ MAE: {mae2:.2f}")
        
        # 3êµ¬ê°„ í‰ê°€
        print("\n3êµ¬ê°„ ì„±ëŠ¥:")
        if mask_low.sum() > 0:
            print(f"  ì €êµ¬ê°„(<200): MAE={np.mean(np.abs(y_test[mask_low] - y_pred2[mask_low])):.2f}")
        if mask_normal.sum() > 0:
            print(f"  ì •ìƒ(200-300): MAE={np.mean(np.abs(y_test[mask_normal] - y_pred2[mask_normal])):.2f}")
        if mask_danger.sum() > 0:
            print(f"  ìœ„í—˜(300+): MAE={np.mean(np.abs(y_test[mask_danger] - y_pred2[mask_danger])):.2f}")
        
        # ê·¹ë‹¨ê°’ ê°ì§€
        print("\nê·¹ë‹¨ê°’ ê°ì§€:")
        for threshold in [310, 335]:
            mask = y_test >= threshold
            if mask.sum() > 0:
                detected = (y_pred2 >= threshold)[mask].sum()
                print(f"  {threshold}+: {detected}/{mask.sum()} ({detected/mask.sum()*100:.1f}%)")
        
        print("\nâœ… V3.1 ì™„ë£Œ!")
        
        # ì™„ë£Œ í›„ ìƒíƒœ íŒŒì¼ ì œê±° ì˜µì…˜
        remove = input("\nìƒíƒœ íŒŒì¼ì„ ì œê±°í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
        if remove == 'y':
            ckpt.clear_state()
            print("ğŸ§¹ ìƒíƒœ íŒŒì¼ ì œê±° ì™„ë£Œ")

if __name__ == "__main__":
    main()