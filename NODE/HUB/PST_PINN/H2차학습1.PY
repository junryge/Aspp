# -*- coding: utf-8 -*-
"""
HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - 300+ ê°ì§€ íŠ¹í™” ë²„ì „
- ê·¹ë‹¨ê°’(300+) ì˜ˆì¸¡ ì •í™•ë„ ìµœìš°ì„ 
- ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥/ë¡œë“œ ì™„ë²½ ì§€ì›
- ë°ì´í„° ë¶ˆê· í˜• í•´ê²°
- PINN ëª¨ë¸ ì™„ì „ ì¬ì„¤ê³„
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight
import warnings
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import pickle
import os
import json
from tqdm import tqdm
import joblib
from collections import Counter

warnings.filterwarnings('ignore')

# TensorFlow ì„¤ì •
print(f"TensorFlow Version: {tf.__version__}")
tf.random.set_seed(42)
np.random.seed(42)

# GPU ë©”ëª¨ë¦¬ ì„¤ì •
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# ===========================
# ğŸ¯ ê·¹ë‹¨ê°’ íŠ¹í™” ì†ì‹¤í•¨ìˆ˜
# ===========================

class ExtremeLoss(tf.keras.losses.Loss):
    """300 ì´ìƒ ê·¹ë‹¨ê°’ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ì†ì‹¤í•¨ìˆ˜"""
    
    def __init__(self, threshold=300, extreme_weight=10.0, name='extreme_loss'):
        super().__init__(name=name)
        self.threshold = threshold
        self.extreme_weight = extreme_weight
    
    def call(self, y_true, y_pred):
        # ê¸°ë³¸ MSE
        mse = tf.square(y_true - y_pred)
        
        # ê·¹ë‹¨ê°’ ë§ˆìŠ¤í¬ (300 ì´ìƒ)
        extreme_mask = tf.cast(y_true >= self.threshold, tf.float32)
        
        # ê°€ì¤‘ì¹˜ ì ìš©
        weights = 1.0 + extreme_mask * (self.extreme_weight - 1.0)
        
        # ê°€ì¤‘ MSE
        weighted_mse = mse * weights
        
        return tf.reduce_mean(weighted_mse)

class FocalMSELoss(tf.keras.losses.Loss):
    """ì˜ˆì¸¡ì´ ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘í•˜ëŠ” Focal Lossì˜ MSE ë²„ì „"""
    
    def __init__(self, gamma=2.0, threshold=300, name='focal_mse_loss'):
        super().__init__(name=name)
        self.gamma = gamma
        self.threshold = threshold
    
    def call(self, y_true, y_pred):
        # ì ˆëŒ€ ì˜¤ì°¨
        abs_error = tf.abs(y_true - y_pred)
        
        # Focal ê°€ì¤‘ì¹˜ (ì˜¤ì°¨ê°€ í´ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
        focal_weight = tf.pow(abs_error / 100.0, self.gamma)
        
        # ê·¹ë‹¨ê°’ ì¶”ê°€ ê°€ì¤‘ì¹˜
        extreme_mask = tf.cast(y_true >= self.threshold, tf.float32)
        extreme_weight = 1.0 + extreme_mask * 4.0
        
        # ìµœì¢… ì†ì‹¤
        mse = tf.square(y_true - y_pred)
        weighted_loss = mse * focal_weight * extreme_weight
        
        return tf.reduce_mean(weighted_loss)

# ===========================
# ğŸ“Š ê·¹ë‹¨ê°’ íŠ¹í™” ë°ì´í„° ì²˜ë¦¬
# ===========================

class ExtremeValueProcessor:
    """ê·¹ë‹¨ê°’ ì²˜ë¦¬ì— íŠ¹í™”ëœ ë°ì´í„° í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, file_path='data/HUB_0509_TO_0730_DATA.CSV'):
        self.file_path = file_path
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        
        # ì„ê³„ê°’ ì„¤ì •
        self.thresholds = {
            'normal': 250,
            'warning': 280,
            'critical': 300,
            'extreme': 400,
            'disaster': 500
        }
        
        # ìŠ¤ì¼€ì¼ëŸ¬ (ê·¹ë‹¨ê°’ì— ê°•ê±´í•œ RobustScaler ì‚¬ìš©)
        self.scaler_X = RobustScaler()
        self.scaler_y = MinMaxScaler(feature_range=(0, 1))  # 0-1ë¡œ ì •ê·œí™”
        self.scaler_physics = StandardScaler()
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ê²½ë¡œ
        self.scaler_dir = './scalers'
        os.makedirs(self.scaler_dir, exist_ok=True)
        
        # ë¬¼ë¦¬ì  ì»¬ëŸ¼
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2', 'M14B_7F_TO_HUB_JOB2'
        ]
        
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB', 'M16A_3F_TO_M14B_7F_JOB'
        ]
    
    def save_scalers(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥"""
        joblib.dump(self.scaler_X, os.path.join(self.scaler_dir, 'scaler_X.pkl'))
        joblib.dump(self.scaler_y, os.path.join(self.scaler_dir, 'scaler_y.pkl'))
        joblib.dump(self.scaler_physics, os.path.join(self.scaler_dir, 'scaler_physics.pkl'))
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ: {self.scaler_dir}")
    
    def load_scalers(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        try:
            self.scaler_X = joblib.load(os.path.join(self.scaler_dir, 'scaler_X.pkl'))
            self.scaler_y = joblib.load(os.path.join(self.scaler_dir, 'scaler_y.pkl'))
            self.scaler_physics = joblib.load(os.path.join(self.scaler_dir, 'scaler_physics.pkl'))
            print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ: {self.scaler_dir}")
            return True
        except:
            print("âš ï¸ ì €ì¥ëœ ìŠ¤ì¼€ì¼ëŸ¬ ì—†ìŒ. ìƒˆë¡œ í•™ìŠµ í•„ìš”.")
            return False
    
    def analyze_extremes(self, df):
        """ê·¹ë‹¨ê°’ ìƒì„¸ ë¶„ì„"""
        if self.target_col not in df.columns:
            return
        
        target = df[self.target_col]
        
        print("\n" + "="*60)
        print("ğŸ” ê·¹ë‹¨ê°’ ë¶„ì„ (HUBROOM ë°˜ì†¡ëŸ‰)")
        print("="*60)
        
        print(f"\nğŸ“Š ê¸°ë³¸ í†µê³„:")
        print(f"  - ì „ì²´ ë²”ìœ„: {target.min():.0f} ~ {target.max():.0f}")
        print(f"  - í‰ê· : {target.mean():.2f}")
        print(f"  - ì¤‘ì•™ê°’: {target.median():.0f}")
        print(f"  - í‘œì¤€í¸ì°¨: {target.std():.2f}")
        
        print(f"\nğŸš¨ ì„ê³„ê°’ë³„ ë¶„í¬:")
        for name, threshold in self.thresholds.items():
            count = (target >= threshold).sum()
            ratio = count / len(target) * 100
            if ratio > 0:
                print(f"  - {name:8} (â‰¥{threshold:3}): {count:5}ê°œ ({ratio:5.2f}%)")
        
        # ê·¹ë‹¨ê°’ êµ¬ê°„ë³„ ìƒì„¸
        extreme_data = target[target >= 300]
        if len(extreme_data) > 0:
            print(f"\nğŸ’¥ ê·¹ë‹¨ê°’ êµ¬ê°„ ìƒì„¸ (â‰¥300):")
            bins = [300, 350, 400, 450, 500, 600, 700, 1000]
            for i in range(len(bins)-1):
                mask = (extreme_data >= bins[i]) & (extreme_data < bins[i+1])
                count = mask.sum()
                if count > 0:
                    print(f"    [{bins[i]:3}-{bins[i+1]:3}): {count:4}ê°œ - "
                          f"í‰ê· : {extreme_data[mask].mean():.1f}, "
                          f"ìµœëŒ€: {extreme_data[mask].max():.0f}")
    
    def create_balanced_sequences(self, df, numeric_cols, seq_len=20, pred_len=10):
        """ê·¹ë‹¨ê°’ ì˜¤ë²„ìƒ˜í”Œë§ì´ ì ìš©ëœ ì‹œí€€ìŠ¤ ìƒì„±"""
        
        X, y, X_physics, sample_weights = [], [], [], []
        
        available_inflow = [col for col in self.inflow_cols if col in df.columns]
        available_outflow = [col for col in self.outflow_cols if col in df.columns]
        
        total_sequences = len(df) - seq_len - pred_len + 1
        
        # ê·¹ë‹¨ê°’ ì¸ë±ìŠ¤ ì°¾ê¸°
        extreme_indices = []
        normal_indices = []
        
        print("\nğŸ“¦ ê· í˜•ì¡íŒ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        
        for i in tqdm(range(total_sequences), desc="ì‹œí€€ìŠ¤ ìŠ¤ìº”"):
            target_value = df[self.target_col].iloc[i + seq_len + pred_len - 1]
            
            if target_value >= 300:
                extreme_indices.append(i)
            else:
                normal_indices.append(i)
        
        print(f"  - ì •ìƒ ì‹œí€€ìŠ¤: {len(normal_indices)}ê°œ")
        print(f"  - ê·¹ë‹¨ê°’ ì‹œí€€ìŠ¤ (â‰¥300): {len(extreme_indices)}ê°œ")
        
        # ê·¹ë‹¨ê°’ ì˜¤ë²„ìƒ˜í”Œë§ (3ë°°)
        oversample_rate = 3
        extreme_indices_oversampled = extreme_indices * oversample_rate
        all_indices = normal_indices + extreme_indices_oversampled
        np.random.shuffle(all_indices)
        
        print(f"  - ì˜¤ë²„ìƒ˜í”Œë§ í›„ ê·¹ë‹¨ê°’: {len(extreme_indices_oversampled)}ê°œ")
        print(f"  - ì „ì²´ í•™ìŠµ ì‹œí€€ìŠ¤: {len(all_indices)}ê°œ")
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        for i in tqdm(all_indices, desc="ì‹œí€€ìŠ¤ ìƒì„±"):
            # ì…ë ¥ ì‹œí€€ìŠ¤
            X_seq = df[numeric_cols].iloc[i:i+seq_len].values
            X.append(X_seq)
            
            # íƒ€ê²Ÿ (10ë¶„ í›„)
            y_val = df[self.target_col].iloc[i + seq_len + pred_len - 1]
            y.append(y_val)
            
            # ê°€ì¤‘ì¹˜ (ê·¹ë‹¨ê°’ì€ ë†’ì€ ê°€ì¤‘ì¹˜)
            if y_val >= 300:
                weight = 10.0
            elif y_val >= 280:
                weight = 5.0
            else:
                weight = 1.0
            sample_weights.append(weight)
            
            # ë¬¼ë¦¬ ë°ì´í„°
            physics_data = np.array([
                df[self.target_col].iloc[i + seq_len - 1],  # í˜„ì¬ HUBROOM
                df[available_inflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_inflow else 0,
                df[available_outflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_outflow else 0
            ])
            X_physics.append(physics_data)
        
        return (np.array(X), np.array(y), np.array(X_physics), np.array(sample_weights))

# ===========================
# ğŸ§  ê·¹ë‹¨ê°’ íŠ¹í™” PatchTST ëª¨ë¸
# ===========================

class ExtremePatchTST(keras.Model):
    """ê·¹ë‹¨ê°’ ì˜ˆì¸¡ì— íŠ¹í™”ëœ PatchTST"""
    
    def __init__(self, config):
        super(ExtremePatchTST, self).__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        self.d_model = config['d_model']
        self.n_heads = config['n_heads']
        self.d_ff = config['d_ff']
        self.n_layers = config['n_layers']
        self.dropout_rate = config['dropout']
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(self.d_model, activation='relu')
        
        # ìœ„ì¹˜ ì¸ì½”ë”©
        self.pos_embedding = self.add_weight(
            name='pos_embedding',
            shape=(1, self.n_patches, self.d_model),
            initializer='random_normal',
            trainable=True
        )
        
        # Transformer ì¸ì½”ë”
        self.encoder_layers = []
        for _ in range(self.n_layers):
            self.encoder_layers.append(
                layers.MultiHeadAttention(
                    num_heads=self.n_heads,
                    key_dim=self.d_model // self.n_heads,
                    dropout=self.dropout_rate
                )
            )
            self.encoder_layers.append(layers.LayerNormalization())
            self.encoder_layers.append(layers.Dropout(self.dropout_rate))
        
        # ê·¹ë‹¨ê°’ ê°ì§€ ë¸Œëœì¹˜
        self.extreme_detector = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dense(1, activation='sigmoid')  # 300 ì´ìƒ í™•ë¥ 
        ])
        
        # ê°’ ì˜ˆì¸¡ ë¸Œëœì¹˜
        self.value_predictor = keras.Sequential([
            layers.Dense(128, activation='relu'),
            layers.Dropout(self.dropout_rate),
            layers.Dense(64, activation='relu'),
            layers.Dense(1)
        ])
        
        self.flatten = layers.Flatten()
    
    def create_patches(self, x):
        batch_size = tf.shape(x)[0]
        x_reshaped = tf.reshape(x, [batch_size, self.n_patches, self.patch_len, self.n_features])
        x_patches = tf.reshape(x_reshaped, [batch_size, self.n_patches, self.patch_len * self.n_features])
        return x_patches
    
    def call(self, x, training=False):
        # íŒ¨ì¹˜ ìƒì„±
        x_patches = self.create_patches(x)
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x_embed = self.patch_embedding(x_patches)
        x_embed = x_embed + self.pos_embedding
        
        # Transformer ì¸ì½”ë”
        for i in range(0, len(self.encoder_layers), 3):
            attention = self.encoder_layers[i](x_embed, x_embed, training=training)
            norm = self.encoder_layers[i+1](attention + x_embed)
            x_embed = self.encoder_layers[i+2](norm, training=training)
        
        # Flatten
        x_flat = self.flatten(x_embed)
        
        # ê·¹ë‹¨ê°’ ê°ì§€ì™€ ê°’ ì˜ˆì¸¡
        extreme_prob = self.extreme_detector(x_flat)
        value_pred = self.value_predictor(x_flat)
        
        # ê·¹ë‹¨ê°’ í™•ë¥ ì„ ì´ìš©í•œ ë³´ì •
        # ê·¹ë‹¨ê°’ í™•ë¥ ì´ ë†’ìœ¼ë©´ ì˜ˆì¸¡ê°’ì„ ìƒí–¥ ì¡°ì •
        adjusted_pred = value_pred + extreme_prob * 50  # ê·¹ë‹¨ê°’ì´ë©´ +50 ë³´ì •
        
        return tf.squeeze(adjusted_pred, axis=-1)

# ===========================
# ğŸ”¬ ê°œì„ ëœ PINN ëª¨ë¸
# ===========================

class ImprovedPINN(keras.Model):
    """ë¬¼ë¦¬ ë²•ì¹™ì´ ê°œì„ ëœ PINN ëª¨ë¸"""
    
    def __init__(self, config):
        super(ImprovedPINN, self).__init__()
        
        # ì‹œê³„ì—´ ì²˜ë¦¬ (ê°„ë‹¨í•œ LSTM)
        self.lstm = layers.LSTM(64, return_sequences=False)
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬
        self.physics_net = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dense(16, activation='relu')
        ])
        
        # í†µí•© ë„¤íŠ¸ì›Œí¬
        self.fusion_net = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dense(1)
        ])
    
    def call(self, inputs, training=False):
        x_seq, x_physics = inputs
        
        # ì‹œê³„ì—´ íŠ¹ì§•
        seq_features = self.lstm(x_seq)
        
        # ë¬¼ë¦¬ íŠ¹ì§•
        physics_features = self.physics_net(x_physics)
        
        # íŠ¹ì§• ê²°í•©
        combined = tf.concat([seq_features, physics_features], axis=-1)
        
        # ìµœì¢… ì˜ˆì¸¡
        output = self.fusion_net(combined)
        
        return tf.squeeze(output, axis=-1)

# ===========================
# ğŸ“ˆ ê·¹ë‹¨ê°’ ëª¨ë‹ˆí„°ë§ ì½œë°±
# ===========================

class ExtremeValueMonitor(Callback):
    """í•™ìŠµ ì¤‘ ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, X_val, y_val, scaler_y, threshold=300):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val
        self.scaler_y = scaler_y
        self.threshold = threshold
    
    def on_epoch_end(self, epoch, logs=None):
        # ì˜ˆì¸¡
        y_pred_scaled = self.model.predict(self.X_val, verbose=0)
        
        # ì—­ì •ê·œí™”
        y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
        y_true = self.scaler_y.inverse_transform(self.y_val.reshape(-1, 1)).flatten()
        
        # ê·¹ë‹¨ê°’ ì •í™•ë„
        true_extreme = y_true >= self.threshold
        pred_extreme = y_pred >= self.threshold
        
        if true_extreme.sum() > 0:
            recall = (true_extreme & pred_extreme).sum() / true_extreme.sum()
            print(f"\n  ğŸ¯ Epoch {epoch+1} - ê·¹ë‹¨ê°’(â‰¥{self.threshold}) Recall: {recall:.2%}")
            
            # ê·¹ë‹¨ê°’ ìƒ˜í”Œ ì¶œë ¥
            extreme_indices = np.where(true_extreme)[0][:3]
            for idx in extreme_indices:
                print(f"     ì‹¤ì œ: {y_true[idx]:.0f}, ì˜ˆì¸¡: {y_pred[idx]:.0f}")

# ===========================
# ğŸ¯ ë©”ì¸ ì‹¤í–‰
# ===========================

def main():
    print("="*80)
    print("ğŸ­ HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
    print("ğŸ¯ ëª©í‘œ: 300 ì´ìƒ ê·¹ë‹¨ê°’ ì •í™•í•œ ì˜ˆì¸¡")
    print("="*80)
    
    # ë°ì´í„° í”„ë¡œì„¸ì„œ
    processor = ExtremeValueProcessor()
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df = pd.read_csv(processor.file_path)
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape}")
    
    # 2. ê·¹ë‹¨ê°’ ë¶„ì„
    processor.analyze_extremes(df)
    
    # 3. ì „ì²˜ë¦¬
    print("\nğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    time_col = df.columns[0]
    df['timestamp'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M', errors='coerce')
    df = df.sort_values('timestamp').reset_index(drop=True)
    df = df.fillna(method='ffill').fillna(0)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # 4. ê· í˜•ì¡íŒ ì‹œí€€ìŠ¤ ìƒì„±
    X, y, X_physics, sample_weights = processor.create_balanced_sequences(
        df, numeric_cols, seq_len=20, pred_len=10
    )
    
    print(f"\nğŸ“Š ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ:")
    print(f"  - X shape: {X.shape}")
    print(f"  - y shape: {y.shape}")
    print(f"  - ê·¹ë‹¨ê°’ ê°€ì¤‘ì¹˜ ì ìš©: {(sample_weights > 1).sum()}ê°œ")
    
    # 5. ë°ì´í„° ë¶„í• 
    # ê°€ì¤‘ì¹˜ë„ í•¨ê»˜ ë¶„í• 
    indices = np.arange(len(X))
    train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=42)
    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)
    
    X_train, y_train = X[train_idx], y[train_idx]
    X_val, y_val = X[val_idx], y[val_idx]
    X_test, y_test = X[test_idx], y[test_idx]
    
    X_physics_train = X_physics[train_idx]
    X_physics_val = X_physics[val_idx]
    X_physics_test = X_physics[test_idx]
    
    weights_train = sample_weights[train_idx]
    
    print(f"\nğŸ”€ ë°ì´í„° ë¶„í• :")
    print(f"  - Train: {len(X_train)} ({(y_train >= 300).sum()} ê·¹ë‹¨ê°’)")
    print(f"  - Valid: {len(X_val)} ({(y_val >= 300).sum()} ê·¹ë‹¨ê°’)")
    print(f"  - Test: {len(X_test)} ({(y_test >= 300).sum()} ê·¹ë‹¨ê°’)")
    
    # 6. ìŠ¤ì¼€ì¼ë§
    print("\nğŸ“ ë°ì´í„° ì •ê·œí™” ì¤‘...")
    n_samples_train, seq_len, n_features = X_train.shape
    
    # X ìŠ¤ì¼€ì¼ë§ (RobustScaler)
    X_train_scaled = processor.scaler_X.fit_transform(
        X_train.reshape(-1, n_features)
    ).reshape(n_samples_train, seq_len, n_features)
    
    X_val_scaled = processor.scaler_X.transform(
        X_val.reshape(-1, n_features)
    ).reshape(X_val.shape[0], seq_len, n_features)
    
    X_test_scaled = processor.scaler_X.transform(
        X_test.reshape(-1, n_features)
    ).reshape(X_test.shape[0], seq_len, n_features)
    
    # y ìŠ¤ì¼€ì¼ë§ (MinMaxScaler)
    y_train_scaled = processor.scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_val_scaled = processor.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
    y_test_scaled = processor.scaler_y.transform(y_test.reshape(-1, 1)).flatten()
    
    # ë¬¼ë¦¬ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    X_physics_train_scaled = processor.scaler_physics.fit_transform(X_physics_train)
    X_physics_val_scaled = processor.scaler_physics.transform(X_physics_val)
    X_physics_test_scaled = processor.scaler_physics.transform(X_physics_test)
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    processor.save_scalers()
    
    # 7. ëª¨ë¸ êµ¬ì„±
    config = {
        'seq_len': 20,
        'n_features': n_features,
        'patch_len': 5,
        'd_model': 128,
        'n_heads': 8,
        'd_ff': 256,
        'n_layers': 4,
        'dropout': 0.2
    }
    
    # ì½œë°±
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),
        ExtremeValueMonitor(X_val_scaled, y_val_scaled, processor.scaler_y)
    ]
    
    results = {}
    
    # ëª¨ë¸ 1: ExtremePatchTST í•™ìŠµ
    print("\n" + "="*60)
    print("ğŸš€ ëª¨ë¸ 1: ExtremePatchTST í•™ìŠµ ì‹œì‘")
    print("="*60)
    model1 = ExtremePatchTST(config)
    
    # ê·¹ë‹¨ê°’ íŠ¹í™” ì†ì‹¤í•¨ìˆ˜ ì‚¬ìš©
    model1.compile(
        optimizer=Adam(learning_rate=0.001),
        loss=ExtremeLoss(threshold=300, extreme_weight=10),
        metrics=['mae']
    )
    
    history1 = model1.fit(
        X_train_scaled, y_train_scaled,
        validation_data=(X_val_scaled, y_val_scaled),
        sample_weight=weights_train,  # ê°€ì¤‘ì¹˜ ì ìš©
        epochs=50,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )
    
    # í‰ê°€
    y_pred1_scaled = model1.predict(X_test_scaled)
    y_pred1 = processor.scaler_y.inverse_transform(y_pred1_scaled.reshape(-1, 1)).flatten()
    results['ExtremePatchTST'] = y_pred1
    
    # ëª¨ë¸ 2: ImprovedPINN í•™ìŠµ
    print("\n" + "="*60)
    print("ğŸš€ ëª¨ë¸ 2: ImprovedPINN í•™ìŠµ ì‹œì‘")
    print("="*60)
    model2 = ImprovedPINN(config)
    
    model2.compile(
        optimizer=Adam(learning_rate=0.001),
        loss=FocalMSELoss(gamma=2.0, threshold=300),
        metrics=['mae']
    )
    
    history2 = model2.fit(
        [X_train_scaled, X_physics_train_scaled], y_train_scaled,
        validation_data=([X_val_scaled, X_physics_val_scaled], y_val_scaled),
        sample_weight=weights_train,
        epochs=50,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )
    
    # í‰ê°€
    y_pred2_scaled = model2.predict([X_test_scaled, X_physics_test_scaled])
    y_pred2 = processor.scaler_y.inverse_transform(y_pred2_scaled.reshape(-1, 1)).flatten()
    results['ImprovedPINN'] = y_pred2
    
    # 8. ìµœì¢… í‰ê°€
    print("\n" + "="*60)
    print("ğŸ“Š ìµœì¢… ì„±ëŠ¥ í‰ê°€")
    print("="*60)
    
    for model_name, y_pred in results.items():
        print(f"\nğŸ” {model_name}:")
        
        # ì „ì²´ ì„±ëŠ¥
        mae = np.mean(np.abs(y_test - y_pred))
        rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
        
        print(f"  ì „ì²´ MAE: {mae:.2f}, RMSE: {rmse:.2f}")
        
        # ê·¹ë‹¨ê°’ ì„±ëŠ¥
        extreme_mask = y_test >= 300
        if extreme_mask.sum() > 0:
            extreme_mae = np.mean(np.abs(y_test[extreme_mask] - y_pred[extreme_mask]))
            
            # ê·¹ë‹¨ê°’ ê°ì§€ ì„±ëŠ¥
            pred_extreme = y_pred >= 300
            tp = (extreme_mask & pred_extreme).sum()
            fp = (~extreme_mask & pred_extreme).sum()
            fn = (extreme_mask & ~pred_extreme).sum()
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            print(f"\n  ğŸš¨ ê·¹ë‹¨ê°’(â‰¥300) ì„±ëŠ¥:")
            print(f"    - MAE: {extreme_mae:.2f}")
            print(f"    - ì •ë°€ë„: {precision:.2%}")
            print(f"    - ì¬í˜„ìœ¨: {recall:.2%}")
            print(f"    - F1-Score: {f1:.2%}")
            print(f"    - ê°ì§€: {tp}/{extreme_mask.sum()} ({recall:.1%})")
            
            # ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ìƒ˜í”Œ
            print(f"\n  ğŸ“ ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ìƒ˜í”Œ:")
            extreme_indices = np.where(extreme_mask)[0][:5]
            for idx in extreme_indices:
                status = "âœ…" if abs(y_test[idx] - y_pred[idx]) < 50 else "âŒ"
                print(f"    {status} ì‹¤ì œ: {y_test[idx]:.0f}, ì˜ˆì¸¡: {y_pred[idx]:.0f}, "
                      f"ì˜¤ì°¨: {abs(y_test[idx] - y_pred[idx]):.0f}")
    
    print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("ğŸ’¾ ìŠ¤ì¼€ì¼ëŸ¬ê°€ ./scalers/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()