# -*- coding: utf-8 -*-
"""
HUBROOM ì „ì²´ ë°ì´í„° ì˜ˆì¸¡ ì‹œìŠ¤í…œ
- 00:20ë¶„ë¶€í„° ì‹œì‘ (20ë¶„ ê³¼ê±° ë°ì´í„° í•„ìš”)
- ê° ì‹œì ì—ì„œ 10ë¶„ í›„ ì˜ˆì¸¡
- ì „ì²´ ê¸°ê°„ì— ëŒ€í•´ ì—°ì†ì ìœ¼ë¡œ ì˜ˆì¸¡
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Flatten, Embedding
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import os
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

# ===========================
# ğŸ”§ ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
# ===========================

class TransformerEncoderLayer(layers.Layer):
    """ì»¤ìŠ¤í…€ Transformer Encoder Layer"""
    
    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1, **kwargs):
        super(TransformerEncoderLayer, self).__init__(**kwargs)
        
        self.mha = MultiHeadAttention(
            num_heads=num_heads, 
            key_dim=d_model // num_heads,
            dropout=dropout_rate
        )
        
        self.ffn = keras.Sequential([
            Dense(dff, activation='relu'),
            Dense(d_model)
        ])
        
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        
        self.dropout1 = Dropout(dropout_rate)
        self.dropout2 = Dropout(dropout_rate)
    
    def call(self, inputs, training=False):
        attn_output = self.mha(inputs, inputs, training=training)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)
        
        return out2

def build_patchtst_model(seq_len=20, n_features=39, patch_len=5, d_model=128, 
                        n_heads=8, d_ff=256, n_layers=3, dropout=0.1):
    """PatchTST ëª¨ë¸ ì¬êµ¬ì„±"""
    
    inputs = Input(shape=(seq_len, n_features))
    
    # Patching
    n_patches = seq_len // patch_len
    patches = tf.reshape(inputs, (-1, n_patches, patch_len * n_features))
    
    # Linear projection
    x = Dense(d_model, name='dense')(patches)
    
    # Positional encoding
    positions = tf.range(start=0, limit=n_patches, delta=1)
    pos_embedding = Embedding(input_dim=n_patches, output_dim=d_model, name='pos_embedding')(positions)
    x = x + pos_embedding
    
    # Transformer layers
    x = TransformerEncoderLayer(d_model, n_heads, d_ff, dropout, name='transformer_encoder_layer')(x)
    x = TransformerEncoderLayer(d_model, n_heads, d_ff, dropout, name='transformer_encoder_layer_1')(x)
    x = TransformerEncoderLayer(d_model, n_heads, d_ff, dropout, name='transformer_encoder_layer_2')(x)
    
    # Flatten
    x = Flatten(name='flatten')(x)
    
    # Dense layers
    x = Dense(128, activation='relu', name='dense_7')(x)
    x = Dropout(dropout, name='dropout_6')(x)
    x = Dense(64, activation='relu', name='dense_8')(x)
    outputs = Dense(1, name='dense_9')(x)
    
    model = Model(inputs=inputs, outputs=outputs, name='patch_tst')
    return model

def build_patchtst_pinn_model(seq_len=20, n_features=39, patch_len=5, d_model=128,
                             n_heads=8, d_ff=256, n_layers=3, dropout=0.1):
    """PatchTST+PINN í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì¬êµ¬ì„±"""
    
    # ë‘ ê°œì˜ ì…ë ¥
    seq_input = Input(shape=(seq_len, n_features), name='sequence_input')
    physics_input = Input(shape=(3,), name='physics_input')
    
    # PatchTST ë¶€ë¶„
    n_patches = seq_len // patch_len
    patches = tf.reshape(seq_input, (-1, n_patches, patch_len * n_features))
    
    # Linear projection
    x = Dense(d_model, name='dense_10')(patches)
    
    # Positional encoding
    positions = tf.range(start=0, limit=n_patches, delta=1)
    pos_embedding = Embedding(input_dim=n_patches, output_dim=d_model, name='pos_embedding')(positions)
    x = x + pos_embedding
    
    # Transformer layers
    for i in range(3):
        x = TransformerEncoderLayer(
            d_model, n_heads, d_ff, dropout,
            name=f'transformer_encoder_layer_{i+3}'
        )(x)
    
    # Global average pooling
    x = GlobalAveragePooling1D()(x)
    
    # Dense layers for TST output
    tst_out = Dense(128, activation='relu', name='dense_17')(x)
    tst_out = Dense(64, activation='relu', name='dense_18')(tst_out)
    tst_out = Dense(1, name='dense_19')(tst_out)
    
    # Physics network
    physics_x = Dense(64, activation='relu', name='dense_20')(physics_input)
    physics_x = Dense(32, activation='relu', name='dense_21')(physics_x)
    
    # Combine
    combined = layers.concatenate([tst_out, physics_x])
    x = Dense(64, activation='relu', name='dense_22')(combined)
    x = Dense(32, activation='relu', name='dense_23')(x)
    outputs = Dense(1, name='dense_24')(x)
    
    # Dropout layer
    outputs = Dropout(dropout, name='dropout_14')(outputs)
    
    model = Model(
        inputs=[seq_input, physics_input],
        outputs=outputs,
        name='patch_tst_pinn_hybrid'
    )
    return model

# ===========================
# ğŸ“Š ì „ì²´ ì˜ˆì¸¡ ì‹œìŠ¤í…œ í´ë˜ìŠ¤
# ===========================

class HUBROOMFullPredictor:
    """HUBROOM ì „ì²´ ì‹œê°„ëŒ€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ"""
    
    def __init__(self, model_type='PINN'):
        """
        Args:
            model_type: 'PatchTST' ë˜ëŠ” 'PINN'
        """
        self.model_type = model_type
        self.seq_len = 20
        self.pred_len = 10
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        self.critical_threshold = 300
        
        # ëª¨ë¸ ê²½ë¡œ
        if model_type == 'PINN':
            self.model_weights = './checkpoints/PatchTST_PINN_best.h5'
        else:
            self.model_weights = './checkpoints/PatchTST_best.h5'
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        self.scaler_X = self.load_scaler('scaler_X.pkl')
        self.scaler_y = self.load_scaler('scaler_y.pkl')
        self.scaler_physics = self.load_scaler('scaler_physics.pkl')
        
        # ë¬¼ë¦¬ ì»¬ëŸ¼
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2', 'M14B_7F_TO_HUB_JOB2'
        ]
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB', 'M16A_3F_TO_M14B_7F_JOB'
        ]
        
        self.model = None
        self.n_features = None
    
    def load_scaler(self, filename):
        """ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        filepath = f'./checkpoints/{filename}'
        if os.path.exists(filepath):
            with open(filepath, 'rb') as f:
                return pickle.load(f)
        else:
            print(f"âš ï¸ {filename}ì´ ì—†ìŠµë‹ˆë‹¤!")
            return None
    
    def load_data(self, data_path):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print(f"\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
        df = pd.read_csv(data_path)
        
        # ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬
        time_col = df.columns[0]
        df['timestamp'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df = df.fillna(method='ffill').fillna(0)
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        self.n_features = len(numeric_cols)
        
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)} í–‰")
        print(f"ğŸ“… ê¸°ê°„: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
        print(f"ğŸ“Š íŠ¹ì„± ìˆ˜: {self.n_features}ê°œ")
        
        return df, numeric_cols
    
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print(f"\nğŸ¤– {self.model_type} ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        if not os.path.exists(self.model_weights):
            print(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.model_weights}")
            return None
        
        try:
            if self.model_type == 'PINN':
                model = build_patchtst_pinn_model(
                    seq_len=self.seq_len,
                    n_features=self.n_features,
                    patch_len=5,
                    d_model=128,
                    n_heads=8,
                    d_ff=256,
                    n_layers=3,
                    dropout=0.1
                )
                # ë”ë¯¸ ë°ì´í„°ë¡œ ëª¨ë¸ ë¹Œë“œ
                dummy_seq = np.zeros((1, self.seq_len, self.n_features))
                dummy_physics = np.zeros((1, 3))
                _ = model([dummy_seq, dummy_physics])
            else:
                model = build_patchtst_model(
                    seq_len=self.seq_len,
                    n_features=self.n_features,
                    patch_len=5,
                    d_model=128,
                    n_heads=8,
                    d_ff=256,
                    n_layers=3,
                    dropout=0.1
                )
                # ë”ë¯¸ ë°ì´í„°ë¡œ ëª¨ë¸ ë¹Œë“œ
                dummy_input = np.zeros((1, self.seq_len, self.n_features))
                _ = model(dummy_input)
            
            # ê°€ì¤‘ì¹˜ ë¡œë“œ
            model.load_weights(self.model_weights, by_name=True, skip_mismatch=True)
            self.model = model
            print(f"âœ… {self.model_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return model
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def predict_all_timestamps(self, df, numeric_cols):
        """ì „ì²´ ì‹œê°„ëŒ€ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰"""
        print(f"\nğŸ”® ì „ì²´ ì˜ˆì¸¡ ì‹œì‘ (00:20ë¶„ë¶€í„°)...")
        
        data = df[numeric_cols].values
        target_idx = numeric_cols.index(self.target_col)
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¼ë¦¬ ì»¬ëŸ¼ í™•ì¸
        available_inflow = [col for col in self.inflow_cols if col in numeric_cols]
        available_outflow = [col for col in self.outflow_cols if col in numeric_cols]
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        predictions = []
        actual_values = []
        timestamps = []
        prediction_times = []
        
        # 00:20ë¶„ë¶€í„° ì‹œì‘ (ì¸ë±ìŠ¤ 19ë¶€í„°)
        start_idx = self.seq_len - 1  # 19
        end_idx = len(data) - self.pred_len
        
        total_predictions = end_idx - start_idx
        print(f"ğŸ“Š ì´ ì˜ˆì¸¡ ìˆ˜: {total_predictions}ê°œ")
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        batch_size = 1000
        n_batches = (total_predictions + batch_size - 1) // batch_size
        
        for batch_idx in range(n_batches):
            start = start_idx + batch_idx * batch_size
            end = min(start_idx + (batch_idx + 1) * batch_size, end_idx)
            
            X_batch = []
            physics_batch = []
            timestamps_batch = []
            actual_batch = []
            
            # ë°°ì¹˜ ë°ì´í„° ìƒì„±
            for i in range(start, end):
                # ì…ë ¥ ì‹œí€€ìŠ¤ (ê³¼ê±° 20ë¶„)
                X_seq = data[i-self.seq_len+1:i+1]
                
                # ì‹¤ì œê°’ (10ë¶„ í›„)
                y_actual = data[i+self.pred_len, target_idx]
                
                # íƒ€ì„ìŠ¤íƒ¬í”„
                current_time = df['timestamp'].iloc[i]
                pred_time = current_time + timedelta(minutes=self.pred_len)
                
                # ë¬¼ë¦¬ ë°ì´í„° (í˜„ì¬ ìƒíƒœ)
                current_state = data[i]
                current_hubroom = current_state[target_idx]
                
                inflow_sum = sum([current_state[numeric_cols.index(col)] 
                                for col in available_inflow])
                outflow_sum = sum([current_state[numeric_cols.index(col)] 
                                 for col in available_outflow])
                
                physics = np.array([current_hubroom, inflow_sum, outflow_sum])
                
                X_batch.append(X_seq)
                physics_batch.append(physics)
                timestamps_batch.append(current_time)
                actual_batch.append(y_actual)
            
            # ë°°ì¹˜ ì˜ˆì¸¡
            X_batch = np.array(X_batch)
            physics_batch = np.array(physics_batch)
            
            # ì •ê·œí™”
            X_scaled = self.scaler_X.transform(
                X_batch.reshape(-1, self.n_features)
            ).reshape(-1, self.seq_len, self.n_features)
            
            if self.model_type == 'PINN':
                physics_scaled = self.scaler_physics.transform(physics_batch)
                y_pred_scaled = self.model.predict([X_scaled, physics_scaled], 
                                                 batch_size=32, verbose=0)
            else:
                y_pred_scaled = self.model.predict(X_scaled, 
                                                 batch_size=32, verbose=0)
            
            # ì—­ì •ê·œí™”
            y_pred = self.scaler_y.inverse_transform(
                y_pred_scaled.reshape(-1, 1)
            ).flatten()
            
            # ê²°ê³¼ ì €ì¥
            predictions.extend(y_pred)
            actual_values.extend(actual_batch)
            timestamps.extend(timestamps_batch)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if (batch_idx + 1) % 10 == 0 or batch_idx == n_batches - 1:
                progress = (batch_idx + 1) / n_batches * 100
                print(f"  ì§„í–‰ë¥ : {progress:.1f}% ({len(predictions)}/{total_predictions})")
        
        print(f"\nâœ… ì˜ˆì¸¡ ì™„ë£Œ!")
        
        # ê²°ê³¼ ë°˜í™˜
        return {
            'timestamps': np.array(timestamps),
            'predictions': np.array(predictions),
            'actual_values': np.array(actual_values)
        }
    
    def analyze_results(self, results):
        """ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„"""
        print("\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„")
        print("="*60)
        
        y_true = results['actual_values']
        y_pred = results['predictions']
        
        # ì „ì²´ ì„±ëŠ¥ ë©”íŠ¸ë¦­
        mae = mean_absolute_error(y_true, y_pred)
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_true, y_pred)
        
        print(f"\nğŸ“ˆ ì „ì²´ ì„±ëŠ¥:")
        print(f"  - MAE: {mae:.4f}")
        print(f"  - RMSE: {rmse:.4f}")
        print(f"  - RÂ²: {r2:.4f}")
        
        # 300 ì´ìƒ ë¶„ì„
        over_300_true = np.sum(y_true >= 300)
        over_300_pred = np.sum(y_pred >= 300)
        
        print(f"\nğŸš¨ 300 ì´ìƒ ë¶„ì„:")
        print(f"  - ì‹¤ì œ 300 ì´ìƒ: {over_300_true}ê°œ ({over_300_true/len(y_true)*100:.1f}%)")
        print(f"  - ì˜ˆì¸¡ 300 ì´ìƒ: {over_300_pred}ê°œ ({over_300_pred/len(y_pred)*100:.1f}%)")
        
        # 300 ì´ìƒì¼ ë•Œ ì„±ëŠ¥
        mask_300 = y_true >= 300
        if np.sum(mask_300) > 0:
            mae_300 = mean_absolute_error(y_true[mask_300], y_pred[mask_300])
            print(f"  - 300 ì´ìƒì¼ ë•Œ MAE: {mae_300:.4f}")
        
        # ì‹œê°„ëŒ€ë³„ ë¶„ì„
        self.analyze_by_hour(results)
        
        return {
            'mae': mae,
            'rmse': rmse,
            'r2': r2,
            'over_300_true': over_300_true,
            'over_300_pred': over_300_pred
        }
    
    def analyze_by_hour(self, results):
        """ì‹œê°„ëŒ€ë³„ ë¶„ì„"""
        print("\nâ° ì‹œê°„ëŒ€ë³„ ë¶„ì„:")
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df_results = pd.DataFrame({
            'timestamp': results['timestamps'],
            'actual': results['actual_values'],
            'predicted': results['predictions'],
            'error': results['predictions'] - results['actual_values']
        })
        
        # ì‹œê°„ëŒ€ ì¶”ì¶œ
        df_results['hour'] = pd.to_datetime(df_results['timestamp']).dt.hour
        
        # ì‹œê°„ëŒ€ë³„ í‰ê·  ì˜¤ì°¨
        hourly_mae = df_results.groupby('hour').apply(
            lambda x: mean_absolute_error(x['actual'], x['predicted'])
        )
        
        print("\nì‹œê°„ëŒ€ë³„ MAE:")
        for hour, mae in hourly_mae.items():
            print(f"  {hour:02d}ì‹œ: {mae:.2f}")
    
    def save_results(self, results, output_path='full_predictions.csv'):
        """ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥"""
        print(f"\nğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df_results = pd.DataFrame({
            'timestamp': results['timestamps'],
            'actual': results['actual_values'],
            'predicted': results['predictions'],
            'error': results['predictions'] - results['actual_values'],
            'is_critical_actual': results['actual_values'] >= 300,
            'is_critical_pred': results['predictions'] >= 300
        })
        
        # ì˜ˆì¸¡ ì‹œê°„ ì¶”ê°€ (10ë¶„ í›„)
        df_results['prediction_for'] = pd.to_datetime(df_results['timestamp']) + timedelta(minutes=10)
        
        # ì €ì¥
        df_results.to_csv(output_path, index=False)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")
        
        # ìš”ì•½ ì¶œë ¥
        print(f"\nğŸ“Š ì €ì¥ëœ ë°ì´í„° ìš”ì•½:")
        print(f"  - ì´ ì˜ˆì¸¡ ìˆ˜: {len(df_results)}ê°œ")
        print(f"  - ê¸°ê°„: {df_results['timestamp'].min()} ~ {df_results['timestamp'].max()}")
        print(f"  - 300 ì´ìƒ ì‹¤ì œ: {df_results['is_critical_actual'].sum()}ê°œ")
        print(f"  - 300 ì´ìƒ ì˜ˆì¸¡: {df_results['is_critical_pred'].sum()}ê°œ")
    
    def visualize_results(self, results, sample_size=1000):
        """ê²°ê³¼ ì‹œê°í™”"""
        print("\nğŸ“Š ê²°ê³¼ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        y_true = results['actual_values']
        y_pred = results['predictions']
        timestamps = results['timestamps']
        
        # í° ë°ì´í„°ì…‹ì¸ ê²½ìš° ìƒ˜í”Œë§
        if len(y_true) > sample_size:
            indices = np.linspace(0, len(y_true)-1, sample_size, dtype=int)
            y_true_sample = y_true[indices]
            y_pred_sample = y_pred[indices]
            timestamps_sample = timestamps[indices]
        else:
            y_true_sample = y_true
            y_pred_sample = y_pred
            timestamps_sample = timestamps
        
        # ê·¸ë˜í”„ ìƒì„±
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'HUBROOM ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ ({self.model_type})', fontsize=16)
        
        # 1. ì‹œê³„ì—´ ê·¸ë˜í”„
        ax1 = axes[0, 0]
        ax1.plot(y_true_sample, label='ì‹¤ì œê°’', alpha=0.7, linewidth=1)
        ax1.plot(y_pred_sample, label='ì˜ˆì¸¡ê°’', alpha=0.7, linewidth=1)
        ax1.axhline(y=300, color='red', linestyle=':', label='ìœ„í—˜ ì„ê³„ê°’')
        ax1.set_title('ì‹œê³„ì—´ ì˜ˆì¸¡ ë¹„êµ')
        ax1.set_xlabel('ì‹œê°„ ì¸ë±ìŠ¤')
        ax1.set_ylabel('HUBROOM ë°˜ì†¡ëŸ‰')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. ì‚°ì ë„
        ax2 = axes[0, 1]
        ax2.scatter(y_true, y_pred, alpha=0.5, s=1)
        ax2.plot([0, max(y_true.max(), y_pred.max())], 
                 [0, max(y_true.max(), y_pred.max())], 
                 'r--', label='Perfect Prediction')
        ax2.axvline(x=300, color='red', linestyle=':', alpha=0.5)
        ax2.axhline(y=300, color='red', linestyle=':', alpha=0.5)
        ax2.set_title('ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’')
        ax2.set_xlabel('ì‹¤ì œê°’')
        ax2.set_ylabel('ì˜ˆì¸¡ê°’')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. ì˜¤ì°¨ íˆìŠ¤í† ê·¸ë¨
        ax3 = axes[1, 0]
        errors = y_pred - y_true
        ax3.hist(errors, bins=50, alpha=0.7, edgecolor='black')
        ax3.axvline(x=0, color='red', linestyle='--', label='Zero Error')
        ax3.set_title('ì˜ˆì¸¡ ì˜¤ì°¨ ë¶„í¬')
        ax3.set_xlabel('ì˜ˆì¸¡ ì˜¤ì°¨')
        ax3.set_ylabel('ë¹ˆë„')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ì‹œê°„ëŒ€ë³„ í‰ê·  ì˜¤ì°¨
        ax4 = axes[1, 1]
        df_temp = pd.DataFrame({
            'timestamp': timestamps,
            'error': np.abs(y_pred - y_true)
        })
        df_temp['hour'] = pd.to_datetime(df_temp['timestamp']).dt.hour
        hourly_mae = df_temp.groupby('hour')['error'].mean()
        
        ax4.bar(hourly_mae.index, hourly_mae.values)
        ax4.set_title('ì‹œê°„ëŒ€ë³„ í‰ê·  ì ˆëŒ€ ì˜¤ì°¨')
        ax4.set_xlabel('ì‹œê°„ (Hour)')
        ax4.set_ylabel('MAE')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'hubroom_full_prediction_{self.model_type}.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… ì‹œê°í™” ì™„ë£Œ!")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("="*80)
    print("ğŸ­ HUBROOM ì „ì²´ ë°ì´í„° ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
    print("ğŸ“… ì˜ˆì¸¡ ì‹œì‘: 00:20ë¶„ë¶€í„° (20ë¶„ ê³¼ê±° ë°ì´í„° í•„ìš”)")
    print("="*80)
    
    # TensorFlow ë¡œê·¸ ë ˆë²¨ ì¡°ì •
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    
    # ëª¨ë¸ ì„ íƒ
    print("\nëª¨ë¸ ì„ íƒ:")
    print("1. PatchTST (ê¸°ë³¸ ëª¨ë¸)")
    print("2. PatchTST+PINN (ë¬¼ë¦¬ ë²•ì¹™ ì ìš© - ê¶Œì¥)")
    
    choice = input("\nì„ íƒ (1 ë˜ëŠ” 2, ê¸°ë³¸ê°’ 2): ").strip()
    if choice == '1':
        model_type = 'PatchTST'
    else:
        model_type = 'PINN'
    
    # ì˜ˆì¸¡ê¸° ìƒì„±
    predictor = HUBROOMFullPredictor(model_type=model_type)
    
    # ìŠ¤ì¼€ì¼ëŸ¬ í™•ì¸
    if predictor.scaler_X is None:
        print("\nâŒ ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”!")
        return
    
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    data_path = input("\në°ì´í„° íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: 20250801_to_20250831.csv): ").strip()
    if not data_path:
        data_path = '20250801_to_20250831.csv'
    
    if not os.path.exists(data_path):
        print(f"âŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        return
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ
        df, numeric_cols = predictor.load_data(data_path)
        
        # 2. ëª¨ë¸ ë¡œë“œ
        model = predictor.load_model()
        if model is None:
            return
        
        # 3. ì „ì²´ ì˜ˆì¸¡ ìˆ˜í–‰
        results = predictor.predict_all_timestamps(df, numeric_cols)
        
        # 4. ê²°ê³¼ ë¶„ì„
        metrics = predictor.analyze_results(results)
        
        # 5. ê²°ê³¼ ì €ì¥
        output_filename = f'full_predictions_{model_type}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        predictor.save_results(results, output_filename)
        
        # 6. ì‹œê°í™”
        predictor.visualize_results(results)
        
        print("\nâœ… ì „ì²´ ì˜ˆì¸¡ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()