# -*- coding: utf-8 -*-
"""
HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ V4.0
- V3 ëŒ€ë¹„ ê°œì„ ì‚¬í•­:
  1. BRIDGE_TIME ì»¬ëŸ¼ ì¶”ê°€ (ê·¹ë‹¨ê°’ ë¬¼ë¦¬ì§€í‘œ)
  2. ì—°ì† 300+ íŒ¨í„´ ë¶„ì„ ê°•í™”
  3. False Positive ë°©ì§€ (300ì´í•˜ë¥¼ 300ì´ìƒìœ¼ë¡œ ì˜¤íƒ ë°©ì§€)
  4. 335+ ê·¹ë‹¨ê°’ ê°ì§€ìœ¨ ê°œì„  (ëª©í‘œ: 50%+)
- Model 1: PatchTST (ì „ì²´ êµ¬ê°„ ê· í˜•)
- Model 2: PatchTST + PINN (335+ ê·¹ë‹¨ê°’ íŠ¹í™”, FP ë°©ì§€)
- ì¤‘ë‹¨/ì¬ê°œ ê¸°ëŠ¥ í¬í•¨
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback
from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
import os
import pickle
import warnings
from tqdm import tqdm
import joblib
import signal
import sys

warnings.filterwarnings('ignore')

np.random.seed(42)
tf.random.set_seed(42)
print(f"TensorFlow Version: {tf.__version__}")
print("="*80)
print("ğŸ­ HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ V4.0")
print("ğŸ¯ ëª©í‘œ: 335+ ê·¹ë‹¨ê°’ ê°ì§€ìœ¨ 50% + False Positive ìµœì†Œí™”")
print("ğŸš€ V4 í•µì‹¬: BRIDGE_TIME í™œìš© + ì—°ì† íŒ¨í„´ ë¶„ì„ + ì˜¤íƒ ë°©ì§€")
print("âœ… ì¤‘ë‹¨/ì¬ê°œ ê¸°ëŠ¥ í™œì„±í™”")
print("="*80)

# ========================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì (ì¤‘ë‹¨/ì¬ì‹œì‘)
# ========================================

class CheckpointManager:
    def __init__(self, checkpoint_dir='./checkpoints'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        self.state_file = os.path.join(checkpoint_dir, 'training_state_v4.pkl')
        self.interrupted = False
        
        # Ctrl+C í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
    
    def _signal_handler(self, sig, frame):
        print('\n\nâš ï¸ ì¤‘ë‹¨ ê°ì§€! ìƒíƒœ ì €ì¥ ì¤‘...')
        self.interrupted = True
        sys.exit(0)
    
    def save_state(self, state):
        with open(self.state_file, 'wb') as f:
            pickle.dump(state, f)
        print("ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ")
    
    def load_state(self):
        if os.path.exists(self.state_file):
            with open(self.state_file, 'rb') as f:
                state = pickle.load(f)
                print("ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ")
                return state
        return None
    
    def clear_state(self):
        if os.path.exists(self.state_file):
            os.remove(self.state_file)
            print("ğŸ—‘ï¸ ì²´í¬í¬ì¸íŠ¸ ì œê±° ì™„ë£Œ")

# ========================================
# V4 ê·¹ë‹¨ê°’ ì†ì‹¤ í•¨ìˆ˜ (False Positive ë°©ì§€)
# ========================================

class ExtremeLossV4(keras.losses.Loss):
    def __init__(self, extreme_focus=True):
        super().__init__()
        self.extreme_focus = extreme_focus
        
    def call(self, y_true, y_pred):
        error = y_true - y_pred
        mae = tf.abs(error)
        
        if self.extreme_focus:
            # V4: 335+ ê·¹ë‹¨ê°’ì— ë” ê°•í•œ í˜ë„í‹°
            weights = tf.where(y_true >= 0.9, 25.0,    # 335+ (ì •ê·œí™” ê¸°ì¤€)
                     tf.where(y_true >= 0.8, 15.0,     # 310-335
                     tf.where(y_true >= 0.7, 8.0,      # 300-310
                     tf.where(y_true >= 0.5, 3.0,      # 250-300
                              1.0))))                    # ë‚˜ë¨¸ì§€
            
            # ê·¹ë‹¨ê°’ ë¯¸íƒì§€ ì¶”ê°€ í˜ë„í‹° (False Negative)
            miss_penalty = tf.where(
                tf.logical_and(y_true >= 0.9, y_pred < 0.85),
                30.0 * mae,  # 335+ ë¯¸íƒì§€ ì‹œ ê°•ë ¥í•œ í˜ë„í‹°
                0.0
            )
            
            # False Positive ë°©ì§€ í˜ë„í‹° (300 ì´í•˜ë¥¼ 300 ì´ìƒìœ¼ë¡œ ì˜ˆì¸¡)
            false_positive_penalty = tf.where(
                tf.logical_and(y_true < 0.7, y_pred >= 0.7),  # 300 ì´í•˜ë¥¼ 300+ ì˜ˆì¸¡
                20.0 * mae,  # ì˜¤íƒ í˜ë„í‹°
                0.0
            )
            
            # íŠ¹íˆ 200-250 êµ¬ê°„ì„ 300+ë¡œ ì˜ˆì¸¡í•˜ë©´ ë” ê°•í•œ í˜ë„í‹°
            severe_fp_penalty = tf.where(
                tf.logical_and(y_true < 0.5, y_pred >= 0.7),  # 250 ì´í•˜ë¥¼ 300+ ì˜ˆì¸¡
                35.0 * mae,  # ì‹¬ê°í•œ ì˜¤íƒ í˜ë„í‹°
                0.0
            )
            
            return tf.reduce_mean(weights * mae + miss_penalty + false_positive_penalty + severe_fp_penalty)
        else:
            # Model 1: ê· í˜•ì¡íŒ ê°€ì¤‘ì¹˜ + ê²½ë¯¸í•œ ì˜¤íƒ ë°©ì§€
            weights = tf.where(y_true >= 0.7, 5.0,
                     tf.where(y_true >= 0.5, 2.0, 1.0))
            
            # Model 1ì—ë„ ê²½ë¯¸í•œ ì˜¤íƒ ë°©ì§€
            fp_penalty = tf.where(
                tf.logical_and(y_true < 0.6, y_pred >= 0.7),
                5.0 * mae,
                0.0
            )
            
            return tf.reduce_mean(weights * mae + fp_penalty)

# ========================================
# ê·¹ë‹¨ê°’ ëª¨ë‹ˆí„°ë§ ì½œë°± V4
# ========================================

class ExtremeValueCallbackV4(Callback):
    def __init__(self, X_val, y_val, scaler_y, X_physics_val=None):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val
        self.scaler_y = scaler_y
        self.X_physics_val = X_physics_val
        
    def on_epoch_end(self, epoch, logs=None):
        if epoch % 5 == 0:
            # ì˜ˆì¸¡
            if self.X_physics_val is not None:
                y_pred_scaled = self.model.predict([self.X_val, self.X_physics_val], verbose=0)
            else:
                y_pred_scaled = self.model.predict(self.X_val, verbose=0)
            
            # ì—­ìŠ¤ì¼€ì¼ë§
            y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
            y_true = self.scaler_y.inverse_transform(self.y_val.reshape(-1, 1)).flatten()
            
            # 335+ ê°ì§€ìœ¨
            mask_335 = y_true >= 335
            if mask_335.sum() > 0:
                detected_335 = (y_pred >= 335)[mask_335].sum()
                recall_335 = detected_335 / mask_335.sum() * 100
                
                # 310+ ê°ì§€ìœ¨
                mask_310 = y_true >= 310
                detected_310 = (y_pred >= 310)[mask_310].sum()
                recall_310 = detected_310 / mask_310.sum() * 100
                
                # False Positive Rate (ì˜¤íƒìœ¨)
                mask_under_300 = y_true < 300
                if mask_under_300.sum() > 0:
                    false_positive_300 = (y_pred >= 300)[mask_under_300].sum()
                    fp_rate = false_positive_300 / mask_under_300.sum() * 100
                else:
                    fp_rate = 0
                
                print(f"\n[Epoch {epoch}] 335+: {recall_335:.1f}% | 310+: {recall_310:.1f}% | FP(300): {fp_rate:.1f}%")

# ========================================
# V4 ë°ì´í„° ì²˜ë¦¬ê¸° (BRIDGE_TIME í¬í•¨)
# ========================================

class DataProcessorV4:
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        self.bridge_time_col = 'BRIDGE_TIME'
        
        # V4 í•„ìˆ˜ ì»¬ëŸ¼ (21ê°œ ì „ì²´ + ì„ íƒ/ê¶Œì¥)
        # ìœ ì… ì»¬ëŸ¼ (6ê°œ)
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB',      # í•„ìˆ˜ 70ì 
            'M16A_2F_TO_HUB_JOB2',     # í•„ìˆ˜ 65ì 
            'M14A_3F_TO_HUB_JOB2',     # í•„ìˆ˜ 65ì 
            'M14B_7F_TO_HUB_JOB2',     # í•„ìˆ˜ 65ì 
            'M16B_10F_TO_HUB_JOB'      # ì„ íƒ 25ì 
        ]
        
        # ìœ ì¶œ ì»¬ëŸ¼ (5ê°œ)
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB',  # í•„ìˆ˜ 60ì 
            'M16A_3F_TO_M16A_2F_JOB',  # í•„ìˆ˜ 55ì 
            'M16A_3F_TO_M14A_3F_JOB',  # í•„ìˆ˜ 55ì 
            'M16A_3F_TO_M14B_7F_JOB',  # í•„ìˆ˜ 55ì 
            'M16A_3F_TO_3F_MLUD_JOB'   # ì„ íƒ 35ì 
        ]
        
        # CMD ì»¬ëŸ¼ (6ê°œ)
        self.cmd_cols = [
            'M16A_3F_CMD',             # í•„ìˆ˜ 85ì  (ë‚´ë¶€)
            'M16A_6F_TO_HUB_CMD',      # ê¶Œì¥ 55ì  (ì™¸ë¶€)
            'M16A_2F_TO_HUB_CMD',      # ì„ íƒ 50ì  (ì™¸ë¶€)
            'M14A_3F_TO_HUB_CMD',      # ì„ íƒ 50ì  (ì™¸ë¶€)
            'M14B_7F_TO_HUB_CMD'       # ì„ íƒ 50ì  (ì™¸ë¶€)
        ]
        
        # MaxCapa ì»¬ëŸ¼ (2ê°œ)
        self.maxcapa_cols = [
            'M16A_6F_LFT_MAXCAPA',     # ì„ íƒ 50ì 
            'M16A_2F_LFT_MAXCAPA'      # ì„ íƒ 45ì 
        ]
        
        # ê·¹ë‹¨ê°’ ì§€í‘œ ì»¬ëŸ¼ (2ê°œ)
        self.extreme_indicator_cols = [
            'M16A_3F_STORAGE_UTIL',    # í•„ìˆ˜ 95ì 
            'BRIDGE_TIME'              # í•„ìˆ˜ 85ì  (ë¬¼ë¦¬ì§€í‘œ/ê·¹ë‹¨ê°’ì§€í‘œ)
        ]
        
        # OFS ì»¬ëŸ¼ (2ê°œ)
        self.ofs_cols = [
            'M14_TO_M16_OFS_CUR',      # ì„ íƒ 40ì 
            'M16_TO_M14_OFS_CUR'       # ì„ íƒ 40ì 
        ]
        
        # ëª¨ë“  V4 ì‚¬ìš© ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (21ê°œ)
        self.all_v4_cols = (
            [self.target_col] +
            self.inflow_cols + 
            self.outflow_cols + 
            self.cmd_cols + 
            self.maxcapa_cols + 
            self.extreme_indicator_cols + 
            self.ofs_cols
        )
        
        # ìŠ¤ì¼€ì¼ëŸ¬
        self.scaler_X = RobustScaler()
        self.scaler_y = MinMaxScaler()
        self.scaler_physics = StandardScaler()
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ê²½ë¡œ
        os.makedirs('./scalers', exist_ok=True)
        
    def analyze_data(self, df):
        """ë°ì´í„° ë¶„ì„"""
        target = df[self.target_col]
        print("\nğŸ“Š ë°ì´í„° ë¶„ì„:")
        print(f"  ë²”ìœ„: {target.min():.0f} ~ {target.max():.0f}")
        print(f"  í‰ê· : {target.mean():.1f}")
        print(f"  ì¤‘ì•™ê°’: {target.median():.1f}")
        
        print("\nğŸ¯ 3êµ¬ê°„ ë¶„í¬:")
        print(f"  ì €êµ¬ê°„(<200): {(target < 200).sum():6}ê°œ ({(target < 200).sum()/len(target)*100:5.2f}%)")
        print(f"  ì •ìƒ(200-300): {((target >= 200) & (target < 300)).sum():6}ê°œ ({((target >= 200) & (target < 300)).sum()/len(target)*100:5.2f}%)")
        print(f"  ìœ„í—˜(300+): {(target >= 300).sum():6}ê°œ ({(target >= 300).sum()/len(target)*100:5.2f}%)")
        
        print("\nğŸš¨ ê·¹ë‹¨ê°’ ì„¸ë¶€:")
        print(f"  310+: {(target >= 310).sum():6}ê°œ ({(target >= 310).sum()/len(target)*100:5.2f}%)")
        print(f"  335+: {(target >= 335).sum():6}ê°œ ({(target >= 335).sum()/len(target)*100:5.2f}%)")
        
    def analyze_consecutive_patterns(self, df, seq_len=20):
        """ì—°ì† 300+ íŒ¨í„´ ë¶„ì„ (V4 í™•ë¥  ë°ì´í„° ê¸°ë°˜)"""
        target = df[self.target_col]
        
        # V4 ì¶”ê°€ë°ì´í„° ê¸°ë°˜ í™•ë¥  ë§¤í•‘
        self.pattern_probability = {
            0: 0.15,   # 0ê°œì¼ ë•Œ 300+ í™•ë¥  (ì‹¤ì œ: 0.003)
            1: 0.15,   # 1ê°œì¼ ë•Œ
            2: 0.25,   # 2ê°œì¼ ë•Œ
            3: 0.31,   # 3ê°œì¼ ë•Œ
            4: 0.43,   # 4ê°œì¼ ë•Œ
            5: 0.43,   # 5ê°œì¼ ë•Œ
            6: 0.35,   # 6ê°œì¼ ë•Œ
            7: 0.42,   # 7ê°œì¼ ë•Œ
            8: 0.53,   # 8ê°œì¼ ë•Œ
            9: 0.49,   # 9ê°œì¼ ë•Œ
            10: 0.42,  # 10ê°œì¼ ë•Œ
            11: 0.47,  # 11ê°œì¼ ë•Œ
            12: 0.52,  # 12ê°œì¼ ë•Œ
            13: 0.60,  # 13ê°œì¼ ë•Œ
            14: 0.54,  # 14ê°œì¼ ë•Œ
            15: 0.66,  # 15ê°œì¼ ë•Œ
            16: 0.62,  # 16ê°œì¼ ë•Œ
            17: 0.71,  # 17ê°œì¼ ë•Œ
            18: 0.79,  # 18ê°œì¼ ë•Œ
            19: 0.83,  # 19ê°œì¼ ë•Œ
            20: 0.99   # 20ê°œì¼ ë•Œ (ì‹¤ì œ: 0.987)
        }
        
        consecutive_300_counts = []
        consecutive_300_probs = []  # í™•ë¥  ì¶”ê°€
        
        for i in range(len(df) - seq_len):
            window = target[i:i+seq_len]
            count_300 = (window >= 300).sum()
            consecutive_300_counts.append(count_300)
            
            # í™•ë¥  ê³„ì‚°
            prob = self.pattern_probability.get(count_300, 0.5)
            consecutive_300_probs.append(prob)
        
        df['consecutive_300_count'] = 0
        df['consecutive_300_prob'] = 0.5
        df.loc[seq_len:, 'consecutive_300_count'] = consecutive_300_counts
        df.loc[seq_len:, 'consecutive_300_prob'] = consecutive_300_probs
        
        print("\nğŸ“Š ì—°ì† 300+ íŒ¨í„´ ë¶„ì„ (V4 ë°ì´í„° ê¸°ë°˜):")
        print(f"  0ê°œ (99.7% 300ì´í•˜): {(df['consecutive_300_count'] == 0).sum()}")
        print(f"  1-5ê°œ (15-43% 300+): {((df['consecutive_300_count'] >= 1) & (df['consecutive_300_count'] <= 5)).sum()}")
        print(f"  6-10ê°œ (35-53% 300+): {((df['consecutive_300_count'] >= 6) & (df['consecutive_300_count'] <= 10)).sum()}")
        print(f"  11-15ê°œ (47-66% 300+): {((df['consecutive_300_count'] >= 11) & (df['consecutive_300_count'] <= 15)).sum()}")
        print(f"  16-19ê°œ (62-83% 300+): {((df['consecutive_300_count'] >= 16) & (df['consecutive_300_count'] <= 19)).sum()}")
        print(f"  20ê°œ (98.7% 300+): {(df['consecutive_300_count'] == 20).sum()}")
        
        return df
    
    def create_sequences_v4(self, df, seq_len=20, pred_len=10):
        """V4 ì‹œí€€ìŠ¤ ìƒì„± (ëª¨ë“  V4 ì»¬ëŸ¼ í¬í•¨)"""
        X, y, X_physics, weights = [], [], [], []
        
        # V4 ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        print("\nğŸ“‹ V4 ì»¬ëŸ¼ í™•ì¸:")
        available_v4_cols = []
        missing_cols = []
        
        for col in self.all_v4_cols:
            if col in df.columns:
                available_v4_cols.append(col)
            else:
                missing_cols.append(col)
        
        print(f"âœ… ì‚¬ìš© ê°€ëŠ¥ V4 ì»¬ëŸ¼: {len(available_v4_cols)}/21ê°œ")
        if missing_cols:
            print(f"âš ï¸ ëˆ„ë½ ì»¬ëŸ¼: {missing_cols}")
        
        # BRIDGE_TIME í™•ì¸
        if self.bridge_time_col in df.columns:
            print("âœ… BRIDGE_TIME ì»¬ëŸ¼ ë°œê²¬ - ê·¹ë‹¨ê°’ ì§€í‘œë¡œ í™œìš©")
            bridge_time_available = True
        else:
            print("âš ï¸ BRIDGE_TIME ì»¬ëŸ¼ ì—†ìŒ")
            bridge_time_available = False
        
        # ì—°ì† íŒ¨í„´ ë¶„ì„
        df = self.analyze_consecutive_patterns(df, seq_len)
        
        # V4 ì»¬ëŸ¼ë§Œ ì‚¬ìš© (ì—†ëŠ” ì»¬ëŸ¼ì€ 0ìœ¼ë¡œ ì±„ì›€)
        numeric_cols = []
        for col in self.all_v4_cols:
            if col in df.columns:
                numeric_cols.append(col)
            else:
                # ëˆ„ë½ëœ ì»¬ëŸ¼ì€ 0ìœ¼ë¡œ ì±„ìš´ ì»¬ëŸ¼ ìƒì„±
                df[col] = 0
                numeric_cols.append(col)
        
        # ì¶”ê°€: ì—°ì† íŒ¨í„´ ì»¬ëŸ¼ë„ í¬í•¨
        if 'consecutive_300_count' in df.columns:
            numeric_cols.append('consecutive_300_count')
        if 'consecutive_300_prob' in df.columns:
            numeric_cols.append('consecutive_300_prob')
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ìœ ì…/ìœ ì¶œ ì»¬ëŸ¼
        available_inflow = [col for col in self.inflow_cols if col in df.columns]
        available_outflow = [col for col in self.outflow_cols if col in df.columns]
        available_cmd = [col for col in self.cmd_cols if col in df.columns]
        
        print(f"\nâœ… ìµœì¢… ì‚¬ìš© ì»¬ëŸ¼: {len(numeric_cols)}ê°œ")
        print(f"  ìœ ì…: {len(available_inflow)}ê°œ")
        print(f"  ìœ ì¶œ: {len(available_outflow)}ê°œ")
        print(f"  CMD: {len(available_cmd)}ê°œ")
        
        # êµ¬ê°„ë³„ ì¸ë±ìŠ¤ ìˆ˜ì§‘
        indices = {
            'low': [],      # <200
            'normal': [],   # 200-300
            '300': [],      # 300-310
            '310': [],      # 310-335
            '335': []       # 335+
        }
        
        for i in range(len(df) - seq_len - pred_len):
            target_val = df[self.target_col].iloc[i + seq_len + pred_len - 1]
            consecutive_count = df['consecutive_300_count'].iloc[i + seq_len - 1] if 'consecutive_300_count' in df.columns else 0
            
            # V4: ì—°ì† íŒ¨í„´ ê¸°ë°˜ ë¶„ë¥˜ (í™•ë¥  ë°ì´í„° í™œìš©)
            if target_val >= 335:
                indices['335'].append(i)
            elif target_val >= 310:
                indices['310'].append(i)
            elif target_val >= 300:
                indices['300'].append(i)
            elif target_val >= 200:
                # ì—°ì† íŒ¨í„´ í™•ë¥ ì— ë”°ë¥¸ ìœ„í—˜ë„ íŒë‹¨
                prob_300 = df['consecutive_300_prob'].iloc[i + seq_len - 1] if 'consecutive_300_prob' in df.columns else 0.5
                
                if prob_300 >= 0.8:  # 80% ì´ìƒ í™•ë¥ 
                    indices['300'].append(i)  # ìœ„í—˜êµ¬ê°„ìœ¼ë¡œ ë¶„ë¥˜
                elif prob_300 >= 0.5:  # 50-80% í™•ë¥ 
                    if np.random.random() < prob_300:
                        indices['300'].append(i)
                    else:
                        indices['normal'].append(i)
                else:
                    indices['normal'].append(i)
            else:
                indices['low'].append(i)
        
        # V4 ì˜¤ë²„ìƒ˜í”Œë§ ì „ëµ (False Positive ë°©ì§€)
        all_indices = []
        all_indices.extend(indices['low'] * 3)      # 3ë°° (FP ë°©ì§€ ê°•í™”)
        all_indices.extend(indices['normal'])        # 1ë°°
        all_indices.extend(indices['300'] * 5)       # 5ë°°
        all_indices.extend(indices['310'] * 15)      # 15ë°°
        all_indices.extend(indices['335'] * 25)      # 25ë°° (ìµœëŒ€ ê°•í™”)
        
        print(f"\nğŸ“Š V4 ì˜¤ë²„ìƒ˜í”Œë§:")
        print(f"  <200: {len(indices['low'])} â†’ {len(indices['low'])*3}")
        print(f"  200-300: {len(indices['normal'])} â†’ {len(indices['normal'])}")
        print(f"  300-310: {len(indices['300'])} â†’ {len(indices['300'])*5}")
        print(f"  310-335: {len(indices['310'])} â†’ {len(indices['310'])*15}")
        print(f"  335+: {len(indices['335'])} â†’ {len(indices['335'])*25}")
        
        np.random.shuffle(all_indices)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        for i in tqdm(all_indices, desc="V4 ì‹œí€€ìŠ¤ ìƒì„±"):
            # ì‹œê³„ì—´ ë°ì´í„° (V4 ì»¬ëŸ¼ë§Œ)
            X.append(df[numeric_cols].iloc[i:i+seq_len].values)
            
            # íƒ€ê²Ÿ
            y_val = df[self.target_col].iloc[i + seq_len + pred_len - 1]
            y.append(y_val)
            
            # V4 ë¬¼ë¦¬ ë°ì´í„° (í™•ì¥ + íŒ¨í„´ í™•ë¥ )
            physics = [
                # ê¸°ë³¸ ë¬¼ë¦¬ ë°ì´í„°
                df[self.target_col].iloc[i + seq_len - 1],  # í˜„ì¬ê°’
                df[available_inflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_inflow else 0,
                df[available_outflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_outflow else 0,
                
                # BRIDGE_TIME
                df[self.bridge_time_col].iloc[i + seq_len - 1] if bridge_time_available else 3.5,
                
                # ì—°ì† íŒ¨í„´ ìˆ˜
                df['consecutive_300_count'].iloc[i + seq_len - 1] if 'consecutive_300_count' in df.columns else 0,
                
                # ì—°ì† íŒ¨í„´ í™•ë¥  (ì¤‘ìš”!)
                df['consecutive_300_prob'].iloc[i + seq_len - 1] if 'consecutive_300_prob' in df.columns else 0.5,
                
                # STORAGE_UTIL
                df['M16A_3F_STORAGE_UTIL'].iloc[i + seq_len - 1] if 'M16A_3F_STORAGE_UTIL' in df.columns else 0,
                
                # CMD í•©ê³„
                df[available_cmd].iloc[i + seq_len - 1].sum() if available_cmd else 0,
                
                # ìµœê·¼ 5ê°œ í‰ê·  (ì¶”ì„¸)
                df[self.target_col].iloc[i+seq_len-5:i+seq_len].mean()
            ]
            
            X_physics.append(physics)
            
            # V4 ê°€ì¤‘ì¹˜ (FP ë°©ì§€ + íŒ¨í„´ í™•ë¥  ë°˜ì˜)
            prob_300 = df['consecutive_300_prob'].iloc[i + seq_len - 1] if 'consecutive_300_prob' in df.columns else 0.5
            
            if y_val >= 335:
                weights.append(30.0)
            elif y_val >= 310:
                weights.append(20.0)
            elif y_val >= 300:
                weights.append(10.0)
            elif y_val < 200:
                # ì €êµ¬ê°„ + ë‚®ì€ í™•ë¥  = FP ë°©ì§€ ê°•í™”
                if prob_300 < 0.2:  # 20% ë¯¸ë§Œ í™•ë¥ 
                    weights.append(8.0)  # ê°•í•œ ê°€ì¤‘ì¹˜ (FP ë°©ì§€)
                elif prob_300 < 0.5:
                    weights.append(5.0)
                else:
                    weights.append(3.0)
            else:  # 200-300 êµ¬ê°„
                # ê²½ê³„ êµ¬ê°„: í™•ë¥ ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
                if prob_300 > 0.7:  # ë†’ì€ í™•ë¥ ì¸ë° 300 ë¯¸ë§Œ
                    weights.append(3.0)  # False Negative ê°€ëŠ¥ì„±
                elif prob_300 < 0.3:  # ë‚®ì€ í™•ë¥ ì´ê³  300 ë¯¸ë§Œ
                    weights.append(2.0)  # ì •ìƒ
                else:
                    weights.append(1.0)
        
        print(f"\nâœ… V4 ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {len(X)}ê°œ")
        print(f"  ë¬¼ë¦¬ ë°ì´í„° ì°¨ì›: {len(physics)}ê°œ")
        
        return np.array(X), np.array(y), np.array(X_physics), np.array(weights)
    
    def save_scalers(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥"""
        joblib.dump(self.scaler_X, './scalers/scaler_X_v4.pkl')
        joblib.dump(self.scaler_y, './scalers/scaler_y_v4.pkl')
        joblib.dump(self.scaler_physics, './scalers/scaler_physics_v4.pkl')
        print("âœ… V4 ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ")
        
    def load_scalers(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        try:
            self.scaler_X = joblib.load('./scalers/scaler_X_v4.pkl')
            self.scaler_y = joblib.load('./scalers/scaler_y_v4.pkl')
            self.scaler_physics = joblib.load('./scalers/scaler_physics_v4.pkl')
            print("âœ… V4 ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
            return True
        except:
            print("âŒ V4 ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨")
            return False

# ========================================
# Model 1: PatchTST (ì „ì²´ ê· í˜•)
# ========================================

class PatchTSTModel(keras.Model):
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(128, activation='relu')
        
        # Transformer
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm1 = layers.LayerNormalization()
        self.norm2 = layers.LayerNormalization()
        
        self.ffn = keras.Sequential([
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(128)
        ])
        
        # ì¶œë ¥
        self.flatten = layers.Flatten()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dropout = layers.Dropout(0.3)
        self.dense2 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(1)
        
    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        
        # íŒ¨ì¹˜ ìƒì„±
        x = tf.reshape(x, [batch_size, self.n_patches, self.patch_len * self.n_features])
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x = self.patch_embedding(x)
        
        # Transformer
        attn = self.attention(x, x, training=training)
        x = self.norm1(x + attn)
        
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        
        # ì¶œë ¥
        x = self.flatten(x)
        x = self.dense1(x)
        x = self.dropout(x, training=training)
        x = self.dense2(x)
        output = self.output_layer(x)
        
        return tf.squeeze(output, axis=-1)

# ========================================
# Model 2: PatchTST + PINN (ê·¹ë‹¨ê°’ íŠ¹í™”)
# ========================================

class PatchTSTPINN(keras.Model):
    def __init__(self, config):
        super().__init__()
        
        # PatchTST ë¶€ë¶„
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(128, activation='relu')
        
        # Transformer
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm = layers.LayerNormalization()
        
        # ì‹œê³„ì—´ ì²˜ë¦¬
        self.flatten = layers.Flatten()
        self.temporal_dense = layers.Dense(64, activation='relu')
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬ (PINN) - V4 í™•ì¥
        self.physics_net = keras.Sequential([
            layers.Dense(64, activation='relu'),  # í™•ì¥
            layers.BatchNormalization(),
            layers.Dense(32, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(16, activation='relu')
        ])
        
        # ìœµí•© ë° ê·¹ë‹¨ê°’ ë³´ì •
        self.fusion = keras.Sequential([
            layers.Dense(128, activation='relu'),  # í™•ì¥
            layers.Dropout(0.3),
            layers.Dense(64, activation='relu'),
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(1)
        ])
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ… ë ˆì´ì–´
        self.extreme_boost = layers.Dense(1, activation='sigmoid')
        
    def call(self, inputs, training=False):
        x_seq, x_physics = inputs
        
        batch_size = tf.shape(x_seq)[0]
        
        # PatchTST ì²˜ë¦¬
        x = tf.reshape(x_seq, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        
        attn = self.attention(x, x, training=training)
        x = self.norm(x + attn)
        
        x = self.flatten(x)
        x_temporal = self.temporal_dense(x)
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬
        x_physics = self.physics_net(x_physics)
        
        # ìœµí•©
        x_combined = tf.concat([x_temporal, x_physics], axis=-1)
        output = self.fusion(x_combined)
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ…
        boost_factor = self.extreme_boost(x_physics)
        output = output * (1 + boost_factor * 0.2)
        
        return tf.squeeze(output, axis=-1)

# ========================================
# ë©”ì¸ í•¨ìˆ˜
# ========================================

def main():
    # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
    ckpt = CheckpointManager()
    
    # ì´ì „ ìƒíƒœ í™•ì¸
    state = ckpt.load_state()
    
    if state:
        print(f"\nğŸ“‚ ì´ì „ í•™ìŠµ ìƒíƒœ ë°œê²¬! (Step {state.get('step', 1)})")
        
        resume = input("ì´ì–´ì„œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y: ì´ì–´ì„œ, n: ì²˜ìŒë¶€í„°): ").lower()
        
        if resume != 'y':
            ckpt.clear_state()
            state = None
            step = 1
        else:
            step = state.get('step', 1)
            print(f"âœ… Step {step}ë¶€í„° ì¬ê°œí•©ë‹ˆë‹¤.")
    else:
        state = {}
        step = 1
        print("\nì´ì „ í•™ìŠµ ìƒíƒœê°€ ì—†ìŠµë‹ˆë‹¤.")
        input("Enterë¥¼ ëˆŒëŸ¬ ì‹œì‘í•˜ì„¸ìš”...")
    
    # ë°ì´í„° ì²˜ë¦¬ê¸°
    processor = DataProcessorV4()
    
    # Step 1: ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    if step == 1:
        print("\n[Step 1/6] ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
        
        # ë©”ì¸ ë°ì´í„° ë¡œë“œ
        df = pd.read_csv('data/HUB_0509_TO_0730_DATA.CSV')
        print(f"âœ… ë©”ì¸ ë°ì´í„° ë¡œë“œ: {df.shape}")
        
        # BRIDGE_TIME ë°ì´í„° ë³‘í•© (ìˆëŠ” ê²½ìš°)
        bridge_time_path = 'data/BRIDGE_TIME.CSV'
        if os.path.exists(bridge_time_path):
            print("ğŸ“Š BRIDGE_TIME ë°ì´í„° ë°œê²¬! ë³‘í•© ì¤‘...")
            try:
                bridge_df = pd.read_csv(bridge_time_path)
                # BRIDGE_TIME ì»¬ëŸ¼ ì¶”ì¶œ (IDC_VAL ì»¬ëŸ¼)
                if 'IDC_VAL' in bridge_df.columns:
                    bridge_df = bridge_df.rename(columns={'IDC_VAL': 'BRIDGE_TIME'})
                    # ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
                    if 'CRT_TM' in bridge_df.columns:
                        bridge_df['timestamp'] = pd.to_datetime(bridge_df['CRT_TM'])
                        df['timestamp'] = pd.to_datetime(df.iloc[:, 0], format='%Y%m%d%H%M', errors='coerce')
                        df = pd.merge(df, bridge_df[['timestamp', 'BRIDGE_TIME']], 
                                    on='timestamp', how='left')
                        df['BRIDGE_TIME'] = df['BRIDGE_TIME'].fillna(3.5)  # ê¸°ë³¸ê°’
                        print("âœ… BRIDGE_TIME ë°ì´í„° ë³‘í•© ì™„ë£Œ")
                else:
                    print("âš ï¸ BRIDGE_TIME ì»¬ëŸ¼ í˜•ì‹ì´ ë‹¤ë¦…ë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©")
                    df['BRIDGE_TIME'] = 3.5
            except Exception as e:
                print(f"âš ï¸ BRIDGE_TIME ë³‘í•© ì‹¤íŒ¨: {e}")
                df['BRIDGE_TIME'] = 3.5
        else:
            print("â„¹ï¸ BRIDGE_TIME ë°ì´í„° ì—†ìŒ. ê¸°ë³¸ê°’(3.5) ì‚¬ìš©")
            df['BRIDGE_TIME'] = 3.5
        
        processor.analyze_data(df)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë¦¬
        if 'timestamp' not in df.columns:
            df['timestamp'] = pd.to_datetime(df.iloc[:, 0], format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('timestamp').reset_index(drop=True)
        df = df.fillna(method='ffill').fillna(0)
        
        state['df_shape'] = df.shape
        state['step'] = 2
        ckpt.save_state(state)
        
        # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•´ df ì €ì¥
        df.to_pickle('./checkpoints/processed_df_v4.pkl')
        print("ğŸ’¾ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
    
    # Step 2: ì‹œí€€ìŠ¤ ìƒì„±
    if step <= 2:
        print("\n[Step 2/6] V4 ì‹œí€€ìŠ¤ ìƒì„±")
        
        df = pd.read_pickle('./checkpoints/processed_df_v4.pkl')
        X, y, X_physics, weights = processor.create_sequences_v4(df)
        
        state['X'] = X
        state['y'] = y
        state['X_physics'] = X_physics
        state['weights'] = weights
        state['n_features'] = X.shape[2]
        state['step'] = 3
        ckpt.save_state(state)
        print("âœ… V4 ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    # Step 3: ë°ì´í„° ë¶„í• 
    if step <= 3:
        print("\n[Step 3/6] ë°ì´í„° ë¶„í• ")
        
        X = state.get('X')
        y = state.get('y')
        X_physics = state.get('X_physics')
        weights = state.get('weights')
        
        if X is None or y is None:
            print("âŒ ì‹œí€€ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. Step 2ë¶€í„° ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            state['step'] = 2
            ckpt.save_state(state)
            return
        
        indices = np.arange(len(X))
        train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)
        val_idx, test_idx = train_test_split(test_idx, test_size=0.5, random_state=42)
        
        state['train_idx'] = train_idx
        state['val_idx'] = val_idx
        state['test_idx'] = test_idx
        state['step'] = 4
        ckpt.save_state(state)
        print(f"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ")
        print(f"  Train: {len(train_idx)}")
        print(f"  Valid: {len(val_idx)}")
        print(f"  Test: {len(test_idx)}")
    
    # Step 4: ìŠ¤ì¼€ì¼ë§
    if step <= 4:
        print("\n[Step 4/6] ë°ì´í„° ìŠ¤ì¼€ì¼ë§")
        
        X = state['X']
        y = state['y']
        X_physics = state['X_physics']
        weights = state['weights']
        n_features = state['n_features']
        
        train_idx = state['train_idx']
        val_idx = state['val_idx']
        test_idx = state['test_idx']
        
        # ë°ì´í„° ë¶„í• 
        X_train, y_train = X[train_idx], y[train_idx]
        X_val, y_val = X[val_idx], y[val_idx]
        X_test, y_test = X[test_idx], y[test_idx]
        
        X_physics_train = X_physics[train_idx]
        X_physics_val = X_physics[val_idx]
        X_physics_test = X_physics[test_idx]
        
        weights_train = weights[train_idx]
        
        # ìŠ¤ì¼€ì¼ë§
        X_train_flat = X_train.reshape(-1, n_features)
        X_train_scaled = processor.scaler_X.fit_transform(X_train_flat)
        X_train_scaled = X_train_scaled.reshape(len(X_train), 20, n_features)
        
        X_val_scaled = processor.scaler_X.transform(X_val.reshape(-1, n_features)).reshape(len(X_val), 20, n_features)
        X_test_scaled = processor.scaler_X.transform(X_test.reshape(-1, n_features)).reshape(len(X_test), 20, n_features)
        
        y_train_scaled = processor.scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = processor.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        y_test_scaled = processor.scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        X_physics_train_scaled = processor.scaler_physics.fit_transform(X_physics_train)
        X_physics_val_scaled = processor.scaler_physics.transform(X_physics_val)
        X_physics_test_scaled = processor.scaler_physics.transform(X_physics_test)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        processor.save_scalers()
        
        # ìƒíƒœ ì €ì¥
        state['X_train_scaled'] = X_train_scaled
        state['X_val_scaled'] = X_val_scaled
        state['X_test_scaled'] = X_test_scaled
        state['y_train_scaled'] = y_train_scaled
        state['y_val_scaled'] = y_val_scaled
        state['y_test'] = y_test
        state['X_physics_train_scaled'] = X_physics_train_scaled
        state['X_physics_val_scaled'] = X_physics_val_scaled
        state['X_physics_test_scaled'] = X_physics_test_scaled
        state['weights_train'] = weights_train
        state['step'] = 5
        ckpt.save_state(state)
        print("âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
    
    # Step 5: ëª¨ë¸ í•™ìŠµ
    if step <= 5:
        print("\n[Step 5/6] ëª¨ë¸ í•™ìŠµ")
        print("="*80)
        
        # ë°ì´í„° ë¡œë“œ
        X_train_scaled = state['X_train_scaled']
        X_val_scaled = state['X_val_scaled']
        y_train_scaled = state['y_train_scaled']
        y_val_scaled = state['y_val_scaled']
        X_physics_train_scaled = state['X_physics_train_scaled']
        X_physics_val_scaled = state['X_physics_val_scaled']
        weights_train = state['weights_train']
        n_features = state.get('n_features', X_train_scaled.shape[2])
        
        config = {
            'seq_len': 20,
            'n_features': n_features,
            'patch_len': 5
        }
        
        # Model 1 í•™ìŠµ
        model1_trained = state.get('model1_trained', False)
        
        if not model1_trained:
            print("\nğŸ¤– Model 1: PatchTST í•™ìŠµ")
            model1 = PatchTSTModel(config)
            model1.compile(
                optimizer=Adam(learning_rate=0.001),
                loss=ExtremeLossV4(extreme_focus=False),
                metrics=['mae']
            )
            
            callbacks_model1 = [
                EarlyStopping(patience=20, restore_best_weights=True),
                ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-6),
                ModelCheckpoint('./checkpoints/model1_v4.h5', save_best_only=True, save_weights_only=True),
                ExtremeValueCallbackV4(X_val_scaled, y_val_scaled, processor.scaler_y)
            ]
            
            history1 = model1.fit(
                X_train_scaled, y_train_scaled,
                validation_data=(X_val_scaled, y_val_scaled),
                sample_weight=weights_train,
                epochs=50,
                batch_size=32,
                callbacks=callbacks_model1,
                verbose=1
            )
            
            state['model1_trained'] = True
            ckpt.save_state(state)
            print("âœ… Model 1 í•™ìŠµ ì™„ë£Œ")
        
        # Model 2 í•™ìŠµ
        model2_trained = state.get('model2_trained', False)
        
        if not model2_trained:
            print("\nğŸ¤– Model 2: PatchTST + PINN í•™ìŠµ")
            model2 = PatchTSTPINN(config)
            model2.compile(
                optimizer=Adam(learning_rate=0.0008),
                loss=ExtremeLossV4(extreme_focus=True),
                metrics=['mae']
            )
            
            callbacks_model2 = [
                EarlyStopping(patience=20, restore_best_weights=True),
                ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-6),
                ModelCheckpoint('./checkpoints/model2_v4.h5', save_best_only=True, save_weights_only=True),
                ExtremeValueCallbackV4((X_val_scaled, X_physics_val_scaled), y_val_scaled, processor.scaler_y, X_physics_val_scaled)
            ]
            
            history2 = model2.fit(
                [X_train_scaled, X_physics_train_scaled], y_train_scaled,
                validation_data=([X_val_scaled, X_physics_val_scaled], y_val_scaled),
                sample_weight=weights_train,
                epochs=60,
                batch_size=32,
                callbacks=callbacks_model2,
                verbose=1
            )
            
            state['model2_trained'] = True
            state['step'] = 6
            ckpt.save_state(state)
            print("âœ… Model 2 í•™ìŠµ ì™„ë£Œ")
    
    # Step 6: í‰ê°€
    if step <= 6:
        print("\n[Step 6/6] ëª¨ë¸ í‰ê°€")
        print("="*80)
        print("ğŸ“Š V4 ìµœì¢… í‰ê°€")
        print("="*80)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        X_test_scaled = state['X_test_scaled']
        y_test = state['y_test']
        X_physics_test_scaled = state['X_physics_test_scaled']
        n_features = state.get('n_features', X_test_scaled.shape[2])
        
        config = {
            'seq_len': 20,
            'n_features': n_features,
            'patch_len': 5
        }
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        if not processor.load_scalers():
            print("âŒ í‰ê°€ ì¤‘ë‹¨: ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨")
            return
        
        # Model 1 í‰ê°€
        print("\n[Model 1: PatchTST]")
        model1 = PatchTSTModel(config)
        model1.compile(optimizer='adam', loss='mse')
        
        # ë”ë¯¸ ë°ì´í„°ë¡œ ë¹Œë“œ
        dummy_input = np.zeros((1, 20, n_features))
        _ = model1(dummy_input)
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model1.load_weights('./checkpoints/model1_v4.h5')
        
        y_pred1_scaled = model1.predict(X_test_scaled, verbose=0)
        y_pred1 = processor.scaler_y.inverse_transform(y_pred1_scaled.reshape(-1, 1)).flatten()
        
        mae1 = np.mean(np.abs(y_test - y_pred1))
        rmse1 = np.sqrt(np.mean((y_test - y_pred1)**2))
        print(f"ì „ì²´ MAE: {mae1:.2f}")
        print(f"ì „ì²´ RMSE: {rmse1:.2f}")
        
        # 3êµ¬ê°„ í‰ê°€
        print("\n3êµ¬ê°„ ì„±ëŠ¥:")
        mask_low = y_test < 200
        mask_normal = (y_test >= 200) & (y_test < 300)
        mask_danger = y_test >= 300
        
        if mask_low.sum() > 0:
            print(f"  ì €êµ¬ê°„(<200): MAE={np.mean(np.abs(y_test[mask_low] - y_pred1[mask_low])):.2f}")
        if mask_normal.sum() > 0:
            print(f"  ì •ìƒ(200-300): MAE={np.mean(np.abs(y_test[mask_normal] - y_pred1[mask_normal])):.2f}")
        if mask_danger.sum() > 0:
            print(f"  ìœ„í—˜(300+): MAE={np.mean(np.abs(y_test[mask_danger] - y_pred1[mask_danger])):.2f}")
        
        # ê·¹ë‹¨ê°’ ê°ì§€ + False Positive
        print("\nê·¹ë‹¨ê°’ ê°ì§€:")
        for threshold in [300, 310, 335]:
            mask = y_test >= threshold
            if mask.sum() > 0:
                detected = (y_pred1 >= threshold)[mask].sum()
                print(f"  {threshold}+: {detected}/{mask.sum()} ({detected/mask.sum()*100:.1f}%)")
        
        # False Positive Rate
        mask_under_300 = y_test < 300
        if mask_under_300.sum() > 0:
            fp_300 = (y_pred1 >= 300)[mask_under_300].sum()
            print(f"\nFalse Positive (300): {fp_300}/{mask_under_300.sum()} ({fp_300/mask_under_300.sum()*100:.1f}%)")
        
        # Model 2 í‰ê°€
        print("\n[Model 2: PatchTST + PINN]")
        model2 = PatchTSTPINN(config)
        model2.compile(optimizer='adam', loss='mse')
        
        # ë”ë¯¸ ë°ì´í„°ë¡œ ë¹Œë“œ
        dummy_seq = np.zeros((1, 20, n_features))
        dummy_physics = np.zeros((1, 9))  # V4: 9ê°œ ë¬¼ë¦¬ ë°ì´í„°
        _ = model2([dummy_seq, dummy_physics])
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model2.load_weights('./checkpoints/model2_v4.h5')
        
        y_pred2_scaled = model2.predict([X_test_scaled, X_physics_test_scaled], verbose=0)
        y_pred2 = processor.scaler_y.inverse_transform(y_pred2_scaled.reshape(-1, 1)).flatten()
        
        mae2 = np.mean(np.abs(y_test - y_pred2))
        rmse2 = np.sqrt(np.mean((y_test - y_pred2)**2))
        print(f"ì „ì²´ MAE: {mae2:.2f}")
        print(f"ì „ì²´ RMSE: {rmse2:.2f}")
        
        # 3êµ¬ê°„ í‰ê°€
        print("\n3êµ¬ê°„ ì„±ëŠ¥:")
        if mask_low.sum() > 0:
            print(f"  ì €êµ¬ê°„(<200): MAE={np.mean(np.abs(y_test[mask_low] - y_pred2[mask_low])):.2f}")
        if mask_normal.sum() > 0:
            print(f"  ì •ìƒ(200-300): MAE={np.mean(np.abs(y_test[mask_normal] - y_pred2[mask_normal])):.2f}")
        if mask_danger.sum() > 0:
            print(f"  ìœ„í—˜(300+): MAE={np.mean(np.abs(y_test[mask_danger] - y_pred2[mask_danger])):.2f}")
        
        # ê·¹ë‹¨ê°’ ê°ì§€ + False Positive
        print("\nê·¹ë‹¨ê°’ ê°ì§€:")
        for threshold in [300, 310, 335]:
            mask = y_test >= threshold
            if mask.sum() > 0:
                detected = (y_pred2 >= threshold)[mask].sum()
                print(f"  {threshold}+: {detected}/{mask.sum()} ({detected/mask.sum()*100:.1f}%)")
        
        # False Positive Rate
        if mask_under_300.sum() > 0:
            fp_300 = (y_pred2 >= 300)[mask_under_300].sum()
            print(f"\nFalse Positive (300): {fp_300}/{mask_under_300.sum()} ({fp_300/mask_under_300.sum()*100:.1f}%)")
        
        print("\n" + "="*80)
        print("âœ… V4.0 í•™ìŠµ ë° í‰ê°€ ì™„ë£Œ!")
        print("="*80)
        
        # ì™„ë£Œ í›„ ìƒíƒœ íŒŒì¼ ì œê±° ì˜µì…˜
        remove = input("\nìƒíƒœ íŒŒì¼ì„ ì œê±°í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
        if remove == 'y':
            ckpt.clear_state()
            print("ğŸ§¹ ìƒíƒœ íŒŒì¼ ì œê±° ì™„ë£Œ")

if __name__ == "__main__":
    main()