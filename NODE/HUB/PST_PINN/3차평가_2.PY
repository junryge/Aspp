# -*- coding: utf-8 -*-
"""
HUBROOM ì˜ˆì¸¡ ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ
- 2025ë…„ 8ì›” 8ì¼ ~ 8ì›” 31ì¼ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì „ì²´ í‰ê°€
- ê³¼ê±° 20ë¶„ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡
- ë‚ ì§œë³„ ì˜ˆì¸¡ ì„±ëŠ¥ ì¶”ì 
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler
import os
import pickle
import warnings
import joblib
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

warnings.filterwarnings('ignore')
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

print("="*80)
print("ğŸ­ HUBROOM ì˜ˆì¸¡ ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ")
print(f"ğŸ“… í‰ê°€ ì‹¤í–‰: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("ğŸ“Š ë°ì´í„°: 2025ë…„ 8ì›” 8ì¼ ~ 8ì›” 31ì¼")
print("="*80)

# ========================================
# ëª¨ë¸ ì •ì˜ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼)
# ========================================

class PatchTSTModel(keras.Model):
    """Model 1: PatchTST (ì „ì²´ ê· í˜•)"""
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(128, activation='relu')
        
        # Transformer
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm1 = layers.LayerNormalization()
        self.norm2 = layers.LayerNormalization()
        
        self.ffn = keras.Sequential([
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(128)
        ])
        
        # ì¶œë ¥
        self.flatten = layers.Flatten()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dropout = layers.Dropout(0.3)
        self.dense2 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(1)
        
    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        
        # íŒ¨ì¹˜ ìƒì„±
        x = tf.reshape(x, [batch_size, self.n_patches, self.patch_len * self.n_features])
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x = self.patch_embedding(x)
        
        # Transformer
        attn = self.attention(x, x, training=training)
        x = self.norm1(x + attn)
        
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        
        # ì¶œë ¥
        x = self.flatten(x)
        x = self.dense1(x)
        x = self.dropout(x, training=training)
        x = self.dense2(x)
        output = self.output_layer(x)
        
        return tf.squeeze(output, axis=-1)

class PatchTSTPINN(keras.Model):
    """Model 2: PatchTST + PINN (ê·¹ë‹¨ê°’ íŠ¹í™”)"""
    def __init__(self, config):
        super().__init__()
        
        # PatchTST ë¶€ë¶„
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(128, activation='relu')
        
        # Transformer
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm = layers.LayerNormalization()
        
        # ì‹œê³„ì—´ ì²˜ë¦¬
        self.flatten = layers.Flatten()
        self.temporal_dense = layers.Dense(64, activation='relu')
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬ (PINN)
        self.physics_net = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dense(16, activation='relu')
        ])
        
        # ìœµí•© ë° ê·¹ë‹¨ê°’ ë³´ì •
        self.fusion = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(1)
        ])
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ… ë ˆì´ì–´
        self.extreme_boost = layers.Dense(1, activation='sigmoid')
        
    def call(self, inputs, training=False):
        x_seq, x_physics = inputs
        
        batch_size = tf.shape(x_seq)[0]
        
        # PatchTST ì²˜ë¦¬
        x = tf.reshape(x_seq, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        
        attn = self.attention(x, x, training=training)
        x = self.norm(x + attn)
        
        x = self.flatten(x)
        temporal_features = self.temporal_dense(x)
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬
        physics_features = self.physics_net(x_physics)
        
        # ìœµí•©
        combined = tf.concat([temporal_features, physics_features], axis=-1)
        output = self.fusion(combined)
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ…
        boost_factor = self.extreme_boost(combined)
        output = output * (1 + boost_factor * 0.2)
        
        return tf.squeeze(output, axis=-1)

# ========================================
# ë°ì´í„° ì²˜ë¦¬ ë° ì˜ˆì¸¡
# ========================================

class EvaluationSystem:
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        
        # ë¬¼ë¦¬ ë²•ì¹™ìš© ì»¬ëŸ¼
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2', 'M14B_7F_TO_HUB_JOB2'
        ]
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB', 'M16A_3F_TO_M14B_7F_JOB'
        ]
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.results = []
        
    def load_scalers(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        try:
            self.scaler_X = joblib.load('./scalers/scaler_X.pkl')
            self.scaler_y = joblib.load('./scalers/scaler_y.pkl')
            self.scaler_physics = joblib.load('./scalers/scaler_physics.pkl')
            print("âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âŒ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
            
    def load_models(self, n_features):
        """ëª¨ë¸ ë¡œë“œ"""
        config = {
            'seq_len': 20,
            'n_features': n_features,
            'patch_len': 5
        }
        
        # Model 1 ë¡œë“œ
        print("\nğŸ“¦ Model 1 ë¡œë“œ ì¤‘...")
        self.model1 = PatchTSTModel(config)
        self.model1.compile(optimizer='adam', loss='mse')
        dummy_input = np.zeros((1, 20, n_features))
        _ = self.model1(dummy_input)
        self.model1.load_weights('./checkpoints/model1_v3.h5')
        print("âœ… Model 1 ë¡œë“œ ì™„ë£Œ")
        
        # Model 2 ë¡œë“œ
        print("ğŸ“¦ Model 2 ë¡œë“œ ì¤‘...")
        self.model2 = PatchTSTPINN(config)
        self.model2.compile(optimizer='adam', loss='mse')
        dummy_seq = np.zeros((1, 20, n_features))
        dummy_physics = np.zeros((1, 3))
        _ = self.model2([dummy_seq, dummy_physics])
        self.model2.load_weights('./checkpoints/model2_v3.h5')
        print("âœ… Model 2 ë¡œë“œ ì™„ë£Œ")
        
    def prepare_data(self, df):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        print("\nğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬
        df['timestamp'] = pd.to_datetime(df.iloc[:, 0], format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df = df.fillna(method='ffill').fillna(0)
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        print(f"  ì „ì²´ ë°ì´í„°: {len(df)} rows")
        print(f"  ê¸°ê°„: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
        print(f"  ìˆ«ìí˜• ì»¬ëŸ¼: {len(numeric_cols)}ê°œ")
        
        # íƒ€ê²Ÿ ë°ì´í„° ë¶„ì„
        if self.target_col in df.columns:
            target = df[self.target_col]
            print(f"\nğŸ“ˆ íƒ€ê²Ÿ({self.target_col}) ë¶„í¬:")
            print(f"  ë²”ìœ„: {target.min():.0f} ~ {target.max():.0f}")
            print(f"  í‰ê· : {target.mean():.1f}")
            print(f"  300+: {(target >= 300).sum()}ê°œ ({(target >= 300).sum()/len(target)*100:.2f}%)")
            print(f"  310+: {(target >= 310).sum()}ê°œ ({(target >= 310).sum()/len(target)*100:.2f}%)")
            print(f"  335+: {(target >= 335).sum()}ê°œ ({(target >= 335).sum()/len(target)*100:.2f}%)")
        
        return df, numeric_cols
    
    def create_sequences_for_evaluation(self, df, numeric_cols, seq_len=20, pred_len=10):
        """í‰ê°€ìš© ì‹œí€€ìŠ¤ ìƒì„±"""
        sequences = []
        physics_data = []
        timestamps = []
        actual_values = []
        
        # ë¬¼ë¦¬ ë°ì´í„°ìš© ì»¬ëŸ¼ í™•ì¸
        available_inflow = [col for col in self.inflow_cols if col in df.columns]
        available_outflow = [col for col in self.outflow_cols if col in df.columns]
        
        total = len(df) - seq_len - pred_len + 1
        
        if total <= 0:
            print("âŒ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None, None, None, None
        
        print(f"\nğŸ“¦ í‰ê°€ìš© ì‹œí€€ìŠ¤ ìƒì„± ì¤‘... (ì´ {total}ê°œ)")
        
        for i in tqdm(range(total)):
            # ì‹œê³„ì—´ ë°ì´í„°
            seq = df[numeric_cols].iloc[i:i+seq_len].values
            sequences.append(seq)
            
            # ë¬¼ë¦¬ ë°ì´í„°
            physics = [
                df[self.target_col].iloc[i + seq_len - 1],  # í˜„ì¬ê°’
                df[available_inflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_inflow else 0,
                df[available_outflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_outflow else 0
            ]
            physics_data.append(physics)
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ì‹¤ì œê°’
            timestamps.append(df['timestamp'].iloc[i + seq_len + pred_len - 1])
            actual_values.append(df[self.target_col].iloc[i + seq_len + pred_len - 1])
        
        return np.array(sequences), np.array(physics_data), timestamps, np.array(actual_values)
    
    def evaluate_models(self, sequences, physics_data, timestamps, actual_values, numeric_cols):
        """ëª¨ë¸ í‰ê°€"""
        print("\nğŸ¤– ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...")
        
        # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        n_features = sequences.shape[2]
        sequences_flat = sequences.reshape(-1, n_features)
        sequences_scaled = self.scaler_X.transform(sequences_flat)
        sequences_scaled = sequences_scaled.reshape(len(sequences), 20, n_features)
        
        physics_scaled = self.scaler_physics.transform(physics_data)
        
        # Model 1 ì˜ˆì¸¡
        print("  Model 1 ì˜ˆì¸¡ ì¤‘...")
        pred1_scaled = self.model1.predict(sequences_scaled, verbose=0)
        pred1 = self.scaler_y.inverse_transform(pred1_scaled.reshape(-1, 1)).flatten()
        
        # Model 2 ì˜ˆì¸¡
        print("  Model 2 ì˜ˆì¸¡ ì¤‘...")
        pred2_scaled = self.model2.predict([sequences_scaled, physics_scaled], verbose=0)
        pred2 = self.scaler_y.inverse_transform(pred2_scaled.reshape(-1, 1)).flatten()
        
        # ê²°ê³¼ ì €ì¥
        results_df = pd.DataFrame({
            'timestamp': timestamps,
            'actual': actual_values,
            'pred_model1': pred1,
            'pred_model2': pred2,
            'error_model1': np.abs(actual_values - pred1),
            'error_model2': np.abs(actual_values - pred2),
            'date': pd.to_datetime(timestamps).dt.date,
            'hour': pd.to_datetime(timestamps).dt.hour
        })
        
        # ê·¹ë‹¨ê°’ í”Œë˜ê·¸ ì¶”ê°€
        results_df['is_300_plus'] = results_df['actual'] >= 300
        results_df['is_310_plus'] = results_df['actual'] >= 310
        results_df['is_335_plus'] = results_df['actual'] >= 335
        
        return results_df
    
    def analyze_results(self, results_df):
        """ê²°ê³¼ ë¶„ì„ ë° ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“Š í‰ê°€ ê²°ê³¼ ë¶„ì„")
        print("="*80)
        
        # ì „ì²´ ì„±ëŠ¥
        print("\n[ì „ì²´ ì„±ëŠ¥]")
        print(f"ë°ì´í„° í¬ì¸íŠ¸: {len(results_df)}")
        print(f"í‰ê°€ ê¸°ê°„: {results_df['date'].min()} ~ {results_df['date'].max()}")
        print(f"\nModel 1 (PatchTST):")
        print(f"  MAE: {results_df['error_model1'].mean():.2f}")
        print(f"  RMSE: {np.sqrt((results_df['error_model1']**2).mean()):.2f}")
        print(f"\nModel 2 (PatchTST + PINN):")
        print(f"  MAE: {results_df['error_model2'].mean():.2f}")
        print(f"  RMSE: {np.sqrt((results_df['error_model2']**2).mean()):.2f}")
        
        # êµ¬ê°„ë³„ ì„±ëŠ¥
        print("\n[êµ¬ê°„ë³„ ì„±ëŠ¥]")
        
        # ì €êµ¬ê°„ (<200)
        mask_low = results_df['actual'] < 200
        if mask_low.sum() > 0:
            print(f"\nì €êµ¬ê°„ (<200): {mask_low.sum()}ê°œ")
            print(f"  Model 1 MAE: {results_df.loc[mask_low, 'error_model1'].mean():.2f}")
            print(f"  Model 2 MAE: {results_df.loc[mask_low, 'error_model2'].mean():.2f}")
        
        # ì •ìƒêµ¬ê°„ (200-300)
        mask_normal = (results_df['actual'] >= 200) & (results_df['actual'] < 300)
        if mask_normal.sum() > 0:
            print(f"\nì •ìƒêµ¬ê°„ (200-300): {mask_normal.sum()}ê°œ")
            print(f"  Model 1 MAE: {results_df.loc[mask_normal, 'error_model1'].mean():.2f}")
            print(f"  Model 2 MAE: {results_df.loc[mask_normal, 'error_model2'].mean():.2f}")
        
        # ìœ„í—˜êµ¬ê°„ (300+)
        mask_danger = results_df['actual'] >= 300
        if mask_danger.sum() > 0:
            print(f"\nìœ„í—˜êµ¬ê°„ (300+): {mask_danger.sum()}ê°œ")
            print(f"  Model 1 MAE: {results_df.loc[mask_danger, 'error_model1'].mean():.2f}")
            print(f"  Model 2 MAE: {results_df.loc[mask_danger, 'error_model2'].mean():.2f}")
        
        # ê·¹ë‹¨ê°’ ê°ì§€ ì„±ëŠ¥
        print("\n[ê·¹ë‹¨ê°’ ê°ì§€ ì„±ëŠ¥]")
        
        for threshold in [300, 310, 335]:
            mask = results_df['actual'] >= threshold
            if mask.sum() > 0:
                detected1 = (results_df.loc[mask, 'pred_model1'] >= threshold).sum()
                detected2 = (results_df.loc[mask, 'pred_model2'] >= threshold).sum()
                
                print(f"\n{threshold}+ ê°ì§€ ({mask.sum()}ê°œ):")
                print(f"  Model 1: {detected1}/{mask.sum()} ({detected1/mask.sum()*100:.1f}%)")
                print(f"  Model 2: {detected2}/{mask.sum()} ({detected2/mask.sum()*100:.1f}%)")
        
        # ë‚ ì§œë³„ ì„±ëŠ¥
        print("\n[ë‚ ì§œë³„ ì„±ëŠ¥]")
        daily_stats = results_df.groupby('date').agg({
            'error_model1': 'mean',
            'error_model2': 'mean',
            'actual': ['min', 'max', 'mean'],
            'is_300_plus': 'sum',
            'is_310_plus': 'sum',
            'is_335_plus': 'sum'
        }).round(2)
        
        print("\në‚ ì§œë³„ MAE:")
        for date in daily_stats.index[:10]:  # ì²˜ìŒ 10ì¼ë§Œ ì¶œë ¥
            mae1 = daily_stats.loc[date, ('error_model1', 'mean')]
            mae2 = daily_stats.loc[date, ('error_model2', 'mean')]
            actual_range = f"{daily_stats.loc[date, ('actual', 'min')]:.0f}-{daily_stats.loc[date, ('actual', 'max')]:.0f}"
            n_extreme = daily_stats.loc[date, ('is_310_plus', 'sum')]
            
            print(f"  {date}: Model1={mae1:.1f}, Model2={mae2:.1f} | ë²”ìœ„:{actual_range} | 310+:{n_extreme:.0f}ê°œ")
        
        if len(daily_stats) > 10:
            print(f"  ... (ì´ {len(daily_stats)}ì¼)")
        
        return results_df
    
    def save_results(self, results_df):
        """ê²°ê³¼ ì €ì¥"""
        print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # CSV ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ìƒì„¸ ê²°ê³¼
        filename_detail = f'evaluation_results_detail_{timestamp}.csv'
        results_df.to_csv(filename_detail, index=False, encoding='utf-8-sig')
        print(f"  ìƒì„¸ ê²°ê³¼: {filename_detail}")
        
        # ìš”ì•½ í†µê³„
        summary_stats = pd.DataFrame({
            'ì§€í‘œ': ['ì „ì²´ MAE', 'ì „ì²´ RMSE', '300+ MAE', '310+ MAE', '335+ MAE',
                   '300+ ê°ì§€ìœ¨', '310+ ê°ì§€ìœ¨', '335+ ê°ì§€ìœ¨'],
            'Model1_PatchTST': [
                results_df['error_model1'].mean(),
                np.sqrt((results_df['error_model1']**2).mean()),
                results_df[results_df['is_300_plus']]['error_model1'].mean() if results_df['is_300_plus'].sum() > 0 else None,
                results_df[results_df['is_310_plus']]['error_model1'].mean() if results_df['is_310_plus'].sum() > 0 else None,
                results_df[results_df['is_335_plus']]['error_model1'].mean() if results_df['is_335_plus'].sum() > 0 else None,
                (results_df[results_df['is_300_plus']]['pred_model1'] >= 300).sum() / results_df['is_300_plus'].sum() * 100 if results_df['is_300_plus'].sum() > 0 else None,
                (results_df[results_df['is_310_plus']]['pred_model1'] >= 310).sum() / results_df['is_310_plus'].sum() * 100 if results_df['is_310_plus'].sum() > 0 else None,
                (results_df[results_df['is_335_plus']]['pred_model1'] >= 335).sum() / results_df['is_335_plus'].sum() * 100 if results_df['is_335_plus'].sum() > 0 else None
            ],
            'Model2_PatchTST_PINN': [
                results_df['error_model2'].mean(),
                np.sqrt((results_df['error_model2']**2).mean()),
                results_df[results_df['is_300_plus']]['error_model2'].mean() if results_df['is_300_plus'].sum() > 0 else None,
                results_df[results_df['is_310_plus']]['error_model2'].mean() if results_df['is_310_plus'].sum() > 0 else None,
                results_df[results_df['is_335_plus']]['error_model2'].mean() if results_df['is_335_plus'].sum() > 0 else None,
                (results_df[results_df['is_300_plus']]['pred_model2'] >= 300).sum() / results_df['is_300_plus'].sum() * 100 if results_df['is_300_plus'].sum() > 0 else None,
                (results_df[results_df['is_310_plus']]['pred_model2'] >= 310).sum() / results_df['is_310_plus'].sum() * 100 if results_df['is_310_plus'].sum() > 0 else None,
                (results_df[results_df['is_335_plus']]['pred_model2'] >= 335).sum() / results_df['is_335_plus'].sum() * 100 if results_df['is_335_plus'].sum() > 0 else None
            ]
        })
        
        filename_summary = f'evaluation_results_summary_{timestamp}.csv'
        summary_stats.to_csv(filename_summary, index=False, encoding='utf-8-sig')
        print(f"  ìš”ì•½ í†µê³„: {filename_summary}")
        
        # ì‹œê°í™” ì €ì¥
        self.create_visualizations(results_df, timestamp)
        
        print("\nâœ… ëª¨ë“  ê²°ê³¼ ì €ì¥ ì™„ë£Œ!")
        
    def create_visualizations(self, results_df, timestamp):
        """ê²°ê³¼ ì‹œê°í™”"""
        print("\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
        
        fig, axes = plt.subplots(3, 2, figsize=(15, 12))
        
        # 1. ì‹œê³„ì—´ ì˜ˆì¸¡ ë¹„êµ (ìƒ˜í”Œ)
        sample_size = min(500, len(results_df))
        sample_df = results_df.iloc[:sample_size]
        
        axes[0, 0].plot(sample_df.index, sample_df['actual'], label='Actual', alpha=0.7, linewidth=1)
        axes[0, 0].plot(sample_df.index, sample_df['pred_model1'], label='Model 1', alpha=0.7, linewidth=1)
        axes[0, 0].plot(sample_df.index, sample_df['pred_model2'], label='Model 2', alpha=0.7, linewidth=1)
        axes[0, 0].axhline(y=310, color='r', linestyle='--', alpha=0.5, label='310 threshold')
        axes[0, 0].set_title(f'Predictions vs Actual (First {sample_size} points)')
        axes[0, 0].set_xlabel('Index')
        axes[0, 0].set_ylabel('Value')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ì‚°ì ë„ - Model 1
        axes[0, 1].scatter(results_df['actual'], results_df['pred_model1'], alpha=0.5, s=1)
        axes[0, 1].plot([results_df['actual'].min(), results_df['actual'].max()], 
                       [results_df['actual'].min(), results_df['actual'].max()], 
                       'r--', alpha=0.5)
        axes[0, 1].set_title('Model 1: Predicted vs Actual')
        axes[0, 1].set_xlabel('Actual')
        axes[0, 1].set_ylabel('Predicted')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. ì‚°ì ë„ - Model 2
        axes[1, 0].scatter(results_df['actual'], results_df['pred_model2'], alpha=0.5, s=1)
        axes[1, 0].plot([results_df['actual'].min(), results_df['actual'].max()], 
                       [results_df['actual'].min(), results_df['actual'].max()], 
                       'r--', alpha=0.5)
        axes[1, 0].set_title('Model 2: Predicted vs Actual')
        axes[1, 0].set_xlabel('Actual')
        axes[1, 0].set_ylabel('Predicted')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. ì˜¤ì°¨ ë¶„í¬
        axes[1, 1].hist(results_df['error_model1'], bins=50, alpha=0.5, label='Model 1', density=True)
        axes[1, 1].hist(results_df['error_model2'], bins=50, alpha=0.5, label='Model 2', density=True)
        axes[1, 1].set_title('Error Distribution')
        axes[1, 1].set_xlabel('Absolute Error')
        axes[1, 1].set_ylabel('Density')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        # 5. ë‚ ì§œë³„ MAE
        daily_mae = results_df.groupby('date')[['error_model1', 'error_model2']].mean()
        axes[2, 0].plot(daily_mae.index, daily_mae['error_model1'], marker='o', label='Model 1')
        axes[2, 0].plot(daily_mae.index, daily_mae['error_model2'], marker='s', label='Model 2')
        axes[2, 0].set_title('Daily MAE')
        axes[2, 0].set_xlabel('Date')
        axes[2, 0].set_ylabel('MAE')
        axes[2, 0].legend()
        axes[2, 0].grid(True, alpha=0.3)
        axes[2, 0].tick_params(axis='x', rotation=45)
        
        # 6. ê·¹ë‹¨ê°’ ê°ì§€ ì„±ëŠ¥
        thresholds = [300, 310, 335]
        detect_rates_m1 = []
        detect_rates_m2 = []
        
        for threshold in thresholds:
            mask = results_df['actual'] >= threshold
            if mask.sum() > 0:
                rate1 = (results_df.loc[mask, 'pred_model1'] >= threshold).sum() / mask.sum() * 100
                rate2 = (results_df.loc[mask, 'pred_model2'] >= threshold).sum() / mask.sum() * 100
                detect_rates_m1.append(rate1)
                detect_rates_m2.append(rate2)
            else:
                detect_rates_m1.append(0)
                detect_rates_m2.append(0)
        
        x_pos = np.arange(len(thresholds))
        width = 0.35
        
        axes[2, 1].bar(x_pos - width/2, detect_rates_m1, width, label='Model 1')
        axes[2, 1].bar(x_pos + width/2, detect_rates_m2, width, label='Model 2')
        axes[2, 1].set_title('Extreme Value Detection Rate')
        axes[2, 1].set_xlabel('Threshold')
        axes[2, 1].set_ylabel('Detection Rate (%)')
        axes[2, 1].set_xticks(x_pos)
        axes[2, 1].set_xticklabels([f'{t}+' for t in thresholds])
        axes[2, 1].legend()
        axes[2, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        filename_plot = f'evaluation_plots_{timestamp}.png'
        plt.savefig(filename_plot, dpi=150, bbox_inches='tight')
        print(f"  ì‹œê°í™” ì €ì¥: {filename_plot}")
        plt.close()

# ========================================
# ë©”ì¸ ì‹¤í–‰
# ========================================

def main():
    evaluator = EvaluationSystem()
    
    # 1. ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    if not evaluator.load_scalers():
        print("âŒ í‰ê°€ ì¤‘ë‹¨: ìŠ¤ì¼€ì¼ëŸ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # 2. ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    data_file = '20250808_TO_20250831.csv'
    
    if not os.path.exists(data_file):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_file}")
        # ë‹¤ë¥¸ ê°€ëŠ¥í•œ íŒŒì¼ëª…ë“¤ ì‹œë„
        alternative_files = [
            'data/20250808_TO_20250831.csv',
            './20250808_TO_20250831.CSV',
            'data/20250808_TO_20250831.CSV'
        ]
        
        for alt_file in alternative_files:
            if os.path.exists(alt_file):
                data_file = alt_file
                print(f"âœ… ëŒ€ì²´ íŒŒì¼ ë°œê²¬: {alt_file}")
                break
        else:
            print("âŒ í‰ê°€í•  ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
    
    df = pd.read_csv(data_file)
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape}")
    
    # 3. ë°ì´í„° ì „ì²˜ë¦¬
    df, numeric_cols = evaluator.prepare_data(df)
    
    # 4. ëª¨ë¸ ë¡œë“œ
    n_features = len(numeric_cols)
    evaluator.load_models(n_features)
    
    # 5. ì‹œí€€ìŠ¤ ìƒì„±
    sequences, physics_data, timestamps, actual_values = evaluator.create_sequences_for_evaluation(
        df, numeric_cols
    )
    
    if sequences is None:
        print("âŒ ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨")
        return
    
    # 6. ëª¨ë¸ í‰ê°€
    results_df = evaluator.evaluate_models(
        sequences, physics_data, timestamps, actual_values, numeric_cols
    )
    
    # 7. ê²°ê³¼ ë¶„ì„
    results_df = evaluator.analyze_results(results_df)
    
    # 8. ê²°ê³¼ ì €ì¥
    evaluator.save_results(results_df)
    
    print("\n" + "="*80)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("="*80)

if __name__ == "__main__":
    main()