# -*- coding: utf-8 -*-
"""
HUBROOM ëª¨ë¸ í‰ê°€ - ì‹¤ì œ CSV ë°ì´í„° í‰ê°€
- 202509ì›”.csv ë°ì´í„° ë¡œë“œ
- ê³¼ê±° 20ë¶„ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import joblib
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler
import os
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("ğŸ¯ HUBROOM ëª¨ë¸ í‰ê°€ - ì‹¤ì œ CSV ë°ì´í„°")
print("ğŸ“… ê³¼ê±° 20ë¶„ â†’ 10ë¶„ í›„ ì˜ˆì¸¡")
print("="*80)

# ========================================
# Model 1: PatchTST
# ========================================

class PatchTSTModel(keras.Model):
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        self.patch_embedding = layers.Dense(128, activation='relu')
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm1 = layers.LayerNormalization()
        self.norm2 = layers.LayerNormalization()
        
        self.ffn = keras.Sequential([
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(128)
        ])
        
        self.flatten = layers.Flatten()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dropout = layers.Dropout(0.3)
        self.dense2 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(1)
        
    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        x = tf.reshape(x, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        attn = self.attention(x, x, training=training)
        x = self.norm1(x + attn)
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        x = self.flatten(x)
        x = self.dense1(x)
        x = self.dropout(x, training=training)
        x = self.dense2(x)
        output = self.output_layer(x)
        return tf.squeeze(output, axis=-1)

# ========================================
# Model 2: PatchTST + PINN
# ========================================

class PatchTSTPINN(keras.Model):
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        self.patch_embedding = layers.Dense(128, activation='relu')
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm = layers.LayerNormalization()
        self.flatten = layers.Flatten()
        self.temporal_dense = layers.Dense(64, activation='relu')
        
        self.physics_net = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dense(16, activation='relu')
        ])
        
        self.fusion = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(1)
        ])
        
        self.extreme_boost = layers.Dense(1, activation='sigmoid')
        
    def call(self, inputs, training=False):
        x_seq, x_physics = inputs
        batch_size = tf.shape(x_seq)[0]
        
        x = tf.reshape(x_seq, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        attn = self.attention(x, x, training=training)
        x = self.norm(x + attn)
        x = self.flatten(x)
        temporal_features = self.temporal_dense(x)
        
        physics_features = self.physics_net(x_physics)
        combined = tf.concat([temporal_features, physics_features], axis=-1)
        output = self.fusion(combined)
        
        boost_factor = self.extreme_boost(combined)
        output = output * (1 + boost_factor * 0.2)
        
        return tf.squeeze(output, axis=-1)

# ========================================
# ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤
# ========================================

class DataProcessor:
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        self.scaler_X = RobustScaler()
        self.scaler_y = MinMaxScaler(feature_range=(0, 1))
        self.scaler_physics = StandardScaler()
        
        # ë¬¼ë¦¬ ë²•ì¹™ìš© ì»¬ëŸ¼
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2', 'M14B_7F_TO_HUB_JOB2'
        ]
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB', 'M16A_3F_TO_M14B_7F_JOB'
        ]
    
    def create_sequences(self, df, seq_len=20, pred_len=10):
        """ê³¼ê±° 20ë¶„ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡ì„ ìœ„í•œ ì‹œí€€ìŠ¤ ìƒì„±"""
        
        X, y, X_physics = [], [], []
        timestamps = []
        
        # íƒ€ê²Ÿ ì»¬ëŸ¼ í™•ì¸
        if self.target_col not in df.columns:
            print(f"âš ï¸ íƒ€ê²Ÿ ì»¬ëŸ¼ '{self.target_col}'ì´ ì—†ìŠµë‹ˆë‹¤.")
            # íƒ€ê²Ÿ ì»¬ëŸ¼ì´ ë‹¤ë¥¸ ì´ë¦„ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í™•ì¸
            possible_targets = [col for col in df.columns if 'CURRENT' in col and 'M16A_3F' in col]
            if possible_targets:
                self.target_col = possible_targets[0]
                print(f"âœ… ëŒ€ì²´ íƒ€ê²Ÿ ì»¬ëŸ¼ ì‚¬ìš©: {self.target_col}")
            else:
                raise ValueError("íƒ€ê²Ÿ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # íŠ¹ì§• ì»¬ëŸ¼ ì„ íƒ
        feature_cols = [col for col in df.columns if col not in ['timestamp', 'datetime']]
        
        # ë¬¼ë¦¬ ë²•ì¹™ ì»¬ëŸ¼ í™•ì¸
        available_inflow = [col for col in self.inflow_cols if col in df.columns]
        available_outflow = [col for col in self.outflow_cols if col in df.columns]
        
        if not available_inflow:
            available_inflow = [col for col in df.columns if 'TO_HUB' in col][:4]
        if not available_outflow:
            available_outflow = [col for col in df.columns if 'M16A_3F_TO' in col][:4]
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        for i in range(len(df) - seq_len - pred_len + 1):
            # ì…ë ¥: ië¶€í„° i+seq_lenê¹Œì§€ (20ë¶„)
            X.append(df[feature_cols].iloc[i:i+seq_len].values)
            
            # ì¶œë ¥: i+seq_len+pred_len-1 ì‹œì  (10ë¶„ í›„)
            y.append(df[self.target_col].iloc[i+seq_len+pred_len-1])
            
            # ë¬¼ë¦¬ íŠ¹ì§•
            if available_inflow and available_outflow:
                inflow = df[available_inflow].iloc[i+seq_len-1].sum()
                outflow = df[available_outflow].iloc[i+seq_len-1].sum()
                net_flow = inflow - outflow
                X_physics.append([inflow, outflow, net_flow])
            else:
                X_physics.append([0, 0, 0])
            
            # íƒ€ì„ìŠ¤íƒ¬í”„
            if 'timestamp' in df.columns:
                timestamps.append(df['timestamp'].iloc[i+seq_len+pred_len-1])
            else:
                timestamps.append(df.index[i+seq_len+pred_len-1])
        
        return np.array(X), np.array(y), np.array(X_physics), timestamps

# ========================================
# CSV ë°ì´í„° ë¡œë“œ ë° í‰ê°€
# ========================================

def evaluate_csv_data(csv_path='data/202509ì›”.csv'):
    """ì‹¤ì œ CSV ë°ì´í„°ë¡œ í‰ê°€"""
    
    print(f"\nğŸ“‚ CSV ë°ì´í„° ë¡œë“œ ì¤‘: {csv_path}")
    
    # CSV ë¡œë“œ
    try:
        df = pd.read_csv(csv_path)
        print(f"âœ… CSV ë¡œë“œ ì™„ë£Œ: {df.shape}")
    except Exception as e:
        print(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ë‹¤ë¥¸ ê²½ë¡œ ì‹œë„...")
        
        # ë‹¤ë¥¸ ê°€ëŠ¥í•œ ê²½ë¡œë“¤
        possible_paths = [
            './data/202509ì›”.csv',
            '../data/202509ì›”.csv',
            './202509ì›”.csv',
            'data/20250901_TO_20250930.CSV',
            'data/HUB_0509_TO_0730_DATA.CSV'
        ]
        
        for path in possible_paths:
            try:
                df = pd.read_csv(path)
                print(f"âœ… CSV ë¡œë“œ ì„±ê³µ: {path}")
                csv_path = path
                break
            except:
                continue
        else:
            raise FileNotFoundError("CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"\nğŸ“Š ë°ì´í„° ì •ë³´:")
    print(f"  - í–‰ ìˆ˜: {len(df)}")
    print(f"  - ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
    print(f"  - ì»¬ëŸ¼ ëª©ë¡: {list(df.columns[:10])}...")
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬
    if df.columns[0].isdigit() or 'timestamp' not in df.columns:
        # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ íƒ€ì„ìŠ¤íƒ¬í”„ì¸ ê²½ìš°
        df['timestamp'] = pd.to_datetime(df.iloc[:, 0], format='%Y%m%d%H%M', errors='coerce')
    
    # ë°ì´í„° ì •ë ¬ ë° ì „ì²˜ë¦¬
    if 'timestamp' in df.columns:
        df = df.sort_values('timestamp').reset_index(drop=True)
    
    df = df.fillna(method='ffill').fillna(0)
    
    # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    df_numeric = df[numeric_cols]
    
    if 'timestamp' in df.columns:
        df_numeric['timestamp'] = df['timestamp']
    
    # ë°ì´í„° í”„ë¡œì„¸ì„œ ìƒì„±
    processor = DataProcessor()
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    print("\nğŸ“ˆ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
    X, y, X_physics, timestamps = processor.create_sequences(df_numeric, seq_len=20, pred_len=10)
    
    print(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ:")
    print(f"  - ìƒ˜í”Œ ìˆ˜: {len(X)}")
    print(f"  - ì…ë ¥ shape: {X.shape}")
    print(f"  - íƒ€ê²Ÿ shape: {y.shape}")
    print(f"  - ë¬¼ë¦¬ íŠ¹ì§• shape: {X_physics.shape}")
    
    # íƒ€ê²Ÿ ë¶„ì„
    print(f"\nğŸ¯ íƒ€ê²Ÿ({processor.target_col}) ë¶„ì„:")
    print(f"  - ë²”ìœ„: {y.min():.0f} ~ {y.max():.0f}")
    print(f"  - í‰ê· : {y.mean():.1f}")
    print(f"  - ê·¹ë‹¨ê°’(310+): {(y >= 310).sum()}ê°œ ({(y >= 310).sum()/len(y)*100:.2f}%)")
    print(f"  - ì´ˆê·¹ë‹¨ê°’(335+): {(y >= 335).sum()}ê°œ ({(y >= 335).sum()/len(y)*100:.2f}%)")
    
    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    print("\nğŸ”§ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì¤‘...")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
    try:
        processor.scaler_X = joblib.load('./scalers/scaler_X.pkl')
        processor.scaler_y = joblib.load('./scalers/scaler_y.pkl')
        processor.scaler_physics = joblib.load('./scalers/scaler_physics.pkl')
        print("âœ… ê¸°ì¡´ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ")
    except:
        print("ğŸ’¡ ìƒˆ ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„±")
        # Fit scalers
        X_flat = X.reshape(-1, X.shape[2])
        processor.scaler_X.fit(X_flat)
        processor.scaler_y.fit(y.reshape(-1, 1))
        processor.scaler_physics.fit(X_physics)
    
    # ìŠ¤ì¼€ì¼ë§ ì ìš©
    X_scaled = processor.scaler_X.transform(X.reshape(-1, X.shape[2])).reshape(X.shape)
    y_scaled = processor.scaler_y.transform(y.reshape(-1, 1)).flatten()
    X_physics_scaled = processor.scaler_physics.transform(X_physics)
    
    # ëª¨ë¸ ì„¤ì •
    n_features = X.shape[2]
    config = {
        'seq_len': 20,
        'n_features': n_features,
        'patch_len': 5
    }
    
    # Model 1 ì˜ˆì¸¡
    print("\nğŸ¤– Model 1 (PatchTST) ì˜ˆì¸¡ ì¤‘...")
    model1 = PatchTSTModel(config)
    model1.compile(optimizer='adam', loss='mse')
    
    # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ëª¨ë¸ ë¹Œë“œ
    dummy_input = np.zeros((1, 20, n_features))
    _ = model1(dummy_input)
    
    try:
        model1.load_weights('./checkpoints/model1_v3.h5')
        print("âœ… Model1 ê°€ì¤‘ì¹˜ ë¡œë“œ")
    except:
        print("âš ï¸ Model1 ê°€ì¤‘ì¹˜ ì—†ìŒ - ëœë¤ ì´ˆê¸°í™” ìƒíƒœë¡œ ì˜ˆì¸¡")
    
    y_pred1_scaled = model1.predict(X_scaled, batch_size=32, verbose=0)
    y_pred1 = processor.scaler_y.inverse_transform(y_pred1_scaled.reshape(-1, 1)).flatten()
    print("âœ… Model 1 ì˜ˆì¸¡ ì™„ë£Œ")
    
    # Model 2 ì˜ˆì¸¡
    print("\nğŸ¤– Model 2 (PatchTST + PINN) ì˜ˆì¸¡ ì¤‘...")
    model2 = PatchTSTPINN(config)
    model2.compile(optimizer='adam', loss='mse')
    
    # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ëª¨ë¸ ë¹Œë“œ
    dummy_seq = np.zeros((1, 20, n_features))
    dummy_physics = np.zeros((1, 3))
    _ = model2([dummy_seq, dummy_physics])
    
    try:
        model2.load_weights('./checkpoints/model2_v3.h5')
        print("âœ… Model2 ê°€ì¤‘ì¹˜ ë¡œë“œ")
    except:
        print("âš ï¸ Model2 ê°€ì¤‘ì¹˜ ì—†ìŒ - ëœë¤ ì´ˆê¸°í™” ìƒíƒœë¡œ ì˜ˆì¸¡")
    
    y_pred2_scaled = model2.predict([X_scaled, X_physics_scaled], batch_size=32, verbose=0)
    y_pred2 = processor.scaler_y.inverse_transform(y_pred2_scaled.reshape(-1, 1)).flatten()
    print("âœ… Model 2 ì˜ˆì¸¡ ì™„ë£Œ")
    
    # ì•™ìƒë¸”
    y_ensemble = 0.4 * y_pred1 + 0.6 * y_pred2
    print("âœ… Ensemble ì˜ˆì¸¡ ì™„ë£Œ")
    
    # ê²°ê³¼ DataFrame ìƒì„±
    print("\nğŸ“Š ê²°ê³¼ ì •ë¦¬ ì¤‘...")
    
    results = []
    for i in range(len(y)):
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬
        if isinstance(timestamps[i], pd.Timestamp):
            target_time = timestamps[i]
            timestamp_str = target_time.strftime('%Y%m%d%H%M')
            readable_time = target_time.strftime('%Y-%m-%d %H:%M')
        else:
            timestamp_str = str(timestamps[i])
            try:
                target_time = datetime.strptime(timestamp_str, '%Y%m%d%H%M')
                readable_time = target_time.strftime('%Y-%m-%d %H:%M')
            except:
                readable_time = timestamp_str
                target_time = datetime.now()
        
        # ì…ë ¥ ì‹œê°„ ê³„ì‚°
        input_start = target_time - timedelta(minutes=30)
        input_end = target_time - timedelta(minutes=10)
        
        result = {
            'index': i,
            'timestamp': timestamp_str,
            'readable_time': readable_time,
            'actual': float(y[i]),
            'predicted_model1': float(y_pred1[i]),
            'predicted_model2': float(y_pred2[i]),
            'predicted_ensemble': float(y_ensemble[i]),
            'Target_time': readable_time,
            'input_start': input_start.strftime('%Y-%m-%d %H:%M'),
            'input_end': input_end.strftime('%Y-%m-%d %H:%M'),
            'error_model1': abs(float(y[i]) - float(y_pred1[i])),
            'error_model2': abs(float(y[i]) - float(y_pred2[i])),
            'error_ensemble': abs(float(y[i]) - float(y_ensemble[i])),
            'is_extreme': float(y[i]) >= 310,
            'is_very_extreme': float(y[i]) >= 335
        }
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        min_error = min(result['error_model1'], result['error_model2'], result['error_ensemble'])
        if result['error_model1'] == min_error:
            result['best_model'] = 'Model1'
        elif result['error_model2'] == min_error:
            result['best_model'] = 'Model2'
        else:
            result['best_model'] = 'Ensemble'
        
        results.append(result)
    
    results_df = pd.DataFrame(results)
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs('./evaluation_results', exist_ok=True)
    results_df.to_csv('./evaluation_results/csv_predictions.csv', index=False)
    print(f"âœ… ê²°ê³¼ ì €ì¥: ./evaluation_results/csv_predictions.csv")
    
    # ê·¹ë‹¨ê°’ë§Œ ì €ì¥
    extreme_df = results_df[results_df['is_extreme']].copy()
    if len(extreme_df) > 0:
        extreme_df.to_csv('./evaluation_results/csv_extreme_predictions.csv', index=False)
        print(f"âœ… ê·¹ë‹¨ê°’ ì €ì¥: ./evaluation_results/csv_extreme_predictions.csv")
    
    # ì„±ëŠ¥ ìš”ì•½
    print("\n" + "="*80)
    print("ğŸ“ˆ ì „ì²´ ì„±ëŠ¥ ìš”ì•½")
    print("="*80)
    
    print(f"\nğŸ“Š í‰ê°€ ë°ì´í„°: {len(results_df)}ê°œ ìƒ˜í”Œ")
    
    mae1 = results_df['error_model1'].mean()
    mae2 = results_df['error_model2'].mean()
    mae_ensemble = results_df['error_ensemble'].mean()
    
    print(f"\nMAE (Mean Absolute Error):")
    print(f"  Model 1: {mae1:.2f}")
    print(f"  Model 2: {mae2:.2f}")
    print(f"  Ensemble: {mae_ensemble:.2f}")
    
    # ê·¹ë‹¨ê°’ ì„±ëŠ¥
    if len(extreme_df) > 0:
        print(f"\nğŸ”´ ê·¹ë‹¨ê°’ ì„±ëŠ¥ ({len(extreme_df)}ê°œ):")
        print(f"  Model 1 MAE: {extreme_df['error_model1'].mean():.2f}")
        print(f"  Model 2 MAE: {extreme_df['error_model2'].mean():.2f}")
        print(f"  Ensemble MAE: {extreme_df['error_ensemble'].mean():.2f}")
        
        # ê·¹ë‹¨ê°’ ê°ì§€ìœ¨
        detected_m1 = (extreme_df['predicted_model1'] >= 305).sum()
        detected_m2 = (extreme_df['predicted_model2'] >= 305).sum()
        print(f"\nê·¹ë‹¨ê°’ ê°ì§€ìœ¨ (ì˜ˆì¸¡ >= 305):")
        print(f"  Model 1: {detected_m1}/{len(extreme_df)} ({detected_m1/len(extreme_df)*100:.1f}%)")
        print(f"  Model 2: {detected_m2}/{len(extreme_df)} ({detected_m2/len(extreme_df)*100:.1f}%)")
    
    # ìƒ˜í”Œ ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ“‹ ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ")
    print("="*80)
    
    # ì²˜ìŒ 10ê°œ
    print("\n[ì²˜ìŒ 10ê°œ]")
    for idx, row in results_df.head(10).iterrows():
        mark = "ğŸ”´" if row['is_extreme'] else "âšª"
        print(f"{mark} {row['readable_time']}: ì‹¤ì œ={row['actual']:.0f}, M1={row['predicted_model1']:.0f}, M2={row['predicted_model2']:.0f}, Ens={row['predicted_ensemble']:.0f}")
    
    # ê·¹ë‹¨ê°’ ìƒ˜í”Œ
    if len(extreme_df) > 0:
        print("\n[ê·¹ë‹¨ê°’ ìƒ˜í”Œ]")
        for idx, row in extreme_df.head(5).iterrows():
            print(f"ğŸ”´ {row['readable_time']}: ì‹¤ì œ={row['actual']:.0f}, M1={row['predicted_model1']:.0f}, M2={row['predicted_model2']:.0f}, Best={row['best_model']}")
    
    print("\nâœ… í‰ê°€ ì™„ë£Œ!")
    return results_df

# ========================================
# ë©”ì¸ ì‹¤í–‰
# ========================================

if __name__ == "__main__":
    try:
        # CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
        csv_path = input("CSV íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸: data/202509ì›”.csv): ").strip()
        if not csv_path:
            csv_path = 'data/202509ì›”.csv'
        
        # í‰ê°€ ì‹¤í–‰
        results = evaluate_csv_data(csv_path)
        
        print("\n" + "="*80)
        print("ğŸ‰ ëª¨ë“  í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: ./evaluation_results/csv_predictions.csv")
        print(f"ğŸ“Š ì´ {len(results)}ê°œ ë°ì´í„° í‰ê°€ ì™„ë£Œ")
        print("="*80)
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()