# -*- coding: utf-8 -*-
"""
HUBROOM ëª¨ë¸ í‰ê°€
- ì´ë¯¸ ì €ì¥ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš© (ì‹œí€€ìŠ¤ ìƒì„± X)
- CSVëŠ” ì‹¤ì œê°’ ë¹„êµìš©ìœ¼ë¡œë§Œ ì‚¬ìš©
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import joblib
from sklearn.metrics import mean_absolute_error, mean_squared_error
import os
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("ğŸ¯ HUBROOM ëª¨ë¸ í‰ê°€")
print("ğŸ“… ì €ì¥ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©")
print("="*80)

# ========================================
# Model 1: PatchTST
# ========================================

class PatchTSTModel(keras.Model):
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        self.patch_embedding = layers.Dense(128, activation='relu')
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm1 = layers.LayerNormalization()
        self.norm2 = layers.LayerNormalization()
        
        self.ffn = keras.Sequential([
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(128)
        ])
        
        self.flatten = layers.Flatten()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dropout = layers.Dropout(0.3)
        self.dense2 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(1)
        
    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        x = tf.reshape(x, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        attn = self.attention(x, x, training=training)
        x = self.norm1(x + attn)
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        x = self.flatten(x)
        x = self.dense1(x)
        x = self.dropout(x, training=training)
        x = self.dense2(x)
        output = self.output_layer(x)
        return tf.squeeze(output, axis=-1)

# ========================================
# Model 2: PatchTST + PINN
# ========================================

class PatchTSTPINN(keras.Model):
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        self.patch_embedding = layers.Dense(128, activation='relu')
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm = layers.LayerNormalization()
        self.flatten = layers.Flatten()
        self.temporal_dense = layers.Dense(64, activation='relu')
        
        self.physics_net = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dense(16, activation='relu')
        ])
        
        self.fusion = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(1)
        ])
        
        self.extreme_boost = layers.Dense(1, activation='sigmoid')
        
    def call(self, inputs, training=False):
        x_seq, x_physics = inputs
        batch_size = tf.shape(x_seq)[0]
        
        x = tf.reshape(x_seq, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        attn = self.attention(x, x, training=training)
        x = self.norm(x + attn)
        x = self.flatten(x)
        temporal_features = self.temporal_dense(x)
        
        physics_features = self.physics_net(x_physics)
        combined = tf.concat([temporal_features, physics_features], axis=-1)
        output = self.fusion(combined)
        
        boost_factor = self.extreme_boost(combined)
        output = output * (1 + boost_factor * 0.2)
        
        return tf.squeeze(output, axis=-1)

# ========================================
# ë©”ì¸ í‰ê°€ í•¨ìˆ˜
# ========================================

def evaluate_model(csv_path=None):
    """ì €ì¥ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€ + CSVì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ë§Œ ì°¸ì¡°"""
    
    print("\nğŸ“‚ ì €ì¥ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    # ì´ë¯¸ ì¤€ë¹„ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    X_test_scaled = np.load('./test_data/X_test_scaled.npy')
    y_test_scaled = np.load('./test_data/y_test_scaled.npy')
    y_test = np.load('./test_data/y_test.npy')
    X_physics_test_scaled = np.load('./test_data/X_physics_test_scaled.npy')
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"  ìƒ˜í”Œ ìˆ˜: {len(y_test)}ê°œ")
    print(f"  ì…ë ¥ shape: {X_test_scaled.shape}")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    scaler_y = joblib.load('./scalers/scaler_y.pkl')
    
    # CSVì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ê°€ì ¸ì˜¤ê¸° (ìˆìœ¼ë©´)
    timestamps = None
    if csv_path:
        try:
            print(f"\nğŸ“‚ CSV íŒŒì¼ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ë¡œë“œ: {csv_path}")
            df = pd.read_csv(csv_path)
            df['timestamp'] = pd.to_datetime(df.iloc[:, 0], format='%Y%m%d%H%M', errors='coerce')
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°ë§Œí¼ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
            timestamps = df['timestamp'].values[-len(y_test):]
            print(f"âœ… íƒ€ì„ìŠ¤íƒ¬í”„ ë¡œë“œ ì™„ë£Œ")
        except:
            print("âš ï¸ íƒ€ì„ìŠ¤íƒ¬í”„ ë¡œë“œ ì‹¤íŒ¨ - ê¸°ë³¸ê°’ ì‚¬ìš©")
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if timestamps is None:
        base_date = datetime(2025, 5, 9, 6, 0)
        timestamps = [base_date + timedelta(minutes=i) for i in range(len(y_test))]
    
    # ë°ì´í„° ë¶„ì„
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„:")
    print(f"  íƒ€ê²Ÿ ë²”ìœ„: {y_test.min():.0f} ~ {y_test.max():.0f}")
    print(f"  í‰ê· : {y_test.mean():.1f}")
    print(f"  ê·¹ë‹¨ê°’(310+): {(y_test >= 310).sum()}ê°œ ({(y_test >= 310).sum()/len(y_test)*100:.2f}%)")
    print(f"  ì´ˆê·¹ë‹¨ê°’(335+): {(y_test >= 335).sum()}ê°œ ({(y_test >= 335).sum()/len(y_test)*100:.2f}%)")
    
    # ëª¨ë¸ ì„¤ì •
    n_features = X_test_scaled.shape[2]
    config = {
        'seq_len': 20,
        'n_features': n_features,
        'patch_len': 5
    }
    
    # Model 1 ì˜ˆì¸¡
    print("\nğŸ¤– Model 1 (PatchTST) ì˜ˆì¸¡ ì¤‘...")
    model1 = PatchTSTModel(config)
    model1.compile(optimizer='adam', loss='mse')
    dummy_input = np.zeros((1, 20, n_features))
    _ = model1(dummy_input)
    model1.load_weights('./checkpoints/model1_v3.h5')
    
    y_pred1_scaled = model1.predict(X_test_scaled, batch_size=64, verbose=0)
    y_pred1 = scaler_y.inverse_transform(y_pred1_scaled.reshape(-1, 1)).flatten()
    print("âœ… Model 1 ì˜ˆì¸¡ ì™„ë£Œ")
    
    # Model 2 ì˜ˆì¸¡
    print("\nğŸ¤– Model 2 (PatchTST + PINN) ì˜ˆì¸¡ ì¤‘...")
    model2 = PatchTSTPINN(config)
    model2.compile(optimizer='adam', loss='mse')
    dummy_seq = np.zeros((1, 20, n_features))
    dummy_physics = np.zeros((1, 3))
    _ = model2([dummy_seq, dummy_physics])
    model2.load_weights('./checkpoints/model2_v3.h5')
    
    y_pred2_scaled = model2.predict([X_test_scaled, X_physics_test_scaled], batch_size=64, verbose=0)
    y_pred2 = scaler_y.inverse_transform(y_pred2_scaled.reshape(-1, 1)).flatten()
    print("âœ… Model 2 ì˜ˆì¸¡ ì™„ë£Œ")
    
    # ì•™ìƒë¸”
    y_ensemble = 0.4 * y_pred1 + 0.6 * y_pred2
    print("âœ… Ensemble ì˜ˆì¸¡ ì™„ë£Œ")
    
    # ê²°ê³¼ ìƒì„±
    print("\nğŸ“Š ê²°ê³¼ ì •ë¦¬ ì¤‘...")
    
    results = []
    for i in range(len(y_test)):
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬
        if pd.notna(timestamps[i]) and hasattr(timestamps[i], 'strftime'):
            target_time = timestamps[i]
            timestamp_str = target_time.strftime('%Y%m%d%H%M')
            readable_time = target_time.strftime('%Y-%m-%d %H:%M')
            input_start = (target_time - timedelta(minutes=30)).strftime('%Y-%m-%d %H:%M')
            input_end = (target_time - timedelta(minutes=10)).strftime('%Y-%m-%d %H:%M')
        else:
            target_time = timestamps[i] if isinstance(timestamps[i], datetime) else datetime.now() + timedelta(minutes=i)
            timestamp_str = target_time.strftime('%Y%m%d%H%M')
            readable_time = target_time.strftime('%Y-%m-%d %H:%M')
            input_start = (target_time - timedelta(minutes=30)).strftime('%Y-%m-%d %H:%M')
            input_end = (target_time - timedelta(minutes=10)).strftime('%Y-%m-%d %H:%M')
        
        result = {
            'index': i,
            'timestamp': timestamp_str,
            'readable_time': readable_time,
            'actual': float(y_test[i]),
            'predicted_model1': float(y_pred1[i]),
            'predicted_model2': float(y_pred2[i]),
            'predicted_ensemble': float(y_ensemble[i]),
            'Target_time': readable_time,
            'input_start': input_start,
            'input_end': input_end,
            'error_model1': abs(float(y_test[i]) - float(y_pred1[i])),
            'error_model2': abs(float(y_test[i]) - float(y_pred2[i])),
            'error_ensemble': abs(float(y_test[i]) - float(y_ensemble[i]))
        }
        
        # ìµœì  ëª¨ë¸
        min_error = min(result['error_model1'], result['error_model2'], result['error_ensemble'])
        if result['error_model1'] == min_error:
            result['best_model'] = 'Model1'
        elif result['error_model2'] == min_error:
            result['best_model'] = 'Model2'
        else:
            result['best_model'] = 'Ensemble'
        
        results.append(result)
    
    results_df = pd.DataFrame(results)
    
    # ì €ì¥
    os.makedirs('./evaluation_results', exist_ok=True)
    results_df.to_csv('./evaluation_results/predictions_all.csv', index=False)
    print(f"âœ… ì „ì²´ ê²°ê³¼ ì €ì¥: ./evaluation_results/predictions_all.csv ({len(results_df)}ê°œ)")
    
    # ê·¹ë‹¨ê°’ë§Œ
    extreme_df = results_df[results_df['actual'] >= 310]
    if len(extreme_df) > 0:
        extreme_df.to_csv('./evaluation_results/predictions_extreme.csv', index=False)
        print(f"âœ… ê·¹ë‹¨ê°’ ì €ì¥: ./evaluation_results/predictions_extreme.csv ({len(extreme_df)}ê°œ)")
    
    # ========================================
    # ì„±ëŠ¥ í‰ê°€
    # ========================================
    
    print("\n" + "="*80)
    print("ğŸ“ˆ ì „ì²´ ì„±ëŠ¥ í‰ê°€")
    print("="*80)
    
    print(f"\nğŸ“Š ì „ì²´: {len(y_test)}ê°œ ìƒ˜í”Œ")
    
    mae1 = mean_absolute_error(y_test, y_pred1)
    mae2 = mean_absolute_error(y_test, y_pred2)
    mae_ens = mean_absolute_error(y_test, y_ensemble)
    
    print(f"\nMAE (Mean Absolute Error):")
    print(f"  Model 1: {mae1:.2f}")
    print(f"  Model 2: {mae2:.2f}")
    print(f"  Ensemble: {mae_ens:.2f}")
    
    # ê·¹ë‹¨ê°’ ì„±ëŠ¥
    extreme_mask = y_test >= 310
    if extreme_mask.sum() > 0:
        print(f"\nğŸ”´ ê·¹ë‹¨ê°’ ì„±ëŠ¥ ({extreme_mask.sum()}ê°œ):")
        print(f"  Model 1: {mean_absolute_error(y_test[extreme_mask], y_pred1[extreme_mask]):.2f}")
        print(f"  Model 2: {mean_absolute_error(y_test[extreme_mask], y_pred2[extreme_mask]):.2f}")
        print(f"  Ensemble: {mean_absolute_error(y_test[extreme_mask], y_ensemble[extreme_mask]):.2f}")
        
        # ê°ì§€ìœ¨
        detected_m1 = (y_pred1[extreme_mask] >= 305).sum()
        detected_m2 = (y_pred2[extreme_mask] >= 305).sum()
        print(f"\nê·¹ë‹¨ê°’ ê°ì§€ìœ¨ (ì˜ˆì¸¡ >= 305):")
        print(f"  Model 1: {detected_m1}/{extreme_mask.sum()} ({detected_m1/extreme_mask.sum()*100:.1f}%)")
        print(f"  Model 2: {detected_m2}/{extreme_mask.sum()} ({detected_m2/extreme_mask.sum()*100:.1f}%)")
    
    # ========================================
    # ê²°ê³¼ ì¶œë ¥
    # ========================================
    
    print("\n" + "="*80)
    print("ğŸ“‹ ì˜ˆì¸¡ ê²°ê³¼ (1ê°œ ìƒ˜í”Œ)")
    print("="*80)
    
    # ëŒ€í‘œ ìƒ˜í”Œ 1ê°œ
    if len(extreme_df) > 0:
        sample = extreme_df.iloc[0]
    else:
        sample = results_df.iloc[0]
    
    print(f"""
Timestamp: {sample['timestamp']}
Readable_time: {sample['readable_time']}
Input_start: {sample['input_start']}
Input_end: {sample['input_end']}
Target_time: {sample['Target_time']}

CURRENT_M16A_3F_JOB_2:
- Actual: {sample['actual']:.0f}
- Predicted_Model1: {sample['predicted_model1']:.0f}
- Predicted_Model2: {sample['predicted_model2']:.0f}
- Predicted_Ensemble: {sample['predicted_ensemble']:.0f}

Error:
- Model1: {sample['error_model1']:.1f}
- Model2: {sample['error_model2']:.1f}
- Ensemble: {sample['error_ensemble']:.1f}

Best: {sample['best_model']}
""")
    
    # ì¶”ê°€ ìƒ˜í”Œ
    print("\n[ì²˜ìŒ 10ê°œ]")
    for idx, row in results_df.head(10).iterrows():
        mark = "ğŸ”´" if row['actual'] >= 310 else "âšª"
        print(f"{mark} {row['readable_time']}: ì‹¤ì œ={row['actual']:.0f}, M1={row['predicted_model1']:.0f}, M2={row['predicted_model2']:.0f}")
    
    print(f"\nâœ… í‰ê°€ ì™„ë£Œ! ì´ {len(results_df)}ê°œ í…ŒìŠ¤íŠ¸ ë°ì´í„°")
    return results_df

# ========================================
# ë©”ì¸ ì‹¤í–‰
# ========================================

if __name__ == "__main__":
    try:
        # CSV íŒŒì¼ ì˜µì…˜
        use_csv = input("CSV íŒŒì¼ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ê°€ì ¸ì˜¬ê¹Œìš”? (y/n): ").strip().lower()
        
        csv_path = None
        if use_csv == 'y':
            csv_path = input("CSV íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: data/202509ì›”.csv): ").strip()
            if not csv_path:
                csv_path = 'data/202509ì›”.csv'
        
        # í‰ê°€ ì‹¤í–‰
        results = evaluate_model(csv_path)
        
        print("\nğŸ‰ í‰ê°€ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()