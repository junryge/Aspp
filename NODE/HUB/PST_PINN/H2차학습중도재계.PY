# -*- coding: utf-8 -*-
"""
HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - 310+ íŠ¹í™” ê°œì„  ë²„ì „
- 310+ ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ê°•í™”
- ë‹¤ë‹¨ê³„ ë³´ì • ì‹œìŠ¤í…œ
- êµ¬ê°„ë³„ ì°¨ë³„í™”ëœ í•™ìŠµ
- ì¤‘ë‹¨/ì¬ì‹œì‘ ì™„ë²½ ì§€ì›
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
import warnings
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import pickle
import os
import json
from tqdm import tqdm
import joblib
from collections import Counter
import signal
import sys

warnings.filterwarnings('ignore')

# TensorFlow ì„¤ì •
print(f"TensorFlow Version: {tf.__version__}")
tf.random.set_seed(42)
np.random.seed(42)

# GPU ë©”ëª¨ë¦¬ ì„¤ì •
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# ===========================
# ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì (ê°œì„ )
# ===========================

class CheckpointManager:
    """í•™ìŠµ ì¤‘ë‹¨/ì¬ê°œë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬"""
    
    def __init__(self, checkpoint_dir='./checkpoints'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        self.interrupted = False
        
        # Ctrl+C ì¸í„°ëŸ½íŠ¸ í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
    
    def _signal_handler(self, sig, frame):
        print('\n\nâš ï¸ í•™ìŠµ ì¤‘ë‹¨ ê°ì§€! ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì¤‘...')
        self.interrupted = True
    
    def save_checkpoint(self, model, model_name, epoch, history=None):
        """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        try:
            # ëª¨ë¸ ê°€ì¤‘ì¹˜
            model_path = os.path.join(self.checkpoint_dir, f'{model_name}_checkpoint.h5')
            model.save_weights(model_path)
            
            # ë©”íƒ€ë°ì´í„°
            meta_path = os.path.join(self.checkpoint_dir, f'{model_name}_meta.pkl')
            meta_data = {
                'epoch': epoch,
                'history': history.history if history else None
            }
            with open(meta_path, 'wb') as f:
                pickle.dump(meta_data, f)
            
            print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {model_name} (Epoch {epoch})")
            return True
        except Exception as e:
            print(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def load_checkpoint(self, model, model_name):
        """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        try:
            model_path = os.path.join(self.checkpoint_dir, f'{model_name}_checkpoint.h5')
            meta_path = os.path.join(self.checkpoint_dir, f'{model_name}_meta.pkl')
            
            if os.path.exists(model_path) and os.path.exists(meta_path):
                # ê°€ì¤‘ì¹˜ ë¡œë“œ
                model.load_weights(model_path)
                
                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                with open(meta_path, 'rb') as f:
                    meta_data = pickle.load(f)
                
                print(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {model_name} (Epoch {meta_data['epoch']})")
                return meta_data
            return None
        except Exception as e:
            print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def check_resume(self, model_name):
        """ì¬ê°œ ì—¬ë¶€ í™•ì¸ (ìë™ ì¬ê°œ)"""
        checkpoint_path = os.path.join(self.checkpoint_dir, f'{model_name}_checkpoint.h5')
        if os.path.exists(checkpoint_path):
            print(f"ğŸ’¾ {model_name}ì˜ ì´ì „ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬! ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.")
            return True
        return False
    
    def save_training_state(self, state_dict, filename='training_state.pkl'):
        """ì „ì²´ í•™ìŠµ ìƒíƒœ ì €ì¥"""
        filepath = os.path.join(self.checkpoint_dir, filename)
        with open(filepath, 'wb') as f:
            pickle.dump(state_dict, f)
        print(f"ğŸ’¾ í•™ìŠµ ìƒíƒœ ì €ì¥: {filepath}")
    
    def load_training_state(self, filename='training_state.pkl'):
        """ì „ì²´ í•™ìŠµ ìƒíƒœ ë¡œë“œ"""
        filepath = os.path.join(self.checkpoint_dir, filename)
        if os.path.exists(filepath):
            with open(filepath, 'rb') as f:
                return pickle.load(f)
        return None

# ===========================
# ğŸ¯ ë‹¤ë‹¨ê³„ ê·¹ë‹¨ê°’ ì†ì‹¤í•¨ìˆ˜ (ê°œì„ )
# ===========================

class MultiLevelExtremeLoss(tf.keras.losses.Loss):
    """310+ ê·¹ë‹¨ê°’ì„ ìœ„í•œ ë‹¤ë‹¨ê³„ ì†ì‹¤í•¨ìˆ˜"""
    
    def __init__(self, name='multi_level_extreme_loss'):
        super().__init__(name=name)
        self.reduction = tf.keras.losses.Reduction.NONE  # ê°œë³„ ì†ì‹¤ê°’ ë°˜í™˜
    
    def call(self, y_true, y_pred):
        # ì°¨ì› ë§ì¶”ê¸°
        y_true = tf.reshape(y_true, [-1])
        y_pred = tf.reshape(y_pred, [-1])
        
        # ê¸°ë³¸ MSE
        mse = tf.square(y_true - y_pred)
        
        # êµ¬ê°„ë³„ ê°€ì¤‘ì¹˜ (310+ íŠ¹ë³„ ê°•í™”)
        weight_280 = tf.where(y_true >= 0.6, 2.0, 1.0)  # MinMaxScaler ì ìš©ëœ ê°’
        weight_300 = tf.where(y_true >= 0.65, 5.0, 1.0)  
        weight_310 = tf.where(y_true >= 0.7, 10.0, 1.0)  # 310+ ê°•ì¡°
        weight_350 = tf.where(y_true >= 0.8, 20.0, 1.0)  # 350+ ë” ê°•ì¡°
        
        # ëˆ„ì  ê°€ì¤‘ì¹˜
        total_weight = weight_280 * weight_300 * weight_310 * weight_350
        
        # ê°€ì¤‘ MSE (ë°°ì¹˜ì˜ ê° ìƒ˜í”Œë³„ ì†ì‹¤ê°’ ë°˜í™˜)
        weighted_mse = mse * total_weight
        
        return weighted_mse  # í‰ê· í•˜ì§€ ì•Šê³  ë°˜í™˜

# ===========================
# ğŸ“Š ê·¹ë‹¨ê°’ íŠ¹í™” ë°ì´í„° ì²˜ë¦¬ (ê°œì„ )
# ===========================

class ImprovedExtremeProcessor:
    """310+ ê·¹ë‹¨ê°’ ì²˜ë¦¬ ê°•í™”"""
    
    def __init__(self, file_path='data/HUB_0509_TO_0730_DATA.CSV'):
        self.file_path = file_path
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        
        # ì„¸ë¶„í™”ëœ ì„ê³„ê°’
        self.thresholds = {
            'normal': 250,
            'warning': 280,
            'critical': 300,
            'severe': 310,     # ì¶”ê°€
            'extreme': 350,    # ìˆ˜ì •
            'disaster': 400
        }
        
        # ìŠ¤ì¼€ì¼ëŸ¬
        self.scaler_X = RobustScaler()
        self.scaler_y = MinMaxScaler(feature_range=(0, 1))
        self.scaler_physics = StandardScaler()
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ê²½ë¡œ
        self.scaler_dir = './scalers'
        os.makedirs(self.scaler_dir, exist_ok=True)
        
        # ë¬¼ë¦¬ì  ì»¬ëŸ¼
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2', 'M14B_7F_TO_HUB_JOB2'
        ]
        
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB', 'M16A_3F_TO_M14B_7F_JOB'
        ]
    
    def save_scalers(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥"""
        joblib.dump(self.scaler_X, os.path.join(self.scaler_dir, 'scaler_X.pkl'))
        joblib.dump(self.scaler_y, os.path.join(self.scaler_dir, 'scaler_y.pkl'))
        joblib.dump(self.scaler_physics, os.path.join(self.scaler_dir, 'scaler_physics.pkl'))
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ: {self.scaler_dir}")
    
    def load_scalers(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        try:
            self.scaler_X = joblib.load(os.path.join(self.scaler_dir, 'scaler_X.pkl'))
            self.scaler_y = joblib.load(os.path.join(self.scaler_dir, 'scaler_y.pkl'))
            self.scaler_physics = joblib.load(os.path.join(self.scaler_dir, 'scaler_physics.pkl'))
            print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ: {self.scaler_dir}")
            return True
        except:
            print("âš ï¸ ì €ì¥ëœ ìŠ¤ì¼€ì¼ëŸ¬ ì—†ìŒ. ìƒˆë¡œ í•™ìŠµ í•„ìš”.")
            return False
    
    def analyze_extremes(self, df):
        """ê·¹ë‹¨ê°’ ìƒì„¸ ë¶„ì„ (310+ ì¶”ê°€)"""
        if self.target_col not in df.columns:
            return
        
        target = df[self.target_col]
        
        print("\n" + "="*60)
        print("ğŸ” ê·¹ë‹¨ê°’ ë¶„ì„ (HUBROOM ë°˜ì†¡ëŸ‰)")
        print("="*60)
        
        print(f"\nğŸ“Š ê¸°ë³¸ í†µê³„:")
        print(f"  - ì „ì²´ ë²”ìœ„: {target.min():.0f} ~ {target.max():.0f}")
        print(f"  - í‰ê· : {target.mean():.2f}")
        print(f"  - ì¤‘ì•™ê°’: {target.median():.0f}")
        print(f"  - í‘œì¤€í¸ì°¨: {target.std():.2f}")
        
        print(f"\nğŸš¨ ì„ê³„ê°’ë³„ ë¶„í¬:")
        for name, threshold in self.thresholds.items():
            count = (target >= threshold).sum()
            ratio = count / len(target) * 100
            if ratio > 0:
                print(f"  - {name:8} (â‰¥{threshold:3}): {count:5}ê°œ ({ratio:5.2f}%)")
        
        # 310+ íŠ¹ë³„ ë¶„ì„
        print(f"\nğŸ’¥ 310+ ê·¹ë‹¨ê°’ ìƒì„¸:")
        bins_310 = [310, 320, 330, 340, 350, 400, 500, 1000]
        extreme_310 = target[target >= 310]
        if len(extreme_310) > 0:
            for i in range(len(bins_310)-1):
                mask = (extreme_310 >= bins_310[i]) & (extreme_310 < bins_310[i+1])
                count = mask.sum()
                if count > 0:
                    print(f"    [{bins_310[i]:3}-{bins_310[i+1]:3}): {count:4}ê°œ")
    
    def create_enhanced_sequences(self, df, numeric_cols, seq_len=20, pred_len=10):
        """êµ¬ê°„ë³„ ì°¨ë³„í™”ëœ ì˜¤ë²„ìƒ˜í”Œë§"""
        
        X, y, X_physics, sample_weights = [], [], [], []
        
        available_inflow = [col for col in self.inflow_cols if col in df.columns]
        available_outflow = [col for col in self.outflow_cols if col in df.columns]
        
        total_sequences = len(df) - seq_len - pred_len + 1
        
        # êµ¬ê°„ë³„ ì¸ë±ìŠ¤ ë¶„ë¥˜
        indices_by_range = {
            'normal': [],      # < 280
            'warning': [],     # 280-300
            'critical': [],    # 300-310
            'severe': [],      # 310-350
            'extreme': []      # 350+
        }
        
        print("\nğŸ“¦ êµ¬ê°„ë³„ ì‹œí€€ìŠ¤ ë¶„ë¥˜ ì¤‘...")
        
        for i in tqdm(range(total_sequences), desc="ì‹œí€€ìŠ¤ ë¶„ë¥˜"):
            target_value = df[self.target_col].iloc[i + seq_len + pred_len - 1]
            
            if target_value >= 350:
                indices_by_range['extreme'].append(i)
            elif target_value >= 310:
                indices_by_range['severe'].append(i)
            elif target_value >= 300:
                indices_by_range['critical'].append(i)
            elif target_value >= 280:
                indices_by_range['warning'].append(i)
            else:
                indices_by_range['normal'].append(i)
        
        # êµ¬ê°„ë³„ í†µê³„ ì¶œë ¥
        for range_name, indices in indices_by_range.items():
            print(f"  - {range_name:8}: {len(indices):5}ê°œ")
        
        # êµ¬ê°„ë³„ ì°¨ë³„í™”ëœ ì˜¤ë²„ìƒ˜í”Œë§
        oversample_rates = {
            'normal': 1,
            'warning': 2,
            'critical': 3,
            'severe': 5,      # 310-350ì€ 5ë°°
            'extreme': 10     # 350+ëŠ” 10ë°°
        }
        
        all_indices = []
        for range_name, indices in indices_by_range.items():
            rate = oversample_rates[range_name]
            all_indices.extend(indices * rate)
            if rate > 1:
                print(f"  - {range_name} ì˜¤ë²„ìƒ˜í”Œë§: {len(indices)} â†’ {len(indices) * rate}")
        
        np.random.shuffle(all_indices)
        print(f"\n  - ì „ì²´ í•™ìŠµ ì‹œí€€ìŠ¤: {len(all_indices)}ê°œ")
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        for i in tqdm(all_indices, desc="ì‹œí€€ìŠ¤ ìƒì„±"):
            # ì…ë ¥ ì‹œí€€ìŠ¤
            X_seq = df[numeric_cols].iloc[i:i+seq_len].values
            X.append(X_seq)
            
            # íƒ€ê²Ÿ (10ë¶„ í›„)
            y_val = df[self.target_col].iloc[i + seq_len + pred_len - 1]
            y.append(y_val)
            
            # ì„¸ë¶„í™”ëœ ê°€ì¤‘ì¹˜
            if y_val >= 350:
                weight = 20.0
            elif y_val >= 310:
                weight = 15.0
            elif y_val >= 300:
                weight = 10.0
            elif y_val >= 280:
                weight = 5.0
            else:
                weight = 1.0
            sample_weights.append(weight)
            
            # ë¬¼ë¦¬ ë°ì´í„°
            physics_data = np.array([
                df[self.target_col].iloc[i + seq_len - 1],  # í˜„ì¬ HUBROOM
                df[available_inflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_inflow else 0,
                df[available_outflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_outflow else 0
            ])
            X_physics.append(physics_data)
        
        return (np.array(X), np.array(y), np.array(X_physics), np.array(sample_weights))

# ===========================
# ğŸ§  310+ íŠ¹í™” PatchTST ëª¨ë¸
# ===========================

class EnhancedExtremePatchTST(keras.Model):
    """310+ ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ê°•í™” ëª¨ë¸"""
    
    def __init__(self, config):
        super(EnhancedExtremePatchTST, self).__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        self.d_model = config['d_model']
        self.n_heads = config['n_heads']
        self.d_ff = config['d_ff']
        self.n_layers = config['n_layers']
        self.dropout_rate = config['dropout']
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(self.d_model, activation='relu')
        
        # ìœ„ì¹˜ ì¸ì½”ë”©
        self.pos_embedding = self.add_weight(
            name='pos_embedding',
            shape=(1, self.n_patches, self.d_model),
            initializer='random_normal',
            trainable=True
        )
        
        # Transformer ì¸ì½”ë”
        self.encoder_layers = []
        for _ in range(self.n_layers):
            self.encoder_layers.append(
                layers.MultiHeadAttention(
                    num_heads=self.n_heads,
                    key_dim=self.d_model // self.n_heads,
                    dropout=self.dropout_rate
                )
            )
            self.encoder_layers.append(layers.LayerNormalization())
            self.encoder_layers.append(layers.Dropout(self.dropout_rate))
        
        # ë‹¤ë‹¨ê³„ ê·¹ë‹¨ê°’ ê°ì§€
        self.extreme_detector_300 = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dense(1, activation='sigmoid')  # 300+ í™•ë¥ 
        ])
        
        self.extreme_detector_310 = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dense(1, activation='sigmoid')  # 310+ í™•ë¥ 
        ])
        
        self.extreme_detector_350 = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dense(1, activation='sigmoid')  # 350+ í™•ë¥ 
        ])
        
        # ê°’ ì˜ˆì¸¡ ë¸Œëœì¹˜
        self.value_predictor = keras.Sequential([
            layers.Dense(256, activation='relu'),  # ë” í° ë„¤íŠ¸ì›Œí¬
            layers.Dropout(self.dropout_rate),
            layers.Dense(128, activation='relu'),
            layers.Dropout(self.dropout_rate),
            layers.Dense(64, activation='relu'),
            layers.Dense(1)
        ])
        
        self.flatten = layers.Flatten()
    
    def create_patches(self, x):
        batch_size = tf.shape(x)[0]
        x_reshaped = tf.reshape(x, [batch_size, self.n_patches, self.patch_len, self.n_features])
        x_patches = tf.reshape(x_reshaped, [batch_size, self.n_patches, self.patch_len * self.n_features])
        return x_patches
    
    def call(self, x, training=False):
        # íŒ¨ì¹˜ ìƒì„±
        x_patches = self.create_patches(x)
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x_embed = self.patch_embedding(x_patches)
        x_embed = x_embed + self.pos_embedding
        
        # Transformer ì¸ì½”ë”
        for i in range(0, len(self.encoder_layers), 3):
            attention = self.encoder_layers[i](x_embed, x_embed, training=training)
            norm = self.encoder_layers[i+1](attention + x_embed)
            x_embed = self.encoder_layers[i+2](norm, training=training)
        
        # Flatten
        x_flat = self.flatten(x_embed)
        
        # ë‹¤ë‹¨ê³„ ê·¹ë‹¨ê°’ ê°ì§€
        prob_300 = self.extreme_detector_300(x_flat)
        prob_310 = self.extreme_detector_310(x_flat)
        prob_350 = self.extreme_detector_350(x_flat)
        
        # ê°’ ì˜ˆì¸¡
        value_pred = self.value_predictor(x_flat)
        
        # ë‹¤ë‹¨ê³„ ë³´ì • (ê°œì„ ëœ ë¡œì§)
        adjustment_300 = prob_300 * 30.0   # 300+ ê¸°ë³¸ ë³´ì •
        adjustment_310 = prob_310 * 40.0   # 310+ ì¶”ê°€ ë³´ì •
        adjustment_350 = prob_350 * 50.0   # 350+ ìµœëŒ€ ë³´ì •
        
        # ëˆ„ì  ë³´ì • (ìµœëŒ€ê°’ ì‚¬ìš©)
        total_adjustment = tf.maximum(
            adjustment_300,
            tf.maximum(adjustment_310, adjustment_350)
        )
        
        # ìµœì¢… ì˜ˆì¸¡
        adjusted_pred = value_pred + total_adjustment
        
        return tf.squeeze(adjusted_pred, axis=-1)

# ===========================
# ğŸ“ˆ êµ¬ê°„ë³„ ëª¨ë‹ˆí„°ë§ ì½œë°±
# ===========================

class DetailedExtremeMonitor(Callback):
    """310+ êµ¬ê°„ë³„ ìƒì„¸ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, X_val, y_val, scaler_y):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val
        self.scaler_y = scaler_y
    
    def on_epoch_end(self, epoch, logs=None):
        # ì˜ˆì¸¡
        y_pred_scaled = self.model.predict(self.X_val, verbose=0)
        
        # ì—­ì •ê·œí™”
        y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
        y_true = self.scaler_y.inverse_transform(self.y_val.reshape(-1, 1)).flatten()
        
        # êµ¬ê°„ë³„ ì •í™•ë„
        ranges = [(300, 310), (310, 350), (350, 1000)]
        
        print(f"\n  ğŸ¯ Epoch {epoch+1} - ê·¹ë‹¨ê°’ êµ¬ê°„ë³„ ì„±ëŠ¥:")
        
        for low, high in ranges:
            mask = (y_true >= low) & (y_true < high)
            if mask.sum() > 0:
                pred_correct = ((y_pred >= low) & (y_pred < high))[mask].sum()
                recall = pred_correct / mask.sum()
                mae = np.mean(np.abs(y_true[mask] - y_pred[mask]))
                print(f"     [{low}-{high}): Recall={recall:.1%}, MAE={mae:.1f}, Count={mask.sum()}")

# ===========================
# ğŸ¯ ë©”ì¸ ì‹¤í–‰
# ===========================

def main():
    print("="*80)
    print("ğŸ­ HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - 310+ ê°•í™” ë²„ì „")
    print("ğŸ¯ ëª©í‘œ: 310 ì´ìƒ ê·¹ë‹¨ê°’ ì •í™•í•œ ì˜ˆì¸¡")
    print("="*80)
    
    # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
    checkpoint_manager = CheckpointManager()
    
    # ì´ì „ í•™ìŠµ ìƒíƒœ í™•ì¸
    training_state = checkpoint_manager.load_training_state()
    if training_state:
        print("ğŸ“‚ ì´ì „ í•™ìŠµ ìƒíƒœ ë°œê²¬! ë³µì› ì¤‘...")
        start_phase = training_state.get('phase', 'data_load')
    else:
        start_phase = 'data_load'
        training_state = {}
    
    # ë°ì´í„° í”„ë¡œì„¸ì„œ
    processor = ImprovedExtremeProcessor()
    
    # Phase 1: ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    if start_phase == 'data_load':
        print("\n[Phase 1/5] ğŸ“‚ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬...")
        df = pd.read_csv(processor.file_path)
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape}")
        
        # ê·¹ë‹¨ê°’ ë¶„ì„
        processor.analyze_extremes(df)
        
        # ì „ì²˜ë¦¬
        time_col = df.columns[0]
        df['timestamp'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('timestamp').reset_index(drop=True)
        df = df.fillna(method='ffill').fillna(0)
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y, X_physics, sample_weights = processor.create_enhanced_sequences(
            df, numeric_cols, seq_len=20, pred_len=10
        )
        
        # ìƒíƒœ ì €ì¥
        training_state['X'] = X
        training_state['y'] = y
        training_state['X_physics'] = X_physics
        training_state['sample_weights'] = sample_weights
        training_state['phase'] = 'data_split'
        checkpoint_manager.save_training_state(training_state)
    
    # Phase 2: ë°ì´í„° ë¶„í• 
    if start_phase in ['data_load', 'data_split']:
        print("\n[Phase 2/5] ğŸ”€ ë°ì´í„° ë¶„í• ...")
        
        if 'X' not in locals():
            X = training_state['X']
            y = training_state['y']
            X_physics = training_state['X_physics']
            sample_weights = training_state['sample_weights']
        
        indices = np.arange(len(X))
        train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=42)
        val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)
        
        X_train, y_train = X[train_idx], y[train_idx]
        X_val, y_val = X[val_idx], y[val_idx]
        X_test, y_test = X[test_idx], y[test_idx]
        
        X_physics_train = X_physics[train_idx]
        X_physics_val = X_physics[val_idx]
        X_physics_test = X_physics[test_idx]
        
        weights_train = sample_weights[train_idx]
        
        print(f"  - Train: {len(X_train)} ({(y_train >= 310).sum()} 310+ê·¹ë‹¨ê°’)")
        print(f"  - Valid: {len(X_val)} ({(y_val >= 310).sum()} 310+ê·¹ë‹¨ê°’)")
        print(f"  - Test: {len(X_test)} ({(y_test >= 310).sum()} 310+ê·¹ë‹¨ê°’)")
        
        training_state['data_split'] = {
            'X_train': X_train, 'y_train': y_train,
            'X_val': X_val, 'y_val': y_val,
            'X_test': X_test, 'y_test': y_test,
            'X_physics_train': X_physics_train,
            'X_physics_val': X_physics_val,
            'X_physics_test': X_physics_test,
            'weights_train': weights_train
        }
        training_state['phase'] = 'scaling'
        checkpoint_manager.save_training_state(training_state)
    
    # Phase 3: ìŠ¤ì¼€ì¼ë§
    if start_phase in ['data_load', 'data_split', 'scaling']:
        print("\n[Phase 3/5] ğŸ“ ë°ì´í„° ì •ê·œí™”...")
        
        if 'X_train' not in locals():
            data_split = training_state['data_split']
            X_train = data_split['X_train']
            X_val = data_split['X_val']
            X_test = data_split['X_test']
            y_train = data_split['y_train']
            y_val = data_split['y_val']
            y_test = data_split['y_test']
            X_physics_train = data_split['X_physics_train']
            X_physics_val = data_split['X_physics_val']
            X_physics_test = data_split['X_physics_test']
            weights_train = data_split['weights_train']
        
        n_samples_train, seq_len, n_features = X_train.shape
        
        # ìŠ¤ì¼€ì¼ë§
        X_train_scaled = processor.scaler_X.fit_transform(
            X_train.reshape(-1, n_features)
        ).reshape(n_samples_train, seq_len, n_features)
        
        X_val_scaled = processor.scaler_X.transform(
            X_val.reshape(-1, n_features)
        ).reshape(X_val.shape[0], seq_len, n_features)
        
        X_test_scaled = processor.scaler_X.transform(
            X_test.reshape(-1, n_features)
        ).reshape(X_test.shape[0], seq_len, n_features)
        
        y_train_scaled = processor.scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = processor.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        y_test_scaled = processor.scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        X_physics_train_scaled = processor.scaler_physics.fit_transform(X_physics_train)
        X_physics_val_scaled = processor.scaler_physics.transform(X_physics_val)
        X_physics_test_scaled = processor.scaler_physics.transform(X_physics_test)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        processor.save_scalers()
        
        training_state['scaled_data'] = {
            'X_train_scaled': X_train_scaled,
            'X_val_scaled': X_val_scaled,
            'X_test_scaled': X_test_scaled,
            'y_train_scaled': y_train_scaled,
            'y_val_scaled': y_val_scaled,
            'y_test_scaled': y_test_scaled,
            'X_physics_train_scaled': X_physics_train_scaled,
            'X_physics_val_scaled': X_physics_val_scaled,
            'X_physics_test_scaled': X_physics_test_scaled,
            'n_features': n_features
        }
        training_state['phase'] = 'training'
        checkpoint_manager.save_training_state(training_state)
    
    # Phase 4: ëª¨ë¸ í•™ìŠµ
    if start_phase in ['data_load', 'data_split', 'scaling', 'training']:
        print("\n[Phase 4/5] ğŸš€ ëª¨ë¸ í•™ìŠµ...")
        
        if 'X_train_scaled' not in locals():
            scaled_data = training_state['scaled_data']
            X_train_scaled = scaled_data['X_train_scaled']
            X_val_scaled = scaled_data['X_val_scaled']
            X_test_scaled = scaled_data['X_test_scaled']
            y_train_scaled = scaled_data['y_train_scaled']
            y_val_scaled = scaled_data['y_val_scaled']
            y_test_scaled = scaled_data['y_test_scaled']
            n_features = scaled_data['n_features']
            weights_train = training_state['data_split']['weights_train']
            y_test = training_state['data_split']['y_test']
        
        # ëª¨ë¸ êµ¬ì„±
        config = {
            'seq_len': 20,
            'n_features': n_features,
            'patch_len': 5,
            'd_model': 128,
            'n_heads': 8,
            'd_ff': 256,
            'n_layers': 4,
            'dropout': 0.2
        }
        
        # ì½œë°±
        def create_callbacks(model_name):
            return [
                ModelCheckpoint(
                    filepath=os.path.join(checkpoint_manager.checkpoint_dir, f'{model_name}_best.h5'),
                    monitor='val_loss',
                    save_best_only=True,
                    save_weights_only=True,
                    verbose=1
                ),
                EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),
                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6),
                DetailedExtremeMonitor(X_val_scaled, y_val_scaled, processor.scaler_y)
            ]
        
        results = {}
        
        # EnhancedExtremePatchTST í•™ìŠµ
        print("\nğŸ¤– EnhancedExtremePatchTST (310+ íŠ¹í™”) í•™ìŠµ")
        model = EnhancedExtremePatchTST(config)
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss=MultiLevelExtremeLoss(),
            metrics=['mae']
        )
        
        # ì²´í¬í¬ì¸íŠ¸ í™•ì¸
        initial_epoch = 0
        if checkpoint_manager.check_resume('EnhancedExtremePatchTST'):
            meta = checkpoint_manager.load_checkpoint(model, 'EnhancedExtremePatchTST')
            if meta:
                initial_epoch = meta['epoch']
        
        try:
            history = model.fit(
                X_train_scaled, y_train_scaled,
                validation_data=(X_val_scaled, y_val_scaled),
                sample_weight=weights_train,
                initial_epoch=initial_epoch,
                epochs=70,  # ë” ë§ì€ ì—í­
                batch_size=32,
                callbacks=create_callbacks('EnhancedExtremePatchTST'),
                verbose=1
            )
            
            # í•™ìŠµ ì™„ë£Œ í›„ ì €ì¥
            checkpoint_manager.save_checkpoint(model, 'EnhancedExtremePatchTST', 70, history)
            
        except KeyboardInterrupt:
            print("\nâš ï¸ í•™ìŠµ ì¤‘ë‹¨ë¨!")
            current_epoch = len(history.history['loss']) if 'history' in locals() else initial_epoch
            checkpoint_manager.save_checkpoint(model, 'EnhancedExtremePatchTST', current_epoch)
            print("ğŸ’¾ ì¬ì‹œì‘ ì‹œ ì´ì–´ì„œ í•™ìŠµ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            sys.exit(0)
        
        # í‰ê°€
        y_pred_scaled = model.predict(X_test_scaled)
        y_pred = processor.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
        results['EnhancedExtremePatchTST'] = y_pred
        
        training_state['results'] = results
        training_state['phase'] = 'evaluation'
        checkpoint_manager.save_training_state(training_state)
    
    # Phase 5: ìµœì¢… í‰ê°€
    print("\n[Phase 5/5] ğŸ“Š ìµœì¢… ì„±ëŠ¥ í‰ê°€")
    
    if 'results' not in locals():
        results = training_state['results']
        y_test = training_state['data_split']['y_test']
    
    for model_name, y_pred in results.items():
        print(f"\nğŸ” {model_name}:")
        
        # ì „ì²´ ì„±ëŠ¥
        mae = np.mean(np.abs(y_test - y_pred))
        rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
        print(f"  ì „ì²´ MAE: {mae:.2f}, RMSE: {rmse:.2f}")
        
        # êµ¬ê°„ë³„ ìƒì„¸ ì„±ëŠ¥
        print(f"\n  ğŸ“ˆ êµ¬ê°„ë³„ ì„±ëŠ¥:")
        ranges = [
            (280, 300, "ê²½ê³ "),
            (300, 310, "ìœ„í—˜"),
            (310, 350, "ì‹¬ê°"),
            (350, 1000, "ì¬ë‚œ")
        ]
        
        for low, high, name in ranges:
            mask = (y_test >= low) & (y_test < high)
            if mask.sum() > 0:
                range_mae = np.mean(np.abs(y_test[mask] - y_pred[mask]))
                pred_in_range = ((y_pred >= low) & (y_pred < high))[mask].sum()
                accuracy = pred_in_range / mask.sum()
                
                print(f"    {name}[{low}-{high}): "
                      f"MAE={range_mae:.1f}, "
                      f"ì •í™•ë„={accuracy:.1%}, "
                      f"ê°œìˆ˜={mask.sum()}")
        
        # 310+ íŠ¹ë³„ ë¶„ì„
        mask_310 = y_test >= 310
        if mask_310.sum() > 0:
            print(f"\n  ğŸ¯ 310+ ê·¹ë‹¨ê°’ ì„±ëŠ¥:")
            mae_310 = np.mean(np.abs(y_test[mask_310] - y_pred[mask_310]))
            pred_310 = y_pred >= 310
            tp = (mask_310 & pred_310).sum()
            recall = tp / mask_310.sum()
            
            print(f"    - MAE: {mae_310:.2f}")
            print(f"    - ê°ì§€ìœ¨: {tp}/{mask_310.sum()} ({recall:.1%})")
            
            # ìƒ˜í”Œ ì¶œë ¥
            print(f"\n  ğŸ“ 310+ ì˜ˆì¸¡ ìƒ˜í”Œ:")
            indices_310 = np.where(mask_310)[0][:5]
            for idx in indices_310:
                status = "âœ…" if abs(y_test[idx] - y_pred[idx]) < 30 else "âš ï¸" if abs(y_test[idx] - y_pred[idx]) < 50 else "âŒ"
                print(f"    {status} ì‹¤ì œ: {y_test[idx]:.0f}, "
                      f"ì˜ˆì¸¡: {y_pred[idx]:.0f}, "
                      f"ì˜¤ì°¨: {abs(y_test[idx] - y_pred[idx]):.0f}")
    
    print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("ğŸ’¾ ìŠ¤ì¼€ì¼ëŸ¬: ./scalers/")
    print("ğŸ’¾ ì²´í¬í¬ì¸íŠ¸: ./checkpoints/")
    print("ğŸ¯ 310+ ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ê°•í™” ì™„ë£Œ")

if __name__ == "__main__":
    main()