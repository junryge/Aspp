# -*- coding: utf-8 -*-
"""
HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - ìµœì¢… ì •ë¦¬ ë²„ì „
- 310+ ê·¹ë‹¨ê°’ ì •í™•í•œ ì˜ˆì¸¡
- ì¤‘ë‹¨/ì¬ì‹œì‘ ì™„ë²½ ì§€ì›
- ê¹”ë”í•œ êµ¬ì¡°
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import RobustScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
import warnings
import os
import pickle
from tqdm import tqdm
import joblib

warnings.filterwarnings('ignore')

print(f"TensorFlow Version: {tf.__version__}")
tf.random.set_seed(42)
np.random.seed(42)

# ===========================
# 1. ë°ì´í„° ì²˜ë¦¬
# ===========================

class DataProcessor:
    def __init__(self, file_path='data/HUB_0509_TO_0730_DATA.CSV'):
        self.file_path = file_path
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        
        # ìŠ¤ì¼€ì¼ëŸ¬
        self.scaler_X = RobustScaler()
        self.scaler_y = MinMaxScaler(feature_range=(0, 1))
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ê²½ë¡œ
        self.scaler_dir = './scalers'
        os.makedirs(self.scaler_dir, exist_ok=True)
        
    def save_scalers(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥"""
        joblib.dump(self.scaler_X, os.path.join(self.scaler_dir, 'scaler_X.pkl'))
        joblib.dump(self.scaler_y, os.path.join(self.scaler_dir, 'scaler_y.pkl'))
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {self.scaler_dir}")
    
    def load_scalers(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        try:
            self.scaler_X = joblib.load(os.path.join(self.scaler_dir, 'scaler_X.pkl'))
            self.scaler_y = joblib.load(os.path.join(self.scaler_dir, 'scaler_y.pkl'))
            print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ: {self.scaler_dir}")
            return True
        except:
            return False
    
    def analyze_data(self, df):
        """ë°ì´í„° ë¶„ì„"""
        target = df[self.target_col]
        
        print("\nğŸ“Š ë°ì´í„° ë¶„ì„")
        print(f"  ë²”ìœ„: {target.min():.0f} ~ {target.max():.0f}")
        print(f"  í‰ê· : {target.mean():.2f}")
        
        # ê·¹ë‹¨ê°’ ë¶„í¬
        print("\nğŸš¨ ê·¹ë‹¨ê°’ ë¶„í¬:")
        thresholds = [280, 300, 310, 350, 400]
        for t in thresholds:
            count = (target >= t).sum()
            ratio = count / len(target) * 100
            print(f"  â‰¥{t}: {count}ê°œ ({ratio:.2f}%)")
    
    def create_sequences(self, df, seq_len=20, pred_len=10):
        """ì‹œí€€ìŠ¤ ìƒì„± (310+ ì˜¤ë²„ìƒ˜í”Œë§)"""
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        X, y, weights = [], [], []
        total_seq = len(df) - seq_len - pred_len + 1
        
        # êµ¬ê°„ë³„ ì¸ë±ìŠ¤ ìˆ˜ì§‘
        indices_normal = []
        indices_300 = []
        indices_310 = []
        indices_350 = []
        
        print("\nğŸ“¦ ì‹œí€€ìŠ¤ ë¶„ë¥˜ ì¤‘...")
        for i in tqdm(range(total_seq)):
            target_val = df[self.target_col].iloc[i + seq_len + pred_len - 1]
            
            if target_val >= 350:
                indices_350.append(i)
            elif target_val >= 310:
                indices_310.append(i)
            elif target_val >= 300:
                indices_300.append(i)
            else:
                indices_normal.append(i)
        
        # ì˜¤ë²„ìƒ˜í”Œë§
        all_indices = []
        all_indices.extend(indices_normal)  # 1ë°°
        all_indices.extend(indices_300 * 2)  # 2ë°°
        all_indices.extend(indices_310 * 5)  # 5ë°° (310+ ê°•í™”)
        all_indices.extend(indices_350 * 8)  # 8ë°° (350+ ë” ê°•í™”)
        
        print(f"  ì •ìƒ: {len(indices_normal)}")
        print(f"  300+: {len(indices_300)} â†’ {len(indices_300)*2}")
        print(f"  310+: {len(indices_310)} â†’ {len(indices_310)*5}")
        print(f"  350+: {len(indices_350)} â†’ {len(indices_350)*8}")
        
        np.random.shuffle(all_indices)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        print(f"\nğŸ“¦ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘... (ì´ {len(all_indices)}ê°œ)")
        for i in tqdm(all_indices):
            X_seq = df[numeric_cols].iloc[i:i+seq_len].values
            y_val = df[self.target_col].iloc[i + seq_len + pred_len - 1]
            
            X.append(X_seq)
            y.append(y_val)
            
            # ê°€ì¤‘ì¹˜
            if y_val >= 350:
                weights.append(15.0)
            elif y_val >= 310:
                weights.append(10.0)
            elif y_val >= 300:
                weights.append(5.0)
            else:
                weights.append(1.0)
        
        return np.array(X), np.array(y), np.array(weights)

# ===========================
# 2. ëª¨ë¸ ì •ì˜
# ===========================

class ExtremePatchTST(keras.Model):
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embed = layers.Dense(128, activation='relu')
        
        # Transformer ë¸”ë¡
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm1 = layers.LayerNormalization()
        self.norm2 = layers.LayerNormalization()
        
        # FFN
        self.ffn = keras.Sequential([
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(128)
        ])
        
        # ì¶œë ¥
        self.flatten = layers.Flatten()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dropout = layers.Dropout(0.3)
        self.dense2 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(1)
        
        # ê·¹ë‹¨ê°’ ë³´ì •
        self.extreme_detector = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.Dense(1, activation='sigmoid')
        ])
    
    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        
        # íŒ¨ì¹˜ ìƒì„±
        x = tf.reshape(x, [batch_size, self.n_patches, self.patch_len * self.n_features])
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x = self.patch_embed(x)
        
        # Transformer
        attn = self.attention(x, x, training=training)
        x = self.norm1(x + attn)
        
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        
        # ì¶œë ¥
        x = self.flatten(x)
        
        # ê·¹ë‹¨ê°’ ê°ì§€
        extreme_prob = self.extreme_detector(x)
        
        # ê°’ ì˜ˆì¸¡
        x = self.dense1(x)
        x = self.dropout(x, training=training)
        x = self.dense2(x)
        value = self.output_layer(x)
        
        # ê·¹ë‹¨ê°’ ë³´ì • (310+ ê°•í™”)
        # extreme_probì´ ë†’ìœ¼ë©´ ë” í° ê°’ìœ¼ë¡œ ë³´ì •
        adjustment = extreme_prob * 80  # ìµœëŒ€ 80 ì¶”ê°€
        final_output = value + adjustment
        
        return tf.squeeze(final_output, axis=-1)

# ===========================
# 3. ì†ì‹¤í•¨ìˆ˜
# ===========================

class ExtremeLoss(tf.keras.losses.Loss):
    def __init__(self):
        super().__init__()
        
    def call(self, y_true, y_pred):
        y_true = tf.reshape(y_true, [-1])
        y_pred = tf.reshape(y_pred, [-1])
        
        # MSE
        mse = tf.square(y_true - y_pred)
        
        # ê·¹ë‹¨ê°’ ê°€ì¤‘ì¹˜ (ì •ê·œí™”ëœ ê°’ ê¸°ì¤€)
        # MinMaxScaler í›„: 300 â†’ ~0.4, 310 â†’ ~0.45, 350 â†’ ~0.55
        weight = tf.where(y_true > 0.55, 20.0,  # 350+
                 tf.where(y_true > 0.45, 10.0,  # 310+
                 tf.where(y_true > 0.4, 5.0,    # 300+
                 1.0)))
        
        return mse * weight

# ===========================
# 4. ë©”ì¸ ì‹¤í–‰
# ===========================

def main():
    print("="*80)
    print("ğŸ­ HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - ìµœì¢… ë²„ì „")
    print("="*80)
    
    # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬
    checkpoint_dir = './checkpoints'
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # ì§„í–‰ ìƒíƒœ íŒŒì¼
    state_file = os.path.join(checkpoint_dir, 'training_state.pkl')
    
    # ì´ì „ ìƒíƒœ í™•ì¸
    if os.path.exists(state_file):
        with open(state_file, 'rb') as f:
            state = pickle.load(f)
        print("ğŸ“‚ ì´ì „ ìƒíƒœ ë¡œë“œ")
        phase = state.get('phase', 1)
    else:
        state = {}
        phase = 1
    
    processor = DataProcessor()
    
    # Phase 1: ë°ì´í„° ë¡œë“œ
    if phase == 1:
        print("\n[Phase 1/4] ë°ì´í„° ì¤€ë¹„")
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(processor.file_path)
        print(f"âœ… ë°ì´í„° ë¡œë“œ: {df.shape}")
        
        # ë¶„ì„
        processor.analyze_data(df)
        
        # ì „ì²˜ë¦¬
        df['timestamp'] = pd.to_datetime(df.iloc[:, 0], format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('timestamp').reset_index(drop=True)
        df = df.fillna(method='ffill').fillna(0)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y, weights = processor.create_sequences(df)
        
        # ë°ì´í„° ë¶„í• 
        indices = np.arange(len(X))
        train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)
        val_idx, test_idx = train_test_split(test_idx, test_size=0.5, random_state=42)
        
        state['X_train'] = X[train_idx]
        state['y_train'] = y[train_idx]
        state['X_val'] = X[val_idx]
        state['y_val'] = y[val_idx]
        state['X_test'] = X[test_idx]
        state['y_test'] = y[test_idx]
        state['weights_train'] = weights[train_idx]
        state['n_features'] = X.shape[2]
        state['phase'] = 2
        
        with open(state_file, 'wb') as f:
            pickle.dump(state, f)
        print("ğŸ’¾ Phase 1 ì™„ë£Œ")
    
    # Phase 2: ìŠ¤ì¼€ì¼ë§
    if phase <= 2:
        print("\n[Phase 2/4] ë°ì´í„° ì •ê·œí™”")
        
        # ë°ì´í„° ë¡œë“œ
        X_train = state['X_train']
        y_train = state['y_train']
        X_val = state['X_val']
        y_val = state['y_val']
        X_test = state['X_test']
        y_test = state['y_test']
        n_features = state['n_features']
        
        # ìŠ¤ì¼€ì¼ë§
        n_train = len(X_train)
        n_val = len(X_val)
        n_test = len(X_test)
        
        X_train_scaled = processor.scaler_X.fit_transform(X_train.reshape(-1, n_features))
        X_train_scaled = X_train_scaled.reshape(n_train, 20, n_features)
        
        X_val_scaled = processor.scaler_X.transform(X_val.reshape(-1, n_features))
        X_val_scaled = X_val_scaled.reshape(n_val, 20, n_features)
        
        X_test_scaled = processor.scaler_X.transform(X_test.reshape(-1, n_features))
        X_test_scaled = X_test_scaled.reshape(n_test, 20, n_features)
        
        y_train_scaled = processor.scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = processor.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        y_test_scaled = processor.scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        processor.save_scalers()
        
        state['X_train_scaled'] = X_train_scaled
        state['y_train_scaled'] = y_train_scaled
        state['X_val_scaled'] = X_val_scaled
        state['y_val_scaled'] = y_val_scaled
        state['X_test_scaled'] = X_test_scaled
        state['y_test_scaled'] = y_test_scaled
        state['phase'] = 3
        
        with open(state_file, 'wb') as f:
            pickle.dump(state, f)
        print("ğŸ’¾ Phase 2 ì™„ë£Œ")
    
    # Phase 3: í•™ìŠµ
    if phase <= 3:
        print("\n[Phase 3/4] ëª¨ë¸ í•™ìŠµ")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        if not processor.load_scalers():
            print("âŒ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨")
            return
        
        # ë°ì´í„° ë¡œë“œ
        X_train_scaled = state['X_train_scaled']
        y_train_scaled = state['y_train_scaled']
        X_val_scaled = state['X_val_scaled']
        y_val_scaled = state['y_val_scaled']
        weights_train = state['weights_train']
        n_features = state['n_features']
        
        # ëª¨ë¸ êµ¬ì„±
        config = {
            'seq_len': 20,
            'n_features': n_features,
            'patch_len': 5
        }
        
        model = ExtremePatchTST(config)
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss=ExtremeLoss(),
            metrics=['mae']
        )
        
        # ì½œë°±
        callbacks = [
            ModelCheckpoint(
                filepath=os.path.join(checkpoint_dir, 'best_model.h5'),
                monitor='val_loss',
                save_best_only=True,
                save_weights_only=True
            ),
            EarlyStopping(patience=15, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=5)
        ]
        
        # í•™ìŠµ
        print(f"í•™ìŠµ ë°ì´í„°: {len(X_train_scaled)}")
        print(f"310+ ìƒ˜í”Œ: {(state['y_train'] >= 310).sum()}")
        
        history = model.fit(
            X_train_scaled, y_train_scaled,
            validation_data=(X_val_scaled, y_val_scaled),
            sample_weight=weights_train,
            epochs=50,
            batch_size=32,
            callbacks=callbacks,
            verbose=1
        )
        
        # ì €ì¥
        model.save_weights(os.path.join(checkpoint_dir, 'final_model.h5'))
        state['phase'] = 4
        
        with open(state_file, 'wb') as f:
            pickle.dump(state, f)
        print("ğŸ’¾ Phase 3 ì™„ë£Œ")
    
    # Phase 4: í‰ê°€
    print("\n[Phase 4/4] ëª¨ë¸ í‰ê°€")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    if not processor.load_scalers():
        print("âŒ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨")
        return
    
    # ëª¨ë¸ ì¬êµ¬ì„±
    config = {
        'seq_len': 20,
        'n_features': state['n_features'],
        'patch_len': 5
    }
    
    model = ExtremePatchTST(config)
    model.compile(optimizer='adam', loss='mse')
    
    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_weights(os.path.join(checkpoint_dir, 'final_model.h5'))
    
    # ì˜ˆì¸¡
    X_test_scaled = state['X_test_scaled']
    y_test_scaled = state['y_test_scaled']
    y_test = state['y_test']
    
    y_pred_scaled = model.predict(X_test_scaled, verbose=0)
    y_pred = processor.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    
    # í‰ê°€
    mae = np.mean(np.abs(y_test - y_pred))
    print(f"\nì „ì²´ MAE: {mae:.2f}")
    
    # 310+ ì„±ëŠ¥
    mask_310 = y_test >= 310
    if mask_310.sum() > 0:
        mae_310 = np.mean(np.abs(y_test[mask_310] - y_pred[mask_310]))
        detected = (y_pred >= 310)[mask_310].sum()
        print(f"\nğŸ¯ 310+ ê·¹ë‹¨ê°’ ì„±ëŠ¥:")
        print(f"  MAE: {mae_310:.2f}")
        print(f"  ê°ì§€: {detected}/{mask_310.sum()} ({detected/mask_310.sum()*100:.1f}%)")
        
        # ìƒ˜í”Œ
        print(f"\nğŸ“ 310+ ì˜ˆì¸¡ ìƒ˜í”Œ:")
        indices = np.where(mask_310)[0][:5]
        for idx in indices:
            print(f"  ì‹¤ì œ: {y_test[idx]:.0f}, ì˜ˆì¸¡: {y_pred[idx]:.0f}")
    
    print("\nâœ… ì™„ë£Œ!")

if __name__ == "__main__":
    main()