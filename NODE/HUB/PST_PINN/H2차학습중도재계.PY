# -*- coding: utf-8 -*-
"""
HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - ì™„ì „ ìƒˆ ë²„ì „
- ExtremePatchTST: Transformer ê¸°ë°˜
- ImprovedPINN: ë¬¼ë¦¬ ë²•ì¹™ ê¸°ë°˜
310+ ê·¹ë‹¨ê°’ ì˜ˆì¸¡ íŠ¹í™”
ì¤‘ë‹¨/ì¬ì‹œì‘ ì§€ì›
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
import os
import pickle
import warnings
from tqdm import tqdm
import joblib
import signal
import sys

warnings.filterwarnings('ignore')

# ì„¤ì •
np.random.seed(42)
tf.random.set_seed(42)
print(f"TensorFlow Version: {tf.__version__}")

# ========================================
# ìƒíƒœ ê´€ë¦¬
# ========================================

class CheckpointManager:
    def __init__(self, checkpoint_dir='./checkpoints'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        self.state_file = os.path.join(checkpoint_dir, 'training_state.pkl')
        self.interrupted = False
        
        # Ctrl+C í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
    
    def _signal_handler(self, sig, frame):
        print('\n\nâš ï¸ ì¤‘ë‹¨ ê°ì§€! ìƒíƒœ ì €ì¥ ì¤‘...')
        self.interrupted = True
        
    def save_state(self, state):
        with open(self.state_file, 'wb') as f:
            pickle.dump(state, f)
        print(f"ğŸ’¾ ìƒíƒœ ì €ì¥ ì™„ë£Œ")
        
    def load_state(self):
        if os.path.exists(self.state_file):
            with open(self.state_file, 'rb') as f:
                return pickle.load(f)
        return None

# ========================================
# ë°ì´í„° ì²˜ë¦¬
# ========================================

class DataProcessor:
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        self.scaler_X = RobustScaler()
        self.scaler_y = MinMaxScaler(feature_range=(0, 1))
        self.scaler_physics = StandardScaler()
        
        # ë¬¼ë¦¬ ì»¬ëŸ¼
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2', 'M14B_7F_TO_HUB_JOB2'
        ]
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB', 'M16A_3F_TO_M14B_7F_JOB'
        ]
        
    def save_scalers(self):
        os.makedirs('./scalers', exist_ok=True)
        joblib.dump(self.scaler_X, './scalers/scaler_X.pkl')
        joblib.dump(self.scaler_y, './scalers/scaler_y.pkl')
        joblib.dump(self.scaler_physics, './scalers/scaler_physics.pkl')
        print("âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ")
        
    def load_scalers(self):
        try:
            self.scaler_X = joblib.load('./scalers/scaler_X.pkl')
            self.scaler_y = joblib.load('./scalers/scaler_y.pkl')
            self.scaler_physics = joblib.load('./scalers/scaler_physics.pkl')
            print("âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
            return True
        except:
            return False
    
    def analyze_data(self, df):
        target = df[self.target_col]
        print("\nğŸ“Š ë°ì´í„° ë¶„ì„:")
        print(f"  ë²”ìœ„: {target.min():.0f} ~ {target.max():.0f}")
        print(f"  í‰ê· : {target.mean():.1f}")
        
        print("\nğŸš¨ ê·¹ë‹¨ê°’ ë¶„í¬:")
        thresholds = [280, 300, 310, 350, 400]
        for t in thresholds:
            count = (target >= t).sum()
            ratio = count / len(target) * 100
            print(f"  â‰¥{t}: {count:6}ê°œ ({ratio:5.2f}%)")
    
    def create_sequences(self, df, seq_len=20, pred_len=10):
        """ì‹œí€€ìŠ¤ ìƒì„± (310+ ì˜¤ë²„ìƒ˜í”Œë§)"""
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # ë¬¼ë¦¬ ë°ì´í„°ìš© ì»¬ëŸ¼ ì²´í¬
        available_inflow = [col for col in self.inflow_cols if col in df.columns]
        available_outflow = [col for col in self.outflow_cols if col in df.columns]
        
        X, y, X_physics, weights = [], [], [], []
        total = len(df) - seq_len - pred_len + 1
        
        # êµ¬ê°„ë³„ ì¸ë±ìŠ¤
        indices = {'normal': [], '300': [], '310': [], '350': []}
        
        print("\nğŸ“¦ ì‹œí€€ìŠ¤ ë¶„ë¥˜ ì¤‘...")
        for i in tqdm(range(total)):
            target_val = df[self.target_col].iloc[i + seq_len + pred_len - 1]
            
            if target_val >= 350:
                indices['350'].append(i)
            elif target_val >= 310:
                indices['310'].append(i)
            elif target_val >= 300:
                indices['300'].append(i)
            else:
                indices['normal'].append(i)
        
        # ì˜¤ë²„ìƒ˜í”Œë§
        all_indices = []
        all_indices.extend(indices['normal'])     # 1ë°°
        all_indices.extend(indices['300'] * 3)    # 3ë°°
        all_indices.extend(indices['310'] * 5)    # 5ë°° (í•µì‹¬)
        all_indices.extend(indices['350'] * 8)    # 8ë°°
        
        print(f"  ì •ìƒ: {len(indices['normal'])}")
        print(f"  300+: {len(indices['300'])} â†’ {len(indices['300'])*3}")
        print(f"  310+: {len(indices['310'])} â†’ {len(indices['310'])*5}")
        print(f"  350+: {len(indices['350'])} â†’ {len(indices['350'])*8}")
        
        np.random.shuffle(all_indices)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        print(f"\nì‹œí€€ìŠ¤ ìƒì„± ì¤‘... (ì´ {len(all_indices)}ê°œ)")
        for i in tqdm(all_indices):
            # ì‹œê³„ì—´ ë°ì´í„°
            X.append(df[numeric_cols].iloc[i:i+seq_len].values)
            
            # íƒ€ê²Ÿ
            y_val = df[self.target_col].iloc[i + seq_len + pred_len - 1]
            y.append(y_val)
            
            # ë¬¼ë¦¬ ë°ì´í„°
            physics = [
                df[self.target_col].iloc[i + seq_len - 1],  # í˜„ì¬ê°’
                df[available_inflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_inflow else 0,
                df[available_outflow].iloc[i+seq_len:i+seq_len+pred_len].sum().sum() if available_outflow else 0
            ]
            X_physics.append(physics)
            
            # ê°€ì¤‘ì¹˜
            if y_val >= 350:
                weights.append(15.0)
            elif y_val >= 310:
                weights.append(10.0)
            elif y_val >= 300:
                weights.append(5.0)
            else:
                weights.append(1.0)
        
        return np.array(X), np.array(y), np.array(X_physics), np.array(weights)

# ========================================
# ëª¨ë¸ 1: ExtremePatchTST
# ========================================

class ExtremePatchTST(keras.Model):
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(128, activation='relu')
        
        # Transformer
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16)
        self.norm1 = layers.LayerNormalization()
        self.norm2 = layers.LayerNormalization()
        
        self.ffn = keras.Sequential([
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(128)
        ])
        
        # ì¶œë ¥
        self.flatten = layers.Flatten()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dropout = layers.Dropout(0.3)
        self.dense2 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(1)
        
    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        
        # íŒ¨ì¹˜ ìƒì„±
        x = tf.reshape(x, [batch_size, self.n_patches, self.patch_len * self.n_features])
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x = self.patch_embedding(x)
        
        # Transformer
        attn = self.attention(x, x, training=training)
        x = self.norm1(x + attn)
        
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        
        # ì¶œë ¥
        x = self.flatten(x)
        x = self.dense1(x)
        x = self.dropout(x, training=training)
        x = self.dense2(x)
        output = self.output_layer(x)
        
        return tf.squeeze(output, axis=-1)

# ========================================
# ëª¨ë¸ 2: ImprovedPINN
# ========================================

class ImprovedPINN(keras.Model):
    def __init__(self, config):
        super().__init__()
        
        # LSTM for ì‹œê³„ì—´
        self.lstm = layers.LSTM(64, return_sequences=False)
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬
        self.physics_net = keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dense(16, activation='relu')
        ])
        
        # ìœµí•©
        self.fusion = keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dense(1)
        ])
        
    def call(self, inputs, training=False):
        x_seq, x_physics = inputs
        
        # ì‹œê³„ì—´ ì²˜ë¦¬
        seq_features = self.lstm(x_seq)
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬
        physics_features = self.physics_net(x_physics)
        
        # ê²°í•©
        combined = tf.concat([seq_features, physics_features], axis=-1)
        
        # ì¶œë ¥
        output = self.fusion(combined)
        
        return tf.squeeze(output, axis=-1)

# ========================================
# ì»¤ìŠ¤í…€ ì†ì‹¤í•¨ìˆ˜
# ========================================

class ExtremeLoss(tf.keras.losses.Loss):
    def __init__(self):
        super().__init__()
        
    def call(self, y_true, y_pred):
        y_true = tf.reshape(y_true, [-1])
        y_pred = tf.reshape(y_pred, [-1])
        
        mse = tf.square(y_true - y_pred)
        
        # MinMaxScaler í›„ ê°’ (300â†’0.4, 310â†’0.45, 350â†’0.55 ëŒ€ëµ)
        weight = tf.where(y_true > 0.55, 15.0,    # 350+
                 tf.where(y_true > 0.45, 10.0,    # 310+
                 tf.where(y_true > 0.4, 5.0,      # 300+
                 1.0)))
        
        return mse * weight

# ========================================
# ë©”ì¸ ì‹¤í–‰
# ========================================

def main():
    print("="*80)
    print("ğŸ­ HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
    print("ğŸ¯ ëª©í‘œ: 310+ ê·¹ë‹¨ê°’ ì •í™• ì˜ˆì¸¡")
    print("="*80)
    
    # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
    ckpt = CheckpointManager()
    
    # ìƒíƒœ ë¡œë“œ
    state = ckpt.load_state()
    if state:
        print("\nğŸ“‚ ì´ì „ ìƒíƒœ ë¡œë“œ")
        step = state.get('step', 1)
    else:
        state = {}
        step = 1
    
    processor = DataProcessor()
    
    # Step 1: ë°ì´í„° ì¤€ë¹„
    if step == 1:
        print("\n[Step 1/5] ë°ì´í„° ì¤€ë¹„")
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv('data/HUB_0509_TO_0730_DATA.CSV')
        print(f"âœ… ë°ì´í„° ë¡œë“œ: {df.shape}")
        
        # ë¶„ì„
        processor.analyze_data(df)
        
        # ì „ì²˜ë¦¬
        df['timestamp'] = pd.to_datetime(df.iloc[:, 0], format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('timestamp').reset_index(drop=True)
        df = df.fillna(method='ffill').fillna(0)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y, X_physics, weights = processor.create_sequences(df)
        
        state['X'] = X
        state['y'] = y
        state['X_physics'] = X_physics
        state['weights'] = weights
        state['n_features'] = X.shape[2]
        state['step'] = 2
        
        ckpt.save_state(state)
        print("âœ… Step 1 ì™„ë£Œ")
    
    # Step 2: ë°ì´í„° ë¶„í• 
    if step <= 2:
        print("\n[Step 2/5] ë°ì´í„° ë¶„í• ")
        
        X = state['X']
        y = state['y']
        X_physics = state['X_physics']
        weights = state['weights']
        
        # ì¸ë±ìŠ¤ ë¶„í• 
        indices = np.arange(len(X))
        train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)
        val_idx, test_idx = train_test_split(test_idx, test_size=0.5, random_state=42)
        
        state['train_idx'] = train_idx
        state['val_idx'] = val_idx
        state['test_idx'] = test_idx
        state['step'] = 3
        
        ckpt.save_state(state)
        
        print(f"  Train: {len(train_idx)} ({(y[train_idx] >= 310).sum()} 310+)")
        print(f"  Valid: {len(val_idx)} ({(y[val_idx] >= 310).sum()} 310+)")
        print(f"  Test: {len(test_idx)} ({(y[test_idx] >= 310).sum()} 310+)")
        print("âœ… Step 2 ì™„ë£Œ")
    
    # Step 3: ìŠ¤ì¼€ì¼ë§
    if step <= 3:
        print("\n[Step 3/5] ë°ì´í„° ìŠ¤ì¼€ì¼ë§")
        
        X = state['X']
        y = state['y']
        X_physics = state['X_physics']
        weights = state['weights']
        n_features = state['n_features']
        
        train_idx = state['train_idx']
        val_idx = state['val_idx']
        test_idx = state['test_idx']
        
        # ë°ì´í„° ë¶„í• 
        X_train, y_train = X[train_idx], y[train_idx]
        X_val, y_val = X[val_idx], y[val_idx]
        X_test, y_test = X[test_idx], y[test_idx]
        
        X_physics_train = X_physics[train_idx]
        X_physics_val = X_physics[val_idx]
        X_physics_test = X_physics[test_idx]
        
        weights_train = weights[train_idx]
        
        # X ìŠ¤ì¼€ì¼ë§
        X_train_flat = X_train.reshape(-1, n_features)
        X_train_scaled = processor.scaler_X.fit_transform(X_train_flat)
        X_train_scaled = X_train_scaled.reshape(len(X_train), 20, n_features)
        
        X_val_flat = X_val.reshape(-1, n_features)
        X_val_scaled = processor.scaler_X.transform(X_val_flat)
        X_val_scaled = X_val_scaled.reshape(len(X_val), 20, n_features)
        
        X_test_flat = X_test.reshape(-1, n_features)
        X_test_scaled = processor.scaler_X.transform(X_test_flat)
        X_test_scaled = X_test_scaled.reshape(len(X_test), 20, n_features)
        
        # y ìŠ¤ì¼€ì¼ë§
        y_train_scaled = processor.scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = processor.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        y_test_scaled = processor.scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        # ë¬¼ë¦¬ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        X_physics_train_scaled = processor.scaler_physics.fit_transform(X_physics_train)
        X_physics_val_scaled = processor.scaler_physics.transform(X_physics_val)
        X_physics_test_scaled = processor.scaler_physics.transform(X_physics_test)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        processor.save_scalers()
        
        state['X_train_scaled'] = X_train_scaled
        state['y_train_scaled'] = y_train_scaled
        state['X_val_scaled'] = X_val_scaled
        state['y_val_scaled'] = y_val_scaled
        state['X_test_scaled'] = X_test_scaled
        state['y_test_scaled'] = y_test_scaled
        state['X_physics_train_scaled'] = X_physics_train_scaled
        state['X_physics_val_scaled'] = X_physics_val_scaled
        state['X_physics_test_scaled'] = X_physics_test_scaled
        state['weights_train'] = weights_train
        state['y_test'] = y_test
        state['step'] = 4
        
        ckpt.save_state(state)
        print("âœ… Step 3 ì™„ë£Œ")
    
    # Step 4: ëª¨ë¸ í•™ìŠµ
    if step <= 4:
        print("\n[Step 4/5] ëª¨ë¸ í•™ìŠµ")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        if not processor.load_scalers():
            print("âŒ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨")
            return
        
        X_train_scaled = state['X_train_scaled']
        y_train_scaled = state['y_train_scaled']
        X_val_scaled = state['X_val_scaled']
        y_val_scaled = state['y_val_scaled']
        X_physics_train_scaled = state['X_physics_train_scaled']
        X_physics_val_scaled = state['X_physics_val_scaled']
        weights_train = state['weights_train']
        n_features = state['n_features']
        
        config = {
            'seq_len': 20,
            'n_features': n_features,
            'patch_len': 5
        }
        
        # ì½œë°±
        callbacks = [
            EarlyStopping(patience=15, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6),
            ModelCheckpoint('./checkpoints/best_model_{epoch:02d}.h5', 
                          save_best_only=True, save_weights_only=True)
        ]
        
        # ëª¨ë¸ 1: ExtremePatchTST
        print("\nğŸ¤– ëª¨ë¸ 1: ExtremePatchTST í•™ìŠµ")
        model1 = ExtremePatchTST(config)
        model1.compile(
            optimizer=Adam(learning_rate=0.001),
            loss=ExtremeLoss(),
            metrics=['mae']
        )
        
        history1 = model1.fit(
            X_train_scaled, y_train_scaled,
            validation_data=(X_val_scaled, y_val_scaled),
            sample_weight=weights_train,
            epochs=50,
            batch_size=32,
            callbacks=callbacks,
            verbose=1
        )
        
        model1.save_weights('./checkpoints/model1_final.h5')
        
        # ëª¨ë¸ 2: ImprovedPINN
        print("\nğŸ¤– ëª¨ë¸ 2: ImprovedPINN í•™ìŠµ")
        model2 = ImprovedPINN(config)
        model2.compile(
            optimizer=Adam(learning_rate=0.001),
            loss=ExtremeLoss(),
            metrics=['mae']
        )
        
        history2 = model2.fit(
            [X_train_scaled, X_physics_train_scaled], y_train_scaled,
            validation_data=([X_val_scaled, X_physics_val_scaled], y_val_scaled),
            sample_weight=weights_train,
            epochs=50,
            batch_size=32,
            callbacks=callbacks,
            verbose=1
        )
        
        model2.save_weights('./checkpoints/model2_final.h5')
        
        state['step'] = 5
        ckpt.save_state(state)
        print("âœ… Step 4 ì™„ë£Œ")
    
    # Step 5: í‰ê°€
    print("\n[Step 5/5] ëª¨ë¸ í‰ê°€")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    if not processor.load_scalers():
        print("âŒ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨")
        return
    
    X_test_scaled = state['X_test_scaled']
    y_test_scaled = state['y_test_scaled']
    X_physics_test_scaled = state['X_physics_test_scaled']
    y_test = state['y_test']
    n_features = state['n_features']
    
    config = {
        'seq_len': 20,
        'n_features': n_features,
        'patch_len': 5
    }
    
    # ëª¨ë¸ 1 í‰ê°€
    print("\nğŸ“Š ëª¨ë¸ 1: ExtremePatchTST í‰ê°€")
    model1 = ExtremePatchTST(config)
    model1.compile(optimizer='adam', loss='mse')
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ ëª¨ë¸ ë¹Œë“œ
    dummy_input = np.zeros((1, 20, n_features))
    _ = model1(dummy_input)
    
    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    model1.load_weights('./checkpoints/model1_final.h5')
    
    y_pred1_scaled = model1.predict(X_test_scaled, verbose=0)
    y_pred1 = processor.scaler_y.inverse_transform(y_pred1_scaled.reshape(-1, 1)).flatten()
    
    mae1 = np.mean(np.abs(y_test - y_pred1))
    print(f"  ì „ì²´ MAE: {mae1:.2f}")
    
    mask_310 = y_test >= 310
    if mask_310.sum() > 0:
        mae_310 = np.mean(np.abs(y_test[mask_310] - y_pred1[mask_310]))
        detected = (y_pred1 >= 310)[mask_310].sum()
        print(f"  310+ MAE: {mae_310:.2f}")
        print(f"  310+ ê°ì§€: {detected}/{mask_310.sum()} ({detected/mask_310.sum()*100:.1f}%)")
    
    # ëª¨ë¸ 2 í‰ê°€
    print("\nğŸ“Š ëª¨ë¸ 2: ImprovedPINN í‰ê°€")
    model2 = ImprovedPINN(config)
    model2.compile(optimizer='adam', loss='mse')
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ ëª¨ë¸ ë¹Œë“œ
    dummy_seq = np.zeros((1, 20, n_features))
    dummy_physics = np.zeros((1, 3))
    _ = model2([dummy_seq, dummy_physics])
    
    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    model2.load_weights('./checkpoints/model2_final.h5')
    
    y_pred2_scaled = model2.predict([X_test_scaled, X_physics_test_scaled], verbose=0)
    y_pred2 = processor.scaler_y.inverse_transform(y_pred2_scaled.reshape(-1, 1)).flatten()
    
    mae2 = np.mean(np.abs(y_test - y_pred2))
    print(f"  ì „ì²´ MAE: {mae2:.2f}")
    
    if mask_310.sum() > 0:
        mae_310 = np.mean(np.abs(y_test[mask_310] - y_pred2[mask_310]))
        detected = (y_pred2 >= 310)[mask_310].sum()
        print(f"  310+ MAE: {mae_310:.2f}")
        print(f"  310+ ê°ì§€: {detected}/{mask_310.sum()} ({detected/mask_310.sum()*100:.1f}%)")
    
    print("\nâœ… ì™„ë£Œ!")

if __name__ == "__main__":
    main()