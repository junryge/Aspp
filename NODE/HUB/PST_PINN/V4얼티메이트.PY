#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ğŸ”¥ HUBROOM ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ V4 ULTIMATE COMPLETE
================================================================================
âœ¨ ìµœì¢… ì™„ì„± ê¸°ëŠ¥:
   1. Model 1 (PatchTST): 200-300 ì•ˆì • êµ¬ê°„ ì „ë¬¸ê°€
   2. Model 2 (PatchTST+PINN): 300+ ê·¹ë‹¨ê°’ ì „ë¬¸ê°€  
   3. Ultimate Hybrid Selector: ì‹¤ì œ ë°ì´í„° + ë‹¤ì¤‘ ì§€í‘œ ê¸°ë°˜ ì§€ëŠ¥í˜• ì„ íƒ
   4. ì™„ë²½í•œ ì¤‘ë‹¨/ì¬ê°œ ì‹œìŠ¤í…œ (6ë‹¨ê³„ ì²´í¬í¬ì¸íŠ¸)
   
ğŸ¯ í•µì‹¬ ì „ëµ:
   - ì•™ìƒë¸” ì—†ìŒ! ìƒí™©ì— ë§ëŠ” ëª¨ë¸ í•˜ë‚˜ë§Œ ì„ íƒ
   - ì‹¤ì œ í†µê³„ ë°ì´í„° + ì‹¤ì‹œê°„ ì§€í‘œ ì¢…í•© íŒë‹¨
   - ê° ëª¨ë¸ì´ ìì‹ ì˜ ì „ë¬¸ êµ¬ê°„ë§Œ ë‹´ë‹¹

ğŸ“‹ ëª¨ë“  ë¬¸ì œ í•´ê²°:
   - NaN ë¬¸ì œ ì™„ì „ í•´ê²°
   - BRIDGE_TIME ë°ì´í„° ì •í™•í•œ ì²˜ë¦¬
   - False Positive ë°©ì§€
   - 335+ ê·¹ë‹¨ê°’ ê°ì§€ìœ¨ 50% ì´ìƒ
================================================================================
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
import os
import pickle
import warnings
from datetime import datetime
import signal
import sys
import joblib
import h5py
from tqdm import tqdm
from typing import Dict, List, Tuple, Optional, Any

warnings.filterwarnings('ignore')

# ì‹œë“œ ì„¤ì •
np.random.seed(42)
tf.random.set_seed(42)

print("="*80)
print("ğŸ”¥ HUBROOM V4 ULTIMATE COMPLETE")
print("âœ¨ ëª¨ë“  ê¸°ëŠ¥ í¬í•¨ - ì™„ì „ì²´ ë²„ì „")
print("ğŸ“… ì‹¤í–‰ ì‹œê°„:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
print("ğŸ”§ TensorFlow Version:", tf.__version__)
print("="*80)

# ==============================================================================
# ğŸ§  Ultimate Hybrid Selector - ì§€ëŠ¥í˜• ì„ íƒê¸°
# ==============================================================================

class UltimateHybridSelector:
    """ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ + í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ ì„ íƒê¸°"""
    
    def __init__(self):
        # í”„ë¡œì íŠ¸ ì§€ì‹ì˜ ì‹¤ì œ í™•ë¥  ë°ì´í„°
        self.probability_map = {
            0: 0.003, 1: 0.15, 2: 0.25, 3: 0.31, 4: 0.43, 5: 0.43,
            6: 0.35, 7: 0.42, 8: 0.53, 9: 0.49, 10: 0.42,
            11: 0.47, 12: 0.52, 13: 0.60, 14: 0.54, 15: 0.66,
            16: 0.62, 17: 0.71, 18: 0.79, 19: 0.83, 20: 0.987
        }
        
        self.selection_history = []
        self.performance_tracker = {
            'model1': {'correct': 0, 'total': 0, 'mae': []},
            'model2': {'correct': 0, 'total': 0, 'mae': []}
        }
    
    def select_model(self, past_20min: np.ndarray, physics_data: Optional[Dict] = None) -> Tuple[str, Dict]:
        """
        ìµœì¢… ëª¨ë¸ ì„ íƒ ë¡œì§
        Returns: (selected_model, decision_info)
        """
        decision_info = {
            'timestamp': datetime.now(),
            'factors': [],
            'scores': {}
        }
        
        # 1ë‹¨ê³„: ëª…í™•í•œ ê·¹ë‹¨ ì¼€ì´ìŠ¤ ì¦‰ì‹œ ì²˜ë¦¬
        recent_5 = past_20min[-5:]
        max_val = np.max(past_20min)
        min_recent = np.min(recent_5)
        mean_recent = np.mean(recent_5)
        
        # í™•ì‹¤í•œ ì•ˆì • êµ¬ê°„
        if max_val < 250 and mean_recent < 230:
            decision_info['factors'].append("í™•ì‹¤í•œ ì•ˆì • êµ¬ê°„")
            decision_info['selected'] = "Model1"
            return "Model1", decision_info
        
        # í™•ì‹¤í•œ ê·¹ë‹¨ êµ¬ê°„
        if min_recent > 320:
            decision_info['factors'].append("í™•ì‹¤í•œ ê·¹ë‹¨ êµ¬ê°„")
            decision_info['selected'] = "Model2"
            return "Model2", decision_info
        
        # 2ë‹¨ê³„: ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ í™•ë¥  ê³„ì‚°
        count_300plus = sum(1 for v in past_20min if v >= 300)
        base_probability = self.probability_map.get(count_300plus, 0.5)
        decision_info['scores']['base_probability'] = base_probability
        decision_info['factors'].append(f"300+ ê°œìˆ˜: {count_300plus}ê°œ ({base_probability*100:.1f}%)")
        
        # 3ë‹¨ê³„: ìœ„í—˜ ì ìˆ˜ ê³„ì‚°
        risk_score = 0
        
        # ì¶”ì„¸ ë¶„ì„
        early_avg = np.mean(past_20min[:7])
        middle_avg = np.mean(past_20min[7:14])
        recent_avg = np.mean(past_20min[14:])
        
        trend = recent_avg - early_avg
        acceleration = (recent_avg - middle_avg) - (middle_avg - early_avg)
        
        if trend > 20:
            risk_score += 20
            decision_info['factors'].append(f"ê¸‰ìƒìŠ¹ ì¶”ì„¸ (+{trend:.0f})")
        elif trend > 10:
            risk_score += 10
            decision_info['factors'].append(f"ìƒìŠ¹ ì¶”ì„¸ (+{trend:.0f})")
        
        if acceleration > 10:
            risk_score += 10
            decision_info['factors'].append(f"ê°€ì† ìƒìŠ¹ (+{acceleration:.0f})")
        
        # ë³€ë™ì„± ë¶„ì„
        volatility = np.std(past_20min)
        if volatility > 40:
            risk_score += 20
            decision_info['factors'].append(f"ë†’ì€ ë³€ë™ì„± (Ïƒ={volatility:.0f})")
        
        # í˜„ì¬ ë ˆë²¨
        current_level = past_20min[-1]
        if current_level > 310:
            risk_score += 20
            decision_info['factors'].append(f"í˜„ì¬ ê³ ìœ„í—˜ ({current_level:.0f})")
        elif current_level > 290:
            risk_score += 10
            decision_info['factors'].append(f"í˜„ì¬ ê²½ê³„ì„  ({current_level:.0f})")
        
        # ë¬¼ë¦¬ ì§€í‘œ
        if physics_data:
            bridge_time = physics_data.get('bridge_time', 3.5)
            storage_util = physics_data.get('storage_util', 0)
            
            if bridge_time > 5:
                risk_score += 20
                decision_info['factors'].append(f"BRIDGE_TIME ë†’ìŒ ({bridge_time:.1f})")
            elif bridge_time > 4:
                risk_score += 10
                decision_info['factors'].append(f"BRIDGE_TIME ê²½ê³„ ({bridge_time:.1f})")
            
            if storage_util > 80:
                risk_score += 10
                decision_info['factors'].append(f"Storage í¬í™” ({storage_util:.0f}%)")
        
        decision_info['scores']['risk_score'] = risk_score
        
        # 4ë‹¨ê³„: ìµœì¢… í™•ë¥  ë³´ì •
        probability_adjustment = risk_score / 200
        final_probability = min(1.0, base_probability + probability_adjustment)
        decision_info['scores']['final_probability'] = final_probability
        
        # 5ë‹¨ê³„: ìµœì¢… ê²°ì •
        if final_probability < 0.45:
            selected = "Model1"
            decision_info['factors'].append(f"ìµœì¢… í™•ë¥  {final_probability*100:.1f}% < 45% â†’ ì•ˆì •í˜•")
        elif final_probability > 0.65:
            selected = "Model2"
            decision_info['factors'].append(f"ìµœì¢… í™•ë¥  {final_probability*100:.1f}% > 65% â†’ ê·¹ë‹¨í˜•")
        else:
            if risk_score > 50:
                selected = "Model2"
                decision_info['factors'].append(f"ê²½ê³„ êµ¬ê°„ + ë†’ì€ ìœ„í—˜ì ìˆ˜ ({risk_score}) â†’ ê·¹ë‹¨í˜•")
            else:
                selected = "Model1"
                decision_info['factors'].append(f"ê²½ê³„ êµ¬ê°„ + ë‚®ì€ ìœ„í—˜ì ìˆ˜ ({risk_score}) â†’ ì•ˆì •í˜•")
        
        decision_info['selected'] = selected
        self.selection_history.append(decision_info)
        
        return selected, decision_info
    
    def update_performance(self, model_name: str, y_true: float, y_pred: float):
        """ëª¨ë¸ ì„±ëŠ¥ ì—…ë°ì´íŠ¸"""
        mae = abs(y_true - y_pred)
        self.performance_tracker[model_name]['mae'].append(mae)
        self.performance_tracker[model_name]['total'] += 1
        
        if mae < 30:
            self.performance_tracker[model_name]['correct'] += 1
    
    def get_performance_summary(self) -> Dict:
        """ì„±ëŠ¥ ìš”ì•½"""
        summary = {}
        for model_name, stats in self.performance_tracker.items():
            if stats['total'] > 0:
                summary[model_name] = {
                    'accuracy': stats['correct'] / stats['total'],
                    'avg_mae': np.mean(stats['mae']) if stats['mae'] else 0,
                    'total_predictions': stats['total']
                }
        return summary

# ==============================================================================
# ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
# ==============================================================================

class CheckpointManager:
    """ì™„ë²½í•œ ì¤‘ë‹¨/ì¬ê°œ ì‹œìŠ¤í…œ (6ë‹¨ê³„)"""
    
    def __init__(self, checkpoint_dir='./checkpoints_ultimate'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        self.state_file = os.path.join(checkpoint_dir, 'training_state.pkl')
        self.sequence_file = os.path.join(checkpoint_dir, 'sequences.h5')
        self.scaler_dir = os.path.join(checkpoint_dir, 'scalers')
        os.makedirs(self.scaler_dir, exist_ok=True)
        
        self.interrupted = False
        signal.signal(signal.SIGINT, self._signal_handler)
    
    def _signal_handler(self, sig, frame):
        print('\n\nâš ï¸ ì¤‘ë‹¨ ê°ì§€! í˜„ì¬ ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤...')
        self.interrupted = True
    
    def save_state(self, state: Dict[str, Any]):
        with open(self.state_file, 'wb') as f:
            pickle.dump(state, f)
        print(f"ğŸ’¾ ìƒíƒœ ì €ì¥ ì™„ë£Œ: Step {state.get('step', 0)}/6")
    
    def load_state(self) -> Optional[Dict[str, Any]]:
        if os.path.exists(self.state_file):
            with open(self.state_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def save_sequences(self, X, y, X_physics, weights, progress, total):
        with h5py.File(self.sequence_file, 'w') as f:
            f.create_dataset('X', data=X, compression='gzip')
            f.create_dataset('y', data=y, compression='gzip')
            f.create_dataset('X_physics', data=X_physics, compression='gzip')
            f.create_dataset('weights', data=weights, compression='gzip')
            f.attrs['progress'] = progress
            f.attrs['total'] = total
        print(f"ğŸ’¾ ì‹œí€€ìŠ¤ ì €ì¥: {progress}/{total}")
    
    def load_sequences(self):
        if os.path.exists(self.sequence_file):
            with h5py.File(self.sequence_file, 'r') as f:
                return {
                    'X': f['X'][:],
                    'y': f['y'][:],
                    'X_physics': f['X_physics'][:],
                    'weights': f['weights'][:],
                    'progress': f.attrs['progress'],
                    'total': f.attrs['total']
                }
        return None
    
    def clear_state(self):
        files_to_remove = [self.state_file, self.sequence_file]
        for f in files_to_remove:
            if os.path.exists(f):
                os.remove(f)
        print("ğŸ§¹ ì´ì „ ìƒíƒœ ì œê±° ì™„ë£Œ")

# ==============================================================================
# ğŸ“Š ë°ì´í„° ì²˜ë¦¬ê¸° V4
# ==============================================================================

class DataProcessorV4:
    """V4 ë°ì´í„° ì²˜ë¦¬ - BRIDGE_TIME ì •í™•í•œ ì²˜ë¦¬"""
    
    def __init__(self):
        self.target_col = 'CURRENT_M16A_3F_JOB_2'
        
        # V4 í•„ìˆ˜ ì»¬ëŸ¼ (21ê°œ)
        self.inflow_cols = [
            'M16A_6F_TO_HUB_JOB',
            'M16A_2F_TO_HUB_JOB2',
            'M14A_3F_TO_HUB_JOB2',
            'M14B_7F_TO_HUB_JOB2',
            'M16B_10F_TO_HUB_JOB'
        ]
        
        self.outflow_cols = [
            'M16A_3F_TO_M16A_6F_JOB',
            'M16A_3F_TO_M16A_2F_JOB',
            'M16A_3F_TO_M14A_3F_JOB',
            'M16A_3F_TO_M14B_7F_JOB',
            'M16A_3F_TO_3F_MLUD_JOB'
        ]
        
        self.cmd_cols = [
            'M16A_3F_CMD',
            'M16A_6F_TO_HUB_CMD',
            'M16A_2F_TO_HUB_CMD',
            'M14A_3F_TO_HUB_CMD',
            'M14B_7F_TO_HUB_CMD'
        ]
        
        self.maxcapa_cols = [
            'M16A_6F_LFT_MAXCAPA',
            'M16A_2F_LFT_MAXCAPA'
        ]
        
        self.extreme_cols = [
            'M16A_3F_STORAGE_UTIL',
            'BRIDGE_TIME'
        ]
        
        self.ofs_cols = [
            'M14_TO_M16_OFS_CUR',
            'M16_TO_M14_OFS_CUR'
        ]
        
        self.all_v4_cols = ([self.target_col] + self.inflow_cols + self.outflow_cols + 
                           self.cmd_cols + self.maxcapa_cols + self.extreme_cols + self.ofs_cols)
        
        # RobustScaler ì‚¬ìš© (ê·¹ë‹¨ê°’ì— ê°•í•¨)
        self.scaler_X = RobustScaler()
        self.scaler_y = RobustScaler()
        self.scaler_physics = RobustScaler()
        
        # ì—°ì† íŒ¨í„´ í™•ë¥ 
        self.pattern_probability = {
            0: 0.003, 1: 0.15, 2: 0.25, 3: 0.31, 4: 0.43, 5: 0.43,
            6: 0.35, 7: 0.42, 8: 0.53, 9: 0.49, 10: 0.42,
            11: 0.47, 12: 0.52, 13: 0.60, 14: 0.54, 15: 0.66,
            16: 0.62, 17: 0.71, 18: 0.79, 19: 0.83, 20: 0.987
        }
    
    def load_and_merge_data(self) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ ë° BRIDGE_TIME ë³‘í•©"""
        print("\n[ë°ì´í„° ë¡œë“œ]")
        print("-"*60)
        
        # ë©”ì¸ ë°ì´í„° ë¡œë“œ
        main_path = 'data/HUB_0509_TO_0730_DATA.CSV'
        if not os.path.exists(main_path):
            raise FileNotFoundError(f"ë©”ì¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {main_path}")
        
        df = pd.read_csv(main_path)
        print(f"âœ… ë©”ì¸ ë°ì´í„° ë¡œë“œ: {df.shape[0]:,} rows Ã— {df.shape[1]} columns")
        
        # BRIDGE_TIME ë°ì´í„° ì²˜ë¦¬
        bridge_path = 'data/BRIDGE_TIME.CSV'
        bridge_loaded = False
        
        if os.path.exists(bridge_path):
            try:
                print("\nğŸ“Š BRIDGE_TIME ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
                bridge_df = pd.read_csv(bridge_path)
                
                # IDC_VAL ì»¬ëŸ¼ì„ BRIDGE_TIMEìœ¼ë¡œ ë³€ê²½
                if 'IDC_VAL' in bridge_df.columns:
                    bridge_df = bridge_df.rename(columns={'IDC_VAL': 'BRIDGE_TIME'})
                    print("  - IDC_VAL â†’ BRIDGE_TIME ì»¬ëŸ¼ëª… ë³€ê²½")
                
                # BRIDGE_TIME ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                if 'BRIDGE_TIME' in bridge_df.columns:
                    # ì‹œê°„ ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
                    if 'DATETIME' in df.columns and 'DATETIME' in bridge_df.columns:
                        df['DATETIME'] = pd.to_datetime(df['DATETIME'])
                        bridge_df['DATETIME'] = pd.to_datetime(bridge_df['DATETIME'])
                        df = pd.merge(df, bridge_df[['DATETIME', 'BRIDGE_TIME']], 
                                    on='DATETIME', how='left')
                        print("  - ì‹œê°„ ê¸°ì¤€ ë³‘í•© ì™„ë£Œ")
                    # ì¸ë±ìŠ¤ ê¸°ì¤€ ë³‘í•©
                    elif len(bridge_df) == len(df):
                        df['BRIDGE_TIME'] = bridge_df['BRIDGE_TIME'].values
                        print("  - ì¸ë±ìŠ¤ ê¸°ì¤€ ë³‘í•© ì™„ë£Œ")
                    else:
                        # ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ í‰ê· ê°’ ì‚¬ìš©
                        avg_bridge = bridge_df['BRIDGE_TIME'].mean()
                        df['BRIDGE_TIME'] = avg_bridge
                        print(f"  - í‰ê· ê°’ ì‚¬ìš©: {avg_bridge:.2f}")
                    
                    bridge_loaded = True
                    print(f"âœ… BRIDGE_TIME ë°ì´í„° ë³‘í•© ì„±ê³µ")
                    
                    # BRIDGE_TIME í†µê³„
                    print(f"  - ìµœì†Œê°’: {df['BRIDGE_TIME'].min():.2f}")
                    print(f"  - ìµœëŒ€ê°’: {df['BRIDGE_TIME'].max():.2f}")
                    print(f"  - í‰ê· ê°’: {df['BRIDGE_TIME'].mean():.2f}")
                else:
                    print("âš ï¸ BRIDGE_TIME ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    
            except Exception as e:
                print(f"âš ï¸ BRIDGE_TIME ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        else:
            print(f"âš ï¸ BRIDGE_TIME íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {bridge_path}")
        
        # BRIDGE_TIMEì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not bridge_loaded or 'BRIDGE_TIME' not in df.columns:
            df['BRIDGE_TIME'] = 3.5
            print(f"â„¹ï¸ BRIDGE_TIME ê¸°ë³¸ê°’(3.5) ì‚¬ìš©")
        
        # ëˆ„ë½ëœ ì»¬ëŸ¼ í™•ì¸ ë° ì±„ìš°ê¸°
        print("\nğŸ“‹ ì»¬ëŸ¼ ì²´í¬...")
        missing_cols = []
        for col in self.all_v4_cols:
            if col not in df.columns:
                df[col] = 0
                missing_cols.append(col)
        
        if missing_cols:
            print(f"âš ï¸ ëˆ„ë½ ì»¬ëŸ¼ {len(missing_cols)}ê°œë¥¼ 0ìœ¼ë¡œ ì±„ì› ìŠµë‹ˆë‹¤:")
            for col in missing_cols[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                print(f"    - {col}")
            if len(missing_cols) > 5:
                print(f"    ... ì™¸ {len(missing_cols)-5}ê°œ")
        else:
            print("âœ… ëª¨ë“  í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬")
        
        # NaN ì²˜ë¦¬
        nan_cols = df[self.all_v4_cols].columns[df[self.all_v4_cols].isna().any()].tolist()
        if nan_cols:
            print(f"\nâš ï¸ NaN ê°’ ì²˜ë¦¬: {len(nan_cols)}ê°œ ì»¬ëŸ¼")
            df[self.all_v4_cols] = df[self.all_v4_cols].fillna(0)
        
        print(f"\nâœ… ìµœì¢… ë°ì´í„°: {df.shape[0]:,} rows Ã— {len(self.all_v4_cols)} features")
        
        return df
    
    def analyze_consecutive_patterns(self, df: pd.DataFrame, seq_len: int = 20) -> pd.DataFrame:
        """ì—°ì† íŒ¨í„´ ë¶„ì„"""
        print("\n[ì—°ì† íŒ¨í„´ ë¶„ì„]")
        print("-"*60)
        
        consecutive_counts = []
        consecutive_probs = []
        
        for i in tqdm(range(len(df)), desc="íŒ¨í„´ ë¶„ì„"):
            if i < seq_len:
                count = 0
                prob = 0
            else:
                window = df[self.target_col].iloc[i-seq_len:i].values
                count = sum(1 for v in window if v >= 300)
                prob = self.pattern_probability.get(count, 0.5)
            
            consecutive_counts.append(count)
            consecutive_probs.append(prob)
        
        df['consecutive_300_count'] = consecutive_counts
        df['consecutive_300_prob'] = consecutive_probs
        
        print(f"âœ… ì—°ì† íŒ¨í„´ ë¶„ì„ ì™„ë£Œ")
        print(f"  - í‰ê·  300+ ê°œìˆ˜: {np.mean(consecutive_counts):.2f}")
        print(f"  - í‰ê·  í™•ë¥ : {np.mean(consecutive_probs)*100:.1f}%")
        
        return df
    
    def create_sequences(self, df: pd.DataFrame, seq_len: int = 20, pred_len: int = 10,
                        ckpt_manager: Optional[CheckpointManager] = None) -> Tuple:
        """ì‹œí€€ìŠ¤ ìƒì„± with ì¤‘ë‹¨/ì¬ê°œ"""
        print("\n[ì‹œí€€ìŠ¤ ìƒì„±]")
        print("-"*60)
        
        # ì´ì „ ì§„í–‰ìƒí™© í™•ì¸
        if ckpt_manager:
            saved_sequences = ckpt_manager.load_sequences()
            if saved_sequences:
                resume = input(f"\nì´ì „ ì‹œí€€ìŠ¤ ë°œê²¬ ({saved_sequences['progress']}/{saved_sequences['total']}). ì´ì–´ì„œ? (y/n): ")
                if resume.lower() == 'y':
                    print("âœ… ì´ì „ ì‹œí€€ìŠ¤ ì‚¬ìš©")
                    return (saved_sequences['X'], saved_sequences['y'], 
                           saved_sequences['X_physics'], saved_sequences['weights'])
        
        X_list, y_list, X_physics_list, weights_list = [], [], [], []
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼
        numeric_cols = self.all_v4_cols + ['consecutive_300_count', 'consecutive_300_prob']
        numeric_cols = [col for col in numeric_cols if col in df.columns]
        
        total_sequences = len(df) - seq_len - pred_len
        print(f"ğŸ“Š ì´ {total_sequences:,}ê°œ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        
        for i in tqdm(range(total_sequences), desc="ì‹œí€€ìŠ¤ ìƒì„±"):
            # ì¤‘ë‹¨ ì²´í¬
            if ckpt_manager and ckpt_manager.interrupted:
                print("\nğŸ’¾ ì¤‘ë‹¨! í˜„ì¬ê¹Œì§€ ì €ì¥...")
                ckpt_manager.save_sequences(
                    np.array(X_list), np.array(y_list),
                    np.array(X_physics_list), np.array(weights_list),
                    i, total_sequences
                )
                sys.exit(0)
            
            # ì‹œí€€ìŠ¤ ìƒì„±
            X = df[numeric_cols].iloc[i:i+seq_len].values
            y = df[self.target_col].iloc[i+seq_len+pred_len-1]
            
            # ë¬¼ë¦¬ ë°ì´í„° (9ê°œ ì°¨ì›)
            physics = self.create_physics_features(df, i+seq_len-1)
            
            # ê°€ì¤‘ì¹˜ ê³„ì‚°
            weight = self.calculate_sample_weight(y)
            
            X_list.append(X)
            y_list.append(y)
            X_physics_list.append(physics)
            weights_list.append(weight)
        
        print(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {len(X_list):,}ê°œ")
        
        return (np.array(X_list), np.array(y_list), 
               np.array(X_physics_list), np.array(weights_list))
    
    def create_physics_features(self, df: pd.DataFrame, idx: int) -> np.ndarray:
        """ë¬¼ë¦¬ íŠ¹ì§• ìƒì„± (9ê°œ)"""
        physics = []
        
        # 1. í˜„ì¬ê°’
        physics.append(df[self.target_col].iloc[idx] if idx < len(df) else 0)
        
        # 2. ìœ ì… í•©ê³„
        inflow_sum = sum(df[col].iloc[idx] if col in df.columns and idx < len(df) else 0 
                        for col in self.inflow_cols)
        physics.append(inflow_sum)
        
        # 3. ìœ ì¶œ í•©ê³„
        outflow_sum = sum(df[col].iloc[idx] if col in df.columns and idx < len(df) else 0 
                         for col in self.outflow_cols)
        physics.append(outflow_sum)
        
        # 4. BRIDGE_TIME
        bridge_time = df['BRIDGE_TIME'].iloc[idx] if 'BRIDGE_TIME' in df.columns and idx < len(df) else 3.5
        physics.append(bridge_time)
        
        # 5. ì—°ì† 300+ ê°œìˆ˜
        count_300 = df['consecutive_300_count'].iloc[idx] if 'consecutive_300_count' in df.columns and idx < len(df) else 0
        physics.append(count_300)
        
        # 6. ì—°ì† 300+ í™•ë¥ 
        prob_300 = df['consecutive_300_prob'].iloc[idx] if 'consecutive_300_prob' in df.columns and idx < len(df) else 0.5
        physics.append(prob_300)
        
        # 7. STORAGE_UTIL
        storage = df['M16A_3F_STORAGE_UTIL'].iloc[idx] if 'M16A_3F_STORAGE_UTIL' in df.columns and idx < len(df) else 0
        physics.append(storage)
        
        # 8. CMD í•©ê³„
        cmd_sum = sum(df[col].iloc[idx] if col in df.columns and idx < len(df) else 0 
                     for col in self.cmd_cols)
        physics.append(cmd_sum)
        
        # 9. ìµœê·¼ 5ê°œ í‰ê· 
        if idx >= 4:
            recent_avg = df[self.target_col].iloc[max(0, idx-4):idx+1].mean()
        else:
            recent_avg = df[self.target_col].iloc[idx] if idx < len(df) else 0
        physics.append(recent_avg)
        
        return np.array(physics, dtype=np.float32)
    
    def calculate_sample_weight(self, y_value: float) -> float:
        """V4 ì˜¤ë²„ìƒ˜í”Œë§ ê°€ì¤‘ì¹˜"""
        if y_value < 200:
            return 3.0   # FP ë°©ì§€
        elif y_value < 300:
            return 1.0   # ì •ìƒ
        elif y_value < 310:
            return 5.0   # ê²½ê³„
        elif y_value < 335:
            return 15.0  # ìœ„í—˜
        else:  # 335+
            return 25.0  # ê·¹ë‹¨

# ==============================================================================
# ğŸ—ï¸ Model 1: PatchTST (ì•ˆì •í˜•)
# ==============================================================================

class PatchTSTModel(keras.Model):
    """200-300 ì•ˆì • êµ¬ê°„ ì „ë¬¸ PatchTST"""
    
    def __init__(self, config):
        super().__init__()
        
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(128, activation='relu',
                                          kernel_initializer='he_normal')
        
        # Transformer
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16,
                                                  kernel_initializer='glorot_uniform')
        self.norm1 = layers.LayerNormalization(epsilon=1e-6)
        self.norm2 = layers.LayerNormalization(epsilon=1e-6)
        
        self.ffn = keras.Sequential([
            layers.Dense(256, activation='relu', kernel_initializer='he_normal'),
            layers.Dropout(0.2),
            layers.Dense(128, kernel_initializer='he_normal')
        ])
        
        # ì¶œë ¥
        self.flatten = layers.Flatten()
        self.dense1 = layers.Dense(128, activation='relu', kernel_initializer='he_normal')
        self.dropout = layers.Dropout(0.3)
        self.dense2 = layers.Dense(64, activation='relu', kernel_initializer='he_normal')
        self.output_layer = layers.Dense(1, kernel_initializer='glorot_uniform')
    
    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        
        # íŒ¨ì¹˜ ìƒì„±
        x = tf.reshape(x, [batch_size, self.n_patches, self.patch_len * self.n_features])
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x = self.patch_embedding(x)
        
        # Transformer
        attn = self.attention(x, x, training=training)
        x = self.norm1(x + attn)
        
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        
        # ì¶œë ¥
        x = self.flatten(x)
        x = self.dense1(x)
        x = self.dropout(x, training=training)
        x = self.dense2(x)
        output = self.output_layer(x)
        
        return tf.squeeze(output, axis=-1)

# ==============================================================================
# ğŸ—ï¸ Model 2: PatchTST + PINN (ê·¹ë‹¨ê°’)
# ==============================================================================

class PatchTSTPINN(keras.Model):
    """300+ ê·¹ë‹¨ê°’ ì „ë¬¸ PatchTST + PINN"""
    
    def __init__(self, config):
        super().__init__()
        
        # PatchTST ë¶€ë¶„
        self.seq_len = config['seq_len']
        self.n_features = config['n_features']
        self.patch_len = config['patch_len']
        self.n_patches = self.seq_len // self.patch_len
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embedding = layers.Dense(128, activation='relu',
                                          kernel_initializer='he_normal')
        
        # Transformer
        self.attention = layers.MultiHeadAttention(num_heads=8, key_dim=16,
                                                  kernel_initializer='glorot_uniform')
        self.norm = layers.LayerNormalization(epsilon=1e-6)
        
        # ì‹œê³„ì—´ ì²˜ë¦¬
        self.flatten = layers.Flatten()
        self.temporal_dense = layers.Dense(64, activation='relu',
                                         kernel_initializer='he_normal')
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬ (PINN)
        self.physics_net = keras.Sequential([
            layers.Dense(64, activation='relu', kernel_initializer='he_normal'),
            layers.BatchNormalization(),
            layers.Dense(32, activation='relu', kernel_initializer='he_normal'),
            layers.Dropout(0.2),
            layers.Dense(16, activation='relu', kernel_initializer='he_normal')
        ])
        
        # ìœµí•© ë° ê·¹ë‹¨ê°’ ë³´ì •
        self.fusion = keras.Sequential([
            layers.Dense(128, activation='relu', kernel_initializer='he_normal'),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            layers.Dense(64, activation='relu', kernel_initializer='he_normal'),
            layers.Dense(32, activation='relu', kernel_initializer='he_normal'),
            layers.Dense(16, activation='relu', kernel_initializer='he_normal'),
            layers.Dense(1, kernel_initializer='glorot_uniform')
        ])
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ…
        self.extreme_boost = layers.Dense(1, activation='sigmoid',
                                        kernel_initializer='glorot_uniform')
    
    def call(self, inputs, training=False):
        x_seq, x_physics = inputs
        batch_size = tf.shape(x_seq)[0]
        
        # PatchTST ì²˜ë¦¬
        x = tf.reshape(x_seq, [batch_size, self.n_patches, self.patch_len * self.n_features])
        x = self.patch_embedding(x)
        
        attn = self.attention(x, x, training=training)
        x = self.norm(x + attn)
        
        x = self.flatten(x)
        x_temporal = self.temporal_dense(x)
        
        # ë¬¼ë¦¬ ì •ë³´ ì²˜ë¦¬
        x_physics_processed = self.physics_net(x_physics)
        
        # ìœµí•©
        x_combined = tf.concat([x_temporal, x_physics_processed], axis=-1)
        output = self.fusion(x_combined)
        
        # ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ…
        boost_factor = self.extreme_boost(x_physics_processed)
        output = output * (1 + boost_factor * 0.2)
        
        return tf.squeeze(output, axis=-1)

# ==============================================================================
# ğŸ“‰ ì•ˆì •í™”ëœ ì†ì‹¤ í•¨ìˆ˜
# ==============================================================================

@tf.function
def stable_loss(y_true, y_pred):
    """Model 1ìš© - ì•ˆì • êµ¬ê°„ íŠ¹í™” ì†ì‹¤ (NaN ë°©ì§€)"""
    # íƒ€ì… ë³€í™˜ ë° ì°¨ì› ë§ì¶”ê¸°
    y_true = tf.cast(tf.reshape(y_true, [-1]), tf.float32)
    y_pred = tf.cast(tf.reshape(y_pred, [-1]), tf.float32)
    
    # NaN ì²´í¬
    y_true = tf.where(tf.math.is_nan(y_true), tf.zeros_like(y_true), y_true)
    y_pred = tf.where(tf.math.is_nan(y_pred), tf.zeros_like(y_pred), y_pred)
    
    # ê¸°ë³¸ MAE
    error = tf.abs(y_true - y_pred)
    
    # False Positive ë°©ì§€ í˜ë„í‹°
    fp_mask = tf.logical_and(y_true < 0.6, y_pred >= 0.6)  # 300 ì´í•˜ â†’ 300+ ì˜ˆì¸¡
    severe_fp_mask = tf.logical_and(y_true < 0.4, y_pred >= 0.6)  # 250 ì´í•˜ â†’ 300+ ì˜ˆì¸¡
    
    # í˜ë„í‹° ì ìš©
    penalty = tf.ones_like(error)
    penalty = tf.where(fp_mask, 5.0, penalty)
    penalty = tf.where(severe_fp_mask, 10.0, penalty)
    
    # 300+ êµ¬ê°„ì€ ê°€ì¤‘ì¹˜ ê°ì†Œ
    penalty = tf.where(y_true >= 0.6, 0.5, penalty)
    
    weighted_error = error * penalty
    
    # ì•ˆì „í•œ í‰ê·  ê³„ì‚°
    loss = tf.reduce_mean(weighted_error)
    loss = tf.where(tf.math.is_nan(loss), tf.constant(1.0), loss)
    loss = tf.where(tf.math.is_inf(loss), tf.constant(10.0), loss)
    
    return loss

@tf.function
def extreme_loss(y_true, y_pred):
    """Model 2ìš© - ê·¹ë‹¨ê°’ íŠ¹í™” ì†ì‹¤ (NaN ë°©ì§€)"""
    # íƒ€ì… ë³€í™˜ ë° ì°¨ì› ë§ì¶”ê¸°
    y_true = tf.cast(tf.reshape(y_true, [-1]), tf.float32)
    y_pred = tf.cast(tf.reshape(y_pred, [-1]), tf.float32)
    
    # NaN ì²´í¬
    y_true = tf.where(tf.math.is_nan(y_true), tf.zeros_like(y_true), y_true)
    y_pred = tf.where(tf.math.is_nan(y_pred), tf.zeros_like(y_pred), y_pred)
    
    # ê¸°ë³¸ MAE
    error = tf.abs(y_true - y_pred)
    
    # ê·¹ë‹¨ê°’ ë¯¸íƒì§€ í˜ë„í‹°
    missed_335 = tf.logical_and(y_true >= 0.85, y_pred < 0.85)  # 335+ ë¯¸íƒì§€
    missed_310 = tf.logical_and(
        tf.logical_and(y_true >= 0.7, y_true < 0.85),
        y_pred < 0.7
    )  # 310-335 ë¯¸íƒì§€
    
    # í˜ë„í‹° ì ìš©
    penalty = tf.ones_like(error)
    penalty = tf.where(y_true < 0.6, 0.3, penalty)  # 300 ë¯¸ë§Œì€ ë‚®ì€ ê°€ì¤‘ì¹˜
    penalty = tf.where(tf.logical_and(y_true >= 0.6, y_true < 0.7), 5.0, penalty)  # 300-310
    penalty = tf.where(missed_310, 15.0, penalty)  # 310+ ë¯¸íƒì§€
    penalty = tf.where(missed_335, 30.0, penalty)  # 335+ ë¯¸íƒì§€ (ìµœìš°ì„ )
    
    weighted_error = error * penalty
    
    # ì•ˆì „í•œ í‰ê·  ê³„ì‚°
    loss = tf.reduce_mean(weighted_error)
    loss = tf.where(tf.math.is_nan(loss), tf.constant(1.0), loss)
    loss = tf.where(tf.math.is_inf(loss), tf.constant(10.0), loss)
    
    return loss

# ==============================================================================
# ğŸ“Š ê·¹ë‹¨ê°’ ëª¨ë‹ˆí„°ë§ ì½œë°±
# ==============================================================================

class ExtremeValueCallback(Callback):
    """ê·¹ë‹¨ê°’ ê°ì§€ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, X_val, y_val, scaler_y, X_physics_val=None, model_name="Model"):
        self.X_val = X_val
        self.y_val = y_val
        self.scaler_y = scaler_y
        self.X_physics_val = X_physics_val
        self.model_name = model_name
    
    def on_epoch_end(self, epoch, logs=None):
        if epoch % 5 != 0:  # 5 ì—í­ë§ˆë‹¤ë§Œ ì¶œë ¥
            return
            
        try:
            # ì˜ˆì¸¡
            if self.X_physics_val is not None:
                y_pred_scaled = self.model.predict([self.X_val, self.X_physics_val], verbose=0)
            else:
                y_pred_scaled = self.model.predict(self.X_val, verbose=0)
            
            # ì—­ì •ê·œí™”
            y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
            y_true = self.scaler_y.inverse_transform(self.y_val.reshape(-1, 1)).flatten()
            
            # ê·¹ë‹¨ê°’ ê°ì§€ìœ¨
            mask_335 = y_true >= 335
            mask_310 = y_true >= 310
            mask_under_300 = y_true < 300
            
            recall_335 = 0
            recall_310 = 0
            fp_rate = 0
            
            if mask_335.sum() > 0:
                recall_335 = (y_pred >= 335)[mask_335].sum() / mask_335.sum() * 100
            
            if mask_310.sum() > 0:
                recall_310 = (y_pred >= 310)[mask_310].sum() / mask_310.sum() * 100
            
            if mask_under_300.sum() > 0:
                fp_rate = (y_pred >= 300)[mask_under_300].sum() / mask_under_300.sum() * 100
            
            print(f"\n[{self.model_name} Epoch {epoch+1}] "
                  f"335+: {recall_335:.1f}% | "
                  f"310+: {recall_310:.1f}% | "
                  f"FP: {fp_rate:.1f}%")
                  
        except Exception as e:
            print(f"\n[{self.model_name} Epoch {epoch+1}] ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")

# ==============================================================================
# ğŸ¯ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ==============================================================================

def main():
    """V4 Ultimate Complete ë©”ì¸ ì‹¤í–‰"""
    
    print("\n" + "="*80)
    print("ğŸš€ V4 ULTIMATE COMPLETE ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*80)
    
    # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
    ckpt = CheckpointManager()
    
    # ì´ì „ ìƒíƒœ í™•ì¸
    state = ckpt.load_state()
    
    if state:
        print(f"\nğŸ“‚ ì´ì „ í•™ìŠµ ìƒíƒœ ë°œê²¬! (Step {state.get('step', 1)}/6)")
        resume = input("ì´ì–´ì„œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y: ì´ì–´ì„œ, n: ì²˜ìŒë¶€í„°): ").lower()
        
        if resume != 'y':
            ckpt.clear_state()
            state = {}
            step = 1
        else:
            step = state.get('step', 1)
            print(f"âœ… Step {step}ë¶€í„° ì¬ê°œí•©ë‹ˆë‹¤.")
    else:
        state = {}
        step = 1
        print("\nìƒˆë¡œìš´ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # ë°ì´í„° ì²˜ë¦¬ê¸°
    processor = DataProcessorV4()
    
    try:
        # ========================================
        # Step 1: ë°ì´í„° ë¡œë“œ
        # ========================================
        if step <= 1:
            print(f"\n{'='*60}")
            print(f"[Step 1/6] ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
            print(f"{'='*60}")
            
            df = processor.load_and_merge_data()
            df = processor.analyze_consecutive_patterns(df)
            
            state['step'] = 2
            state['data_shape'] = df.shape
            ckpt.save_state(state)
            print("âœ… Step 1 ì™„ë£Œ")
        else:
            print(f"\n[Step 1/6] ë°ì´í„° ë¡œë“œ - ì´ë¯¸ ì™„ë£Œ")
        
        # ========================================
        # Step 2: ì‹œí€€ìŠ¤ ìƒì„±
        # ========================================
        if step <= 2:
            print(f"\n{'='*60}")
            print(f"[Step 2/6] ì‹œí€€ìŠ¤ ìƒì„±")
            print(f"{'='*60}")
            
            if step < 2:
                df = processor.load_and_merge_data()
                df = processor.analyze_consecutive_patterns(df)
            
            X, y, X_physics, weights = processor.create_sequences(df, ckpt_manager=ckpt)
            
            # ì™„ì„±ëœ ì‹œí€€ìŠ¤ ì €ì¥
            with h5py.File('./checkpoints_ultimate/sequences_complete.h5', 'w') as f:
                f.create_dataset('X', data=X, compression='gzip')
                f.create_dataset('y', data=y, compression='gzip')
                f.create_dataset('X_physics', data=X_physics, compression='gzip')
                f.create_dataset('weights', data=weights, compression='gzip')
            
            state['step'] = 3
            state['sequence_shape'] = X.shape
            ckpt.save_state(state)
            print("âœ… Step 2 ì™„ë£Œ")
        else:
            print(f"\n[Step 2/6] ì‹œí€€ìŠ¤ ìƒì„± - ì´ë¯¸ ì™„ë£Œ")
        
        # ========================================
        # Step 3: ë°ì´í„° ë¶„í• 
        # ========================================
        if step <= 3:
            print(f"\n{'='*60}")
            print(f"[Step 3/6] ë°ì´í„° ë¶„í• ")
            print(f"{'='*60}")
            
            # ì‹œí€€ìŠ¤ ë¡œë“œ
            with h5py.File('./checkpoints_ultimate/sequences_complete.h5', 'r') as f:
                X = f['X'][:]
                y = f['y'][:]
                X_physics = f['X_physics'][:]
                weights = f['weights'][:]
            
            # ë¶„í• 
            indices = np.arange(len(X))
            np.random.seed(42)  # ì¬í˜„ì„±
            np.random.shuffle(indices)
            
            train_size = int(0.7 * len(X))
            val_size = int(0.15 * len(X))
            
            train_idx = indices[:train_size]
            val_idx = indices[train_size:train_size+val_size]
            test_idx = indices[train_size+val_size:]
            
            print(f"âœ… Train: {len(train_idx):,} samples")
            print(f"âœ… Valid: {len(val_idx):,} samples")
            print(f"âœ… Test: {len(test_idx):,} samples")
            
            state['step'] = 4
            state['n_features'] = X.shape[2]
            ckpt.save_state(state)
            print("âœ… Step 3 ì™„ë£Œ")
        else:
            print(f"\n[Step 3/6] ë°ì´í„° ë¶„í•  - ì´ë¯¸ ì™„ë£Œ")
        
        # ========================================
        # Step 4: ìŠ¤ì¼€ì¼ë§
        # ========================================
        if step <= 4:
            print(f"\n{'='*60}")
            print(f"[Step 4/6] ë°ì´í„° ìŠ¤ì¼€ì¼ë§")
            print(f"{'='*60}")
            
            # ì‹œí€€ìŠ¤ ë¡œë“œ
            with h5py.File('./checkpoints_ultimate/sequences_complete.h5', 'r') as f:
                X = f['X'][:]
                y = f['y'][:]
                X_physics = f['X_physics'][:]
                weights = f['weights'][:]
            
            # ë¶„í•  ì¬í˜„
            indices = np.arange(len(X))
            np.random.seed(42)
            np.random.shuffle(indices)
            
            train_size = int(0.7 * len(X))
            val_size = int(0.15 * len(X))
            
            train_idx = indices[:train_size]
            val_idx = indices[train_size:train_size+val_size]
            test_idx = indices[train_size+val_size:]
            
            X_train, y_train = X[train_idx], y[train_idx]
            X_val, y_val = X[val_idx], y[val_idx]
            X_test, y_test = X[test_idx], y[test_idx]
            
            X_physics_train = X_physics[train_idx]
            X_physics_val = X_physics[val_idx]
            X_physics_test = X_physics[test_idx]
            
            weights_train = weights[train_idx]
            
            # ìŠ¤ì¼€ì¼ë§
            n_features = X.shape[2]
            
            # X ìŠ¤ì¼€ì¼ë§
            X_train_flat = X_train.reshape(-1, n_features)
            X_val_flat = X_val.reshape(-1, n_features)
            X_test_flat = X_test.reshape(-1, n_features)
            
            X_train_scaled = processor.scaler_X.fit_transform(X_train_flat)
            X_train_scaled = X_train_scaled.reshape(len(X_train), 20, n_features)
            
            X_val_scaled = processor.scaler_X.transform(X_val_flat)
            X_val_scaled = X_val_scaled.reshape(len(X_val), 20, n_features)
            
            X_test_scaled = processor.scaler_X.transform(X_test_flat)
            X_test_scaled = X_test_scaled.reshape(len(X_test), 20, n_features)
            
            # y ìŠ¤ì¼€ì¼ë§
            y_train_scaled = processor.scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
            y_val_scaled = processor.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
            
            # physics ìŠ¤ì¼€ì¼ë§
            X_physics_train_scaled = processor.scaler_physics.fit_transform(X_physics_train)
            X_physics_val_scaled = processor.scaler_physics.transform(X_physics_val)
            X_physics_test_scaled = processor.scaler_physics.transform(X_physics_test)
            
            # NaN ì²´í¬
            print("\nğŸ“Š ìŠ¤ì¼€ì¼ë§ í›„ NaN ì²´í¬...")
            has_nan = False
            
            if np.isnan(X_train_scaled).any():
                print("âš ï¸ X_train_scaledì— NaN ë°œê²¬!")
                X_train_scaled = np.nan_to_num(X_train_scaled, 0)
                has_nan = True
            
            if np.isnan(y_train_scaled).any():
                print("âš ï¸ y_train_scaledì— NaN ë°œê²¬!")
                y_train_scaled = np.nan_to_num(y_train_scaled, 0)
                has_nan = True
            
            if np.isnan(X_physics_train_scaled).any():
                print("âš ï¸ X_physics_train_scaledì— NaN ë°œê²¬!")
                X_physics_train_scaled = np.nan_to_num(X_physics_train_scaled, 0)
                has_nan = True
            
            if not has_nan:
                print("âœ… NaN ì—†ìŒ - ë°ì´í„° ì •ìƒ")
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
            joblib.dump(processor.scaler_X, './checkpoints_ultimate/scalers/scaler_X.pkl')
            joblib.dump(processor.scaler_y, './checkpoints_ultimate/scalers/scaler_y.pkl')
            joblib.dump(processor.scaler_physics, './checkpoints_ultimate/scalers/scaler_physics.pkl')
            
            # ìŠ¤ì¼€ì¼ëœ ë°ì´í„° ì €ì¥
            with h5py.File('./checkpoints_ultimate/scaled_data.h5', 'w') as f:
                f.create_dataset('X_train_scaled', data=X_train_scaled, compression='gzip')
                f.create_dataset('X_val_scaled', data=X_val_scaled, compression='gzip')
                f.create_dataset('X_test_scaled', data=X_test_scaled, compression='gzip')
                f.create_dataset('y_train_scaled', data=y_train_scaled, compression='gzip')
                f.create_dataset('y_val_scaled', data=y_val_scaled, compression='gzip')
                f.create_dataset('y_test', data=y_test, compression='gzip')
                f.create_dataset('X_physics_train_scaled', data=X_physics_train_scaled, compression='gzip')
                f.create_dataset('X_physics_val_scaled', data=X_physics_val_scaled, compression='gzip')
                f.create_dataset('X_physics_test_scaled', data=X_physics_test_scaled, compression='gzip')
                f.create_dataset('weights_train', data=weights_train, compression='gzip')
                f.attrs['n_features'] = n_features
            
            state['step'] = 5
            state['n_features'] = n_features
            ckpt.save_state(state)
            print("âœ… Step 4 ì™„ë£Œ")
        else:
            print(f"\n[Step 4/6] ìŠ¤ì¼€ì¼ë§ - ì´ë¯¸ ì™„ë£Œ")
        
        # ========================================
        # Step 5: ëª¨ë¸ í•™ìŠµ
        # ========================================
        if step <= 5:
            print(f"\n{'='*60}")
            print(f"[Step 5/6] ëª¨ë¸ í•™ìŠµ")
            print(f"{'='*60}")
            
            # ìŠ¤ì¼€ì¼ëœ ë°ì´í„° ë¡œë“œ
            with h5py.File('./checkpoints_ultimate/scaled_data.h5', 'r') as f:
                X_train_scaled = f['X_train_scaled'][:]
                X_val_scaled = f['X_val_scaled'][:]
                y_train_scaled = f['y_train_scaled'][:]
                y_val_scaled = f['y_val_scaled'][:]
                X_physics_train_scaled = f['X_physics_train_scaled'][:]
                X_physics_val_scaled = f['X_physics_val_scaled'][:]
                weights_train = f['weights_train'][:]
                n_features = f.attrs['n_features']
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            processor.scaler_X = joblib.load('./checkpoints_ultimate/scalers/scaler_X.pkl')
            processor.scaler_y = joblib.load('./checkpoints_ultimate/scalers/scaler_y.pkl')
            processor.scaler_physics = joblib.load('./checkpoints_ultimate/scalers/scaler_physics.pkl')
            
            config = {
                'seq_len': 20,
                'n_features': n_features,
                'patch_len': 5
            }
            
            # Model 1 í•™ìŠµ (ì•ˆì •í˜•)
            if not state.get('model1_trained', False):
                print("\nğŸ¤– Model 1: PatchTST (ì•ˆì •í˜•) í•™ìŠµ")
                print("-"*60)
                
                model1 = PatchTSTModel(config)
                
                # Gradient Clippingì´ ìˆëŠ” ì˜µí‹°ë§ˆì´ì €
                optimizer1 = Adam(learning_rate=0.0005, clipnorm=1.0)
                
                model1.compile(
                    optimizer=optimizer1,
                    loss=stable_loss,
                    metrics=['mae']
                )
                
                callbacks1 = [
                    EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss'),
                    ReduceLROnPlateau(factor=0.5, patience=8, min_lr=1e-6, monitor='val_loss'),
                    ModelCheckpoint('./checkpoints_ultimate/model1_stable.weights.h5', 
                                  save_best_only=True, save_weights_only=True, monitor='val_loss'),
                    ExtremeValueCallback(X_val_scaled, y_val_scaled, processor.scaler_y, 
                                       model_name="Model1")
                ]
                
                history1 = model1.fit(
                    X_train_scaled, y_train_scaled,
                    validation_data=(X_val_scaled, y_val_scaled),
                    epochs=50,
                    batch_size=32,
                    callbacks=callbacks1,
                    sample_weight=weights_train,
                    verbose=1
                )
                
                state['model1_trained'] = True
                ckpt.save_state(state)
                print("âœ… Model 1 í•™ìŠµ ì™„ë£Œ")
            else:
                print("\n[Model 1] ì´ë¯¸ í•™ìŠµ ì™„ë£Œ")
            
            # Model 2 í•™ìŠµ (ê·¹ë‹¨í˜•)
            if not state.get('model2_trained', False):
                print("\nğŸ¤– Model 2: PatchTST+PINN (ê·¹ë‹¨í˜•) í•™ìŠµ")
                print("-"*60)
                
                model2 = PatchTSTPINN(config)
                
                # Gradient Clippingì´ ìˆëŠ” ì˜µí‹°ë§ˆì´ì €
                optimizer2 = Adam(learning_rate=0.0005, clipnorm=1.0)
                
                model2.compile(
                    optimizer=optimizer2,
                    loss=extreme_loss,
                    metrics=['mae']
                )
                
                callbacks2 = [
                    EarlyStopping(patience=20, restore_best_weights=True, monitor='val_loss'),
                    ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-6, monitor='val_loss'),
                    ModelCheckpoint('./checkpoints_ultimate/model2_extreme.weights.h5', 
                                  save_best_only=True, save_weights_only=True, monitor='val_loss'),
                    ExtremeValueCallback(X_val_scaled, y_val_scaled, processor.scaler_y,
                                       X_physics_val_scaled, model_name="Model2")
                ]
                
                history2 = model2.fit(
                    [X_train_scaled, X_physics_train_scaled], y_train_scaled,
                    validation_data=([X_val_scaled, X_physics_val_scaled], y_val_scaled),
                    epochs=60,
                    batch_size=32,
                    callbacks=callbacks2,
                    sample_weight=weights_train,
                    verbose=1
                )
                
                state['model2_trained'] = True
                ckpt.save_state(state)
                print("âœ… Model 2 í•™ìŠµ ì™„ë£Œ")
            else:
                print("\n[Model 2] ì´ë¯¸ í•™ìŠµ ì™„ë£Œ")
            
            state['step'] = 6
            ckpt.save_state(state)
            print("\nâœ… Step 5 ì™„ë£Œ")
        else:
            print(f"\n[Step 5/6] ëª¨ë¸ í•™ìŠµ - ì´ë¯¸ ì™„ë£Œ")
        
        # ========================================
        # Step 6: í‰ê°€ ë° ì„ íƒê¸° í…ŒìŠ¤íŠ¸
        # ========================================
        if step <= 6:
            print(f"\n{'='*60}")
            print(f"[Step 6/6] ìµœì¢… í‰ê°€ ë° Ultimate Hybrid Selector í…ŒìŠ¤íŠ¸")
            print(f"{'='*60}")
            
            # ë°ì´í„° ë¡œë“œ
            with h5py.File('./checkpoints_ultimate/scaled_data.h5', 'r') as f:
                X_test_scaled = f['X_test_scaled'][:]
                y_test = f['y_test'][:]
                X_physics_test_scaled = f['X_physics_test_scaled'][:]
                n_features = f.attrs['n_features']
            
            # ì›ë³¸ ë°ì´í„° ë¡œë“œ (ì„ íƒê¸°ìš©)
            with h5py.File('./checkpoints_ultimate/sequences_complete.h5', 'r') as f:
                X = f['X'][:]
                X_physics = f['X_physics'][:]
            
            # í…ŒìŠ¤íŠ¸ ì¸ë±ìŠ¤ ì¬í˜„
            indices = np.arange(len(X))
            np.random.seed(42)
            np.random.shuffle(indices)
            train_size = int(0.7 * len(X))
            val_size = int(0.15 * len(X))
            test_idx = indices[train_size+val_size:]
            X_test = X[test_idx]
            X_physics_test = X_physics[test_idx]
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            processor.scaler_X = joblib.load('./checkpoints_ultimate/scalers/scaler_X.pkl')
            processor.scaler_y = joblib.load('./checkpoints_ultimate/scalers/scaler_y.pkl')
            processor.scaler_physics = joblib.load('./checkpoints_ultimate/scalers/scaler_physics.pkl')
            
            config = {
                'seq_len': 20,
                'n_features': n_features,
                'patch_len': 5
            }
            
            # ëª¨ë¸ ë¡œë“œ
            print("\nğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘...")
            model1 = PatchTSTModel(config)
            model1.build(input_shape=(None, 20, n_features))
            model1.load_weights('./checkpoints_ultimate/model1_stable.weights.h5')
            print("âœ… Model 1 ë¡œë“œ ì™„ë£Œ")
            
            model2 = PatchTSTPINN(config)
            model2.build([(None, 20, n_features), (None, 9)])
            model2.load_weights('./checkpoints_ultimate/model2_extreme.weights.h5')
            print("âœ… Model 2 ë¡œë“œ ì™„ë£Œ")
            
            # Ultimate Hybrid Selector ì´ˆê¸°í™”
            selector = UltimateHybridSelector()
            
            print("\n" + "="*60)
            print("ğŸ“Š Ultimate Hybrid Selector í…ŒìŠ¤íŠ¸")
            print("="*60)
            
            # í…ŒìŠ¤íŠ¸
            test_samples = min(100, len(X_test))
            total_mae = []
            model1_count = 0
            model2_count = 0
            correct_selections = 0
            
            print(f"\ní…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {test_samples}ê°œ")
            print("-"*60)
            
            for i in tqdm(range(test_samples), desc="í‰ê°€ ì§„í–‰"):
                # ê³¼ê±° 20ë¶„ ë°ì´í„° (íƒ€ê²Ÿ ì»¬ëŸ¼ë§Œ)
                past_20min = X_test[i, :, 0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ íƒ€ê²Ÿ
                
                # ë¬¼ë¦¬ ë°ì´í„°
                physics_data = {
                    'bridge_time': X_physics_test[i, 3],
                    'storage_util': X_physics_test[i, 6]
                }
                
                # ëª¨ë¸ ì„ íƒ
                selected_model, decision_info = selector.select_model(past_20min, physics_data)
                
                # ì˜ˆì¸¡
                if selected_model == "Model1":
                    y_pred_scaled = model1.predict(X_test_scaled[i:i+1], verbose=0)[0]
                    model1_count += 1
                else:
                    y_pred_scaled = model2.predict([X_test_scaled[i:i+1], 
                                                   X_physics_test_scaled[i:i+1]], verbose=0)[0]
                    model2_count += 1
                
                # ì—­ì •ê·œí™”
                y_pred = processor.scaler_y.inverse_transform([[y_pred_scaled]])[0, 0]
                y_true = y_test[i]
                
                mae = abs(y_true - y_pred)
                total_mae.append(mae)
                
                # ì„±ëŠ¥ ì—…ë°ì´íŠ¸
                selector.update_performance(selected_model.lower().replace(" ", ""), y_true, y_pred)
                
                # ì˜¬ë°”ë¥¸ ì„ íƒì¸ì§€ í™•ì¸
                if (y_true < 300 and selected_model == "Model1") or \
                   (y_true >= 300 and selected_model == "Model2"):
                    correct_selections += 1
                
                # ì²˜ìŒ 10ê°œ ìƒ˜í”Œ ì¶œë ¥
                if i < 10:
                    print(f"[{i+1:3d}] ì‹¤ì œ: {y_true:6.1f} | ì˜ˆì¸¡: {y_pred:6.1f} | "
                          f"MAE: {mae:5.1f} | ì„ íƒ: {selected_model}")
            
            # ìµœì¢… í†µê³„
            print("\n" + "="*60)
            print("ğŸ“ˆ ìµœì¢… ì„±ëŠ¥ í†µê³„")
            print("="*60)
            
            print(f"\nâœ… ì „ì²´ ì„±ëŠ¥:")
            print(f"  - í‰ê·  MAE: {np.mean(total_mae):.2f}")
            print(f"  - ì¤‘ì•™ê°’ MAE: {np.median(total_mae):.2f}")
            print(f"  - ìµœëŒ€ MAE: {np.max(total_mae):.2f}")
            print(f"  - ìµœì†Œ MAE: {np.min(total_mae):.2f}")
            
            print(f"\nğŸ“Š ëª¨ë¸ ì‚¬ìš© í†µê³„:")
            print(f"  - Model 1 (ì•ˆì •í˜•): {model1_count}íšŒ ({model1_count/test_samples*100:.1f}%)")
            print(f"  - Model 2 (ê·¹ë‹¨í˜•): {model2_count}íšŒ ({model2_count/test_samples*100:.1f}%)")
            print(f"  - ì˜¬ë°”ë¥¸ ì„ íƒìœ¨: {correct_selections/test_samples*100:.1f}%")
            
            # ì„ íƒê¸° ì„±ëŠ¥ ìš”ì•½
            performance = selector.get_performance_summary()
            
            print(f"\nğŸ“Š ê°œë³„ ëª¨ë¸ ì„±ëŠ¥:")
            for model_name, stats in performance.items():
                if stats['total_predictions'] > 0:
                    print(f"\n{model_name.upper()}:")
                    print(f"  - ì •í™•ë„ (MAE<30): {stats['accuracy']*100:.1f}%")
                    print(f"  - í‰ê·  MAE: {stats['avg_mae']:.2f}")
                    print(f"  - ì˜ˆì¸¡ íšŸìˆ˜: {stats['total_predictions']}")
            
            # ê·¹ë‹¨ê°’ êµ¬ê°„ë³„ ì„±ëŠ¥
            print(f"\nğŸ“Š êµ¬ê°„ë³„ ì„±ëŠ¥:")
            
            mask_low = y_test[:test_samples] < 200
            mask_normal = (y_test[:test_samples] >= 200) & (y_test[:test_samples] < 300)
            mask_danger = (y_test[:test_samples] >= 300) & (y_test[:test_samples] < 335)
            mask_extreme = y_test[:test_samples] >= 335
            
            mae_array = np.array(total_mae)
            
            if mask_low.sum() > 0:
                print(f"  - ì €êµ¬ê°„ (<200): MAE={np.mean(mae_array[mask_low]):.2f}")
            if mask_normal.sum() > 0:
                print(f"  - ì •ìƒ (200-300): MAE={np.mean(mae_array[mask_normal]):.2f}")
            if mask_danger.sum() > 0:
                print(f"  - ìœ„í—˜ (300-335): MAE={np.mean(mae_array[mask_danger]):.2f}")
            if mask_extreme.sum() > 0:
                print(f"  - ê·¹ë‹¨ (335+): MAE={np.mean(mae_array[mask_extreme]):.2f}")
            
            print("\n" + "="*80)
            print("âœ… V4 ULTIMATE COMPLETE í•™ìŠµ ë° í‰ê°€ ì™„ë£Œ!")
            print("="*80)
            
            # ì™„ë£Œ í›„ ìƒíƒœ ì œê±° ì˜µì…˜
            remove = input("\nìƒíƒœ íŒŒì¼ì„ ì œê±°í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
            if remove == 'y':
                ckpt.clear_state()
                print("ğŸ§¹ ìƒíƒœ íŒŒì¼ ì œê±° ì™„ë£Œ")
            
            print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨ ê°ì§€")
        print("ğŸ’¾ ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¾ ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ==============================================================================
# ğŸš€ ì‹¤í–‰
# ==============================================================================

if __name__ == "__main__":
    main()