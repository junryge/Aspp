import numpy as np
import pandas as pd
import xgboost as xgb
import pickle
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

warnings.filterwarnings('ignore')
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def calculate_surge_risk(features):
    """
    ê¸‰ì¦ ìœ„í—˜ë„ ë° ì˜ˆìƒ ê¸‰ì¦ëŸ‰ ê³„ì‚°
    
    Returns:
        risk_level: ìœ„í—˜ë„ (HIGH/MEDIUM/LOW)
        surge_prob: ê¸‰ì¦ í™•ë¥  (0~1)
        expected_surge: ì˜ˆìƒ ê¸‰ì¦ëŸ‰
    """
    # ì£¼ìš” ì§€í‘œ ì¶”ì¶œ
    cmd_last = features.get('M16A_3F_CMD_last_value', 999)
    cmd_mean = features.get('M16A_3F_CMD_mean', 999)
    storage_last = features.get('M16A_3F_STORAGE_UTIL_last_value', 0)
    storage_mean = features.get('M16A_3F_STORAGE_UTIL_mean', 0)
    
    # CMD ë³€í™”ëŸ‰ ê³„ì‚°
    cmd_slope = features.get('M16A_3F_CMD_slope', 0)
    
    # ìœ„í—˜ë„ íŒì • (ë¶„ì„ ê²°ê³¼ ê¸°ë°˜)
    if cmd_last < 180:
        # CMDê°€ ë§¤ìš° ë‚®ìŒ â†’ í° ê¸‰ì¦ ê°€ëŠ¥ì„±
        risk_level = "HIGH"
        surge_prob = 0.70
        expected_surge = 60
    elif storage_last >= 30 and storage_last <= 40:
        # Storage Util ìœ„í—˜ êµ¬ê°„
        risk_level = "HIGH"
        surge_prob = 0.45
        expected_surge = 50
    elif cmd_last >= 260 and cmd_last <= 280:
        # CMD ë†’ì€ êµ¬ê°„ì—ì„œë„ ê¸‰ì¦ ê°€ëŠ¥
        risk_level = "HIGH"
        surge_prob = 0.67
        expected_surge = 60
    elif cmd_last < 220:
        # CMD ì¤‘ê°„ êµ¬ê°„
        risk_level = "MEDIUM"
        surge_prob = 0.30
        expected_surge = 35
    else:
        # ì¼ë°˜ ìƒí™©
        risk_level = "LOW"
        surge_prob = 0.05
        expected_surge = 20
    
    # ì¶”ê°€ ë³´ì • ìš”ì†Œ
    # 1. CMD ê¸‰ê° íŒ¨í„´ (slopeê°€ ìŒìˆ˜ì´ê³  í¼)
    if cmd_slope < -5:
        surge_prob *= 1.2  # 20% ì¦ê°€
        expected_surge *= 1.1
    
    # 2. Storage Utilì´ ì¦ê°€ ì¶”ì„¸
    storage_slope = features.get('M16A_3F_STORAGE_UTIL_slope', 0)
    if storage_slope > 2:
        surge_prob *= 1.1  # 10% ì¦ê°€
    
    # í™•ë¥  ìƒí•œ ì¡°ì •
    surge_prob = min(surge_prob, 0.85)
    
    return risk_level, surge_prob, expected_surge

def apply_adaptive_correction(prediction, features, seq_target):
    """
    ì ì‘í˜• ì˜ˆì¸¡ê°’ ë³´ì •
    
    Args:
        prediction: ì›ë˜ ëª¨ë¸ ì˜ˆì¸¡ê°’
        features: íŠ¹ì„± ë”•ì…”ë„ˆë¦¬
        seq_target: 30ê°œ ì‹œí€€ìŠ¤ íƒ€ê²Ÿê°’
    
    Returns:
        ë³´ì •ëœ ì˜ˆì¸¡ê°’, ìœ„í—˜ë„, ê¸‰ì¦ í™•ë¥ , ë³´ì •ëŸ‰
    """
    # ìœ„í—˜ë„ ê³„ì‚°
    risk_level, surge_prob, expected_surge = calculate_surge_risk(features)
    
    # í˜„ì¬ ê°’ (ì‹œí€€ìŠ¤ ë§ˆì§€ë§‰)
    current_value = seq_target[-1]
    
    # ì‹œí€€ìŠ¤ í†µê³„
    seq_mean = np.mean(seq_target)
    seq_std = np.std(seq_target)
    seq_max = np.max(seq_target)
    recent_mean = np.mean(seq_target[-5:])
    
    # ë³´ì • ì „ëµ
    correction = 0
    
    # 1. ê¸°ë³¸ ë³´ì •: ìœ„í—˜ë„ì— ë”°ë¥¸ ê¸‰ì¦ ë°˜ì˜
    if current_value < 300:  # í˜„ì¬ 300 ë¯¸ë§Œì¼ ë•Œë§Œ
        if risk_level == "HIGH":
            # ë†’ì€ ìœ„í—˜ë„: ê¸‰ì¦ ê°€ëŠ¥ì„± ì ê·¹ ë°˜ì˜
            if prediction < 300:
                # ì˜ˆì¸¡ê°’ì´ 300 ë¯¸ë§Œì´ë©´ ê¸‰ì¦ ê°€ëŠ¥ì„± ë°˜ì˜
                potential_value = current_value + expected_surge * surge_prob
                
                # 300 ë„ë‹¬ ê°€ëŠ¥ì„± ì²´í¬
                if potential_value >= 300:
                    # 300 ë„˜ì„ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë©´ ë³´ì •
                    correction = (potential_value - prediction) * 0.7  # 70% ë°˜ì˜
                else:
                    # 300 ë¯¸ë‹¬ì´ì–´ë„ ìƒí–¥ ë³´ì •
                    correction = expected_surge * surge_prob * 0.5
                    
        elif risk_level == "MEDIUM":
            # ì¤‘ê°„ ìœ„í—˜ë„: ë³´ìˆ˜ì  ë³´ì •
            if prediction < 300 and surge_prob > 0.25:
                correction = expected_surge * surge_prob * 0.3
                
        else:  # LOW
            # ë‚®ì€ ìœ„í—˜ë„: ìµœì†Œ ë³´ì •
            correction = expected_surge * surge_prob * 0.1
    
    # 2. ì¶”ê°€ ë³´ì •: ì‹œí€€ìŠ¤ íŒ¨í„´ ë°˜ì˜
    # ìµœê·¼ ê¸‰ì¦ ì¶”ì„¸
    recent_increase = recent_mean - seq_mean
    if recent_increase > 10:  # ìµœê·¼ ê¸‰ì¦ ì¤‘
        correction += recent_increase * 0.2
    
    # ì‹œí€€ìŠ¤ ë‚´ 300 ì´ìƒ ìˆì—ˆë˜ ê²½ìš°
    if seq_max >= 300:
        correction += 5  # ì¶”ê°€ ìƒí–¥
    
    # 3. ë³´ì •ê°’ ì œí•œ
    corrected_prediction = prediction + correction
    
    # ìƒí•œ/í•˜í•œ ì œí•œ
    corrected_prediction = max(corrected_prediction, current_value - 50)  # ê¸‰ë½ ì œí•œ
    corrected_prediction = min(corrected_prediction, current_value + 100)  # ê¸‰ë“± ì œí•œ
    
    # í˜„ì‹¤ì  ë²”ìœ„ ì œí•œ
    if current_value < 250 and corrected_prediction > 350:
        corrected_prediction = 320  # ê³¼ë„í•œ ê¸‰ì¦ ì œí•œ
    
    return corrected_prediction, risk_level, surge_prob, correction

def train_and_evaluate_complete():
    """
    í•™ìŠµ(aas.csv) â†’ í‰ê°€(BBB.CSV) ì „ì²´ í”„ë¡œì„¸ìŠ¤ + ê¸‰ì¦ ìœ„í—˜ë„ ê¸°ë°˜ ë³´ì •
    V6 Ultimate + ë³´ì • ë¡œì§ í†µí•©
    """
    print("="*80)
    print("XGBoost 30ë¶„â†’10ë¶„ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ + ê¸‰ì¦ ìœ„í—˜ë„ ë³´ì •")
    print("="*80)
    
    # í•µì‹¬ 12ê°œ ì»¬ëŸ¼ë§Œ ì‚¬ìš© (ë¬¼ë¦¬ì ìœ¼ë¡œ ì¤‘ìš”í•œ ê²ƒë§Œ)
    FEATURE_COLS = {
        'storage': ['M16A_3F_STORAGE_UTIL'],  # ê·¹ë‹¨ê°’ í•µì‹¬ ì§€í‘œ
        'cmd': ['M16A_3F_CMD', 'M16A_6F_TO_HUB_CMD'],  # ì£¼ìš” CMD
        'inflow': ['M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2', 'M14A_3F_TO_HUB_JOB2'],  # ì£¼ìš” ìœ ì…
        'outflow': ['M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB', 'M16A_3F_TO_M14A_3F_JOB'],  # ì£¼ìš” ìœ ì¶œ
        'maxcapa': ['M16A_6F_LFT_MAXCAPA', 'M16A_2F_LFT_MAXCAPA']  # ìš©ëŸ‰ ì œí•œ
    }
    
    TARGET_COL = 'CURRENT_M16A_3F_JOB_2'
    
    # Feature ìƒì„± í•¨ìˆ˜
    def create_features(df, start_idx=30):
        features_list = []
        labels = []
        seq_max_list = []
        seq_min_list = []
        indices = []
        seq_target_list = []
        
        for i in range(start_idx, len(df)):
            seq_target = df[TARGET_COL].iloc[i-30:i].values
            
            features = {
                # íƒ€ê²Ÿ ì»¬ëŸ¼ íŠ¹ì„±
                'target_mean': np.mean(seq_target),
                'target_std': np.std(seq_target),
                'target_last_5_mean': np.mean(seq_target[-5:]),
                'target_max': np.max(seq_target),
                'target_min': np.min(seq_target),
                'target_slope': np.polyfit(np.arange(30), seq_target, 1)[0],
                'target_last_10_mean': np.mean(seq_target[-10:]),
                'target_first_10_mean': np.mean(seq_target[:10]),
            }
            
            # ê° ì»¬ëŸ¼ ê·¸ë£¹ë³„ íŠ¹ì„± ì¶”ê°€
            for group_name, cols in FEATURE_COLS.items():
                for col in cols:
                    if col in df.columns:
                        seq_data = df[col].iloc[i-30:i].values
                        
                        # ê¸°ë³¸ í†µê³„
                        features[f'{col}_mean'] = np.mean(seq_data)
                        features[f'{col}_std'] = np.std(seq_data)
                        features[f'{col}_max'] = np.max(seq_data)
                        features[f'{col}_min'] = np.min(seq_data)
                        
                        # ìµœê·¼ íŠ¹ì„±
                        features[f'{col}_last_5_mean'] = np.mean(seq_data[-5:])
                        features[f'{col}_last_10_mean'] = np.mean(seq_data[-10:])
                        
                        # ì¶”ì„¸
                        features[f'{col}_slope'] = np.polyfit(np.arange(30), seq_data, 1)[0]
                        
                        # êµ¬ê°„ë³„ í‰ê·  (ì´ˆê¸°/ì¤‘ê°„/ìµœê·¼)
                        features[f'{col}_first_10_mean'] = np.mean(seq_data[:10])
                        features[f'{col}_mid_10_mean'] = np.mean(seq_data[10:20])
                        features[f'{col}_last_value'] = seq_data[-1]
            
            # ìœ ì…-ìœ ì¶œ ì°¨ì´ (Net Flow)
            inflow_sum = 0
            outflow_sum = 0
            for col in FEATURE_COLS['inflow']:
                if col in df.columns:
                    inflow_sum += df[col].iloc[i-1]
            for col in FEATURE_COLS['outflow']:
                if col in df.columns:
                    outflow_sum += df[col].iloc[i-1]
            features['net_flow'] = inflow_sum - outflow_sum
            
            # CMD ì´í•©
            cmd_sum = 0
            for col in FEATURE_COLS['cmd']:
                if col in df.columns:
                    cmd_sum += df[col].iloc[i-1]
            features['total_cmd'] = cmd_sum
            
            features_list.append(features)
            labels.append(df[TARGET_COL].iloc[i])
            # NaN/Inf ì²´í¬
            if np.isnan(labels[-1]) or np.isinf(labels[-1]):
                features_list.pop()
                continue
            seq_max_list.append(np.max(seq_target))
            seq_min_list.append(np.min(seq_target))
            indices.append(i)
            seq_target_list.append(seq_target)
        
        return pd.DataFrame(features_list), np.array(labels), seq_max_list, seq_min_list, indices, seq_target_list
    
    # ===== 1. í•™ìŠµ ë‹¨ê³„ (aas.csv) =====
    print("\n[STEP 1] í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ")
    print("-"*40)
    
    # ì¸ì½”ë”© ìë™ ê°ì§€ ë° ë¡œë“œ
    try:
        df_train = pd.read_csv('81.csv', on_bad_lines='skip', encoding='utf-8')
    except UnicodeDecodeError:
        try:
            df_train = pd.read_csv('81.csv', on_bad_lines='skip', encoding='cp949')
            print("âš ï¸ ì¸ì½”ë”©: CP949ë¡œ íŒŒì¼ ë¡œë“œ")
        except:
            df_train = pd.read_csv('81.csv', on_bad_lines='skip', encoding='euc-kr')
            print("âš ï¸ ì¸ì½”ë”©: EUC-KRë¡œ íŒŒì¼ ë¡œë“œ")
    
    print(f"í•™ìŠµ ë°ì´í„°: {len(df_train)}ê°œ í–‰")
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ í™•ì¸:")
    
    all_feature_cols = []
    for group_name, cols in FEATURE_COLS.items():
        available = [col for col in cols if col in df_train.columns]
        all_feature_cols.extend(available)
        print(f"  - {group_name}: {len(available)}/{len(cols)}ê°œ")
    
    # í•™ìŠµ ë°ì´í„° ìƒì„±
    X_train, y_train, _, _, _, _ = create_features(df_train)
    
    print(f"\nìƒì„±ëœ Feature ìˆ˜: {len(X_train.columns)}ê°œ")
    print(f"í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(X_train)}ê°œ")
    
    # í•™ìŠµ/ê²€ì¦ ë¶„í• 
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42
    )
    
    # GPU/CPU ìë™ ì„ íƒ (ì‹¤ì œ í•™ìŠµìœ¼ë¡œ í…ŒìŠ¤íŠ¸)
    print("\nğŸ” í•™ìŠµ í™˜ê²½ ê°ì§€ ì¤‘...")
    use_gpu = False
    
    # 1ì°¨ ì‹œë„: GPU
    try:
        print("  â†’ GPU ëª¨ë“œ ì‹œë„ ì¤‘...")
        test_model = xgb.XGBRegressor(
            n_estimators=5,
            max_depth=3,
            random_state=42,
            tree_method='gpu_hist',
            gpu_id=0
        )
        # ì‹¤ì œë¡œ í•™ìŠµ ì‹œë„ (ì‘ì€ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸)
        test_model.fit(X_tr[:100], y_tr[:100], verbose=False)
        use_gpu = True
        print("  âœ… GPU ì‚¬ìš© ê°€ëŠ¥! GPU ëª¨ë“œë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n")
    except Exception as e:
        print(f"  âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€: {str(e)[:100]}")
        print("  â†’ CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.\n")
        use_gpu = False
    
    # ì‹¤ì œ ëª¨ë¸ ìƒì„±
    if use_gpu:
        model = xgb.XGBRegressor(
            n_estimators=150,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            tree_method='gpu_hist',
            gpu_id=0,
            predictor='gpu_predictor'
        )
    else:
        model = xgb.XGBRegressor(
            n_estimators=150,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            tree_method='hist',  # CPU ìµœì í™”
            n_jobs=-1
        )
    
    print("ëª¨ë¸ í•™ìŠµ ì¤‘...")
    model.fit(
        X_tr, y_tr,
        eval_set=[(X_val, y_val)],
        verbose=False
    )
    
    # í•™ìŠµ ë°ì´í„° í‰ê°€
    y_val_pred = model.predict(X_val)
    train_mae = mean_absolute_error(y_val, y_val_pred)
    train_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
    train_r2 = r2_score(y_val, y_val_pred)
    
    print(f"\ní•™ìŠµ ë°ì´í„° ì„±ëŠ¥:")
    print(f"  MAE:  {train_mae:.4f}")
    print(f"  RMSE: {train_rmse:.4f}")
    print(f"  RÂ²:   {train_r2:.4f}")
    
    # ëª¨ë¸ ì €ì¥
    with open('xgboost_model_30min_10min.pkl', 'wb') as f:
        pickle.dump(model, f)
    print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: xgboost_model_30min_10min.pkl")
    
    # ===== 2. í‰ê°€ ë‹¨ê³„ (BBB.CSV) + ë³´ì • ë¡œì§ =====
    print("\n[STEP 2] BBB.CSVë¡œ ëª¨ë¸ í‰ê°€ + ê¸‰ì¦ ìœ„í—˜ë„ ë³´ì •")
    print("-"*40)
    
    # ì¸ì½”ë”© ìë™ ê°ì§€ ë° ë¡œë“œ
    try:
        df_test = pd.read_csv('91.CSV', on_bad_lines='skip', encoding='utf-8')
    except UnicodeDecodeError:
        try:
            df_test = pd.read_csv('91.CSV', on_bad_lines='skip', encoding='cp949')
            print("âš ï¸ ì¸ì½”ë”©: CP949ë¡œ íŒŒì¼ ë¡œë“œ")
        except:
            df_test = pd.read_csv('91.CSV', on_bad_lines='skip', encoding='euc-kr')
            print("âš ï¸ ì¸ì½”ë”©: EUC-KRë¡œ íŒŒì¼ ë¡œë“œ")
    
    print(f"í‰ê°€ ë°ì´í„°: {len(df_test)}ê°œ í–‰")
    
    # BBB.CSV Feature ìƒì„±
    X_test, y_test, seq_max_list, seq_min_list, indices, seq_target_list = create_features(df_test)
    
    # ê¸°ë³¸ ì˜ˆì¸¡
    y_pred_original = model.predict(X_test)
    
    # ë³´ì •ëœ ì˜ˆì¸¡ ìƒì„±
    y_pred_corrected = []
    risk_levels = []
    surge_probs = []
    corrections = []
    
    print("ë³´ì • ë¡œì§ ì ìš© ì¤‘...")
    for i in range(len(X_test)):
        features_dict = X_test.iloc[i].to_dict()
        seq_target = seq_target_list[i]
        
        pred_corrected, risk_level, surge_prob, correction = \
            apply_adaptive_correction(y_pred_original[i], features_dict, seq_target)
        
        y_pred_corrected.append(pred_corrected)
        risk_levels.append(risk_level)
        surge_probs.append(surge_prob)
        corrections.append(correction)
    
    y_pred_corrected = np.array(y_pred_corrected)
    
    # í‰ê°€ ì§€í‘œ (ë³´ì • ì „)
    test_mae_original = mean_absolute_error(y_test, y_pred_original)
    test_rmse_original = np.sqrt(mean_squared_error(y_test, y_pred_original))
    test_r2_original = r2_score(y_test, y_pred_original)
    
    # í‰ê°€ ì§€í‘œ (ë³´ì • í›„)
    test_mae_corrected = mean_absolute_error(y_test, y_pred_corrected)
    test_rmse_corrected = np.sqrt(mean_squared_error(y_test, y_pred_corrected))
    test_r2_corrected = r2_score(y_test, y_pred_corrected)
    
    print(f"\nBBB.CSV í‰ê°€ ê²°ê³¼ (ë³´ì • ì „):")
    print(f"  MAE:  {test_mae_original:.4f}")
    print(f"  RMSE: {test_rmse_original:.4f}")
    print(f"  RÂ²:   {test_r2_original:.4f}")
    
    print(f"\nBBB.CSV í‰ê°€ ê²°ê³¼ (ë³´ì • í›„):")
    print(f"  MAE:  {test_mae_corrected:.4f}")
    print(f"  RMSE: {test_rmse_corrected:.4f}")
    print(f"  RÂ²:   {test_r2_corrected:.4f}")
    print(f"  ê°œì„ ë„: {test_mae_original - test_mae_corrected:.4f}")
    
    # ===== 3. ìƒì„¸ ë¶„ì„ ê²°ê³¼ ìƒì„± =====
    print("\n[STEP 3] ìƒì„¸ ë¶„ì„ ê²°ê³¼ ìƒì„±")
    print("-"*40)
    
    # STAT_DT ì²˜ë¦¬
    if 'STAT_DT' in df_test.columns:
        try:
            df_test['STAT_DT'] = pd.to_datetime(df_test['STAT_DT'], format='%Y%m%d%H%M')
        except:
            try:
                df_test['STAT_DT'] = pd.to_datetime(df_test['STAT_DT'])
            except:
                base_date = datetime(2024, 1, 1)
                df_test['STAT_DT'] = [base_date + timedelta(minutes=i) for i in range(len(df_test))]
    else:
        base_date = datetime(2024, 1, 1)
        df_test['STAT_DT'] = [base_date + timedelta(minutes=i) for i in range(len(df_test))]
    
    # ê²°ê³¼ DataFrame ìƒì„±
    results = []
    jump_count = 0
    extreme_count = 0
    extreme_detected_original = 0
    extreme_detected_corrected = 0
    surge_count = 0
    surge_detected_original = 0
    surge_detected_corrected = 0
    
    for i, idx in enumerate(indices):
        current_time = df_test['STAT_DT'].iloc[idx]
        seq_start_time = df_test['STAT_DT'].iloc[idx-30]
        prediction_time = current_time + timedelta(minutes=10)
        current_value = seq_target_list[i][-1]
        
        is_jump = (seq_max_list[i] < 280) and (y_test[i] >= 300)
        is_surge = (current_value < 300) and (y_test[i] >= 300)
        
        if is_jump:
            jump_count += 1
        
        if is_surge:
            surge_count += 1
            if y_pred_original[i] >= 300:
                surge_detected_original += 1
            if y_pred_corrected[i] >= 300:
                surge_detected_corrected += 1
        
        if y_test[i] >= 300:
            extreme_count += 1
            if y_pred_original[i] >= 290:
                extreme_detected_original += 1
            if y_pred_corrected[i] >= 290:
                extreme_detected_corrected += 1
        
        results.append({
            'í˜„ì¬ì‹œê°„': current_time.strftime('%Y-%m-%d %H:%M'),
            'ì˜ˆì¸¡ì‹œê°„(+10ë¶„)': prediction_time.strftime('%Y-%m-%d %H:%M'),
            'ì‹œí€€ìŠ¤ì‹œì‘': seq_start_time.strftime('%Y-%m-%d %H:%M'),
            'ì‹œí€€ìŠ¤ì™„ë£Œ': current_time.strftime('%Y-%m-%d %H:%M'),
            'ì‹¤ì œê°’': round(y_test[i], 2),
            'ì˜ˆì¸¡ê°’': round(y_pred_original[i], 2),
            'ì˜ˆì¸¡ê°’_ë³´ì •í›„': round(y_pred_corrected[i], 2),
            'ìœ„í—˜ë„': risk_levels[i],
            'ê¸‰ì¦í™•ë¥ ': f"{surge_probs[i]*100:.0f}%",
            'ë³´ì •ëŸ‰': round(corrections[i], 2),
            'ì˜¤ì°¨': round(abs(y_test[i] - y_pred_original[i]), 2),
            'ì˜¤ì°¨_ë³´ì •í›„': round(abs(y_test[i] - y_pred_corrected[i]), 2),
            'ì˜¤ì°¨ìœ¨(%)': round(abs(y_test[i] - y_pred_original[i]) / y_test[i] * 100, 2) if y_test[i] > 0 else 0,
            'ì˜¤ì°¨ìœ¨_ë³´ì •í›„(%)': round(abs(y_test[i] - y_pred_corrected[i]) / y_test[i] * 100, 2) if y_test[i] > 0 else 0,
            'ì‹œí€€ìŠ¤MAX': round(seq_max_list[i], 2),
            'ì‹œí€€ìŠ¤MIN': round(seq_min_list[i], 2),
            'ì‹œí€€ìŠ¤ë²”ìœ„': round(seq_max_list[i] - seq_min_list[i], 2),
            'ì í”„ì¼€ì´ìŠ¤': 'O' if is_jump else '-',
            '300ê¸‰ì¦': 'âœ…' if is_surge else '-',
            'ê¸‰ì¦ì˜ˆì¸¡ì„±ê³µ_ë³´ì •ì „': 'O' if (is_surge and y_pred_original[i] >= 300) else '-',
            'ê¸‰ì¦ì˜ˆì¸¡ì„±ê³µ_ë³´ì •í›„': 'âœ…' if (is_surge and y_pred_corrected[i] >= 300) else '-',
            'ê·¹ë‹¨ê°’(300+)': 'O' if y_test[i] >= 300 else '-',
            'ê·¹ë‹¨ê°’ê°ì§€_ë³´ì •ì „': 'O' if (y_test[i] >= 300 and y_pred_original[i] >= 290) else '-',
            'ê·¹ë‹¨ê°’ê°ì§€_ë³´ì •í›„': 'O' if (y_test[i] >= 300 and y_pred_corrected[i] >= 290) else '-'
        })
    
    df_results = pd.DataFrame(results)
    
    # í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ë¶„ì„:")
    print(f"  - ì „ì²´ ì˜ˆì¸¡: {len(df_results)}ê°œ")
    print(f"\n[ê·¹ë‹¨ê°’ (300+) ë¶„ì„]")
    print(f"  - ê·¹ë‹¨ê°’ ë°œìƒ: {extreme_count}ê°œ ({extreme_count/len(df_results)*100:.1f}%)")
    print(f"  - ê°ì§€(ë³´ì •ì „): {extreme_detected_original}/{extreme_count}ê°œ ({extreme_detected_original/extreme_count*100 if extreme_count > 0 else 0:.1f}%)")
    print(f"  - ê°ì§€(ë³´ì •í›„): {extreme_detected_corrected}/{extreme_count}ê°œ ({extreme_detected_corrected/extreme_count*100 if extreme_count > 0 else 0:.1f}%)")
    
    print(f"\n[300 ê¸‰ì¦ (300ë¯¸ë§Œâ†’300ì´ìƒ) ë¶„ì„]")
    print(f"  - ê¸‰ì¦ ë°œìƒ: {surge_count}ê°œ ({surge_count/len(df_results)*100:.1f}%)")
    print(f"  - ì˜ˆì¸¡ì„±ê³µ(ë³´ì •ì „): {surge_detected_original}/{surge_count}ê°œ ({surge_detected_original/surge_count*100 if surge_count > 0 else 0:.1f}%)")
    print(f"  - ì˜ˆì¸¡ì„±ê³µ(ë³´ì •í›„): {surge_detected_corrected}/{surge_count}ê°œ ({surge_detected_corrected/surge_count*100 if surge_count > 0 else 0:.1f}%)")
    print(f"  - ê°œì„ : +{surge_detected_corrected - surge_detected_original}ê°œ")
    
    print(f"\n[ì í”„ ì¼€ì´ìŠ¤ (ì‹œí€€ìŠ¤MAX<280 â†’ ì‹¤ì œê°’â‰¥300)]")
    print(f"  - ì í”„ ì¼€ì´ìŠ¤: {jump_count}ê°œ ({jump_count/len(df_results)*100:.1f}%)")
    
    # ìœ„í—˜ë„ë³„ í†µê³„
    print(f"\n[ìœ„í—˜ë„ë³„ ë¶„ì„]")
    for risk in ['HIGH', 'MEDIUM', 'LOW']:
        risk_df = df_results[df_results['ìœ„í—˜ë„'] == risk]
        if len(risk_df) > 0:
            surge_in_risk = (risk_df['300ê¸‰ì¦'] == 'âœ…').sum()
            print(f"  {risk}: {len(risk_df)}ê°œ - ê¸‰ì¦ ë°œìƒ {surge_in_risk}ê°œ ({surge_in_risk/len(risk_df)*100:.1f}%)")
    
    # CSV ì €ì¥
    df_results.to_csv('BBB_evaluation_results_with_correction.csv', index=False, encoding='utf-8-sig')
    print(f"\nâœ… ìƒì„¸ ê²°ê³¼ ì €ì¥: BBB_evaluation_results_with_correction.csv")
    
    # ===== 4. ê·¸ë˜í”„ ìƒì„± =====
    print("\n[STEP 4] í‰ê°€ ê·¸ë˜í”„ ìƒì„±")
    print("-"*40)
    
    fig, axes = plt.subplots(3, 3, figsize=(20, 15))
    
    # 1. ì˜ˆì¸¡ vs ì‹¤ì œ ì‚°ì ë„ (ë³´ì • ì „)
    ax1 = axes[0, 0]
    ax1.scatter(y_test, y_pred_original, alpha=0.5, s=10, color='blue')
    ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    ax1.set_xlabel('Actual')
    ax1.set_ylabel('Predicted')
    ax1.set_title(f'[Before Correction] Actual vs Predicted\nMAE={test_mae_original:.2f}, RÂ²={test_r2_original:.3f}')
    ax1.grid(True, alpha=0.3)
    
    # 2. ì˜ˆì¸¡ vs ì‹¤ì œ ì‚°ì ë„ (ë³´ì • í›„)
    ax2 = axes[0, 1]
    ax2.scatter(y_test, y_pred_corrected, alpha=0.5, s=10, color='green')
    ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    ax2.set_xlabel('Actual')
    ax2.set_ylabel('Predicted (Corrected)')
    ax2.set_title(f'[After Correction] Actual vs Predicted\nMAE={test_mae_corrected:.2f}, RÂ²={test_r2_corrected:.3f}')
    ax2.grid(True, alpha=0.3)
    
    # 3. ì‹œê³„ì—´ ë¹„êµ (ë³´ì • ì „)
    ax3 = axes[0, 2]
    plot_size = min(300, len(y_test))
    ax3.plot(range(plot_size), y_test[:plot_size], 'b-', label='Actual', alpha=0.7, linewidth=1)
    ax3.plot(range(plot_size), y_pred_original[:plot_size], 'r--', label='Predicted', alpha=0.7, linewidth=1)
    ax3.axhline(y=300, color='orange', linestyle='--', label='Extreme(300)', alpha=0.5)
    ax3.set_xlabel('Time Index')
    ax3.set_ylabel('Value')
    ax3.set_title('Time Series Comparison (Before Correction)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. ì‹œê³„ì—´ ë¹„êµ (ë³´ì • í›„)
    ax4 = axes[1, 0]
    ax4.plot(range(plot_size), y_test[:plot_size], 'b-', label='Actual', alpha=0.7, linewidth=1)
    ax4.plot(range(plot_size), y_pred_corrected[:plot_size], 'g--', label='Predicted (Corrected)', alpha=0.7, linewidth=1)
    ax4.axhline(y=300, color='orange', linestyle='--', label='Extreme(300)', alpha=0.5)
    ax4.set_xlabel('Time Index')
    ax4.set_ylabel('Value')
    ax4.set_title('Time Series Comparison (After Correction)')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # 5. ì˜¤ì°¨ ë¶„í¬ ë¹„êµ
    ax5 = axes[1, 1]
    errors_original = y_pred_original - y_test
    errors_corrected = y_pred_corrected - y_test
    ax5.hist(errors_original, bins=50, alpha=0.5, label='Before', color='blue', edgecolor='black')
    ax5.hist(errors_corrected, bins=50, alpha=0.5, label='After', color='green', edgecolor='black')
    ax5.axvline(x=0, color='r', linestyle='--', linewidth=2)
    ax5.set_xlabel('Prediction Error')
    ax5.set_ylabel('Frequency')
    ax5.set_title(f'Error Distribution Comparison')
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # 6. ê·¹ë‹¨ê°’ ì„±ëŠ¥ (ë³´ì • í›„)
    ax6 = axes[1, 2]
    extreme_mask = y_test >= 300
    if extreme_mask.any():
        ax6.scatter(y_test[~extreme_mask], y_pred_corrected[~extreme_mask], 
                   alpha=0.3, s=5, label='Normal', color='blue')
        ax6.scatter(y_test[extreme_mask], y_pred_corrected[extreme_mask], 
                   alpha=0.8, s=20, label='Extreme(300+)', color='red')
        ax6.plot([200, 500], [200, 500], 'k--', lw=1)
        ax6.axhline(y=300, color='orange', linestyle='--', alpha=0.5)
        ax6.axvline(x=300, color='orange', linestyle='--', alpha=0.5)
        ax6.set_xlabel('Actual')
        ax6.set_ylabel('Predicted (Corrected)')
        ax6.set_title(f'Extreme Value Performance (After Correction)\nDetected: {extreme_detected_corrected}/{extreme_count}')
        ax6.legend()
        ax6.grid(True, alpha=0.3)
    
    # 7. ì í”„ ì¼€ì´ìŠ¤ ë¶„ì„
    ax7 = axes[2, 0]
    if jump_count > 0:
        jump_mask = df_results['ì í”„ì¼€ì´ìŠ¤'] == 'O'
        jump_actual = df_results[jump_mask]['ì‹¤ì œê°’'].values
        jump_pred_corrected = df_results[jump_mask]['ì˜ˆì¸¡ê°’_ë³´ì •í›„'].values
        jump_seq_max = df_results[jump_mask]['ì‹œí€€ìŠ¤MAX'].values
        
        ax7.scatter(jump_seq_max, jump_actual, label='Actual Jump', s=50, alpha=0.8, color='red')
        ax7.scatter(jump_seq_max, jump_pred_corrected, label='Predicted (Corrected)', s=30, alpha=0.6, color='blue')
        
        for i in range(len(jump_seq_max)):
            ax7.plot([jump_seq_max[i], jump_seq_max[i]], 
                    [jump_pred_corrected[i], jump_actual[i]], 
                    'gray', alpha=0.3, linewidth=0.5)
        
        ax7.axhline(y=300, color='orange', linestyle='--', label='Threshold')
        ax7.axvline(x=280, color='green', linestyle='--', alpha=0.5, label='SeqMax=280')
        ax7.set_xlabel('Sequence MAX')
        ax7.set_ylabel('Value')
        ax7.set_title(f'Jump Cases Analysis (After Correction)\n(SeqMax<280 â†’ Actualâ‰¥300: {jump_count} cases)')
        ax7.legend()
        ax7.grid(True, alpha=0.3)
    else:
        ax7.text(0.5, 0.5, 'No Jump Cases', ha='center', va='center', fontsize=14)
        ax7.set_title('Jump Cases Analysis')
    
    # 8. ìœ„í—˜ë„ë³„ ë¶„í¬
    ax8 = axes[2, 1]
    risk_counts = df_results['ìœ„í—˜ë„'].value_counts()
    colors_risk = {'HIGH': 'red', 'MEDIUM': 'orange', 'LOW': 'green'}
    bars = ax8.bar(risk_counts.index, risk_counts.values, 
                   color=[colors_risk.get(x, 'gray') for x in risk_counts.index])
    ax8.set_xlabel('Risk Level')
    ax8.set_ylabel('Count')
    ax8.set_title('Risk Level Distribution')
    for bar in bars:
        height = bar.get_height()
        ax8.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}',
                ha='center', va='bottom')
    ax8.grid(True, alpha=0.3)
    
    # 9. Feature ì¤‘ìš”ë„ (Top 20)
    ax9 = axes[2, 2]
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False).head(20)
    
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(feature_importance)))
    ax9.barh(range(len(feature_importance)), feature_importance['importance'].values[::-1], color=colors[::-1])
    ax9.set_yticks(range(len(feature_importance)))
    ax9.set_yticklabels(feature_importance['feature'].values[::-1], fontsize=7)
    ax9.set_xlabel('Importance')
    ax9.set_title('Top 20 Feature Importance')
    ax9.grid(True, alpha=0.3)
    
    plt.suptitle('BBB.CSV Evaluation Results with Surge Risk Correction', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig('BBB_evaluation_graphs_with_correction.png', dpi=150, bbox_inches='tight')
    print("âœ… ê·¸ë˜í”„ ì €ì¥: BBB_evaluation_graphs_with_correction.png")
    
    # ===== 5. ìµœì¢… ìš”ì•½ =====
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… í‰ê°€ ìš”ì•½ (ê¸‰ì¦ ìœ„í—˜ë„ ë³´ì • í¬í•¨)")
    print("="*80)
    
    print(f"\n1. ëª¨ë¸ ì„±ëŠ¥:")
    print(f"   [í•™ìŠµ ë°ì´í„°]")
    print(f"   - MAE: {train_mae:.2f}")
    print(f"   - RÂ²:  {train_r2:.3f}")
    
    print(f"\n   [í‰ê°€ ë°ì´í„° - ë³´ì • ì „]")
    print(f"   - MAE:  {test_mae_original:.2f}")
    print(f"   - RMSE: {test_rmse_original:.2f}")
    print(f"   - RÂ²:   {test_r2_original:.3f}")
    
    print(f"\n   [í‰ê°€ ë°ì´í„° - ë³´ì • í›„]")
    print(f"   - MAE:  {test_mae_corrected:.2f} (ê°œì„ : {test_mae_original - test_mae_corrected:.2f})")
    print(f"   - RMSE: {test_rmse_corrected:.2f}")
    print(f"   - RÂ²:   {test_r2_corrected:.3f}")
    
    print(f"\n2. íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì„±ëŠ¥:")
    print(f"   [ê·¹ë‹¨ê°’ (300+)]")
    print(f"   - ë°œìƒ: {extreme_count}ê°œ")
    print(f"   - ê°ì§€(ë³´ì •ì „): {extreme_detected_original}ê°œ ({extreme_detected_original/extreme_count*100 if extreme_count > 0 else 0:.1f}%)")
    print(f"   - ê°ì§€(ë³´ì •í›„): {extreme_detected_corrected}ê°œ ({extreme_detected_corrected/extreme_count*100 if extreme_count > 0 else 0:.1f}%)")
    
    print(f"\n   [300 ê¸‰ì¦ (300ë¯¸ë§Œâ†’300ì´ìƒ)]")
    print(f"   - ë°œìƒ: {surge_count}ê°œ")
    print(f"   - ì˜ˆì¸¡ì„±ê³µ(ë³´ì •ì „): {surge_detected_original}ê°œ ({surge_detected_original/surge_count*100 if surge_count > 0 else 0:.1f}%)")
    print(f"   - ì˜ˆì¸¡ì„±ê³µ(ë³´ì •í›„): {surge_detected_corrected}ê°œ ({surge_detected_corrected/surge_count*100 if surge_count > 0 else 0:.1f}%)")
    print(f"   - ê°œì„ : +{surge_detected_corrected - surge_detected_original}ê°œ")
    
    print(f"\n   [ì í”„ ì¼€ì´ìŠ¤]")
    print(f"   - ë°œìƒ: {jump_count}ê°œ")
    
    print(f"\n3. Feature ì •ë³´:")
    print(f"   - ì´ Feature ìˆ˜: {len(X_train.columns)}ê°œ")
    print(f"   - ì‚¬ìš©ëœ ì»¬ëŸ¼ ê·¸ë£¹: inflow(3), outflow(3), cmd(2), maxcapa(2), storage(1)")
    
    print(f"\n4. Top 10 ì¤‘ìš” Features:")
    top_features = feature_importance.head(10)
    for idx, row in top_features.iterrows():
        print(f"   - {row['feature']}: {row['importance']:.4f}")
    
    print(f"\n5. ì €ì¥ íŒŒì¼:")
    print(f"   - ëª¨ë¸: xgboost_model_30min_10min.pkl")
    print(f"   - ê²°ê³¼: BBB_evaluation_results_with_correction.csv")
    print(f"   - ê·¸ë˜í”„: BBB_evaluation_graphs_with_correction.png")
    
    print(f"\n6. ë³´ì • ë¡œì§ ìš”ì•½:")
    print(f"   - HIGH ìœ„í—˜ë„: CMD<180, STORAGE 30~40, CMD 260~280")
    print(f"   - MEDIUM ìœ„í—˜ë„: CMD<220")
    print(f"   - LOW ìœ„í—˜ë„: ê¸°íƒ€")
    
    return model, df_results

# ì‹¤í–‰
if __name__ == '__main__':
    model, results = train_and_evaluate_complete()