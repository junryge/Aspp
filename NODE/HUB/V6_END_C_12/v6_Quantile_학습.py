import numpy as np
import pandas as pd
import lightgbm as lgb
import pickle
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

warnings.filterwarnings('ignore')
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def train_and_evaluate_quantile():
    """
    LightGBM Quantile Regression (alpha=0.9)
    ë³´ìˆ˜ì  ì˜ˆì¸¡ìœ¼ë¡œ ì‚¬ì „ê°ì§€ ì„±ëŠ¥ í–¥ìƒ
    """
    print("="*80)
    print("LightGBM ë¶„ìœ„ íšŒê·€ (Quantile Regression) - alpha=0.9")
    print("30ë¶„ â†’ 10ë¶„ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€")
    print("="*80)
    
    # í•µì‹¬ 12ê°œ ì»¬ëŸ¼
    FEATURE_COLS = {
        'storage': ['M16A_3F_STORAGE_UTIL'],
        'cmd': ['M16A_3F_CMD', 'M16A_6F_TO_HUB_CMD'],
        'inflow': ['M16A_6F_TO_HUB_JOB', 'M16A_2F_TO_HUB_JOB2', 'M14A_3F_TO_HUB_JOB2'],
        'outflow': ['M16A_3F_TO_M16A_6F_JOB', 'M16A_3F_TO_M16A_2F_JOB', 'M16A_3F_TO_M14A_3F_JOB'],
        'maxcapa': ['M16A_6F_LFT_MAXCAPA', 'M16A_2F_LFT_MAXCAPA']
    }
    
    TARGET_COL = 'CURRENT_M16A_3F_JOB_2'
    
    # Feature ìƒì„± í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
    def create_features(df, start_idx=30):
        features_list = []
        labels = []
        seq_max_list = []
        seq_min_list = []
        indices = []
        
        for i in range(start_idx, len(df) - 10):
            seq_target = df[TARGET_COL].iloc[i-30:i].values
            
            features = {
                # íƒ€ê²Ÿ ì»¬ëŸ¼ íŠ¹ì„±
                'target_mean': np.mean(seq_target),
                'target_std': np.std(seq_target),
                'target_last_5_mean': np.mean(seq_target[-5:]),
                'target_max': np.max(seq_target),
                'target_min': np.min(seq_target),
                'target_slope': np.polyfit(np.arange(30), seq_target, 1)[0],
                'target_last_10_mean': np.mean(seq_target[-10:]),
                'target_first_10_mean': np.mean(seq_target[:10]),
            }
            
            # ê° ì»¬ëŸ¼ ê·¸ë£¹ë³„ íŠ¹ì„± ì¶”ê°€
            for group_name, cols in FEATURE_COLS.items():
                for col in cols:
                    if col in df.columns:
                        seq_data = df[col].iloc[i-30:i].values
                        
                        features[f'{col}_mean'] = np.mean(seq_data)
                        features[f'{col}_std'] = np.std(seq_data)
                        features[f'{col}_max'] = np.max(seq_data)
                        features[f'{col}_min'] = np.min(seq_data)
                        features[f'{col}_last_5_mean'] = np.mean(seq_data[-5:])
                        features[f'{col}_last_10_mean'] = np.mean(seq_data[-10:])
                        features[f'{col}_slope'] = np.polyfit(np.arange(30), seq_data, 1)[0]
                        features[f'{col}_first_10_mean'] = np.mean(seq_data[:10])
                        features[f'{col}_mid_10_mean'] = np.mean(seq_data[10:20])
                        features[f'{col}_last_value'] = seq_data[-1]
            
            # ìœ ì…-ìœ ì¶œ ì°¨ì´
            inflow_sum = 0
            outflow_sum = 0
            for col in FEATURE_COLS['inflow']:
                if col in df.columns:
                    inflow_sum += df[col].iloc[i-1]
            for col in FEATURE_COLS['outflow']:
                if col in df.columns:
                    outflow_sum += df[col].iloc[i-1]
            features['net_flow'] = inflow_sum - outflow_sum
            
            # CMD ì´í•©
            cmd_sum = 0
            for col in FEATURE_COLS['cmd']:
                if col in df.columns:
                    cmd_sum += df[col].iloc[i-1]
            features['total_cmd'] = cmd_sum
            
            features_list.append(features)
            labels.append(df[TARGET_COL].iloc[i:i+10].max())
            seq_max_list.append(np.max(seq_target))
            seq_min_list.append(np.min(seq_target))
            indices.append(i)
        
        return pd.DataFrame(features_list), np.array(labels), seq_max_list, seq_min_list, indices
    
    # ===== 1. í•™ìŠµ ë‹¨ê³„ =====
    print("\n[STEP 1] í•™ìŠµ ë°ì´í„° ë¡œë“œ ë° Feature ìƒì„±")
    print("-"*40)
    
    df_train = pd.read_csv('HUB_0509_TO_0929_DATA.csv', on_bad_lines='skip')
    
    print(f"í•™ìŠµ ë°ì´í„°: {len(df_train)}ê°œ í–‰")
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ í™•ì¸:")
    
    all_feature_cols = []
    for group_name, cols in FEATURE_COLS.items():
        available = [col for col in cols if col in df_train.columns]
        all_feature_cols.extend(available)
        print(f"  - {group_name}: {len(available)}/{len(cols)}ê°œ")
    
    # í•™ìŠµ ë°ì´í„° ìƒì„±
    X_train, y_train, _, _, _ = create_features(df_train)
    
    print(f"\nìƒì„±ëœ Feature ìˆ˜: {len(X_train.columns)}ê°œ")
    print(f"í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(X_train)}ê°œ")
    
    # í•™ìŠµ/ê²€ì¦ ë¶„í• 
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42
    )
    
    # ===== 2. LightGBM Quantile Regression í•™ìŠµ =====
    print("\n[STEP 2] LightGBM ë¶„ìœ„ íšŒê·€ ëª¨ë¸ í•™ìŠµ")
    print("-"*40)
    print("ğŸ“Š ëª¨ë¸ ì„¤ì •:")
    print("  - objective: quantile (ë¶„ìœ„ íšŒê·€)")
    print("  - alpha: 0.9 (90% ë¶„ìœ„ìˆ˜ ì˜ˆì¸¡)")
    print("  - íš¨ê³¼: ë³´ìˆ˜ì  ì˜ˆì¸¡ (ë†’ê²Œ ì˜ˆì¸¡)")
    print()
    
    model = lgb.LGBMRegressor(
        objective='quantile',  # ë¶„ìœ„ íšŒê·€!
        alpha=0.9,              # 90% ë¶„ìœ„ìˆ˜
        n_estimators=150,
        max_depth=6,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1,
        verbose=-1
    )
    
    print("ëª¨ë¸ í•™ìŠµ ì¤‘...")
    model.fit(
        X_tr, y_tr,
        eval_set=[(X_val, y_val)],
        callbacks=[lgb.log_evaluation(period=0)]
    )
    
    # í•™ìŠµ ë°ì´í„° í‰ê°€
    y_val_pred = model.predict(X_val)
    train_mae = mean_absolute_error(y_val, y_val_pred)
    train_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
    train_r2 = r2_score(y_val, y_val_pred)
    
    print(f"\ní•™ìŠµ ë°ì´í„° ì„±ëŠ¥:")
    print(f"  MAE:  {train_mae:.4f}")
    print(f"  RMSE: {train_rmse:.4f}")
    print(f"  RÂ²:   {train_r2:.4f}")
    
    # ë³´ìˆ˜ì  ì˜ˆì¸¡ í™•ì¸
    over_predict = np.sum(y_val_pred > y_val)
    print(f"\në³´ìˆ˜ì  ì˜ˆì¸¡ í™•ì¸:")
    print(f"  ì‹¤ì œë³´ë‹¤ ë†’ê²Œ ì˜ˆì¸¡: {over_predict}/{len(y_val)} ({over_predict/len(y_val)*100:.1f}%)")
    print(f"  í‰ê·  ì˜ˆì¸¡ - ì‹¤ì œ: {np.mean(y_val_pred - y_val):.2f}")
    
    # ëª¨ë¸ ì €ì¥
    model_file = 'lightgbm_quantile_model_30min_10min_12ì»¬ëŸ¼.pkl'
    with open(model_file, 'wb') as f:
        pickle.dump(model, f)
    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_file}")
    
    # ===== 3. í‰ê°€ ë‹¨ê³„ (BBB.CSV) =====
    print("\n[STEP 3] í‰ê°€ ë°ì´í„°ë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("-"*40)
    
    df_test = pd.read_csv('HUB_20250916_to_20250929.CSV', on_bad_lines='skip')
    
    print(f"í‰ê°€ ë°ì´í„°: {len(df_test)}ê°œ í–‰")
    
    # Feature ìƒì„±
    X_test, y_test, seq_max_list, seq_min_list, indices = create_features(df_test)
    
    # ì˜ˆì¸¡
    y_pred = model.predict(X_test)
    
    # í‰ê°€ ì§€í‘œ
    test_mae = mean_absolute_error(y_test, y_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    test_r2 = r2_score(y_test, y_pred)
    
    print(f"\ní‰ê°€ ë°ì´í„° ì„±ëŠ¥:")
    print(f"  MAE:  {test_mae:.4f}")
    print(f"  RMSE: {test_rmse:.4f}")
    print(f"  RÂ²:   {test_r2:.4f}")
    
    # ë³´ìˆ˜ì  ì˜ˆì¸¡ í™•ì¸
    over_predict_test = np.sum(y_pred > y_test)
    print(f"\në³´ìˆ˜ì  ì˜ˆì¸¡ í™•ì¸:")
    print(f"  ì‹¤ì œë³´ë‹¤ ë†’ê²Œ ì˜ˆì¸¡: {over_predict_test}/{len(y_test)} ({over_predict_test/len(y_test)*100:.1f}%)")
    print(f"  í‰ê·  ì˜ˆì¸¡ - ì‹¤ì œ: {np.mean(y_pred - y_test):.2f}")
    
    # ì‚¬ì „ê°ì§€ ì„±ëŠ¥
    extreme_actual = np.sum(y_test >= 300)
    extreme_predicted = np.sum(y_pred >= 300)
    extreme_detected = np.sum((y_test >= 300) & (y_pred >= 300))
    
    print(f"\nğŸš¨ ì‚¬ì „ê°ì§€ ì„±ëŠ¥:")
    print(f"  ì‹¤ì œ 300+: {extreme_actual}ê°œ")
    print(f"  ì˜ˆì¸¡ 300+: {extreme_predicted}ê°œ")
    print(f"  ì •í™• ê°ì§€: {extreme_detected}/{extreme_actual}ê°œ ({extreme_detected/extreme_actual*100 if extreme_actual > 0 else 0:.1f}%)")
    
    # ===== 4. ìƒì„¸ ê²°ê³¼ ìƒì„± =====
    print("\n[STEP 4] ìƒì„¸ ë¶„ì„ ê²°ê³¼ ìƒì„±")
    print("-"*40)
    
    # STAT_DT ì²˜ë¦¬
    if 'STAT_DT' in df_test.columns:
        try:
            df_test['STAT_DT'] = pd.to_datetime(df_test['STAT_DT'], format='%Y%m%d%H%M')
        except:
            try:
                df_test['STAT_DT'] = pd.to_datetime(df_test['STAT_DT'])
            except:
                base_date = datetime(2024, 1, 1)
                df_test['STAT_DT'] = [base_date + timedelta(minutes=i) for i in range(len(df_test))]
    else:
        base_date = datetime(2024, 1, 1)
        df_test['STAT_DT'] = [base_date + timedelta(minutes=i) for i in range(len(df_test))]
    
    # ê²°ê³¼ DataFrame ìƒì„±
    results = []
    
    for i, idx in enumerate(indices):
        current_time = df_test['STAT_DT'].iloc[idx]
        seq_start_time = df_test['STAT_DT'].iloc[idx-30]
        prediction_time = current_time + timedelta(minutes=10)
        
        results.append({
            'í˜„ì¬ì‹œê°„': current_time.strftime('%Y-%m-%d %H:%M'),
            'ì˜ˆì¸¡ì‹œê°„(+10ë¶„)': prediction_time.strftime('%Y-%m-%d %H:%M'),
            'ì‹œí€€ìŠ¤ì‹œì‘': seq_start_time.strftime('%Y-%m-%d %H:%M'),
            'ì‹¤ì œê°’': round(y_test[i], 2),
            'ì˜ˆì¸¡ê°’': round(y_pred[i], 2),
            'ì˜¤ì°¨': round(y_pred[i] - y_test[i], 2),  # ì˜ˆì¸¡ - ì‹¤ì œ
            'ì ˆëŒ€ì˜¤ì°¨': round(abs(y_test[i] - y_pred[i]), 2),
            'ì˜¤ì°¨ìœ¨(%)': round(abs(y_test[i] - y_pred[i]) / y_test[i] * 100, 2),
            'ì‹œí€€ìŠ¤MAX': round(seq_max_list[i], 2),
            'ì‹œí€€ìŠ¤MIN': round(seq_min_list[i], 2),
            'ë³´ìˆ˜ì ì˜ˆì¸¡': 'O' if y_pred[i] > y_test[i] else 'X',
            'ì‹¤ì œ_300+': 'O' if y_test[i] >= 300 else '-',
            'ì˜ˆì¸¡_300+': 'O' if y_pred[i] >= 300 else '-',
            'ì‚¬ì „ê°ì§€ì„±ê³µ': 'O' if (y_test[i] >= 300 and y_pred[i] >= 300) else '-'
        })
    
    df_results = pd.DataFrame(results)
    
    # CSV ì €ì¥
    output_file = 'BBB_evaluation_quantile_results.csv'
    df_results.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"âœ… ìƒì„¸ ê²°ê³¼ ì €ì¥: {output_file}")
    
    # ===== 5. ê·¸ë˜í”„ ìƒì„± =====
    print("\n[STEP 5] í‰ê°€ ê·¸ë˜í”„ ìƒì„±")
    print("-"*40)
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. ì˜ˆì¸¡ vs ì‹¤ì œ
    ax1 = axes[0, 0]
    ax1.scatter(y_test, y_pred, alpha=0.5, s=10)
    ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    ax1.axhline(y=300, color='orange', linestyle='--', alpha=0.5)
    ax1.axvline(x=300, color='orange', linestyle='--', alpha=0.5)
    ax1.set_xlabel('Actual')
    ax1.set_ylabel('Predicted (Quantile 0.9)')
    ax1.set_title(f'Quantile Regression (alpha=0.9)\nMAE={test_mae:.2f}, RÂ²={test_r2:.3f}')
    ax1.grid(True, alpha=0.3)
    
    # 2. ì‹œê³„ì—´ ë¹„êµ
    ax2 = axes[0, 1]
    plot_size = min(300, len(y_test))
    ax2.plot(range(plot_size), y_test[:plot_size], 'b-', label='Actual', alpha=0.7, linewidth=1)
    ax2.plot(range(plot_size), y_pred[:plot_size], 'r--', label='Predicted', alpha=0.7, linewidth=1)
    ax2.axhline(y=300, color='orange', linestyle='--', label='Critical(300)', alpha=0.5)
    ax2.set_xlabel('Time Index')
    ax2.set_ylabel('Value')
    ax2.set_title('Time Series Comparison (First 300)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. ì˜¤ì°¨ ë¶„í¬
    ax3 = axes[1, 0]
    errors = y_pred - y_test
    ax3.hist(errors, bins=50, edgecolor='black', alpha=0.7, color='skyblue')
    ax3.axvline(x=0, color='r', linestyle='--', linewidth=2)
    ax3.axvline(x=np.mean(errors), color='g', linestyle='--', linewidth=2, label=f'Mean={np.mean(errors):.2f}')
    ax3.set_xlabel('Prediction Error (Predicted - Actual)')
    ax3.set_ylabel('Frequency')
    ax3.set_title(f'Error Distribution (Quantile 0.9)\nMean={np.mean(errors):.2f}, Std={np.std(errors):.2f}')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. ê·¹ë‹¨ê°’ ì„±ëŠ¥
    ax4 = axes[1, 1]
    extreme_mask = y_test >= 300
    if extreme_mask.any():
        ax4.scatter(y_test[~extreme_mask], y_pred[~extreme_mask], 
                   alpha=0.3, s=5, label='Normal', color='blue')
        ax4.scatter(y_test[extreme_mask], y_pred[extreme_mask], 
                   alpha=0.8, s=20, label='Critical(300+)', color='red')
        ax4.plot([200, 500], [200, 500], 'k--', lw=1)
        ax4.axhline(y=300, color='orange', linestyle='--', alpha=0.5)
        ax4.axvline(x=300, color='orange', linestyle='--', alpha=0.5)
        ax4.set_xlabel('Actual')
        ax4.set_ylabel('Predicted')
        ax4.set_title(f'Critical Value Detection\nDetected: {extreme_detected}/{extreme_actual}')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
    
    plt.suptitle('LightGBM Quantile Regression (alpha=0.9) Evaluation', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    graph_file = 'BBB_evaluation_quantile_graphs.png'
    plt.savefig(graph_file, dpi=150, bbox_inches='tight')
    print(f"âœ… ê·¸ë˜í”„ ì €ì¥: {graph_file}")
    
    # ===== 6. ìµœì¢… ìš”ì•½ =====
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… í‰ê°€ ìš”ì•½")
    print("="*80)
    print(f"1. ëª¨ë¸ ì„±ëŠ¥:")
    print(f"   - í•™ìŠµ MAE: {train_mae:.2f}")
    print(f"   - í‰ê°€ MAE: {test_mae:.2f}")
    
    print(f"\n2. ë³´ìˆ˜ì  ì˜ˆì¸¡ íŠ¹ì„±:")
    print(f"   - í‰ê·  (ì˜ˆì¸¡ - ì‹¤ì œ): {np.mean(y_pred - y_test):.2f}")
    print(f"   - ì‹¤ì œë³´ë‹¤ ë†’ê²Œ ì˜ˆì¸¡: {over_predict_test}/{len(y_test)} ({over_predict_test/len(y_test)*100:.1f}%)")
    
    print(f"\n3. ì‚¬ì „ê°ì§€ ì„±ëŠ¥:")
    print(f"   - ì‹¤ì œ 300+: {extreme_actual}ê°œ")
    print(f"   - ê°ì§€ ì„±ê³µ: {extreme_detected}ê°œ ({extreme_detected/extreme_actual*100 if extreme_actual > 0 else 0:.1f}%)")
    
    print(f"\n4. ì €ì¥ íŒŒì¼:")
    print(f"   - ëª¨ë¸: {model_file}")
    print(f"   - ê²°ê³¼: {output_file}")
    print(f"   - ê·¸ë˜í”„: {graph_file}")
    
    return model, df_results

# ì‹¤í–‰
if __name__ == '__main__':
    model, results = train_and_evaluate_quantile()