================================================================================
          ML/DL 자원 확보 요청 (V100 1장, DDR 64GB 확보 요청)
================================================================================

1. 현재 보유 자원
--------------------------------------------------------------------------------
  구분                 현재 스펙
--------------------------------------------------------------------------------
  GPU                  V100 32GB × 2장 (총 64GB)
  시스템 메모리 (RAM)   64GB
--------------------------------------------------------------------------------


2. 운영 중인 예측 모델 (FAB 1개 기준)
================================================================================

[CPU 처리 모델 - XGBoost V7]
--------------------------------------------------------------------------------
  모델                 입력          출력           용도             처리방식
--------------------------------------------------------------------------------
  XGBoost V7 (10분)    30분 시퀀스   10분 후 예측   단기 급증 감지   CPU 가능
  XGBoost V7 (30분)    30분 시퀀스   30분 후 예측   중기 급증 감지   CPU 가능
--------------------------------------------------------------------------------
  * XGBoost는 tree 기반 모델로 CPU에서도 충분히 학습/추론 가능
  * 학습 시 메모리: 4-8GB (RAM)
--------------------------------------------------------------------------------

[GPU 필수 모델 - HUBROOM 딥러닝 앙상블]
--------------------------------------------------------------------------------
  모델                    입력           출력           용도              학습 시 GPU
--------------------------------------------------------------------------------
  HUBROOM 앙상블 (10분)   280분 시퀀스   10분 후 예측   장기 패턴 단기    24-32GB
  HUBROOM 앙상블 (30분)   280분 시퀀스   30분 후 예측   장기 패턴 중기    24-32GB
--------------------------------------------------------------------------------
  * 딥러닝 모델은 GPU 없이 학습 시 수십 배 느려짐 → GPU 필수
  * 합계 (1개 FAB): 48-64GB GPU 필요
--------------------------------------------------------------------------------

[HUBROOM 앙상블 세부 구성]
--------------------------------------------------------------------------------
  모델             가중치    처리방식      메모리        역할
--------------------------------------------------------------------------------
  XGBoost          20%       CPU           4-8GB RAM     기본 예측 모델
  LSTM             20%       GPU           8GB           장기 시계열 패턴 학습
  GRU              15%       GPU           6GB           단기 변화 감지
  CNN-LSTM         20%       GPU           10GB          공간-시간 통합 패턴
  Spike Detector   15%       GPU           4GB           급증 특화 감지
  Rule-Based       10%       -             -             도메인 지식 기반 규칙
--------------------------------------------------------------------------------
  * XGBoost: CPU로 처리 가능
  * LSTM/GRU/CNN-LSTM/Spike Detector: 딥러닝 모델 → GPU 필수
  * 280분 시퀀스 × 11개 Feature → 대용량 텐서 연산 필요
--------------------------------------------------------------------------------


3. 현재 상황 (V100 2장 / 64GB RAM)
================================================================================

[GPU - 정상 운영 중]
--------------------------------------------------------------------------------
  * 현재 1개 FAB 기준 모델 학습 정상 수행 중
  * XGBoost V7: CPU로 문제없이 학습
  * HUBROOM 앙상블: V100 2장(64GB)으로 정상 학습
--------------------------------------------------------------------------------

[RAM - 메모리 부족 오류 발생 중]
--------------------------------------------------------------------------------
  * 현재 64GB RAM으로 메모리 부족 오류 간헐적 발생
  * 데이터 양이 증가함에 따라 메모리 사용량 지속 증가
  * 대용량 데이터 로딩 + 전처리 + 학습 동시 수행 시 메모리 초과
  * RAM 추가 확보 필수
--------------------------------------------------------------------------------
================================================================================


4. FAB 11개 확장 예정 - 자원 부족 예상
================================================================================

[확장 대상 FAB]
--------------------------------------------------------------------------------
  사이트     FAB                      적용 모델
--------------------------------------------------------------------------------
  이천       M14A, M14B, M16A, M16B   V7(CPU) + HUBROOM 앙상블(GPU)
  청주       M15, M15X                V7(CPU) + HUBROOM 앙상블(GPU)
  우시       C2, C3                   V7(CPU)
  신규       추가 3개 예정            V7(CPU) + HUBROOM 앙상블(GPU)
--------------------------------------------------------------------------------

[11개 FAB 전체 모델 현황]
--------------------------------------------------------------------------------
  모델 유형              처리방식     모델 수     학습 자원
--------------------------------------------------------------------------------
  XGBoost V7 (10분)      CPU          11개        RAM 4-8GB (문제없음)
  XGBoost V7 (30분)      CPU          11개        RAM 4-8GB (문제없음)
  HUBROOM 앙상블 (10분)  GPU 필수     9개*        GPU 24-32GB
  HUBROOM 앙상블 (30분)  GPU 필수     9개*        GPU 24-32GB
--------------------------------------------------------------------------------
  합계                   -            40개        -
--------------------------------------------------------------------------------
  * 우시(C2, C3)는 V7만 적용, HUBROOM 앙상블 제외

[FAB 11개 확장 시 예상 문제]
--------------------------------------------------------------------------------
  작업                                필요량       현재 자원     예상 상태
--------------------------------------------------------------------------------
  XGBoost V7 (11개 FAB)               4-8GB RAM    64GB RAM      [O] 문제없음
  HUBROOM 앙상블 1개 FAB 학습         48-64GB      64GB GPU      [O] 가능
  HUBROOM 앙상블 2개 FAB 동시 학습    96-128GB     64GB GPU      [X] 불가
  9개 FAB HUBROOM 순차 학습           48-64GB      64GB GPU      [!] 18시간+ 소요
  다중 FAB 데이터 동시 로딩           100GB+       64GB RAM      [X] 불가
--------------------------------------------------------------------------------
  * GPU: 현재 1개 FAB는 문제없으나, FAB 11개 확장 시 순차 학습만 가능 (18시간+)
  * RAM: 현재도 메모리 부족 오류 발생, FAB 확장 시 더욱 심각해짐
--------------------------------------------------------------------------------


5. GPU가 필요한 이유 (딥러닝 모델)
================================================================================

[CPU vs GPU 학습 시간 비교 - HUBROOM 앙상블 1개 FAB 기준]
--------------------------------------------------------------------------------
  처리방식          LSTM      GRU       CNN-LSTM    전체 앙상블
--------------------------------------------------------------------------------
  GPU (V100)        30분      20분      40분        약 2시간
  CPU only          8시간     5시간     12시간      약 25시간+
--------------------------------------------------------------------------------
  * GPU 사용 시 10배 이상 빠름
  * FAB 11개 전체 학습 시 CPU only는 사실상 불가능 (200시간+)
--------------------------------------------------------------------------------

[딥러닝 모델이 GPU 필수인 이유]
--------------------------------------------------------------------------------
  1. 대용량 텐서 연산
     - 입력: 280분 × 11개 Feature = 3,080개 데이터 포인트/샘플
     - 수만 개 샘플 × 수백만 파라미터 → 병렬 연산 필수

  2. 행렬 곱셈 집약적
     - LSTM: 게이트 연산 (forget, input, output, cell)
     - CNN: 컨볼루션 필터 연산
     - GPU의 수천 개 코어로 병렬 처리 필요

  3. 역전파(Backpropagation) 연산
     - 수백만 개 가중치 동시 업데이트
     - CPU 순차 처리 시 수십 배 느림
--------------------------------------------------------------------------------


6. 추가 요청 자원
--------------------------------------------------------------------------------
  구분              현재                      추가 요청           변경 후
--------------------------------------------------------------------------------
  GPU               V100 32GB × 2장 (64GB)    V100 32GB × 1장     V100 3장 (96GB)
  시스템 메모리     64GB                      64GB                128GB
--------------------------------------------------------------------------------


7. 추가 후 기대 효과 (V100 3장 / 128GB RAM)
================================================================================

[GPU 필수 모델 (HUBROOM 앙상블) 학습 성능 비교]
--------------------------------------------------------------------------------
  항목                              현재 (V100 2장)      추가 후 (V100 3장)
--------------------------------------------------------------------------------
  1개 FAB 앙상블 학습               [O] 가능 (2시간)     [O] 가능 (2시간)
  2개 FAB 앙상블 동시 학습          [X] 불가             [O] 가능 (2시간)
  9개 FAB 앙상블 전체 학습          18시간+              6-8시간
--------------------------------------------------------------------------------

[FAB 11개 확장 시 효과]
--------------------------------------------------------------------------------
  항목                         현재 (V100 2장/64GB)   추가 후 (V100 3장/128GB)
--------------------------------------------------------------------------------
  HUBROOM 앙상블 동시 학습     1개 FAB만 가능         2개 FAB 동시 가능
  9개 FAB 앙상블 전체 학습     18시간+                6-8시간 (약 65% 단축)
  모델 재학습 주기             주 1회                 주 2-3회 가능
  긴급 재학습 대응             [X] 어려움             [O] 빠른 학습 가능
  신규 FAB 추가 시             기존 학습 중단 필요    병렬 처리 가능
  데이터 처리 안정성           메모리 오류 발생       메모리 오류 해결
--------------------------------------------------------------------------------


8. 요약
================================================================================

[모델별 처리 방식]
--------------------------------------------------------------------------------
  모델                    처리방식     GPU 필요 여부
--------------------------------------------------------------------------------
  XGBoost V7 (10분/30분)  CPU          불필요 (CPU로 충분)
  HUBROOM 앙상블          GPU          필수 (LSTM, GRU, CNN-LSTM)
--------------------------------------------------------------------------------

[자원 요청 근거]
--------------------------------------------------------------------------------
  항목           내용
--------------------------------------------------------------------------------
  현재 상황      GPU: 1개 FAB 정상 운영 중
                 RAM: 64GB 부족으로 메모리 오류 발생 중 (데이터 증가로 필수 추가)
  확장 계획      FAB 11개 (9개 FAB × 2개 앙상블 모델 = 18개 딥러닝 모델)
  예상 문제      FAB 11개 확장 시 동시 학습 불가, 순차 처리로 18시간+ 소요
  요청 자원      V100 1장 추가 (총 3장, 96GB) + RAM 64GB 추가 (총 128GB, 필수)
  기대 효과      학습 시간 65% 단축, 2개 FAB 동시 학습, 메모리 오류 해결
================================================================================