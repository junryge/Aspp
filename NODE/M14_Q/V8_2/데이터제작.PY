#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ðŸ”¥ 1700+ ë°ì´í„° ìƒì„±ê¸°
ì‹¤ì œ 1700+ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„° ìƒì„±
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

def analyze_1700_patterns(df):
    """1700+ íŒ¨í„´ ë¶„ì„"""
    print("\nðŸ“Š 1700+ íŒ¨í„´ ë¶„ì„")
    print("="*80)
    
    high_risk = df[df['TOTALCNT'] >= 1700].copy()
    
    if len(high_risk) == 0:
        print("âŒ 1700+ ìƒ˜í”Œ ì—†ìŒ!")
        return None
    
    print(f"1700+ ìƒ˜í”Œ: {len(high_risk)}ê°œ")
    
    # íŒ¨í„´ í†µê³„
    patterns = {
        'TOTALCNT': {
            'mean': high_risk['TOTALCNT'].mean(),
            'std': high_risk['TOTALCNT'].std(),
            'min': high_risk['TOTALCNT'].min(),
            'max': high_risk['TOTALCNT'].max()
        },
        'M14AM14B': {
            'mean': high_risk['M14AM14B'].mean(),
            'std': high_risk['M14AM14B'].std(),
            'min': high_risk['M14AM14B'].min(),
            'max': high_risk['M14AM14B'].max()
        },
        'M14AM10A': {
            'mean': high_risk['M14AM10A'].mean(),
            'std': high_risk['M14AM10A'].std(),
            'min': high_risk['M14AM10A'].min(),
            'max': high_risk['M14AM10A'].max()
        },
        'M14AM16': {
            'mean': high_risk['M14AM16'].mean(),
            'std': high_risk['M14AM16'].std(),
            'min': high_risk['M14AM16'].min(),
            'max': high_risk['M14AM16'].max()
        },
        'M14.QUE.OHT.OHTUTIL': {
            'mean': high_risk['M14.QUE.OHT.OHTUTIL'].mean(),
            'std': high_risk['M14.QUE.OHT.OHTUTIL'].std(),
            'min': high_risk['M14.QUE.OHT.OHTUTIL'].min(),
            'max': high_risk['M14.QUE.OHT.OHTUTIL'].max()
        },
        'M14.QUE.ALL.CURRENTQCREATED': {
            'mean': high_risk['M14.QUE.ALL.CURRENTQCREATED'].mean(),
            'std': high_risk['M14.QUE.ALL.CURRENTQCREATED'].std(),
            'min': high_risk['M14.QUE.ALL.CURRENTQCREATED'].min(),
            'max': high_risk['M14.QUE.ALL.CURRENTQCREATED'].max()
        },
        'M14.QUE.ALL.CURRENTQCOMPLETED': {
            'mean': high_risk['M14.QUE.ALL.CURRENTQCOMPLETED'].mean(),
            'std': high_risk['M14.QUE.ALL.CURRENTQCOMPLETED'].std(),
            'min': high_risk['M14.QUE.ALL.CURRENTQCOMPLETED'].min(),
            'max': high_risk['M14.QUE.ALL.CURRENTQCOMPLETED'].max()
        }
    }
    
    print(f"\nTOTALCNT: {patterns['TOTALCNT']['mean']:.1f} Â± {patterns['TOTALCNT']['std']:.1f}")
    print(f"M14AM14B: {patterns['M14AM14B']['mean']:.1f} Â± {patterns['M14AM14B']['std']:.1f}")
    print(f"OHT_UTIL: {patterns['M14.QUE.OHT.OHTUTIL']['mean']:.1f} Â± {patterns['M14.QUE.OHT.OHTUTIL']['std']:.1f}")
    
    return patterns, high_risk

def generate_new_sample(base_sample, patterns, variation=0.3):
    """ê¸°ì¡´ ìƒ˜í”Œ ê¸°ë°˜ìœ¼ë¡œ ìƒˆ ìƒ˜í”Œ ìƒì„± (í° ë³€í˜•!)"""
    new_sample = base_sample.copy()
    
    # TOTALCNT: 1700~1900 ë²”ìœ„ì—ì„œ ìƒì„± (ì •ìˆ˜!)
    new_sample['TOTALCNT'] = int(np.random.uniform(1700, 1900))
    
    # M14AM14B: í° ë³€í˜• (200~600)
    base_val = base_sample['M14AM14B']
    noise = np.random.normal(0, patterns['M14AM14B']['std'] * 0.8)
    new_sample['M14AM14B'] = int(np.clip(base_val + noise, 200, 600))
    
    # M14AM10A: í° ë³€í˜• (40~130)
    base_val = base_sample['M14AM10A']
    noise = np.random.normal(0, patterns['M14AM10A']['std'] * 0.8)
    new_sample['M14AM10A'] = int(np.clip(base_val + noise, 40, 130))
    
    # M14AM16: í° ë³€í˜• (60~120)
    base_val = base_sample['M14AM16']
    noise = np.random.normal(0, patterns['M14AM16']['std'] * 0.8)
    new_sample['M14AM16'] = int(np.clip(base_val + noise, 60, 120))
    
    # OHT_UTIL: 80~98 ë²”ìœ„ (ê³ ë¶€í•˜, í° ë³€í˜•)
    base_val = base_sample['M14.QUE.OHT.OHTUTIL']
    noise = np.random.normal(0, patterns['M14.QUE.OHT.OHTUTIL']['std'] * 0.8)
    new_sample['M14.QUE.OHT.OHTUTIL'] = round(np.clip(base_val + noise, 80, 98), 2)
    
    # CURRENTQCREATED: í° ë³€í˜•
    base_val = base_sample['M14.QUE.ALL.CURRENTQCREATED']
    noise = np.random.normal(0, patterns['M14.QUE.ALL.CURRENTQCREATED']['std'] * 0.8)
    new_sample['M14.QUE.ALL.CURRENTQCREATED'] = int(np.clip(base_val + noise, 2700, 3300))
    
    # CURRENTQCOMPLETED: í° ë³€í˜•
    base_val = base_sample['M14.QUE.ALL.CURRENTQCOMPLETED']
    noise = np.random.normal(0, patterns['M14.QUE.ALL.CURRENTQCOMPLETED']['std'] * 0.8)
    new_sample['M14.QUE.ALL.CURRENTQCOMPLETED'] = int(np.clip(base_val + noise, 2600, 3100))
    
    # M14.QUE.ALL.TRANSPORT4MINOVERCNT: í° ë³€í˜•
    base_val = base_sample['M14.QUE.ALL.TRANSPORT4MINOVERCNT']
    noise = np.random.uniform(-50, 50)
    new_sample['M14.QUE.ALL.TRANSPORT4MINOVERCNT'] = int(np.clip(base_val + noise, 60, 200))
    
    # M14B.QUE.SENDFAB.VERTICALQUEUECOUNT: í° ë³€í˜•
    base_val = base_sample['M14B.QUE.SENDFAB.VERTICALQUEUECOUNT']
    noise = np.random.uniform(-60, 60)
    new_sample['M14B.QUE.SENDFAB.VERTICALQUEUECOUNT'] = int(np.clip(base_val + noise, 130, 320))
    
    # M10A.QUE.ALL.CURRENTQCNT: í° ë³€í˜•
    base_val = base_sample['M10A.QUE.ALL.CURRENTQCNT']
    noise = np.random.uniform(-50, 50)
    new_sample['M10A.QUE.ALL.CURRENTQCNT'] = int(np.clip(base_val + noise, 280, 480))
    
    # M10A.QUE.OHT_OHS.OHTUTIL: í° ë³€í˜•
    base_val = base_sample['M10A.QUE.OHT_OHS.OHTUTIL']
    noise = np.random.uniform(-15, 15)
    new_sample['M10A.QUE.OHT_OHS.OHTUTIL'] = round(np.clip(base_val + noise, 25, 95), 2)
    
    # ê¸°íƒ€ ì»¬ëŸ¼ë“¤ (í° ë²”ìœ„)
    new_sample['M10AM14A'] = int(np.random.uniform(35, 65))
    new_sample['M14AM10ASUM'] = new_sample['M14AM10A'] + new_sample['M10AM14A']
    new_sample['M14BM14A'] = int(np.random.uniform(50, 120))
    new_sample['M14AM14BSUM'] = new_sample['M14AM14B'] + new_sample['M14BM14A']
    new_sample['M16M14A'] = int(np.random.uniform(110, 170))
    new_sample['M14AM16SUM'] = new_sample['M14AM16'] + new_sample['M16M14A']
    
    return new_sample

def generate_synthetic_data(df, num_samples=2000):
    """í•©ì„± ë°ì´í„° ìƒì„±"""
    print(f"\nðŸ”¥ í•©ì„± ë°ì´í„° ìƒì„± ì‹œìž‘")
    print(f"ëª©í‘œ: {num_samples}ê°œ ìƒì„±")
    print("="*80)
    
    # 1700+ íŒ¨í„´ ë¶„ì„
    result = analyze_1700_patterns(df)
    if result is None:
        return None
    
    patterns, high_risk_samples = result
    
    # ìƒˆ ë°ì´í„° ìƒì„±
    synthetic_data = []
    
    # ë§ˆì§€ë§‰ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
    last_time = pd.to_datetime(df['CURRTIME'].iloc[-1], format='%Y%m%d%H%M')
    
    print(f"\nâš™ï¸ ìƒì„± ì¤‘...")
    for i in range(num_samples):
        # ëžœë¤í•˜ê²Œ ê¸°ì¡´ 1700+ ìƒ˜í”Œ ì„ íƒ
        base_sample = high_risk_samples.sample(1).iloc[0]
        
        # ìƒˆ ìƒ˜í”Œ ìƒì„± (í° ë³€í˜•!)
        new_sample = generate_new_sample(base_sample, patterns, variation=0.5)
        
        # ì‹œê°„ ì„¤ì • (1ë¶„ì”© ì¦ê°€)
        new_time = last_time + timedelta(minutes=i+1)
        new_sample['CURRTIME'] = new_time.strftime('%Y%m%d%H%M')
        new_sample['TIME'] = new_time.strftime('%Y%m%d%H%M')
        
        synthetic_data.append(new_sample)
        
        if (i+1) % 500 == 0:
            print(f"  ì§„í–‰: {i+1}/{num_samples} ({(i+1)/num_samples*100:.1f}%)")
    
    print(f"âœ… ìƒì„± ì™„ë£Œ: {len(synthetic_data)}ê°œ")
    
    # DataFrame ë³€í™˜
    synthetic_df = pd.DataFrame(synthetic_data)
    
    # í†µê³„
    print(f"\nðŸ“Š ìƒì„±ëœ ë°ì´í„° í†µê³„:")
    print(f"  TOTALCNT ë²”ìœ„: {synthetic_df['TOTALCNT'].min():.0f} ~ {synthetic_df['TOTALCNT'].max():.0f}")
    print(f"  TOTALCNT í‰ê· : {synthetic_df['TOTALCNT'].mean():.1f}")
    print(f"  1700+ ìƒ˜í”Œ: {(synthetic_df['TOTALCNT'] >= 1700).sum()}ê°œ (100%)")
    
    return synthetic_df

def merge_data(original_df, synthetic_df):
    """ì›ë³¸ + í•©ì„± ë°ì´í„° ë³‘í•©"""
    print(f"\nðŸ”— ë°ì´í„° ë³‘í•©")
    print("="*80)
    
    print(f"ì›ë³¸: {len(original_df):,}ê°œ")
    print(f"í•©ì„±: {len(synthetic_df):,}ê°œ")
    
    # ë³‘í•©
    merged_df = pd.concat([original_df, synthetic_df], ignore_index=True)
    
    print(f"ë³‘í•© í›„: {len(merged_df):,}ê°œ")
    print(f"1700+ ìƒ˜í”Œ: {(merged_df['TOTALCNT'] >= 1700).sum():,}ê°œ")
    
    return merged_df

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*80)
    print("ðŸš€ 1700+ í•©ì„± ë°ì´í„° ìƒì„±ê¸°")
    print("="*80)
    
    # ì›ë³¸ ë°ì´í„° ë¡œë“œ
    csv_file = 'Ois.csv'
    print(f"\nðŸ“‚ ë°ì´í„° ë¡œë”©: {csv_file}")
    
    df = pd.read_csv(csv_file)
    print(f"âœ… ì›ë³¸: {len(df):,}í–‰")
    print(f"1700+ ìƒ˜í”Œ: {(df['TOTALCNT'] >= 1700).sum()}ê°œ")
    
    # í•©ì„± ë°ì´í„° ìƒì„± (ë” ë§Žì´!)
    num_synthetic = 3000  # ìƒì„±í•  ê°œìˆ˜
    synthetic_df = generate_synthetic_data(df, num_samples=num_synthetic)
    
    if synthetic_df is None:
        print("âŒ í•©ì„± ë°ì´í„° ìƒì„± ì‹¤íŒ¨")
        return
    
    # ë³‘í•©
    merged_df = merge_data(df, synthetic_df)
    
    # ì €ìž¥
    output_file = 'Ois_augmented.csv'
    merged_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nðŸ’¾ ì €ìž¥: {output_file}")
    
    print("\n" + "="*80)
    print("âœ… ì™„ë£Œ!")
    print("="*80)
    print(f"ì›ë³¸ íŒŒì¼: {csv_file}")
    print(f"ì¦ê°• íŒŒì¼: {output_file}")
    print(f"\nì›ë³¸: {len(df):,}ê°œ (1700+: {(df['TOTALCNT'] >= 1700).sum()}ê°œ)")
    print(f"ì¦ê°•: {len(merged_df):,}ê°œ (1700+: {(merged_df['TOTALCNT'] >= 1700).sum()}ê°œ)")
    print(f"\n1700+ ì¦ê°€: {(df['TOTALCNT'] >= 1700).sum()}ê°œ â†’ {(merged_df['TOTALCNT'] >= 1700).sum()}ê°œ")
    print("="*80)

if __name__ == '__main__':
    main()