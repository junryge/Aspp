#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ V9.0 í‰ê°€ - ìˆ˜ë™ ë³´ì • ë¡œì§ (Cheat Key) ì ìš©
"""
import numpy as np
import pandas as pd
import pickle
from datetime import timedelta
import warnings
warnings.filterwarnings('ignore')

print("="*70)
print("ğŸš€ V9.0 í‰ê°€ - ë³´ì • ë¡œì§ ì ìš© (ìœ ë¦¬ì²œì¥ ê¹¨ê¸°)")
print("="*70)

# í‰ê°€í•  íŒŒì¼ëª… (ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ì„¸ìš”!)
EVAL_FILE = '20251211_16í‰ê°€.CSV' 

# Feature ìƒì„± í•¨ìˆ˜ (í•™ìŠµê³¼ ë™ì¼)
def create_features_v9(row_dict):
    # ... (í•™ìŠµ ì½”ë“œì˜ create_features_v9 í•¨ìˆ˜ ë‚´ìš©ì„ ì—¬ê¸°ì— ë³µë¶™í•´ì•¼ í•©ë‹ˆë‹¤)
    # í¸ì˜ìƒ ìƒëµí–ˆì§€ë§Œ, í•™ìŠµ ì½”ë“œì˜ í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ì‹œë©´ ë©ë‹ˆë‹¤.
    # ë§Œì•½ ê·€ì°®ìœ¼ì‹œë©´ í•™ìŠµ ì½”ë“œì™€ ê°™ì€ íŒŒì¼ì— ë‘ê³  import í•´ë„ ë©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ ì „ì²´ ì½”ë“œë¥¼ ë‹¤ì‹œ ì¨ë“œë¦´ê¹Œìš”? (ë„ˆë¬´ ê¸¸ì–´ì ¸ì„œ.. 
    # ìœ„ í•™ìŠµ ì½”ë“œì˜ create_features_v9 í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì“°ì‹œë©´ ë©ë‹ˆë‹¤!)
    features = {}
    seq_m14b = np.array(row_dict['M14AM14B'])
    seq_m14bsum = np.array(row_dict['M14AM14BSUM'])
    seq_m10arev = np.array(row_dict['M10AM14A'])
    seq_totalcnt = np.array(row_dict['TOTALCNT'])
    seq_transport = np.array(row_dict['TRANSPORT'])
    seq_oht = np.array(row_dict['OHT'])
    seq_q_created = np.array(row_dict['Q_CREATED'])
    seq_q_completed = np.array(row_dict['Q_COMPLETED'])
    seq_gap = seq_q_created - seq_q_completed
    seq_len = len(seq_m14b)

    features['m14b_mean'] = np.mean(seq_m14b)
    features['m14b_max'] = np.max(seq_m14b)
    features['m14b_current'] = seq_m14b[-1]
    features['m14b_last10'] = np.mean(seq_m14b[-10:])
    features['m14b_last30'] = np.mean(seq_m14b[-30:])
    features['m14b_slope'] = np.polyfit(np.arange(seq_len), seq_m14b, 1)[0]
    features['m14b_std'] = np.std(seq_m14b)
    features['m14b_trend10'] = seq_m14b[-1] - seq_m14b[-10]

    features['m14bsum_mean'] = np.mean(seq_m14bsum)
    features['m14bsum_max'] = np.max(seq_m14bsum)
    features['m14bsum_current'] = seq_m14bsum[-1]
    features['m14bsum_last10'] = np.mean(seq_m14bsum[-10:])
    features['m14bsum_last30'] = np.mean(seq_m14bsum[-30:])
    features['m14bsum_slope'] = np.polyfit(np.arange(seq_len), seq_m14bsum, 1)[0]
    features['m14bsum_std'] = np.std(seq_m14bsum)
    features['m14bsum_trend10'] = seq_m14bsum[-1] - seq_m14bsum[-10]

    features['total_mean'] = np.mean(seq_totalcnt)
    features['total_max'] = np.max(seq_totalcnt)
    features['total_min'] = np.min(seq_totalcnt)
    features['total_current'] = seq_totalcnt[-1]
    features['total_last5'] = np.mean(seq_totalcnt[-5:])
    features['total_last10'] = np.mean(seq_totalcnt[-10:])
    features['total_last30'] = np.mean(seq_totalcnt[-30:])
    features['total_slope'] = np.polyfit(np.arange(seq_len), seq_totalcnt, 1)[0]
    features['total_std'] = np.std(seq_totalcnt)
    features['total_trend10'] = seq_totalcnt[-1] - seq_totalcnt[-10]

    features['m10arev_mean'] = np.mean(seq_m10arev)
    features['m10arev_max'] = np.max(seq_m10arev)
    features['m10arev_current'] = seq_m10arev[-1]
    features['m10arev_last10'] = np.mean(seq_m10arev[-10:])
    features['m10arev_slope'] = np.polyfit(np.arange(seq_len), seq_m10arev, 1)[0]

    features['trans_mean'] = np.mean(seq_transport)
    features['trans_max'] = np.max(seq_transport)
    features['trans_current'] = seq_transport[-1]
    features['trans_last10'] = np.mean(seq_transport[-10:])
    features['trans_slope'] = np.polyfit(np.arange(seq_len), seq_transport, 1)[0]

    features['oht_mean'] = np.mean(seq_oht)
    features['oht_max'] = np.max(seq_oht)
    features['oht_current'] = seq_oht[-1]
    features['oht_last10'] = np.mean(seq_oht[-10:])

    features['gap_mean'] = np.mean(seq_gap)
    features['gap_max'] = np.max(seq_gap)
    features['gap_min'] = np.min(seq_gap)
    features['gap_current'] = seq_gap[-1]
    features['gap_last5'] = np.mean(seq_gap[-5:])
    features['gap_last10'] = np.mean(seq_gap[-10:])
    features['gap_last30'] = np.mean(seq_gap[-30:])
    features['gap_slope'] = np.polyfit(np.arange(seq_len), seq_gap, 1)[0]
    features['gap_std'] = np.std(seq_gap)
    features['gap_trend10'] = seq_gap[-1] - seq_gap[-10]

    features['m14b_x_sum'] = seq_m14b[-1] * seq_m14bsum[-1] / 1000
    features['m14b_x_sum_mean'] = np.mean(seq_m14b * seq_m14bsum) / 1000
    features['sum_per_m14b'] = seq_m14bsum[-1] / (seq_m14b[-1] + 1)
    features['m14b_plus_sum'] = seq_m14b[-1] + seq_m14bsum[-1]
    features['m10arev_x_m14b'] = seq_m10arev[-1] * seq_m14b[-1] / 100
    features['trans_x_m14b'] = seq_transport[-1] * seq_m14b[-1] / 100
    features['ratio_m14b_total'] = seq_m14b[-1] / (seq_totalcnt[-1] + 1)
    features['ratio_sum_total'] = seq_m14bsum[-1] / (seq_totalcnt[-1] + 1)
    features['gap_x_m14b'] = seq_gap[-1] * seq_m14b[-1] / 1000
    features['gap_x_total'] = seq_gap[-1] * seq_totalcnt[-1] / 1000
    features['gap_x_trans'] = seq_gap[-1] * seq_transport[-1] / 100
    features['ratio_gap_total'] = seq_gap[-1] / (seq_totalcnt[-1] + 1)

    features['m14b_over_520'] = np.sum(seq_m14b > 520)
    features['m14b_over_540'] = np.sum(seq_m14b > 540)
    features['m14bsum_over_600'] = np.sum(seq_m14bsum > 600)
    features['m14bsum_over_620'] = np.sum(seq_m14bsum > 620)
    features['m10arev_over_55'] = np.sum(seq_m10arev > 55)
    features['total_over_1600'] = np.sum(seq_totalcnt >= 1600)
    features['total_over_1650'] = np.sum(seq_totalcnt >= 1650)
    features['total_over_1700'] = np.sum(seq_totalcnt >= 1700)
    features['gap_over_200'] = np.sum(seq_gap > 200)
    features['gap_over_250'] = np.sum(seq_gap > 250)
    features['gap_over_300'] = np.sum(seq_gap > 300)
    features['gap_over_350'] = np.sum(seq_gap > 350)

    features['gold_strict'] = 1 if (seq_m14b[-1] > 540 and seq_m14bsum[-1] > 620) else 0
    features['gold_normal'] = 1 if (seq_m14b[-1] > 520 and seq_m14bsum[-1] > 600) else 0
    features['in_danger'] = 1 if seq_totalcnt[-1] >= 1700 else 0
    features['near_danger'] = 1 if seq_totalcnt[-1] >= 1600 else 0
    features['danger_gap'] = 1 if seq_gap[-1] > 300 else 0
    features['danger_trans'] = 1 if seq_transport[-1] > 151 else 0
    features['triple_check'] = 1 if (seq_m14b[-1] > 520 and seq_m14bsum[-1] > 600 and seq_gap[-1] > 250) else 0
    features['quad_check'] = 1 if (seq_m14b[-1] > 520 and seq_m14bsum[-1] > 600 and seq_gap[-1] > 250 and seq_transport[-1] > 145) else 0

    return features

# ğŸ”¥ [í•µì‹¬] ê°•ì œ ë³´ì • í•¨ìˆ˜ (Glass Ceiling Break)
def adjust_prediction(pred, current_total, m14b, m14bsum, gap, trans):
    # 1. í•˜í•œì„  ì•ˆì „ì¥ì¹˜
    floor = current_total - 80
    if pred < floor: pred = floor
    
    # 2. ë¶€ìŠ¤íŠ¸ ë¡œì§ (ì¡°ê±´ ë§Œì¡± ì‹œ ê°•ì œ +ì ìˆ˜)
    boost = 0
    # í™©ê¸ˆ íŒ¨í„´
    if m14b > 540 and m14bsum > 620: boost += 50
    elif m14b > 520 and m14bsum > 600: boost += 40
    
    # Gap ë¶€ìŠ¤íŠ¸
    if gap > 350: boost += 40
    elif gap > 300: boost += 35
    elif gap > 250: boost += 25
    
    # Transport ë¶€ìŠ¤íŠ¸
    if trans > 180: boost += 30
    elif trans > 151: boost += 20
    
    pred = pred + boost
    
    # 3. í˜„ì¬ ì´ë¯¸ 1700 ë„˜ì—ˆìœ¼ë©´, ì˜ˆì¸¡ê°’ë„ ëª» ë–¨ì–´ì§€ê²Œ ë§‰ìŒ
    if current_total >= 1700:
        if pred < 1680: pred = max(pred, current_total - 50)
        
    return pred

# ë©”ì¸ ì‹¤í–‰
if __name__ == '__main__':
    with open('model_v9_10min.pkl', 'rb') as f: model = pickle.load(f)
    try: df = pd.read_csv(EVAL_FILE, encoding='utf-8')
    except: df = pd.read_csv(EVAL_FILE, encoding='cp949')
    
    df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), format='%Y%m%d%H%M')
    
    results = []
    seq_len = 280
    pred_offset = 10
    
    print("ğŸš€ í‰ê°€ ì‹œì‘...")
    for i in range(seq_len, len(df) - pred_offset):
        row_dict = {
            'M14AM14B': df['M14AM14B'].iloc[i-seq_len:i].values,
            'M14AM14BSUM': df['M14AM14BSUM'].iloc[i-seq_len:i].values,
            'M10AM14A': df['M10AM14A'].iloc[i-seq_len:i].values,
            'TOTALCNT': df['TOTALCNT'].iloc[i-seq_len:i].values,
            'TRANSPORT': df['M14.QUE.ALL.TRANSPORT4MINOVERCNT'].iloc[i-seq_len:i].values,
            'OHT': df['M14.QUE.OHT.OHTUTIL'].iloc[i-seq_len:i].values,
            'Q_CREATED': df['M14.QUE.ALL.CURRENTQCREATED'].iloc[i-seq_len:i].values,
            'Q_COMPLETED': df['M14.QUE.ALL.CURRENTQCOMPLETED'].iloc[i-seq_len:i].values,
        }
        
        # ê¸°ë³¸ ì˜ˆì¸¡
        feat = create_features_v9(row_dict)
        pred_raw = model.predict(pd.DataFrame([feat]))[0]
        
        # ğŸ”¥ ë³´ì • ì˜ˆì¸¡ ì ìš©
        current_total = df['TOTALCNT'].iloc[i-1]
        gap_val = feat['gap_current']
        trans_val = feat['trans_current']
        m14b_val = feat['m14b_current']
        m14bsum_val = feat['m14bsum_current']
        
        pred_adj = adjust_prediction(pred_raw, current_total, m14b_val, m14bsum_val, gap_val, trans_val)
        
        actual = df['TOTALCNT'].iloc[i+pred_offset-1]
        
        results.append({
            'í˜„ì¬ì‹œê°„': df['CURRTIME'].iloc[i-1],
            'ì‹¤ì œê°’': actual,
            'ì˜ˆì¸¡ê°’': pred_adj, # ë³´ì •ëœ ê°’ ì‚¬ìš©
            'ì˜¤ì°¨': actual - pred_adj,
            'ì˜ˆì¸¡ìœ„í—˜': 'O' if pred_adj >= 1650 else '' # 1650 ê¸°ì¤€
        })
        
    res_df = pd.DataFrame(results)
    res_df.to_csv('Evaluation_Result_V9.CSV', index=False)
    print("âœ… ì™„ë£Œ! ì €ì¥ë¨: Evaluation_Result_V9.CSV")
