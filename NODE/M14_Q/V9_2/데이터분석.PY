#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ­ AMHS ì¥ì• ì˜ˆì¸¡ íŒë‹¨ ë°ì´í„° EDA
ëª©ì : ë°˜ë„ì²´ ê³µì¥ ë¬¼ë¥˜ëŸ‰(TOTALCNT) ì˜ˆì¸¡ì„ ìœ„í•œ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

print("="*70)
print("ğŸ­ AMHS ì¥ì• ì˜ˆì¸¡ EDA ë¶„ì„")
print("="*70)

# ============================================
# 1. ë°ì´í„° ë¡œë“œ
# ============================================
DATA_PATH = 'your_data.csv'  # â† ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½!

df = pd.read_csv(DATA_PATH, on_bad_lines='skip')
print(f"\nâœ… ë°ì´í„° ë¡œë“œ: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")

# ê¸°ë³¸ ì •ë³´
print("\n" + "="*70)
print("ğŸ“Œ 1. ê¸°ë³¸ ì •ë³´")
print("="*70)
print(df.info())
print("\nê¸°ìˆ í†µê³„:")
print(df.describe())

# ============================================
# 2. ë³€ìˆ˜ íƒ€ì… ì •ì˜ (ì¹´í…Œê³ ë¦¬ vs ì—°ì†í˜•)
# ============================================
print("\n" + "="*70)
print("ğŸ“Œ 2. ë³€ìˆ˜ íƒ€ì… ë¶„ì„")
print("="*70)

def analyze_variable_types(df, unique_threshold=20):
    results = []
    for col in df.columns:
        unique_count = df[col].nunique()
        dtype = str(df[col].dtype)
        
        if col == 'CURRTIME':
            var_type = 'DATETIME'
        elif dtype == 'object':
            var_type = 'CATEGORY'
        elif unique_count <= unique_threshold:
            var_type = 'CATEGORY(í›„ë³´)'
        else:
            var_type = 'CONTINUOUS'
        
        results.append({
            'ì»¬ëŸ¼ëª…': col,
            'ë°ì´í„°íƒ€ì…': dtype,
            'ê³ ìœ ê°’ìˆ˜': unique_count,
            'ë³€ìˆ˜íƒ€ì…': var_type,
            'ê²°ì¸¡ì¹˜': df[col].isna().sum(),
            'ê²°ì¸¡ë¥ (%)': round(df[col].isna().sum() / len(df) * 100, 2)
        })
    return pd.DataFrame(results)

var_types = analyze_variable_types(df)
print(var_types.to_string())
print("\nğŸ“Š ë³€ìˆ˜ íƒ€ì…ë³„ ê°œìˆ˜:")
print(var_types['ë³€ìˆ˜íƒ€ì…'].value_counts())

# ì¹´í…Œê³ ë¦¬ ì„¸ê·¸ë¨¼íŠ¸ ì •ì˜
print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ ì„¸ê·¸ë¨¼íŠ¸:")
cat_cols = var_types[var_types['ë³€ìˆ˜íƒ€ì…'].str.contains('CATEGORY')]['ì»¬ëŸ¼ëª…'].tolist()
for col in cat_cols[:10]:  # ìƒìœ„ 10ê°œë§Œ
    if col in df.columns:
        print(f"\nâ–¶ {col}")
        vc = df[col].value_counts()
        for val, cnt in vc.head(5).items():
            print(f"  '{val}': {cnt:,}ê°œ ({cnt/len(df)*100:.1f}%)")

# ============================================
# 3. ê²°ì¸¡ì¹˜ ë¶„ì„
# ============================================
print("\n" + "="*70)
print("ğŸ“Œ 3. ê²°ì¸¡ì¹˜ ë¶„ì„")
print("="*70)

missing_df = pd.DataFrame({
    'ê²°ì¸¡ì¹˜ìˆ˜': df.isna().sum(),
    'ê²°ì¸¡ë¥ (%)': round(df.isna().sum() / len(df) * 100, 2)
}).sort_values('ê²°ì¸¡ì¹˜ìˆ˜', ascending=False)

missing_df = missing_df[missing_df['ê²°ì¸¡ì¹˜ìˆ˜'] > 0]

if len(missing_df) > 0:
    print(f"ê²°ì¸¡ì¹˜ ìˆëŠ” ì»¬ëŸ¼: {len(missing_df)}ê°œ")
    print(missing_df.head(20))
else:
    print("âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ!")

# missingno ì‹œê°í™” (ì„¤ì¹˜ëœ ê²½ìš°)
try:
    import missingno as msno
    if df.isna().sum().sum() > 0:
        fig, ax = plt.subplots(figsize=(16, 8))
        msno.matrix(df, ax=ax, sparkline=False)
        plt.title('ê²°ì¸¡ì¹˜ Matrix')
        plt.tight_layout()
        plt.savefig('missing_analysis.png', dpi=150)
        plt.close()
        print("ğŸ’¾ missing_analysis.png ì €ì¥")
except ImportError:
    print("âš ï¸ missingno ë¯¸ì„¤ì¹˜ (pip install missingno)")

# ============================================
# 4. ì´ìƒì¹˜ ë¶„ì„
# ============================================
print("\n" + "="*70)
print("ğŸ“Œ 4. ì´ìƒì¹˜ ë¶„ì„ (IQR)")
print("="*70)

def detect_outliers_iqr(series, multiplier=1.5):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - multiplier * IQR
    upper = Q3 + multiplier * IQR
    outliers = (series < lower) | (series > upper)
    return outliers.sum(), lower, upper, Q1, Q3

numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
if 'CURRTIME' in numeric_cols:
    numeric_cols.remove('CURRTIME')

outlier_results = []
for col in numeric_cols:
    if df[col].notna().sum() > 0:
        count, lower, upper, q1, q3 = detect_outliers_iqr(df[col].dropna())
        outlier_results.append({
            'ì»¬ëŸ¼': col,
            'Q1': round(q1, 2),
            'Q3': round(q3, 2),
            'í•˜í•œ': round(lower, 2),
            'ìƒí•œ': round(upper, 2),
            'ì´ìƒì¹˜ìˆ˜': count,
            'ì´ìƒì¹˜ìœ¨(%)': round(count / len(df) * 100, 2)
        })

outlier_df = pd.DataFrame(outlier_results).sort_values('ì´ìƒì¹˜ìœ¨(%)', ascending=False)
print(outlier_df.head(20).to_string(index=False))

# Boxplot
key_cols = ['TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M10AM14A']
key_cols = [c for c in key_cols if c in df.columns]

if key_cols:
    fig, axes = plt.subplots(1, len(key_cols), figsize=(4*len(key_cols), 5))
    if len(key_cols) == 1:
        axes = [axes]
    for i, col in enumerate(key_cols):
        axes[i].boxplot(df[col].dropna())
        axes[i].set_title(col)
    plt.suptitle('ì£¼ìš” ë³€ìˆ˜ Boxplot')
    plt.tight_layout()
    plt.savefig('boxplot_analysis.png', dpi=150)
    plt.close()
    print("ğŸ’¾ boxplot_analysis.png ì €ì¥")

# TOTALCNT ë¶„í¬
if 'TOTALCNT' in df.columns:
    print("\nğŸ“Š TOTALCNT êµ¬ê°„ë³„ ë¶„í¬:")
    print(f"  < 900 (LOW): {(df['TOTALCNT'] < 900).sum():,}ê°œ ({(df['TOTALCNT'] < 900).sum()/len(df)*100:.2f}%)")
    print(f"  900~1599 (NORMAL): {((df['TOTALCNT'] >= 900) & (df['TOTALCNT'] < 1600)).sum():,}ê°œ")
    print(f"  1600~1699 (CAUTION): {((df['TOTALCNT'] >= 1600) & (df['TOTALCNT'] < 1700)).sum():,}ê°œ")
    print(f"  >= 1700 (CRITICAL): {(df['TOTALCNT'] >= 1700).sum():,}ê°œ ({(df['TOTALCNT'] >= 1700).sum()/len(df)*100:.2f}%)")

# ============================================
# 5. ì¤‘ë³µê°’ ë¶„ì„
# ============================================
print("\n" + "="*70)
print("ğŸ“Œ 5. ì¤‘ë³µê°’ ë¶„ì„")
print("="*70)

# ì™„ì „ ì¤‘ë³µ
full_dup = df.duplicated().sum()
print(f"1ï¸âƒ£ ì™„ì „ ì¤‘ë³µ: {full_dup:,}ê°œ ({full_dup/len(df)*100:.2f}%)")

# CURRTIME ì œì™¸ ì¤‘ë³µ
cols_except_time = [c for c in df.columns if c != 'CURRTIME']
if cols_except_time:
    time_diff_dup = df.duplicated(subset=cols_except_time).sum()
    print(f"2ï¸âƒ£ CURRTIME ì œì™¸ ì¤‘ë³µ: {time_diff_dup:,}ê°œ ({time_diff_dup/len(df)*100:.2f}%)")

# ì—°ì† ì¤‘ë³µ
consecutive_dup = 0
for i in range(1, len(df)):
    if df[cols_except_time].iloc[i].equals(df[cols_except_time].iloc[i-1]):
        consecutive_dup += 1
print(f"3ï¸âƒ£ ì—°ì† ì¤‘ë³µ (ì§ì „ê³¼ ë™ì¼): {consecutive_dup:,}ê°œ")

# ============================================
# 6. ìƒê´€ê´€ê³„ ë¶„ì„
# ============================================
print("\n" + "="*70)
print("ğŸ“Œ 6. ìƒê´€ê´€ê³„ ë¶„ì„")
print("="*70)

numeric_df = df.select_dtypes(include=[np.number])
if 'CURRTIME' in numeric_df.columns:
    numeric_df = numeric_df.drop(columns=['CURRTIME'])

corr_matrix = numeric_df.corr()

# TOTALCNT ìƒê´€ê´€ê³„
if 'TOTALCNT' in corr_matrix.columns:
    totalcnt_corr = corr_matrix['TOTALCNT'].drop('TOTALCNT').sort_values(key=abs, ascending=False)
    print("\nğŸ† TOTALCNT ìƒê´€ê´€ê³„ TOP 20:")
    print(totalcnt_corr.head(20))

# ë†’ì€ ìƒê´€ê´€ê³„ ìŒ (0.95 ì´ìƒ)
print("\nğŸ“Š ë†’ì€ ìƒê´€ê´€ê³„ ìŒ (|r| >= 0.95):")
high_corr_pairs = []
for i in range(len(corr_matrix.columns)):
    for j in range(i+1, len(corr_matrix.columns)):
        col1, col2 = corr_matrix.columns[i], corr_matrix.columns[j]
        corr_val = corr_matrix.iloc[i, j]
        if abs(corr_val) >= 0.95:
            high_corr_pairs.append({'ë³€ìˆ˜1': col1, 'ë³€ìˆ˜2': col2, 'ìƒê´€ê³„ìˆ˜': round(corr_val, 4)})

if high_corr_pairs:
    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('ìƒê´€ê³„ìˆ˜', key=abs, ascending=False)
    print(f"ë°œê²¬: {len(high_corr_df)}ìŒ")
    print(high_corr_df.to_string(index=False))
else:
    print("âœ… 0.95 ì´ìƒ ì—†ìŒ")

# íˆíŠ¸ë§µ
if 'TOTALCNT' in corr_matrix.columns:
    top_cols = ['TOTALCNT'] + totalcnt_corr.head(15).index.tolist()
    top_cols = [c for c in top_cols if c in numeric_df.columns]
    
    plt.figure(figsize=(14, 12))
    sns.heatmap(numeric_df[top_cols].corr(), annot=True, fmt='.2f', cmap='RdBu_r', center=0)
    plt.title('ì£¼ìš” ë³€ìˆ˜ ìƒê´€ê´€ê³„')
    plt.tight_layout()
    plt.savefig('correlation_heatmap.png', dpi=150)
    plt.close()
    print("ğŸ’¾ correlation_heatmap.png ì €ì¥")

# ============================================
# 7. íŒŒìƒ ë³€ìˆ˜ (queue_gap)
# ============================================
print("\n" + "="*70)
print("ğŸ“Œ 7. íŒŒìƒ ë³€ìˆ˜ ë¶„ì„")
print("="*70)

# M14 queue_gap
if 'M14.QUE.ALL.CURRENTQCREATED' in df.columns and 'M14.QUE.ALL.CURRENTQCOMPLETED' in df.columns:
    df['M14_queue_gap'] = df['M14.QUE.ALL.CURRENTQCREATED'] - df['M14.QUE.ALL.CURRENTQCOMPLETED']
    print(f"\nâœ… M14_queue_gap = CURRENTQCREATED - CURRENTQCOMPLETED")
    print(f"   ë²”ìœ„: {df['M14_queue_gap'].min():.0f} ~ {df['M14_queue_gap'].max():.0f}")
    print(f"   í‰ê· : {df['M14_queue_gap'].mean():.2f}")
    if 'TOTALCNT' in df.columns:
        gap_corr = df['M14_queue_gap'].corr(df['TOTALCNT'])
        print(f"   ğŸ”¥ TOTALCNT ìƒê´€ê³„ìˆ˜: {gap_corr:.4f}")

# M14B queue_gap
if 'M14B.QUE.ALL.CURRENTQCREATED' in df.columns and 'M14B.QUE.ALL.CURRENTQCOMPLETED' in df.columns:
    df['M14B_queue_gap'] = df['M14B.QUE.ALL.CURRENTQCREATED'] - df['M14B.QUE.ALL.CURRENTQCOMPLETED']
    print(f"\nâœ… M14B_queue_gap")
    print(f"   ë²”ìœ„: {df['M14B_queue_gap'].min():.0f} ~ {df['M14B_queue_gap'].max():.0f}")
    if 'TOTALCNT' in df.columns:
        gap_corr = df['M14B_queue_gap'].corr(df['TOTALCNT'])
        print(f"   TOTALCNT ìƒê´€ê³„ìˆ˜: {gap_corr:.4f}")

# ============================================
# 8. Feature Importance (XGBoost)
# ============================================
print("\n" + "="*70)
print("ğŸ“Œ 8. Feature Importance (XGBoost)")
print("="*70)

try:
    from sklearn.model_selection import train_test_split
    import xgboost as xgb
    
    feature_cols = [c for c in numeric_df.columns if c != 'TOTALCNT']
    X = numeric_df[feature_cols].fillna(0)
    y = numeric_df['TOTALCNT'] if 'TOTALCNT' in numeric_df.columns else None
    
    if y is not None and len(X) > 100:
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
        
        model = xgb.XGBRegressor(max_depth=6, learning_rate=0.1, n_estimators=100, random_state=42, n_jobs=-1)
        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
        
        importance_df = pd.DataFrame({
            'feature': feature_cols,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\nğŸ† Feature Importance TOP 30:")
        print(importance_df.head(30).to_string(index=False))
        
        # ì‹œê°í™”
        plt.figure(figsize=(12, 10))
        plt.barh(importance_df['feature'].head(30)[::-1], importance_df['importance'].head(30)[::-1])
        plt.xlabel('Importance')
        plt.title('XGBoost Feature Importance TOP 30')
        plt.tight_layout()
        plt.savefig('feature_importance.png', dpi=150)
        plt.close()
        print("ğŸ’¾ feature_importance.png ì €ì¥")
        
        importance_df.to_csv('feature_importance.csv', index=False, encoding='utf-8-sig')
except ImportError:
    print("âš ï¸ xgboost ë¯¸ì„¤ì¹˜ (pip install xgboost)")

# ============================================
# 9. ê²°ê³¼ ì €ì¥
# ============================================
print("\n" + "="*70)
print("ğŸ“Œ 9. ê²°ê³¼ ì €ì¥")
print("="*70)

var_types.to_csv('variable_types.csv', index=False, encoding='utf-8-sig')
print("ğŸ’¾ variable_types.csv")

outlier_df.to_csv('outlier_analysis.csv', index=False, encoding='utf-8-sig')
print("ğŸ’¾ outlier_analysis.csv")

if high_corr_pairs:
    high_corr_df.to_csv('high_correlation_pairs.csv', index=False, encoding='utf-8-sig')
    print("ğŸ’¾ high_correlation_pairs.csv")

# ============================================
# ìš”ì•½ ë¦¬í¬íŠ¸
# ============================================
print("\n" + "="*70)
print("ğŸ“‹ EDA ìš”ì•½")
print("="*70)
print(f"""
ë°ì´í„°: {len(df):,}í–‰ Ã— {len(df.columns)}ì—´
ìˆ˜ì¹˜í˜•: {len(numeric_cols)}ê°œ
ê²°ì¸¡ì¹˜ ì»¬ëŸ¼: {len(missing_df)}ê°œ
1700+ ë¹„ìœ¨: {(df['TOTALCNT'] >= 1700).sum()/len(df)*100:.2f}%
ë†’ì€ìƒê´€(0.95+): {len(high_corr_pairs)}ìŒ

ğŸ“Œ 1ì°¨ íšŒì˜ ì•ˆê±´:
â–¡ ì¹´í…Œê³ ë¦¬/ì—°ì†í˜• êµ¬ë¶„ í™•ì •
â–¡ ìƒê´€ê´€ê³„ ê¸°ë°˜ ë³€ìˆ˜ ì œê±° (ë„ë©”ì¸ ì§€ì‹)
â–¡ ê²°ì¸¡ì¹˜/ì´ìƒì¹˜ ì²˜ë¦¬ ì „ëµ
â–¡ ì¤‘ë³µê°’ ì²˜ë¦¬
""")

print("\nâœ… EDA ì™„ë£Œ!")