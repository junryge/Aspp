#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ V12.0 í•™ìŠµ - Load_Category ë°¸ëŸ°ì‹± ì ìš©
íŠ¹ì§•: êµ¬ê°„ë³„ ë°ì´í„° ê· í˜•ì„ ë§ì¶° ê³ ë¶€í•˜(1700+) íŒ¨í„´ í•™ìŠµ ê°•í™”
"""
import numpy as np
import pandas as pd
import xgboost as xgb
import pickle
import glob
import warnings
warnings.filterwarnings('ignore')

print("="*70)
print("ğŸš€ V12.0 í•™ìŠµ - Load_Category ë°¸ëŸ°ì‹± ì ìš© (280ë¶„ ì‹œí€€ìŠ¤)")
print("="*70)

PRED_OFFSET = 10
SEQ_LEN = 280

# --- êµ¬ê°„ ë¶„ë¥˜ í•¨ìˆ˜ ---
def get_load_category(cnt):
    if cnt <= 1200: return 'Under_1200'
    elif 1200 < cnt <= 1300: return '1200_1300'
    elif 1300 < cnt <= 1400: return '1300_1400'
    elif 1400 < cnt <= 1500: return '1400_1500'
    elif 1500 < cnt <= 1600: return '1500_1600'
    elif 1600 < cnt < 1700: return '1600_1700'
    else: return 'Over_1700'

def load_data(path_pattern):
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {path_pattern}")
    files = glob.glob(path_pattern)
    if not files:
        # íŒŒì¼ ì—†ìœ¼ë©´ ê¸°ë³¸ íŒŒì¼ ì‹œë„
        files = glob.glob('M14_Q_*.CSV')
        
    dfs = []
    for f in sorted(files):
        try:
            df = pd.read_csv(f, on_bad_lines='skip', encoding='utf-8')
            dfs.append(df)
        except:
            df = pd.read_csv(f, on_bad_lines='skip', encoding='cp949')
            dfs.append(df)
    
    if not dfs: raise ValueError("ë°ì´í„° íŒŒì¼ ì—†ìŒ!")
    return pd.concat(dfs, ignore_index=True)

def create_vectorized_features(df):
    """ë²¡í„°í™”ëœ í”¼ì²˜ ìƒì„± (280ë¶„ ì‹œí€€ìŠ¤)"""
    df = df.copy()
    
    # ìˆ«ìí˜• ë³€í™˜
    cols = ['TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M10AM14A', 
            'M14.QUE.ALL.TRANSPORT4MINOVERCNT', 'M14.QUE.OHT.OHTUTIL',
            'M14.QUE.ALL.CURRENTQCREATED', 'M14.QUE.ALL.CURRENTQCOMPLETED']
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)

    df['Gap'] = df['M14.QUE.ALL.CURRENTQCREATED'] - df['M14.QUE.ALL.CURRENTQCOMPLETED']
    
    # Lag ë° Rolling í”¼ì²˜ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    lags = [1, 3, 5, 10, 20, 50, 100, 200, 280]
    for lag in lags:
        df[f'Total_Lag_{lag}'] = df['TOTALCNT'].shift(lag)
        df[f'M14B_Lag_{lag}'] = df['M14AM14B'].shift(lag)
        df[f'Sum_Lag_{lag}'] = df['M14AM14BSUM'].shift(lag)
        df[f'Gap_Lag_{lag}'] = df['Gap'].shift(lag)

    windows = [10, 30, 60, 100, 200, 280]
    for w in windows:
        df[f'Total_Mean_{w}'] = df['TOTALCNT'].rolling(w).mean()
        df[f'Total_Std_{w}'] = df['TOTALCNT'].rolling(w).std()
        df[f'M14B_Mean_{w}'] = df['M14AM14B'].rolling(w).mean()
        df[f'Gap_Mean_{w}'] = df['Gap'].rolling(w).mean()

    # íƒ€ê²Ÿ ìƒì„±
    df['Target_Total_Future'] = df['TOTALCNT'].shift(-PRED_OFFSET)
    df['Target_Delta'] = df['Target_Total_Future'] - df['TOTALCNT']
    
    # ê²°ì¸¡ ì œê±° (ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ì•ë¶€ë¶„ ë°ì´í„° ì‚­ì œë¨)
    df_clean = df.dropna().reset_index(drop=True)
    return df_clean

# --- ë©”ì¸ ì‹¤í–‰ ---
# 1. ë°ì´í„° ë¡œë“œ
df = load_data('M14_Q_*.CSV')

# 2. í”¼ì²˜ ìƒì„±
print("\nğŸ”„ í”¼ì²˜ ìƒì„± ì¤‘...")
df_features = create_vectorized_features(df)

# 3. Load_Category ìƒì„± ë° ë°ì´í„° ë°¸ëŸ°ì‹±
print("\nâš–ï¸ ë°ì´í„° ë°¸ëŸ°ì‹± ì‘ì—… ì‹œì‘ (Load_Category ê¸°ë°˜)...")

# íƒ€ê²Ÿ ê°’(ë¯¸ë˜ ì‹œì ì˜ ë¶€í•˜)ì„ ê¸°ì¤€ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
df_features['Load_Category'] = df_features['Target_Total_Future'].apply(get_load_category)

# ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜ í™•ì¸
counts = df_features['Load_Category'].value_counts()
print("  [ì›ë³¸ ë¶„í¬]")
print(counts)

# ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ê¸°ì¤€ ì„¤ì • (ë„ˆë¬´ ì ìœ¼ë©´ í•™ìŠµ ì•ˆë˜ë¯€ë¡œ ìµœì†Œ 5000ê°œ í˜¹ì€ ê°€ì¥ ì ì€ í´ë˜ìŠ¤ ê°œìˆ˜ ì‚¬ìš©)
# ì—¬ê¸°ì„œëŠ” 'Over_1700' ê°œìˆ˜ í˜¹ì€ 10000ê°œ ì¤‘ ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì–¸ë”ìƒ˜í”Œë§
min_samples = counts.min() 
target_samples = max(min_samples, 5000) # ìµœì†Œ 5000ê°œëŠ” í™•ë³´ ì‹œë„, ë°ì´í„°ê°€ ëª¨ìë¥´ë©´ minê°’ ì‚¬ìš©

print(f"  ğŸ‘‰ ë°¸ëŸ°ì‹± ëª©í‘œ ìƒ˜í”Œ ìˆ˜: {target_samples} (ê° í´ë˜ìŠ¤ë³„)")

# ë°¸ëŸ°ì‹± ìˆ˜í–‰ (ê° ê·¸ë£¹ë³„ë¡œ ìƒ˜í”Œë§)
df_balanced = df_features.groupby('Load_Category', group_keys=False).apply(
    lambda x: x.sample(n=min(len(x), target_samples), random_state=42)
)
# ë°ì´í„° ì„ê¸°
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

print(f"âœ… ë°¸ëŸ°ì‹± ì™„ë£Œ: ì´ {len(df_balanced):,}í–‰")
print("  [ë°¸ëŸ°ì‹± í›„ ë¶„í¬]")
print(df_balanced['Load_Category'].value_counts())

# 4. í•™ìŠµ ì¤€ë¹„
exclude_cols = ['CURRTIME', 'Target_Total_Future', 'Target_Delta', 'Load_Category']
feature_cols = [c for c in df_balanced.columns if c not in exclude_cols]

X = df_balanced[feature_cols]
y_delta = df_balanced['Target_Delta']

# 5. ëª¨ë¸ í•™ìŠµ
print(f"\nğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (í”¼ì²˜ {len(feature_cols)}ê°œ)...")
model = xgb.XGBRegressor(
    n_estimators=2000,     # ë°¸ëŸ°ì‹± ë˜ì—ˆìœ¼ë¯€ë¡œ í•™ìŠµëŸ‰ ì¶©ë¶„íˆ
    learning_rate=0.01,
    max_depth=12,          # ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ
    subsample=0.8,
    colsample_bytree=0.8,
    n_jobs=4,
    device='cuda',         # GPU ì‚¬ìš© (ì—†ìœ¼ë©´ 'cpu')
    tree_method='hist',
    random_state=42
)

model.fit(X, y_delta, verbose=False)

# 6. ì €ì¥
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
with open('model_v12_balanced.pkl', 'wb') as f:
    pickle.dump(model, f)
with open('features_v12_balanced.pkl', 'wb') as f:
    pickle.dump(feature_cols, f)
    
print("âœ… í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!")
