#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ V9.0 í•™ìŠµ - 100ë¶„ ì‹œí€€ìŠ¤ / 10ë¶„ í›„ ì˜ˆì¸¡
íŒ¨í„´ ìš°ì„  + ML ë³´ì¡° í•˜ì´ë¸Œë¦¬ë“œ!

í•µì‹¬ ë³€ê²½ (V8.3 â†’ V9.0):
1. ì‹œí€€ìŠ¤: 280ë¶„ â†’ 100ë¶„
2. ì»¬ëŸ¼: 8ê°œ â†’ 5ê°œ í•„ìˆ˜ë§Œ
3. Feature: 82ê°œ â†’ 34ê°œ
4. íŒ¨í„´: ML Feature â†’ ì¦‰ì‹œ ê²½ë³´!
"""

import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import warnings
warnings.filterwarnings('ignore')

print("="*70)
print("ğŸš€ V9.0 í•™ìŠµ - 100ë¶„/10ë¶„ ì˜ˆì¸¡")
print("   íŒ¨í„´ ìš°ì„  + ML ë³´ì¡° í•˜ì´ë¸Œë¦¬ë“œ!")
print("="*70)

# ============================================
# ì„¤ì •
# ============================================
SEQ_LEN = 100          # 280 â†’ 100ë¶„!
PRED_MINUTES = 10      # 10ë¶„ í›„ ì˜ˆì¸¡
AUGMENT_MULTIPLIER = 50
CSV_FILE = "YOUR_CSV_FILE.csv"  # â† ì‹¤ì œ íŒŒì¼ ê²½ë¡œ!

# ============================================
# íŒ¨í„´ ì²´í¬ í•¨ìˆ˜ (ì¦‰ì‹œ ê²½ë³´ìš©)
# ============================================
def check_pattern(m14b, m14bsum, gap, totalcnt):
    """
    íŒ¨í„´ ë§¤ì¹­ â†’ ì¦‰ì‹œ ê²½ë³´!
    ë°˜í™˜: (ë ˆë²¨, ì˜ˆì¸¡ê°’, í™•ë¥ ) or None
    """
    # 1ë‹¨ê³„: í™©ê¸ˆíŒ¨í„´ & gap>400 (100% ì •ë°€ë„)
    if m14b > 520 and m14bsum > 600 and gap > 400:
        return ('CRITICAL', 1800, 100.0)
    
    # 2ë‹¨ê³„: gapë§Œ ë§¤ìš° ë†’ìŒ (99.7%)
    if gap > 500:
        return ('CRITICAL', 1780, 99.7)
    
    # 3ë‹¨ê³„: Triple Check (96.5%)
    if m14b > 520 and m14bsum > 600 and gap > 250:
        return ('CRITICAL', 1750, 96.5)
    
    # 4ë‹¨ê³„: gap>400 ë‹¨ë… (96.2%)
    if gap > 400:
        return ('HIGH', 1730, 96.2)
    
    # 5ë‹¨ê³„: í˜„ì¬ê°’ ì´ë¯¸ ë†’ìŒ
    if totalcnt >= 1680:
        return ('HIGH', 1720, 85.0)
    
    if totalcnt >= 1650:
        return ('WARNING', 1700, 71.0)
    
    # íŒ¨í„´ ì—†ìŒ â†’ MLë¡œ
    return None

# ============================================
# Feature ìƒì„± (34ê°œ)
# ============================================
def create_features_v9(row_dict):
    """í•„ìˆ˜ 5ê°œ ì»¬ëŸ¼ ê¸°ë°˜ 34ê°œ Feature"""
    features = {}
    
    seq_m14b = np.array(row_dict['M14B'])
    seq_m14bsum = np.array(row_dict['M14BSUM'])
    seq_totalcnt = np.array(row_dict['TOTALCNT'])
    seq_q_created = np.array(row_dict['Q_CREATED'])
    seq_q_completed = np.array(row_dict['Q_COMPLETED'])
    seq_gap = seq_q_created - seq_q_completed
    
    seq_len = len(seq_m14b)
    
    # ========== TOTALCNT (8ê°œ) ==========
    features['total_current'] = seq_totalcnt[-1]
    features['total_mean'] = np.mean(seq_totalcnt)
    features['total_max'] = np.max(seq_totalcnt)
    features['total_min'] = np.min(seq_totalcnt)
    features['total_last10'] = np.mean(seq_totalcnt[-10:])
    features['total_slope'] = np.polyfit(np.arange(seq_len), seq_totalcnt, 1)[0]
    features['total_std'] = np.std(seq_totalcnt)
    features['total_trend10'] = seq_totalcnt[-1] - seq_totalcnt[-10]
    
    # ========== M14AM14B (6ê°œ) ==========
    features['m14b_current'] = seq_m14b[-1]
    features['m14b_mean'] = np.mean(seq_m14b)
    features['m14b_max'] = np.max(seq_m14b)
    features['m14b_last10'] = np.mean(seq_m14b[-10:])
    features['m14b_slope'] = np.polyfit(np.arange(seq_len), seq_m14b, 1)[0]
    features['m14b_trend10'] = seq_m14b[-1] - seq_m14b[-10]
    
    # ========== M14AM14BSUM (6ê°œ) ==========
    features['m14bsum_current'] = seq_m14bsum[-1]
    features['m14bsum_mean'] = np.mean(seq_m14bsum)
    features['m14bsum_max'] = np.max(seq_m14bsum)
    features['m14bsum_last10'] = np.mean(seq_m14bsum[-10:])
    features['m14bsum_slope'] = np.polyfit(np.arange(seq_len), seq_m14bsum, 1)[0]
    features['m14bsum_trend10'] = seq_m14bsum[-1] - seq_m14bsum[-10]
    
    # ========== Queue Gap (8ê°œ) - í•µì‹¬! ==========
    features['gap_current'] = seq_gap[-1]
    features['gap_mean'] = np.mean(seq_gap)
    features['gap_max'] = np.max(seq_gap)
    features['gap_min'] = np.min(seq_gap)
    features['gap_last10'] = np.mean(seq_gap[-10:])
    features['gap_slope'] = np.polyfit(np.arange(seq_len), seq_gap, 1)[0]
    features['gap_std'] = np.std(seq_gap)
    features['gap_trend10'] = seq_gap[-1] - seq_gap[-10]
    
    # ========== í•µì‹¬ ì¡°í•© (6ê°œ) ==========
    features['m14b_x_sum'] = seq_m14b[-1] * seq_m14bsum[-1] / 1000
    features['gap_x_m14b'] = seq_gap[-1] * seq_m14b[-1] / 1000
    features['ratio_gap_total'] = seq_gap[-1] / (seq_totalcnt[-1] + 1)
    features['ratio_m14b_total'] = seq_m14b[-1] / (seq_totalcnt[-1] + 1)
    features['m14b_plus_sum'] = seq_m14b[-1] + seq_m14bsum[-1]
    features['gap_per_m14b'] = seq_gap[-1] / (seq_m14b[-1] + 1)
    
    return features

# ============================================
# ë°ì´í„° ì¤€ë¹„
# ============================================
def prepare_data(csv_path):
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {csv_path}")
    df = pd.read_csv(csv_path, on_bad_lines='skip')
    print(f"âœ… {len(df):,}í–‰ Ã— {df.shape[1]}ì—´")
    
    # í•„ìˆ˜ ì»¬ëŸ¼
    required = {
        'TOTALCNT': 'TOTALCNT',
        'M14B': 'M14AM14B',
        'M14BSUM': 'M14AM14BSUM',
        'Q_CREATED': 'M14.QUE.ALL.CURRENTQCREATED',
        'Q_COMPLETED': 'M14.QUE.ALL.CURRENTQCOMPLETED',
    }
    
    for key, col in required.items():
        if col not in df.columns:
            print(f"âŒ ëˆ„ë½: {col}")
            return None, None
    
    print(f"âœ… í•„ìˆ˜ 5ê°œ ì»¬ëŸ¼ í™•ì¸!")
    
    # NaN ì œê±°
    df = df.dropna(subset=list(required.values()))
    print(f"âœ… NaN ì œê±° í›„: {len(df):,}í–‰")
    
    # í†µê³„
    df['queue_gap'] = df['M14.QUE.ALL.CURRENTQCREATED'] - df['M14.QUE.ALL.CURRENTQCOMPLETED']
    high_ratio = (df['TOTALCNT'] >= 1700).sum() / len(df) * 100
    print(f"ğŸ“Š í˜„ì¬ 1700+ ë¹„ìœ¨: {high_ratio:.2f}%")
    print(f"ğŸ“Š queue_gap ë²”ìœ„: {df['queue_gap'].min():.0f} ~ {df['queue_gap'].max():.0f}")
    
    # Feature ìƒì„±
    print(f"\nğŸ”„ Feature ìƒì„± ì¤‘... (ì‹œí€€ìŠ¤={SEQ_LEN}ë¶„, ì˜ˆì¸¡={PRED_MINUTES}ë¶„ í›„)")
    
    X_list, y_list = [], []
    pattern_stats = {'CRITICAL': 0, 'HIGH': 0, 'WARNING': 0}
    
    total = len(df) - SEQ_LEN - PRED_MINUTES
    
    for i in range(SEQ_LEN, len(df) - PRED_MINUTES):
        row_dict = {
            'TOTALCNT': df['TOTALCNT'].iloc[i-SEQ_LEN:i].values,
            'M14B': df['M14AM14B'].iloc[i-SEQ_LEN:i].values,
            'M14BSUM': df['M14AM14BSUM'].iloc[i-SEQ_LEN:i].values,
            'Q_CREATED': df['M14.QUE.ALL.CURRENTQCREATED'].iloc[i-SEQ_LEN:i].values,
            'Q_COMPLETED': df['M14.QUE.ALL.CURRENTQCOMPLETED'].iloc[i-SEQ_LEN:i].values,
        }
        
        target = df['TOTALCNT'].iloc[i + PRED_MINUTES - 1]
        if pd.isna(target):
            continue
        
        # í˜„ì¬ ìƒíƒœ
        current_total = row_dict['TOTALCNT'][-1]
        current_m14b = row_dict['M14B'][-1]
        current_m14bsum = row_dict['M14BSUM'][-1]
        current_gap = row_dict['Q_CREATED'][-1] - row_dict['Q_COMPLETED'][-1]
        
        # íŒ¨í„´ í†µê³„
        pattern = check_pattern(current_m14b, current_m14bsum, current_gap, current_total)
        if pattern:
            pattern_stats[pattern[0]] += 1
        
        # Feature ìƒì„±
        X_list.append(create_features_v9(row_dict))
        y_list.append(target)
        
        if (i - SEQ_LEN) % 10000 == 0:
            print(f"  {i-SEQ_LEN:,}/{total:,} ({(i-SEQ_LEN)/total*100:.1f}%)")
    
    print(f"âœ… ì™„ë£Œ!")
    print(f"\nğŸ“Š íŒ¨í„´ ë§¤ì¹­ í†µê³„ (ì‹¤ì‹œê°„ì—ì„œ ì¦‰ì‹œ ì²˜ë¦¬):")
    for level, count in pattern_stats.items():
        print(f"  {level}: {count:,}ê°œ")
    
    X = pd.DataFrame(X_list)
    y = pd.Series(y_list)
    
    # NaN ì œê±°
    mask = ~(X.isna().any(axis=1) | y.isna())
    X = X[mask].reset_index(drop=True)
    y = y[mask].reset_index(drop=True)
    
    print(f"\nğŸ“Š ìµœì¢… ë°ì´í„°:")
    print(f"  Feature: {X.shape[1]}ê°œ")
    print(f"  ìƒ˜í”Œ: {len(X):,}ê°œ")
    print(f"  íƒ€ê²Ÿ 1700+: {(y>=1700).sum():,}ê°œ ({(y>=1700).sum()/len(y)*100:.2f}%)")
    
    return X, y

# ============================================
# ë°ì´í„° ì¦ê°•
# ============================================
def augment_data(X, y, threshold=1700, multiplier=AUGMENT_MULTIPLIER):
    print(f"\nğŸ”¥ 1700+ ë°ì´í„° {multiplier}ë°° ì¦ê°•")
    high_mask = y >= threshold
    count = high_mask.sum()
    print(f"  1700+ ì›ë³¸: {count:,}ê°œ")
    
    if count == 0:
        print("  âš ï¸ 1700+ ë°ì´í„° ì—†ìŒ!")
        return X, y
    
    X_aug = pd.concat([X] + [X[high_mask]] * (multiplier - 1), ignore_index=True)
    y_aug = pd.concat([y] + [y[high_mask]] * (multiplier - 1), ignore_index=True)
    
    print(f"  ì¦ê°• í›„: {len(X_aug):,}ê°œ")
    print(f"  1700+ ë¹„ìœ¨: {(y_aug>=threshold).sum()/len(y_aug)*100:.1f}%")
    
    return X_aug, y_aug

# ============================================
# ëª¨ë¸ í•™ìŠµ
# ============================================
def train_model(X, y):
    print(f"\nğŸš€ XGBoost í•™ìŠµ")
    
    # Train/Val ë¶„ë¦¬
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
    print(f"  Train: {len(X_train):,}, Val: {len(X_val):,}")
    
    model = xgb.XGBRegressor(
        objective='reg:squarederror',
        max_depth=6,
        learning_rate=0.05,
        n_estimators=300,
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_weight=3,
        gamma=0.1,
        reg_alpha=0.1,
        reg_lambda=1.0,
        random_state=42,
        n_jobs=-1,
        tree_method='hist'
    )
    
    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        verbose=50
    )
    
    # ê²€ì¦
    y_val_pred = model.predict(X_val)
    mae = mean_absolute_error(y_val, y_val_pred)
    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
    r2 = r2_score(y_val, y_val_pred)
    
    print(f"\nğŸ“Š ê²€ì¦ ì„±ëŠ¥:")
    print(f"  MAE: {mae:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  RÂ²: {r2:.4f}")
    
    # 1700+ ê°ì§€ìœ¨
    val_high = y_val >= 1700
    if val_high.sum() > 0:
        detected = ((y_val >= 1700) & (y_val_pred >= 1680)).sum()
        print(f"  1700+ ê°ì§€ìœ¨: {detected}/{val_high.sum()} ({detected/val_high.sum()*100:.1f}%)")
    
    return model

# ============================================
# Feature ì¤‘ìš”ë„
# ============================================
def show_importance(model, X):
    print(f"\nğŸ† Feature ì¤‘ìš”ë„ TOP 15")
    imp = pd.DataFrame({
        'feature': X.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(imp.head(15).to_string(index=False))
    return imp

# ============================================
# ë©”ì¸
# ============================================
if __name__ == '__main__':
    # ë°ì´í„° ì¤€ë¹„
    X, y = prepare_data(CSV_FILE)
    if X is None:
        print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨!")
        exit(1)
    
    # ì¦ê°•
    X_aug, y_aug = augment_data(X, y)
    
    # í•™ìŠµ
    model = train_model(X_aug, y_aug)
    
    # Feature ì¤‘ìš”ë„
    show_importance(model, X_aug)
    
    # ì €ì¥
    with open('model_v9_100min_10min.pkl', 'wb') as f:
        pickle.dump(model, f)
    with open('features_v9.pkl', 'wb') as f:
        pickle.dump(list(X.columns), f)
    
    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ:")
    print(f"  ëª¨ë¸: model_v9_100min_10min.pkl")
    print(f"  Feature: features_v9.pkl")
    
    print(f"\n" + "="*70)
    print(f"âœ… V9.0 í•™ìŠµ ì™„ë£Œ!")
    print(f"   ì‹œí€€ìŠ¤: {SEQ_LEN}ë¶„ / ì˜ˆì¸¡: {PRED_MINUTES}ë¶„ í›„")
    print(f"   Feature: {X.shape[1]}ê°œ")
    print("="*70)