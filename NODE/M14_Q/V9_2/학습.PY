#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ V11.5 í•™ìŠµ - ë³€í™”ëŸ‰(Delta) ì§‘ì¤‘ ê³µëµ (ìˆ˜ì •ë¨)
íŠ¹ì§•: ê³ ë¶€í•˜(1700) + 'ê¸‰ìƒìŠ¹(Delta í­ë°œ)' êµ¬ê°„ ë™ì‹œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
"""
import numpy as np
import pandas as pd
import xgboost as xgb
import pickle
import glob
import warnings
warnings.filterwarnings('ignore')

print("="*70)
print("ğŸš€ V11.5 í•™ìŠµ - ë³€í™”ëŸ‰(Delta) ì§‘ì¤‘ í•™ìŠµ")
print("="*70)

PRED_OFFSET = 10

def load_data(path_pattern):
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {path_pattern}")
    files = glob.glob(path_pattern)
    if not files:
        print("âš  ì§€ì •ëœ íŒŒì¼ì´ ì—†ì–´ ê¸°ë³¸ íŒŒì¼(M14_Q_*.CSV)ì„ ì°¾ìŠµë‹ˆë‹¤.")
        files = glob.glob('M14_Q_*.CSV')
        
    dfs = []
    for f in sorted(files):
        try: dfs.append(pd.read_csv(f, on_bad_lines='skip', encoding='utf-8'))
        except: dfs.append(pd.read_csv(f, on_bad_lines='skip', encoding='cp949'))
            
    if not dfs: raise ValueError("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    return pd.concat(dfs, ignore_index=True)

def create_vectorized_features(df):
    """ë²¡í„°í™”ëœ í”¼ì²˜ ìƒì„±"""
    df = df.copy()
    
    # ìˆ«ìí˜• ë³€í™˜
    cols = ['TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M10AM14A', 
            'M14.QUE.ALL.TRANSPORT4MINOVERCNT', 'M14.QUE.OHT.OHTUTIL',
            'M14.QUE.ALL.CURRENTQCREATED', 'M14.QUE.ALL.CURRENTQCOMPLETED']
    for c in cols: df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)

    df['Gap'] = df['M14.QUE.ALL.CURRENTQCREATED'] - df['M14.QUE.ALL.CURRENTQCOMPLETED']
    
    # 1. ì‹œì°¨ ë³€ìˆ˜ (Lags)
    lags = [1, 3, 5, 10, 20]
    for lag in lags:
        df[f'Total_Lag_{lag}'] = df['TOTALCNT'].shift(lag)
        df[f'M14B_Lag_{lag}'] = df['M14AM14B'].shift(lag)
    
    # 2. ì´ë™ í‰ê·  (Rolling)
    windows = [10, 30, 60]
    for w in windows:
        df[f'Total_Mean_{w}'] = df['TOTALCNT'].rolling(w).mean()
        df[f'Total_Std_{w}'] = df['TOTALCNT'].rolling(w).std()

    # 3. ğŸ”¥ [ê°•í™”] ë³€í™”ëŸ‰ ë° ê¸°ìš¸ê¸° (Lag í˜„ìƒ ë°©ì§€ í•µì‹¬)
    # 1ë¶„ ì „ ëŒ€ë¹„ ì–¼ë§ˆë‚˜ ë³€í–ˆë‚˜? (ê°€ì¥ ë¯¼ê°í•œ ì§€í‘œ)
    df['Slope_1'] = df['TOTALCNT'] - df['TOTALCNT'].shift(1)
    df['Slope_3'] = df['TOTALCNT'] - df['TOTALCNT'].shift(3)
    
    # ê°€ì†ë„ (ë³€í™”ì˜ ì†ë„ê°€ ë¹¨ë¼ì§€ëŠ”ê°€?)
    df['Accel_1'] = df['Slope_1'] - df['Slope_1'].shift(1)
    
    # ê¸°ì¡´ ë³€í™”ëŸ‰ ì§€í‘œ ìœ ì§€
    df['Total_Delta_10'] = df['TOTALCNT'] - df['Total_Lag_10']

    # 4. íƒ€ê²Ÿ ìƒì„± (10ë¶„ í›„ ì˜ˆì¸¡)
    df['Target_Total_Future'] = df['TOTALCNT'].shift(-PRED_OFFSET)
    df['Target_Delta'] = df['Target_Total_Future'] - df['TOTALCNT']
    
    return df.dropna().reset_index(drop=True)

# --- ë©”ì¸ ì‹¤í–‰ ---
# 1. ë°ì´í„° ë¡œë“œ
df = load_data('M14_Q_*.CSV')

# 2. í”¼ì²˜ ìƒì„±
print("\nğŸ”„ í”¼ì²˜ ìƒì„± ì¤‘...")
df_features = create_vectorized_features(df)
print(f"âœ… ìƒì„± ì™„ë£Œ: {len(df_features):,}í–‰, ì»¬ëŸ¼ {df_features.shape[1]}ê°œ")

# 3. í•™ìŠµ ì¤€ë¹„
exclude_cols = ['CURRTIME', 'Target_Total_Future', 'Target_Delta']
feature_cols = [c for c in df_features.columns if c not in exclude_cols]
X = df_features[feature_cols]
y_delta = df_features['Target_Delta']        # ëª©í‘œ: ë³€í™”ëŸ‰ ë§ì¶”ê¸°
y_total = df_features['Target_Total_Future'] # ê°€ì¤‘ì¹˜ ì„¤ì •ìš© ì°¸ì¡°ê°’

# 4. ğŸ”¥ [í•µì‹¬] ê°€ì¤‘ì¹˜ ì „ëµ (Weighting)
# ëª©í‘œ: ëª¨ë¸ì´ "ê¸‰ë³€í•˜ëŠ” ìˆœê°„"ì— ì§‘ì¤‘í•˜ê²Œ ë§Œë“¦
weights = np.ones(len(y_total))

# ê·œì¹™ A: ì ˆëŒ€ê°’ì´ ë†’ìœ¼ë©´ ì¤‘ìš” (ê¸°ì¡´ ìœ ì§€)
weights[y_total >= 1500] = 5
weights[y_total >= 1700] = 20

# ê·œì¹™ B: ğŸ”¥ ë³€í™”ëŸ‰ì´ í¬ë©´ ì¤‘ìš” (Delta >= 50ì´ë©´ ê°€ì¤‘ì¹˜ 10ë°°)
# -> ì´ê±¸ ì¶”ê°€í•´ì•¼ ëª¨ë¸ì´ "ë©ë•Œë¦¬ë‹¤ê°€ 1700 ê°€ëŠ” ê²ƒ"ì„ ì¡ìŠµë‹ˆë‹¤.
weights[y_delta >= 50] = 10 

print("\nğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ë³€í™”ëŸ‰ + ê³ ë¶€í•˜ ì§‘ì¤‘)...")
model_base = xgb.XGBRegressor(
    n_estimators=2000,      # í•™ìŠµëŸ‰ ì¶©ë¶„íˆ
    learning_rate=0.015,    # ì •êµí•˜ê²Œ
    max_depth=10,           # ë³µì¡í•œ íŒ¨í„´ ì¸ì‹
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=1,
    n_jobs=4,
    device='cuda',          # GPU ì‚¬ìš© (ì—†ìœ¼ë©´ 'cpu')
    tree_method='hist',
    random_state=42
)

# í•™ìŠµ (ë³€í™”ëŸ‰ y_delta í•™ìŠµ)
model_base.fit(X, y_delta, sample_weight=weights, verbose=False)

# ì €ì¥
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
with open('model_v11_delta.pkl', 'wb') as f:
    pickle.dump(model_base, f)
with open('features_v11_delta.pkl', 'wb') as f:
    pickle.dump(list(feature_cols), f)
    
print("âœ… í•™ìŠµ ì™„ë£Œ! (model_v11_delta.pkl)")