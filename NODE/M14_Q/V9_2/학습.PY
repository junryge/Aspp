#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ V12.2 í•™ìŠµ - ê°€ì¤‘ì¹˜ í­íƒ„ (Weight Bomb) ì ìš©
íŠ¹ì§•: ë°¸ëŸ°ì‹±ëœ ë°ì´í„°ì— 1700+ êµ¬ê°„ ê°€ì¤‘ì¹˜ 50ë°°ë¥¼ ë¶€ì—¬í•˜ì—¬ ê³ ë¶€í•˜ ì˜ˆì¸¡ ê°•ì œí™”
"""
import numpy as np
import pandas as pd
import xgboost as xgb
import pickle
import glob
import warnings
warnings.filterwarnings('ignore')

print("="*70)
print("ğŸš€ V12.2 í•™ìŠµ - ê°€ì¤‘ì¹˜ í­íƒ„ (Over_1700 ì§‘ì¤‘ íƒ€ê²©)")
print("="*70)

PRED_OFFSET = 10
SEQ_LEN = 280

def get_load_category(cnt):
    """ë¶€í•˜ êµ¬ê°„ ë¶„ë¥˜ (ë°¸ëŸ°ì‹±ìš©)"""
    if cnt <= 1200: return 'Under_1200'
    elif 1200 < cnt <= 1300: return '1200_1300'
    elif 1300 < cnt <= 1400: return '1300_1400'
    elif 1400 < cnt <= 1500: return '1400_1500'
    elif 1500 < cnt <= 1600: return '1500_1600'
    elif 1600 < cnt < 1700: return '1600_1700'
    else: return 'Over_1700'

def load_data(path_pattern):
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {path_pattern}")
    files = glob.glob(path_pattern)
    if not files: 
        print("âš  ì§€ì •ëœ íŒŒì¼ì´ ì—†ì–´ ê¸°ë³¸ íŒ¨í„´(M14_Q_*.CSV)ì„ ì°¾ìŠµë‹ˆë‹¤.")
        files = glob.glob('M14_Q_*.CSV')
    
    dfs = []
    for f in sorted(files):
        try: dfs.append(pd.read_csv(f, on_bad_lines='skip', encoding='utf-8'))
        except: dfs.append(pd.read_csv(f, on_bad_lines='skip', encoding='cp949'))
    
    if not dfs: raise ValueError("âŒ í•™ìŠµí•  ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
    return pd.concat(dfs, ignore_index=True)

def create_vectorized_features(df):
    """í”¼ì²˜ ìƒì„± (280ë¶„ ì‹œí€€ìŠ¤ & ê¸°ìš¸ê¸° ê°•í™”)"""
    df = df.copy()
    
    # ìˆ«ìí˜• ë³€í™˜
    cols = ['TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M10AM14A', 
            'M14.QUE.ALL.TRANSPORT4MINOVERCNT', 'M14.QUE.OHT.OHTUTIL',
            'M14.QUE.ALL.CURRENTQCREATED', 'M14.QUE.ALL.CURRENTQCOMPLETED']
    for c in cols: df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)
    
    df['Gap'] = df['M14.QUE.ALL.CURRENTQCREATED'] - df['M14.QUE.ALL.CURRENTQCOMPLETED']
    
    # 1. Lag (ì‹œì°¨)
    lags = [1, 3, 5, 10, 20, 50, 100, 200, 280]
    for lag in lags:
        df[f'Total_Lag_{lag}'] = df['TOTALCNT'].shift(lag)
        df[f'M14B_Lag_{lag}'] = df['M14AM14B'].shift(lag)
        df[f'Sum_Lag_{lag}'] = df['M14AM14BSUM'].shift(lag)
        df[f'Gap_Lag_{lag}'] = df['Gap'].shift(lag)
    
    # 2. Rolling (ì´ë™í‰ê· )
    windows = [10, 30, 60, 100, 200, 280]
    for w in windows:
        df[f'Total_Mean_{w}'] = df['TOTALCNT'].rolling(w).mean()
        df[f'Total_Std_{w}'] = df['TOTALCNT'].rolling(w).std()
        df[f'M14B_Mean_{w}'] = df['M14AM14B'].rolling(w).mean()
        df[f'Gap_Mean_{w}'] = df['Gap'].rolling(w).mean()

    # 3. Slope (ê¸‰ìƒìŠ¹ ê°ì§€ìš© ê¸°ìš¸ê¸°)
    df['Slope_1'] = df['TOTALCNT'] - df['TOTALCNT'].shift(1)
    df['Slope_3'] = df['TOTALCNT'] - df['TOTALCNT'].shift(3)
    df['Slope_10'] = df['TOTALCNT'] - df['TOTALCNT'].shift(10)
    
    # íƒ€ê²Ÿ ìƒì„± (10ë¶„ ë’¤ ì˜ˆì¸¡)
    df['Target_Total_Future'] = df['TOTALCNT'].shift(-PRED_OFFSET)
    df['Target_Delta'] = df['Target_Total_Future'] - df['TOTALCNT']
    
    # ê²°ì¸¡ ì œê±°
    return df.dropna().reset_index(drop=True)

# --- ë©”ì¸ ì‹¤í–‰ ---
# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
df = load_data('M14_Q_*.CSV') 
print("\nğŸ”„ í”¼ì²˜ ìƒì„± ì¤‘...")
df_features = create_vectorized_features(df)

# 2. ë°ì´í„° ë°¸ëŸ°ì‹± (Under-sampling)
print("\nâš–ï¸ ë°ì´í„° ë°¸ëŸ°ì‹± ì‘ì—…...")
# íƒ€ê²Ÿê°’ ê¸°ì¤€ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
df_features['Load_Category'] = df_features['Target_Total_Future'].apply(get_load_category)

# ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ê³„ì‚° (ë„ˆë¬´ ì ìœ¼ë©´ 5000ê°œë¡œ ê³ ì • ì‹œë„)
counts = df_features['Load_Category'].value_counts()
min_samples = counts.min()
target_samples = max(min_samples, 5000) 

print(f"  ğŸ‘‰ í´ë˜ìŠ¤ë³„ ëª©í‘œ ìƒ˜í”Œ ìˆ˜: {target_samples} (ìµœì†Œ í´ë˜ìŠ¤: {min_samples})")

df_balanced = df_features.groupby('Load_Category', group_keys=False).apply(
    lambda x: x.sample(n=min(len(x), target_samples), random_state=42)
)
# ìˆœì„œ ì„ê¸°
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

# 3. [í•µì‹¬] ê°€ì¤‘ì¹˜(Sample Weight) ìƒì„± - ê¸ˆìœµ ì¹˜ë£Œ ì ìš©
# ê¸°ë³¸ ê°€ì¤‘ì¹˜ 1.0
weights = np.ones(len(df_balanced))
y_future = df_balanced['Target_Total_Future'].values

# êµ¬ê°„ë³„ ê°€ì¤‘ì¹˜ í• ì¦
weights[(y_future >= 1500) & (y_future < 1600)] = 2.0   # ì£¼ì˜ êµ¬ê°„: 2ë°°
weights[(y_future >= 1600) & (y_future < 1700)] = 10.0  # ìœ„í—˜ êµ¬ê°„: 10ë°°
weights[y_future >= 1700] = 50.0                        # ğŸš¨ ê³ ë¶€í•˜ êµ¬ê°„: 50ë°° (ê°•ë ¥ ì²˜ë°©)

print(f"ğŸ”¥ ê°€ì¤‘ì¹˜ ì ìš© ì™„ë£Œ: 1700+ ìƒ˜í”Œ {sum(y_future >= 1700)}ê°œì— 50ë°° ê°€ì¤‘ì¹˜ ë¶€ì—¬")

# 4. í•™ìŠµ ì¤€ë¹„
exclude_cols = ['CURRTIME', 'Target_Total_Future', 'Target_Delta', 'Load_Category']
feature_cols = [c for c in df_balanced.columns if c not in exclude_cols]

X = df_balanced[feature_cols]
y = df_balanced['Target_Delta']

print(f"\nğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (XGBoost, ê°€ì¤‘ì¹˜ ì ìš©)...")
model = xgb.XGBRegressor(
    n_estimators=3000,      # ì¶©ë¶„í•œ í•™ìŠµëŸ‰
    learning_rate=0.01,     # ì •êµí•œ í•™ìŠµ
    max_depth=10,           # ë³µì¡í•œ íŒ¨í„´ ì¸ì‹
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=1,     # ì†Œìˆ˜ ë°ì´í„° ë¬´ì‹œ ë°©ì§€
    n_jobs=4,
    device='cuda',          # GPU ì‚¬ìš© (ì—†ìœ¼ë©´ 'cpu'ë¡œ ë³€ê²½)
    tree_method='hist',
    random_state=42
)

# í•™ìŠµ ì‹œ weights ì „ë‹¬
model.fit(X, y, sample_weight=weights, verbose=False)

# 5. ì €ì¥
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
with open('model_v12_weighted.pkl', 'wb') as f:
    pickle.dump(model, f)
with open('features_v12_weighted.pkl', 'wb') as f:
    pickle.dump(feature_cols, f)
    
print("âœ… í•™ìŠµ ì™„ë£Œ! (model_v12_weighted.pkl)")
