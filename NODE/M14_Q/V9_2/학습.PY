#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ V11.5 í•™ìŠµ - 280ë¶„ ì‹œí€€ìŠ¤ & 10ë¶„ ì˜ˆì¸¡
íŠ¹ì§•: ê³¼ê±° 280ë¶„(4ì‹œê°„ 40ë¶„) ë°ì´í„°ë¥¼ ë³´ê³  10ë¶„ ë’¤ ë³€í™”ëŸ‰(Delta)ì„ ì˜ˆì¸¡
"""
import numpy as np
import pandas as pd
import xgboost as xgb
import pickle
import glob
import warnings
warnings.filterwarnings('ignore')

print("="*70)
print("ğŸš€ V11.5 í•™ìŠµ - 280ë¶„ ì‹œí€€ìŠ¤ / 10ë¶„ ì˜ˆì¸¡")
print("="*70)

PRED_OFFSET = 10 # 10ë¶„ ë’¤ ì˜ˆì¸¡

def load_data(path_pattern):
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {path_pattern}")
    files = glob.glob(path_pattern)
    if not files: files = glob.glob('M14_Q_*.CSV')
    dfs = []
    for f in sorted(files):
        try: dfs.append(pd.read_csv(f, on_bad_lines='skip', encoding='utf-8'))
        except: dfs.append(pd.read_csv(f, on_bad_lines='skip', encoding='cp949'))
    return pd.concat(dfs, ignore_index=True)

def create_vectorized_features(df):
    """280ë¶„ ì‹œí€€ìŠ¤ í”¼ì²˜ ìƒì„±"""
    df = df.copy()
    
    # ìˆ«ìí˜• ë³€í™˜
    cols = ['TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M10AM14A', 
            'M14.QUE.ALL.TRANSPORT4MINOVERCNT', 'M14.QUE.OHT.OHTUTIL',
            'M14.QUE.ALL.CURRENTQCREATED', 'M14.QUE.ALL.CURRENTQCOMPLETED']
    for c in cols: df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)

    df['Gap'] = df['M14.QUE.ALL.CURRENTQCREATED'] - df['M14.QUE.ALL.CURRENTQCOMPLETED']
    
    # ğŸ”¥ [ìˆ˜ì •] 280ë¶„ê¹Œì§€ í™•ì¥
    lags = [1, 3, 5, 10, 20, 50, 100, 280] 
    for lag in lags:
        df[f'Total_Lag_{lag}'] = df['TOTALCNT'].shift(lag)
        df[f'M14B_Lag_{lag}'] = df['M14AM14B'].shift(lag)
    
    # ğŸ”¥ [ìˆ˜ì •] 280ë¶„ê¹Œì§€ í™•ì¥
    windows = [10, 30, 60, 100, 280]
    for w in windows:
        df[f'Total_Mean_{w}'] = df['TOTALCNT'].rolling(w).mean()
        df[f'Total_Std_{w}'] = df['TOTALCNT'].rolling(w).std()

    # ë³€í™”ëŸ‰ ë° ê¸°ìš¸ê¸° (Delta ì§‘ì¤‘)
    df['Slope_1'] = df['TOTALCNT'] - df['TOTALCNT'].shift(1)
    df['Slope_3'] = df['TOTALCNT'] - df['TOTALCNT'].shift(3)
    df['Accel_1'] = df['Slope_1'] - df['Slope_1'].shift(1)
    df['Total_Delta_10'] = df['TOTALCNT'] - df['Total_Lag_10']

    # íƒ€ê²Ÿ ìƒì„± (10ë¶„ í›„ ì˜ˆì¸¡)
    df['Target_Total_Future'] = df['TOTALCNT'].shift(-PRED_OFFSET)
    df['Target_Delta'] = df['Target_Total_Future'] - df['TOTALCNT']
    
    return df.dropna().reset_index(drop=True)

# 1. ì‹¤í–‰
df = load_data('M14_Q_*.CSV')
# (ë§Œì•½ 1:1 íŒŒì¼ ì“°ì‹¤ê±°ë©´: df = pd.read_csv('Final_Train_Data_1to1.csv') í•˜ì‹œë©´ ë©ë‹ˆë‹¤)

print("\nğŸ”„ 280ë¶„ ì‹œí€€ìŠ¤ í”¼ì²˜ ìƒì„± ì¤‘...")
df_features = create_vectorized_features(df)

# 2. í•™ìŠµ ì¤€ë¹„
exclude = ['CURRTIME', 'Target_Total_Future', 'Target_Delta']
cols = [c for c in df_features.columns if c not in exclude]
X = df_features[cols]
y_delta = df_features['Target_Delta']
y_total = df_features['Target_Total_Future']

# 3. ê°€ì¤‘ì¹˜ (ê³ ë¶€í•˜ + ê¸‰ìƒìŠ¹)
weights = np.ones(len(y_total))
weights[y_total >= 1500] = 5
weights[y_total >= 1700] = 20
weights[y_delta >= 50] = 10 

print(f"\nğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (280ë¶„ Sequence)...")
model = xgb.XGBRegressor(
    n_estimators=2000,
    learning_rate=0.015,
    max_depth=10,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=1,
    n_jobs=4,
    device='cuda',
    tree_method='hist',
    random_state=42
)

model.fit(X, y_delta, sample_weight=weights, verbose=False)

# ì €ì¥
with open('model_v11_280seq.pkl', 'wb') as f: pickle.dump(model, f)
with open('features_v11_280seq.pkl', 'wb') as f: pickle.dump(cols, f)
print("âœ… í•™ìŠµ ì™„ë£Œ! (model_v11_280seq.pkl)")