#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ V13.0 í•™ìŠµ - ë°ì´í„° ì¦í­ (Oversampling) & ì¼ë°˜í™”
íŠ¹ì§•: í‰ê°€ì¼(12ì›” 11ì¼) ì œì™¸í•˜ê³ , ë‹¤ë¥¸ ê³ ë¶€í•˜ ë‚ ì§œë¥¼ 5ë°° ì¦í­í•˜ì—¬ í•™ìŠµ
"""
import numpy as np
import pandas as pd
import xgboost as xgb
import pickle
import glob
import warnings
warnings.filterwarnings('ignore')

print("="*70)
print("ğŸš€ V13.0 í•™ìŠµ - ë°ì´í„° ì¦í­ (ìª½ì§‘ê²Œ ê³¼ì™¸ ëª¨ë“œ)")
print("="*70)

PRED_OFFSET = 10
EVAL_DATE = '2025-12-11' # ğŸ”´ ì´ ë‚ ì§œëŠ” í•™ìŠµì—ì„œ ì² ì €íˆ ë°°ì œ (í‰ê°€ìš©)

def load_data(path_pattern):
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {path_pattern}")
    files = glob.glob(path_pattern)
    if not files: files = glob.glob('M14_Q_*.CSV')
    
    dfs = []
    for f in sorted(files):
        try: dfs.append(pd.read_csv(f, on_bad_lines='skip', encoding='utf-8'))
        except: dfs.append(pd.read_csv(f, on_bad_lines='skip', encoding='cp949'))
    
    if not dfs: raise ValueError("âŒ ë°ì´í„° íŒŒì¼ ì—†ìŒ!")
    return pd.concat(dfs, ignore_index=True)

def create_vectorized_features(df):
    """V11 ê¸°ë°˜ í”¼ì²˜ ìƒì„± (ê²€ì¦ëœ ë¡œì§)"""
    df = df.copy()
    cols = ['TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M10AM14A', 
            'M14.QUE.ALL.TRANSPORT4MINOVERCNT', 'M14.QUE.OHT.OHTUTIL',
            'M14.QUE.ALL.CURRENTQCREATED', 'M14.QUE.ALL.CURRENTQCOMPLETED']
    for c in cols: df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)
    
    df['Gap'] = df['M14.QUE.ALL.CURRENTQCREATED'] - df['M14.QUE.ALL.CURRENTQCOMPLETED']
    
    # Lag & Rolling (V11 ì„¤ì • ìœ ì§€)
    lags = [1, 3, 5, 10, 20]
    for lag in lags:
        df[f'Total_Lag_{lag}'] = df['TOTALCNT'].shift(lag)
        df[f'M14B_Lag_{lag}'] = df['M14AM14B'].shift(lag)
    
    windows = [10, 30, 60]
    for w in windows:
        df[f'Total_Mean_{w}'] = df['TOTALCNT'].rolling(w).mean()
        df[f'Total_Std_{w}'] = df['TOTALCNT'].rolling(w).std()

    # íƒ€ê²Ÿ ìƒì„±
    df['Target_Total_Future'] = df['TOTALCNT'].shift(-PRED_OFFSET)
    df['Target_Delta'] = df['Target_Total_Future'] - df['TOTALCNT']
    
    return df.dropna().reset_index(drop=True)

# 1. ë°ì´í„° ë¡œë“œ
df_raw = load_data('M14_Q_*.CSV')

# 2. ë‚ ì§œ íŒŒì‹± ë° í‰ê°€ì¼ ì œì™¸ (Cheating ë°©ì§€)
df_raw['datetime'] = pd.to_datetime(df_raw['CURRTIME'].astype(str), format='%Y%m%d%H%M', errors='coerce')
df_train_source = df_raw[df_raw['datetime'].dt.date.astype(str) != EVAL_DATE].copy()

print(f"ğŸ“‰ í•™ìŠµ ë°ì´í„° í•„í„°ë§: {EVAL_DATE} ë°ì´í„° ì œì™¸ ì™„ë£Œ")
print(f"   (ë‚¨ì€ ë°ì´í„°ë¡œ 12ì›” íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤)")

# 3. í”¼ì²˜ ìƒì„±
print("\nğŸ”„ í”¼ì²˜ ìƒì„± ì¤‘...")
df_features = create_vectorized_features(df_train_source)

# 4. [í•µì‹¬] ë°ì´í„° ì¦í­ (Oversampling)
# 1650 ì´ìƒì¸ 'ì§„ì§œ' ìœ„í—˜ ë°ì´í„°ë¥¼ ì°¾ì•„ì„œ 5ë°°ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.
print("\nâš–ï¸ ë°ì´í„° ì¦í­ ì‘ì—… (Oversampling)...")

high_load_indices = df_features[df_features['Target_Total_Future'] >= 1650].index
if len(high_load_indices) > 0:
    high_load_data = df_features.loc[high_load_indices]
    # 5ë°° ë»¥íŠ€ê¸° (ì›ë³¸ í¬í•¨ ì´ 6ë°° íš¨ê³¼)
    augmented_data = pd.concat([high_load_data] * 5, ignore_index=True)
    df_final = pd.concat([df_features, augmented_data], ignore_index=True)
    print(f"ğŸ”¥ ê³ ë¶€í•˜(1650+) ë°ì´í„° {len(high_load_data)}ê°œë¥¼ 5ë°° ë³µì‚¬í•˜ì—¬ ì¶”ê°€í•¨!")
else:
    df_final = df_features
    print("âš  ê³ ë¶€í•˜ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ì¦í­í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# ìˆœì„œ ì„ê¸°
df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)

# 5. ê°€ì¤‘ì¹˜ ì„¤ì • (V11 ë¡œì§)
y_future = df_final['Target_Total_Future'].values
weights = np.ones(len(df_final))
weights[y_future >= 1500] = 5
weights[y_future >= 1700] = 20  # ì¤‘ìš”ë„ ìœ ì§€

# 6. í•™ìŠµ ì¤€ë¹„
exclude = ['CURRTIME', 'datetime', 'Target_Total_Future', 'Target_Delta']
cols = [c for c in df_final.columns if c not in exclude]
X = df_final[cols]
y = df_final['Target_Delta']

print(f"\nğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ì¦í­ëœ ë°ì´í„° {len(df_final):,}ê±´)...")
model = xgb.XGBRegressor(
    n_estimators=2000,
    learning_rate=0.02,
    max_depth=10,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=1,
    n_jobs=4,
    device='cuda',
    tree_method='hist',
    random_state=42
)

model.fit(X, y, sample_weight=weights, verbose=False)

# 7. ì €ì¥
with open('model_v13_final.pkl', 'wb') as f: pickle.dump(model, f)
with open('features_v13_final.pkl', 'wb') as f: pickle.dump(cols, f)
print("âœ… V13.0 í•™ìŠµ ì™„ë£Œ! (model_v13_final.pkl)")
