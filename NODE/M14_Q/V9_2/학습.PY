#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ V11.0 í•™ìŠµ - 10ë¶„ ì˜ˆì¸¡ (ì‹œí€€ìŠ¤ 280ë¶„)
ì‹œí€€ìŠ¤: 280ë¶„, ì˜ˆì¸¡: 10ë¶„ í›„
íŠ¹ì§•: ë²¡í„°í™” ì—°ì‚°, ê°€ì¤‘ì¹˜ í•™ìŠµ, 280ë¶„ ì‹œê³„ì—´ í™œìš©
"""
import numpy as np
import pandas as pd
import xgboost as xgb
import pickle
import glob
import warnings
from sklearn.metrics import mean_absolute_error
warnings.filterwarnings('ignore')
print("="*70)
print("ğŸš€ V11.0 í•™ìŠµ - 10ë¶„ ì˜ˆì¸¡ (ì‹œí€€ìŠ¤ 280ë¶„)")
print("="*70)
PRED_OFFSET = 10
SEQ_LEN = 280

def load_data(path_pattern):
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {path_pattern}")
    files = glob.glob(path_pattern)
    if not files:
        print("âš  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ íŒŒì¼ëª…ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        files = glob.glob('M14_Q_20241201_202512010_í•™ìŠµ.CSV')
        
    dfs = []
    for f in sorted(files):
        try:
            df = pd.read_csv(f, on_bad_lines='skip', encoding='utf-8')
            print(f"  {f}: {len(df):,}í–‰")
            dfs.append(df)
        except UnicodeDecodeError:
            df = pd.read_csv(f, on_bad_lines='skip', encoding='cp949')
            print(f"  {f} (cp949): {len(df):,}í–‰")
            dfs.append(df)
            
    if not dfs:
        raise ValueError("ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        
    return pd.concat(dfs, ignore_index=True)

def create_vectorized_features(df):
    """ë²¡í„°í™”ëœ í”¼ì²˜ ìƒì„± (280ë¶„ ì‹œí€€ìŠ¤)"""
    df = df.copy()
    
    # ìˆ«ìí˜• ë³€í™˜
    cols = ['TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M10AM14A', 
            'M14.QUE.ALL.TRANSPORT4MINOVERCNT', 'M14.QUE.OHT.OHTUTIL',
            'M14.QUE.ALL.CURRENTQCREATED', 'M14.QUE.ALL.CURRENTQCOMPLETED']
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)
    # ê¸°ë³¸ ë³€ìˆ˜
    df['Gap'] = df['M14.QUE.ALL.CURRENTQCREATED'] - df['M14.QUE.ALL.CURRENTQCOMPLETED']
    df['Transport'] = df['M14.QUE.ALL.TRANSPORT4MINOVERCNT']
    df['OHT'] = df['M14.QUE.OHT.OHTUTIL']
    
    # 1. ì‹œì°¨ ë³€ìˆ˜ (Lags) - 280ë¶„ê¹Œì§€ í™•ì¥
    lags = [1, 3, 5, 10, 20, 50, 100, 200, 280]
    for lag in lags:
        df[f'Total_Lag_{lag}'] = df['TOTALCNT'].shift(lag)
        df[f'M14B_Lag_{lag}'] = df['M14AM14B'].shift(lag)
        df[f'Sum_Lag_{lag}'] = df['M14AM14BSUM'].shift(lag)
        df[f'Gap_Lag_{lag}'] = df['Gap'].shift(lag)
    # 2. ì´ë™ í‰ê· /í‘œì¤€í¸ì°¨ (Rolling Stats) - 280ë¶„ê¹Œì§€ í™•ì¥
    windows = [10, 30, 60, 100, 200, 280]
    for w in windows:
        df[f'Total_Mean_{w}'] = df['TOTALCNT'].rolling(w).mean()
        df[f'Total_Std_{w}'] = df['TOTALCNT'].rolling(w).std()
        df[f'Total_Max_{w}'] = df['TOTALCNT'].rolling(w).max()
        df[f'Total_Min_{w}'] = df['TOTALCNT'].rolling(w).min()
        df[f'M14B_Mean_{w}'] = df['M14AM14B'].rolling(w).mean()
        df[f'Gap_Mean_{w}'] = df['Gap'].rolling(w).mean()
    # 3. ë³€í™”ëŸ‰ (Delta)
    df['Total_Delta_1'] = df['TOTALCNT'] - df['Total_Lag_1']
    df['Total_Delta_5'] = df['TOTALCNT'] - df['Total_Lag_5']
    df['Total_Delta_10'] = df['TOTALCNT'] - df['Total_Lag_10']
    df['Total_Delta_50'] = df['TOTALCNT'] - df['Total_Lag_50']
    df['Total_Delta_100'] = df['TOTALCNT'] - df['Total_Lag_100']
    df['Total_Delta_280'] = df['TOTALCNT'] - df['Total_Lag_280']
    
    df['M14B_Delta_10'] = df['M14AM14B'] - df['M14B_Lag_10']
    df['M14B_Delta_100'] = df['M14AM14B'] - df['M14B_Lag_100']
    df['Sum_Delta_10'] = df['M14AM14BSUM'] - df['Sum_Lag_10']
    df['Sum_Delta_100'] = df['M14AM14BSUM'] - df['Sum_Lag_100']
    
    # 4. ìƒí˜¸ì‘ìš© (Interaction)
    df['Load_Ratio'] = df['M14AM14B'] / (df['TOTALCNT'] + 1)
    df['Gap_Load'] = df['Gap'] * df['M14AM14B']
    
    # 5. íŒ¨í„´ (Explicit Patterns)
    df['Gold_Pattern'] = ((df['M14AM14B'] > 500) & (df['M14AM14BSUM'] > 600)).astype(int)
    df['Danger_Gap'] = (df['Gap'] > 250).astype(int)
    df['Danger_Total'] = (df['TOTALCNT'] >= 1600).astype(int)
    df['Triple_Check'] = ((df['M14AM14B'] > 450) & (df['M14AM14BSUM'] > 550) & (df['Gap'] > 200)).astype(int)
    
    # 6. ê°€ì†ë„ (Acceleration) ë° ë‹¨ê¸° ë³€í™”
    df['Accel_1'] = df['Total_Delta_1'] - df['Total_Delta_1'].shift(1)
    df['Accel_3'] = df['Total_Delta_1'].diff(3)
    
    # ê¸°ìš¸ê¸° (Slope)
    df['Slope_3'] = (df['TOTALCNT'] - df['TOTALCNT'].shift(3)) / 3
    df['Slope_5'] = (df['TOTALCNT'] - df['TOTALCNT'].shift(5)) / 5
    df['Slope_30'] = (df['TOTALCNT'] - df['TOTALCNT'].shift(30)) / 30
    df['Slope_100'] = (df['TOTALCNT'] - df['TOTALCNT'].shift(100)) / 100
    df['Gap_Slope_3'] = (df['Gap'] - df['Gap'].shift(3)) / 3
    df['Gap_Slope_30'] = (df['Gap'] - df['Gap'].shift(30)) / 30
    # íƒ€ê²Ÿ ìƒì„± (10ë¶„ í›„ ì˜ˆì¸¡)
    df['Target_Total_Future'] = df['TOTALCNT'].shift(-PRED_OFFSET)
    df['Target_Delta'] = df['Target_Total_Future'] - df['TOTALCNT']
    
    # ê²°ì¸¡ ì œê±°
    df_clean = df.dropna().reset_index(drop=True)
    return df_clean

# --- ë©”ì¸ ì‹¤í–‰ ---
# 1. ë°ì´í„° ë¡œë“œ
df = load_data('M14_Q_*.CSV')
# 2. í”¼ì²˜ ìƒì„±
print("\nğŸ”„ í”¼ì²˜ ìƒì„± ì¤‘ (280ë¶„ ì‹œí€€ìŠ¤)...")
df_features = create_vectorized_features(df)
print(f"âœ… ìƒì„± ì™„ë£Œ: {len(df_features):,}í–‰, ì»¬ëŸ¼ {df_features.shape[1]}ê°œ")
# 3. í•™ìŠµ ì¤€ë¹„
exclude_cols = ['CURRTIME', 'Target_Total_Future', 'Target_Delta']
feature_cols = [c for c in df_features.columns if c not in exclude_cols]
X = df_features[feature_cols]
y_delta = df_features['Target_Delta']
y_total = df_features['Target_Total_Future']
print(f"ğŸ“Š í”¼ì²˜ ìˆ˜: {len(feature_cols)}ê°œ")
# ê°€ì¤‘ì¹˜: ê³ ë¶€í•˜ 20ë°°, ì¤‘ë¶€í•˜ 5ë°°
weights = np.ones(len(y_total))
weights[y_total >= 1500] = 5
weights[y_total >= 1700] = 20
# ëª¨ë¸ í•™ìŠµ (ì „ì²´ ë°ì´í„° ì‚¬ìš©)
print("\nğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (GPU ê°€ì†, ë°˜ì‘ì„± ê°•í™”)...")
# ì›ë³¸ ëª¨ë¸ (Base Model)
print("  ë©”ì¸ ëª¨ë¸ í•™ìŠµ...")
model_base = xgb.XGBRegressor(
    n_estimators=1500, # í•™ìŠµëŸ‰ ì¦ê°€
    learning_rate=0.02, # ì •êµí•˜ê²Œ
    max_depth=10, # ê¹Šì´ ì¦ê°€ (ë³µì¡í•œ íŒ¨í„´)
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=1, # ë¯¼ê°í•˜ê²Œ
    reg_alpha=0.0, # ê·œì œ ì™„í™”
    reg_lambda=0.5, # ê·œì œ ì™„í™”
    n_jobs=4,
    device='cuda',
    tree_method='hist',
    random_state=42
)
model_base.fit(X, y_delta, sample_weight=weights, verbose=False)
# ì €ì¥
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
with open('model_v11_280min.pkl', 'wb') as f:
    pickle.dump(model_base, f)
with open('features_v11_280min.pkl', 'wb') as f:
    pickle.dump(list(feature_cols), f)
print(f"âœ… í•™ìŠµ ì™„ë£Œ! (model_v11_280min.pkl)")
print(f"ğŸ“Š ì‹œí€€ìŠ¤: {SEQ_LEN}ë¶„, í”¼ì²˜: {len(feature_cols)}ê°œ")