#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”¬ AMHS 3ê°œì›” ë°ì´í„° ì¢…í•© ë¶„ì„
- EDA ë° ë³€ìˆ˜ íƒ€ì… ì •ì˜
- ë°ì´í„° ì •í•©ì„± í™•ì¸
- Feature Selection (XGBoost, SHAP, Boruta)
- ì‹œê°í™” + ë¦¬í¬íŠ¸ ìƒì„±
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['font.size'] = 10

print("="*80)
print("ğŸ”¬ AMHS 3ê°œì›” ë°ì´í„° ì¢…í•© ë¶„ì„")
print("="*80)

# ============================================================
# 1. ë°ì´í„° ë¡œë“œ
# ============================================================
def load_data(file_path):
    """ë°ì´í„° ë¡œë“œ"""
    print("\n" + "="*60)
    print("ğŸ“‚ 1. ë°ì´í„° ë¡œë“œ")
    print("="*60)
    
    df = pd.read_csv(file_path, on_bad_lines='skip')
    print(f"âœ… íŒŒì¼: {file_path}")
    print(f"âœ… í–‰: {len(df):,}ê°œ, ì—´: {df.shape[1]}ê°œ")
    
    # CURRTIME íŒŒì‹±
    if 'CURRTIME' in df.columns:
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), format='%Y%m%d%H%M', errors='coerce')
        if df['CURRTIME'].notna().sum() > 0:
            print(f"âœ… ê¸°ê°„: {df['CURRTIME'].min()} ~ {df['CURRTIME'].max()}")
            days = (df['CURRTIME'].max() - df['CURRTIME'].min()).days
            print(f"   ì´ {days}ì¼ ({days/30:.1f}ê°œì›”)")
    
    # queue_gap ìƒì„± (í•µì‹¬ íŒŒìƒë³€ìˆ˜!)
    if 'M14.QUE.ALL.CURRENTQCREATED' in df.columns and 'M14.QUE.ALL.CURRENTQCOMPLETED' in df.columns:
        df['queue_gap'] = df['M14.QUE.ALL.CURRENTQCREATED'] - df['M14.QUE.ALL.CURRENTQCOMPLETED']
        print(f"âœ… queue_gap ìƒì„± ì™„ë£Œ")
    
    return df


# ============================================================
# 2. EDA - ë³€ìˆ˜ íƒ€ì… ë¶„ì„
# ============================================================
def analyze_variable_types(df):
    """ë³€ìˆ˜ íƒ€ì… ë¶„ì„ (ì¹´í…Œê³ ë¦¬ vs ì—°ì†í˜•)"""
    print("\n" + "="*60)
    print("ğŸ“Š 2. EDA - ë³€ìˆ˜ íƒ€ì… ë¶„ì„")
    print("="*60)
    
    results = []
    for col in df.columns:
        if col == 'CURRTIME':
            continue
        
        unique_count = df[col].nunique()
        total_count = len(df)
        unique_ratio = unique_count / total_count * 100
        
        # ë³€ìˆ˜ íƒ€ì… íŒì •
        if unique_count <= 10:
            var_type = 'ì¹´í…Œê³ ë¦¬(ëª…ëª©)'
        elif unique_count <= 30:
            var_type = 'ì¹´í…Œê³ ë¦¬(ìˆœì„œ)'
        elif unique_ratio < 5:
            var_type = 'ì¤€ì¹´í…Œê³ ë¦¬'
        else:
            var_type = 'ì—°ì†í˜•'
        
        results.append({
            'ì»¬ëŸ¼ëª…': col,
            'ë°ì´í„°íƒ€ì…': str(df[col].dtype),
            'ê³ ìœ ê°’ìˆ˜': unique_count,
            'ê³ ìœ ê°’ë¹„ìœ¨(%)': round(unique_ratio, 2),
            'ë³€ìˆ˜íƒ€ì…': var_type,
            'ê²°ì¸¡ì¹˜': df[col].isna().sum(),
            'ìµœì†Œê°’': df[col].min() if pd.api.types.is_numeric_dtype(df[col]) else '-',
            'ìµœëŒ€ê°’': df[col].max() if pd.api.types.is_numeric_dtype(df[col]) else '-',
            'í‰ê· ': round(df[col].mean(), 2) if pd.api.types.is_numeric_dtype(df[col]) else '-'
        })
    
    df_types = pd.DataFrame(results)
    
    print(f"\nğŸ“Œ ë³€ìˆ˜ íƒ€ì… ë¶„í¬:")
    print(df_types['ë³€ìˆ˜íƒ€ì…'].value_counts().to_string())
    
    return df_types


# ============================================================
# 3. íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬ ë¶„ì„
# ============================================================
def analyze_target(df, target='TOTALCNT'):
    """TOTALCNT ë¶„í¬ ë¶„ì„"""
    print("\n" + "="*60)
    print(f"ğŸ“Š 3. íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬: {target}")
    print("="*60)
    
    if target not in df.columns:
        print(f"âŒ {target} ì—†ìŒ")
        return None
    
    print(f"  í‰ê· : {df[target].mean():.2f}")
    print(f"  ì¤‘ì•™ê°’: {df[target].median():.2f}")
    print(f"  í‘œì¤€í¸ì°¨: {df[target].std():.2f}")
    print(f"  ìµœì†Œ/ìµœëŒ€: {df[target].min():.0f} ~ {df[target].max():.0f}")
    
    # ìœ„í—˜ êµ¬ê°„ ë¶„ì„
    print(f"\nğŸ“Œ ìœ„í—˜ êµ¬ê°„ ë¶„í¬:")
    thresholds = [1600, 1650, 1700, 1750, 1800]
    for t in thresholds:
        cnt = (df[target] >= t).sum()
        print(f"  {t}+ : {cnt:,}ê°œ ({cnt/len(df)*100:.3f}%)")
    
    return df[target].describe()


# ============================================================
# 4. ìƒê´€ê´€ê³„ ë¶„ì„
# ============================================================
def correlation_analysis(df, target='TOTALCNT', threshold=0.95):
    """ìƒê´€ê´€ê³„ ë¶„ì„"""
    print("\n" + "="*60)
    print("ğŸ“Š 4. ìƒê´€ê´€ê³„ ë¶„ì„")
    print("="*60)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols = [c for c in numeric_cols if c != 'CURRTIME']
    
    print(f"âœ… ìˆ«ìí˜• ë³€ìˆ˜: {len(numeric_cols)}ê°œ")
    
    corr_matrix = df[numeric_cols].corr()
    
    # íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„
    if target in corr_matrix.columns:
        target_corr = corr_matrix[target].drop(target).sort_values(ascending=False)
        
        print(f"\nğŸ“Œ {target}ê³¼ì˜ ìƒê´€ê´€ê³„ TOP 20:")
        for i, (col, corr) in enumerate(target_corr.head(20).items()):
            print(f"  {i+1:2d}. {col}: {corr:.4f}")
    
    # ë†’ì€ ìƒê´€ê´€ê³„ ìŒ (ì œê±° í›„ë³´)
    print(f"\nğŸ“Œ ìƒê´€ê³„ìˆ˜ {threshold} ì´ìƒ ë³€ìˆ˜ìŒ (ì œê±° í›„ë³´):")
    high_corr = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) >= threshold:
                high_corr.append({
                    'ë³€ìˆ˜1': corr_matrix.columns[i],
                    'ë³€ìˆ˜2': corr_matrix.columns[j],
                    'ìƒê´€ê³„ìˆ˜': round(corr_matrix.iloc[i, j], 4)
                })
    
    if high_corr:
        df_high = pd.DataFrame(high_corr).sort_values('ìƒê´€ê³„ìˆ˜', ascending=False)
        print(f"  ì´ {len(df_high)}ê°œ ìŒ")
        print(df_high.head(15).to_string(index=False))
    else:
        print(f"  ì—†ìŒ")
    
    return corr_matrix, high_corr


# ============================================================
# 5. ë°ì´í„° ì •í•©ì„± í™•ì¸
# ============================================================
def check_data_integrity(df):
    """ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, ì¤‘ë³µê°’ í™•ì¸"""
    print("\n" + "="*60)
    print("ğŸ” 5. ë°ì´í„° ì •í•©ì„± í™•ì¸")
    print("="*60)
    
    # 5-1. ê²°ì¸¡ì¹˜
    print("\nğŸ“Œ 5-1. ê²°ì¸¡ì¹˜ ë¶„ì„")
    missing = df.isnull().sum()
    missing_cols = missing[missing > 0].sort_values(ascending=False)
    if len(missing_cols) > 0:
        print(f"  ê²°ì¸¡ì¹˜ ìˆëŠ” ì»¬ëŸ¼: {len(missing_cols)}ê°œ")
        for col, cnt in missing_cols.head(10).items():
            print(f"    {col}: {cnt:,}ê°œ ({cnt/len(df)*100:.2f}%)")
    else:
        print("  âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ")
    
    # 5-2. ì´ìƒì¹˜ (IQR)
    print("\nğŸ“Œ 5-2. ì´ìƒì¹˜ ë¶„ì„ (IQR)")
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    outlier_cols = []
    
    for col in numeric_cols[:50]:  # ìƒìœ„ 50ê°œë§Œ
        Q1, Q3 = df[col].quantile([0.25, 0.75])
        IQR = Q3 - Q1
        outliers = ((df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)).sum()
        if outliers > len(df) * 0.05:  # 5% ì´ìƒ
            outlier_cols.append({'ì»¬ëŸ¼': col, 'ì´ìƒì¹˜ìˆ˜': outliers, 'ë¹„ìœ¨(%)': round(outliers/len(df)*100, 2)})
    
    if outlier_cols:
        print(f"  ì´ìƒì¹˜ 5%+ ì»¬ëŸ¼: {len(outlier_cols)}ê°œ")
        for o in outlier_cols[:10]:
            print(f"    {o['ì»¬ëŸ¼']}: {o['ì´ìƒì¹˜ìˆ˜']:,}ê°œ ({o['ë¹„ìœ¨(%)']}%)")
    
    # 5-3. ì¤‘ë³µê°’
    print("\nğŸ“Œ 5-3. ì¤‘ë³µê°’ ë¶„ì„")
    full_dup = df.duplicated().sum()
    cols_no_time = [c for c in df.columns if c != 'CURRTIME']
    time_only_dup = df.duplicated(subset=cols_no_time).sum()
    
    print(f"  ì™„ì „ ì¤‘ë³µ: {full_dup:,}ê°œ")
    print(f"  ì‹œê°„ë§Œ ë‹¤ë¥¸ ì¤‘ë³µ: {time_only_dup:,}ê°œ")
    
    # 5-4. ì‹œê°„ ì—°ì†ì„±
    if 'CURRTIME' in df.columns and df['CURRTIME'].notna().sum() > 0:
        print("\nğŸ“Œ 5-4. ì‹œê°„ ì—°ì†ì„±")
        df_sorted = df.dropna(subset=['CURRTIME']).sort_values('CURRTIME')
        time_diff = df_sorted['CURRTIME'].diff().dt.total_seconds() / 60
        gaps = (time_diff > 1).sum()
        print(f"  ëˆ„ë½ êµ¬ê°„: {gaps:,}ê°œ")
        if gaps > 0:
            print(f"  ìµœëŒ€ ëˆ„ë½: {time_diff.max():.0f}ë¶„")
    
    return {'missing': len(missing_cols), 'duplicates': full_dup}


# ============================================================
# 6. queue_gap ë¶„ì„ (í•µì‹¬!)
# ============================================================
def analyze_queue_gap(df, target='TOTALCNT'):
    """queue_gap íŠ¹ë³„ ë¶„ì„"""
    print("\n" + "="*60)
    print("ğŸ”¥ 6. queue_gap ë¶„ì„ (í•µì‹¬ Feature!)")
    print("="*60)
    
    if 'queue_gap' not in df.columns:
        print("âŒ queue_gap ì—†ìŒ")
        return None
    
    print(f"  í‰ê· : {df['queue_gap'].mean():.2f}")
    print(f"  í‘œì¤€í¸ì°¨: {df['queue_gap'].std():.2f}")
    print(f"  ìµœì†Œ/ìµœëŒ€: {df['queue_gap'].min():.0f} ~ {df['queue_gap'].max():.0f}")
    
    # TOTALCNT ìƒê´€ê³„ìˆ˜
    if target in df.columns:
        corr = df['queue_gap'].corr(df[target])
        print(f"\nğŸ“Œ TOTALCNT ìƒê´€ê³„ìˆ˜: {corr:.4f}")
    
    # êµ¬ê°„ë³„ ë¶„ì„
    print(f"\nğŸ“Œ êµ¬ê°„ë³„ í‰ê·  TOTALCNT:")
    bins = [-999, 100, 150, 200, 250, 300, 350, 400, 9999]
    labels = ['<100', '100-150', '150-200', '200-250', '250-300', '300-350', '350-400', '400+']
    df['gap_bin'] = pd.cut(df['queue_gap'], bins=bins, labels=labels)
    
    for label in labels:
        mask = df['gap_bin'] == label
        if mask.sum() > 0:
            avg = df[mask][target].mean()
            rate_1700 = (df[mask][target] >= 1700).sum() / mask.sum() * 100
            print(f"    {label}: í‰ê·  {avg:.0f}, 1700+ í™•ë¥  {rate_1700:.1f}%")
    
    df.drop('gap_bin', axis=1, inplace=True)
    return df['queue_gap'].describe()


# ============================================================
# 7. í™©ê¸ˆ íŒ¨í„´ ë¶„ì„
# ============================================================
def analyze_golden_pattern(df, target='TOTALCNT'):
    """í™©ê¸ˆ íŒ¨í„´ ë¶„ì„"""
    print("\n" + "="*60)
    print("âœ¨ 7. í™©ê¸ˆ íŒ¨í„´ ë¶„ì„")
    print("="*60)
    
    if 'M14AM14B' not in df.columns or 'M14AM14BSUM' not in df.columns:
        print("âŒ M14AM14B/M14AM14BSUM ì—†ìŒ")
        return None
    
    patterns = {
        'ì—„ê²©(540/620)': (df['M14AM14B'] > 540) & (df['M14AM14BSUM'] > 620),
        'ë³´í†µ(520/600)': (df['M14AM14B'] > 520) & (df['M14AM14BSUM'] > 600),
        'ì™„í™”(500/580)': (df['M14AM14B'] > 500) & (df['M14AM14BSUM'] > 580),
    }
    
    for name, mask in patterns.items():
        cnt = mask.sum()
        if cnt > 0:
            avg = df[mask][target].mean()
            rate_1700 = (df[mask][target] >= 1700).sum() / cnt * 100
            print(f"  {name}:")
            print(f"    ë°œìƒ: {cnt:,}ê°œ ({cnt/len(df)*100:.2f}%)")
            print(f"    í‰ê·  TOTALCNT: {avg:.0f}")
            print(f"    1700+ í™•ë¥ : {rate_1700:.1f}%")
    
    return patterns


# ============================================================
# 8. Feature Importance (XGBoost)
# ============================================================
def feature_importance_analysis(df, target='TOTALCNT'):
    """XGBoost Feature Importance"""
    print("\n" + "="*60)
    print("ğŸ¯ 8. Feature Importance (XGBoost)")
    print("="*60)
    
    try:
        import xgboost as xgb
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import mean_absolute_error, r2_score
    except ImportError:
        print("âŒ xgboost í•„ìš”: pip install xgboost")
        return None
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    feature_cols = [c for c in numeric_cols if c != target and c != 'CURRTIME']
    
    X = df[feature_cols].fillna(df[feature_cols].median())
    y = df[target]
    
    print(f"âœ… Feature: {len(feature_cols)}ê°œ, ìƒ˜í”Œ: {len(X):,}ê°œ")
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = xgb.XGBRegressor(max_depth=6, learning_rate=0.1, n_estimators=100, 
                              random_state=42, n_jobs=-1)
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
    
    # ì„±ëŠ¥
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"\nğŸ“Œ ëª¨ë¸ ì„±ëŠ¥: MAE={mae:.2f}, RÂ²={r2:.4f}")
    
    # Importance
    importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\nğŸ“Œ Feature Importance TOP 30:")
    print(importance.head(30).to_string(index=False))
    
    return importance, model


# ============================================================
# 9. SHAP ë¶„ì„ (ì„ íƒ)
# ============================================================
def shap_analysis(df, target='TOTALCNT', sample_size=3000):
    """SHAP ë¶„ì„"""
    print("\n" + "="*60)
    print("ğŸ¯ 9. SHAP ë¶„ì„")
    print("="*60)
    
    try:
        import shap
        import xgboost as xgb
    except ImportError:
        print("âŒ shap í•„ìš”: pip install shap")
        return None
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    feature_cols = [c for c in numeric_cols if c != target and c != 'CURRTIME']
    
    X = df[feature_cols].fillna(df[feature_cols].median())
    y = df[target]
    
    # ìƒ˜í”Œë§
    if len(X) > sample_size:
        idx = np.random.choice(len(X), sample_size, replace=False)
        X_sample, y_sample = X.iloc[idx], y.iloc[idx]
    else:
        X_sample, y_sample = X, y
    
    print(f"âœ… SHAP ìƒ˜í”Œ: {len(X_sample):,}ê°œ")
    
    model = xgb.XGBRegressor(max_depth=6, n_estimators=100, random_state=42, n_jobs=-1)
    model.fit(X_sample, y_sample, verbose=False)
    
    print("ğŸ”„ SHAP ê³„ì‚° ì¤‘...")
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_sample)
    
    shap_imp = pd.DataFrame({
        'feature': feature_cols,
        'mean_abs_shap': np.abs(shap_values).mean(axis=0)
    }).sort_values('mean_abs_shap', ascending=False)
    
    print(f"\nğŸ“Œ SHAP ì¤‘ìš”ë„ TOP 30:")
    print(shap_imp.head(30).to_string(index=False))
    
    return shap_imp


# ============================================================
# 10. Boruta (ì„ íƒ)
# ============================================================
def boruta_analysis(df, target='TOTALCNT', max_iter=30):
    """Boruta Feature Selection"""
    print("\n" + "="*60)
    print("ğŸ¯ 10. Boruta Feature Selection")
    print("="*60)
    
    try:
        from boruta import BorutaPy
        from sklearn.ensemble import RandomForestRegressor
    except ImportError:
        print("âŒ boruta í•„ìš”: pip install boruta")
        return None
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    feature_cols = [c for c in numeric_cols if c != target and c != 'CURRTIME']
    
    X = df[feature_cols].fillna(df[feature_cols].median()).values
    y = df[target].values
    
    print(f"âœ… Feature: {len(feature_cols)}ê°œ")
    print(f"ğŸ”„ Boruta ì‹¤í–‰ ì¤‘... (ì‹œê°„ ì†Œìš”)")
    
    rf = RandomForestRegressor(n_jobs=-1, max_depth=7, random_state=42)
    boruta = BorutaPy(rf, n_estimators='auto', max_iter=max_iter, random_state=42, verbose=0)
    boruta.fit(X, y)
    
    results = pd.DataFrame({
        'feature': feature_cols,
        'ranking': boruta.ranking_,
        'confirmed': boruta.support_,
        'tentative': boruta.support_weak_
    }).sort_values('ranking')
    
    confirmed = results[results['confirmed']]['feature'].tolist()
    tentative = results[results['tentative']]['feature'].tolist()
    
    print(f"\nğŸ“Œ Boruta ê²°ê³¼:")
    print(f"  Confirmed: {len(confirmed)}ê°œ")
    print(f"  Tentative: {len(tentative)}ê°œ")
    print(f"  Rejected: {len(results) - len(confirmed) - len(tentative)}ê°œ")
    
    print(f"\nğŸ“Œ Confirmed Features:")
    for i, f in enumerate(confirmed[:20]):
        print(f"  {i+1}. {f}")
    
    return results


# ============================================================
# 11. ì‹œê°í™”
# ============================================================
def create_visualizations(df, target='TOTALCNT', output_prefix='fig'):
    """ì‹œê°í™” ìƒì„±"""
    print("\n" + "="*60)
    print("ğŸ“ˆ 11. ì‹œê°í™” ìƒì„±")
    print("="*60)
    
    # 1. TOTALCNT ë¶„í¬
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    ax1 = axes[0, 0]
    ax1.hist(df[target], bins=50, edgecolor='black', alpha=0.7, color='steelblue')
    ax1.axvline(1600, color='orange', linestyle='--', label='1600')
    ax1.axvline(1700, color='red', linestyle='--', label='1700')
    ax1.set_xlabel(target)
    ax1.set_title(f'{target} Distribution')
    ax1.legend()
    
    # 2. ì‹œê³„ì—´
    ax2 = axes[0, 1]
    if 'CURRTIME' in df.columns and df['CURRTIME'].notna().any():
        df_plot = df.dropna(subset=['CURRTIME']).sort_values('CURRTIME')
        sample_idx = np.linspace(0, len(df_plot)-1, min(3000, len(df_plot)), dtype=int)
        ax2.plot(df_plot['CURRTIME'].iloc[sample_idx], df_plot[target].iloc[sample_idx], lw=0.5)
        ax2.axhline(1700, color='red', linestyle='--', alpha=0.7)
    ax2.set_title(f'{target} Time Series')
    
    # 3. queue_gap vs TOTALCNT
    ax3 = axes[1, 0]
    if 'queue_gap' in df.columns:
        sample = df.sample(min(3000, len(df)))
        ax3.scatter(sample['queue_gap'], sample[target], alpha=0.3, s=10)
        ax3.axhline(1700, color='red', linestyle='--')
        ax3.axvline(300, color='red', linestyle='--')
        ax3.set_xlabel('queue_gap')
        ax3.set_ylabel(target)
    ax3.set_title('queue_gap vs TOTALCNT')
    
    # 4. í™©ê¸ˆ íŒ¨í„´
    ax4 = axes[1, 1]
    if 'M14AM14B' in df.columns:
        sample = df.sample(min(3000, len(df)))
        colors = ['red' if t >= 1700 else 'blue' for t in sample[target]]
        ax4.scatter(sample['M14AM14B'], sample['M14AM14BSUM'], c=colors, alpha=0.3, s=10)
        ax4.axvline(520, color='orange', linestyle='--')
        ax4.axhline(600, color='orange', linestyle='--')
        ax4.set_xlabel('M14AM14B')
        ax4.set_ylabel('M14AM14BSUM')
    ax4.set_title('Golden Pattern (Red=1700+)')
    
    plt.tight_layout()
    plt.savefig(f'{output_prefix}_analysis.png', dpi=150)
    plt.close()
    print(f"âœ… ì €ì¥: {output_prefix}_analysis.png")


# ============================================================
# 12. ê²°ê³¼ ì €ì¥
# ============================================================
def save_results(df_types, importance, corr_high, output_prefix='results'):
    """ê²°ê³¼ ì €ì¥"""
    print("\n" + "="*60)
    print("ğŸ’¾ 12. ê²°ê³¼ ì €ì¥")
    print("="*60)
    
    if df_types is not None:
        df_types.to_csv(f'{output_prefix}_variable_types.csv', index=False, encoding='utf-8-sig')
        print(f"âœ… {output_prefix}_variable_types.csv")
    
    if importance is not None:
        importance.to_csv(f'{output_prefix}_feature_importance.csv', index=False, encoding='utf-8-sig')
        print(f"âœ… {output_prefix}_feature_importance.csv")
    
    if corr_high:
        pd.DataFrame(corr_high).to_csv(f'{output_prefix}_high_correlation.csv', index=False, encoding='utf-8-sig')
        print(f"âœ… {output_prefix}_high_correlation.csv")


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================
if __name__ == '__main__':
    # ========================================
    # ğŸ“Œ ì—¬ê¸° íŒŒì¼ ê²½ë¡œë§Œ ìˆ˜ì •í•˜ì„¸ìš”!
    # ========================================
    DATA_FILE = "data/M14_Q_3MONTH.csv"  # 3ê°œì›” ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    
    # 1. ë°ì´í„° ë¡œë“œ
    df = load_data(DATA_FILE)
    
    # 2. ë³€ìˆ˜ íƒ€ì… ë¶„ì„
    df_types = analyze_variable_types(df)
    
    # 3. íƒ€ê²Ÿ ë¶„í¬
    analyze_target(df, 'TOTALCNT')
    
    # 4. ìƒê´€ê´€ê³„
    corr_matrix, high_corr = correlation_analysis(df, 'TOTALCNT', threshold=0.95)
    
    # 5. ì •í•©ì„± í™•ì¸
    check_data_integrity(df)
    
    # 6. queue_gap ë¶„ì„
    analyze_queue_gap(df, 'TOTALCNT')
    
    # 7. í™©ê¸ˆ íŒ¨í„´
    analyze_golden_pattern(df, 'TOTALCNT')
    
    # 8. Feature Importance
    importance, model = feature_importance_analysis(df, 'TOTALCNT')
    
    # 9. SHAP (ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼ - í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
    # shap_imp = shap_analysis(df, 'TOTALCNT', sample_size=3000)
    
    # 10. Boruta (ì‹œê°„ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦¼ - í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
    # boruta_results = boruta_analysis(df, 'TOTALCNT', max_iter=30)
    
    # 11. ì‹œê°í™”
    create_visualizations(df, 'TOTALCNT', output_prefix='amhs')
    
    # 12. ê²°ê³¼ ì €ì¥
    save_results(df_types, importance, high_corr, output_prefix='amhs')
    
    print("\n" + "="*80)
    print("âœ… ë¶„ì„ ì™„ë£Œ!")
    print("="*80)#