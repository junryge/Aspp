#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ Baseline ëª¨ë¸ ë° Feature Selection ì „ëµ ìˆ˜ë¦½
ëª©ì : 3ê°œì›” ë°ì´í„° ê¸°ë°˜ ìµœì  Feature Set ë„ì¶œ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import (
    SelectKBest, f_regression, mutual_info_regression,
    RFE, VarianceThreshold
)
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.linear_model import LassoCV, Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import xgboost as xgb
import warnings
import glob
import os
from datetime import datetime

warnings.filterwarnings('ignore')
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['figure.figsize'] = (14, 8)

print("="*80)
print("ğŸš€ Baseline ëª¨ë¸ ë° Feature Selection ì „ëµ ìˆ˜ë¦½")
print("="*80)

# ========================================
# 1. ë°ì´í„° ë¡œë”©
# ========================================
def load_all_data(path_pattern='*.CSV'):
    """3ê°œì›”ì¹˜ CSV íŒŒì¼ ë¡œë“œ"""
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {path_pattern}")
    
    files = glob.glob(path_pattern)
    if not files:
        # ê¸°ë³¸ ê²½ë¡œ ì‹œë„
        files = glob.glob('data/*.CSV') + glob.glob('M14_Q_*.CSV')
    
    if not files:
        print("âš ï¸ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        return None
    
    dfs = []
    for f in sorted(files):
        try:
            df = pd.read_csv(f, on_bad_lines='skip', encoding='utf-8')
            print(f"  âœ… {os.path.basename(f)}: {len(df):,}í–‰, {df.shape[1]}ì»¬ëŸ¼")
            dfs.append(df)
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(f, on_bad_lines='skip', encoding='cp949')
                print(f"  âœ… {os.path.basename(f)} (cp949): {len(df):,}í–‰")
                dfs.append(df)
            except Exception as e:
                print(f"  âŒ {f}: {e}")
    
    if not dfs:
        return None
    
    df_all = pd.concat(dfs, ignore_index=True)
    print(f"\nğŸ“Š ì´ ë°ì´í„°: {len(df_all):,}í–‰, {df_all.shape[1]}ì»¬ëŸ¼")
    return df_all


# ========================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬
# ========================================
def preprocess_data(df):
    """ë°ì´í„° ì „ì²˜ë¦¬ ë° ê¸°ë³¸ Feature ìƒì„±"""
    print("\nğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬...")
    
    df = df.copy()
    
    # CURRTIME íŒŒì‹±
    if 'CURRTIME' in df.columns:
        df['CURRTIME'] = df['CURRTIME'].astype(str).str.strip()
        df = df[df['CURRTIME'].str.len() == 12].copy()
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'], format='%Y%m%d%H%M', errors='coerce')
        df = df.dropna(subset=['CURRTIME']).copy()
        df = df.sort_values('CURRTIME').reset_index(drop=True)
    
    # ìˆ«ìí˜• ë³€í™˜
    exclude_cols = ['CURRTIME']
    numeric_cols = [c for c in df.columns if c not in exclude_cols]
    
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (forward fill â†’ backward fill â†’ 0)
    df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)
    
    # í•µì‹¬ íŒŒìƒ ë³€ìˆ˜ ìƒì„±
    if 'M14.QUE.ALL.CURRENTQCREATED' in df.columns and 'M14.QUE.ALL.CURRENTQCOMPLETED' in df.columns:
        df['queue_gap'] = df['M14.QUE.ALL.CURRENTQCREATED'] - df['M14.QUE.ALL.CURRENTQCOMPLETED']
    
    if 'M14B.QUE.ALL.CURRENTQCREATED' in df.columns and 'M14B.QUE.ALL.CURRENTQCOMPLETED' in df.columns:
        df['queue_gap_B'] = df['M14B.QUE.ALL.CURRENTQCREATED'] - df['M14B.QUE.ALL.CURRENTQCOMPLETED']
    
    print(f"  âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df):,}í–‰")
    return df


# ========================================
# 3. EDA (íƒìƒ‰ì  ë°ì´í„° ë¶„ì„)
# ========================================
def perform_eda(df, target_col='TOTALCNT'):
    """íƒìƒ‰ì  ë°ì´í„° ë¶„ì„"""
    print("\nğŸ“Š EDA (íƒìƒ‰ì  ë°ì´í„° ë¶„ì„)")
    print("-"*60)
    
    # íƒ€ê²Ÿ ë¶„í¬
    print(f"\nğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜ ({target_col}) ë¶„í¬:")
    print(f"  í‰ê· : {df[target_col].mean():.2f}")
    print(f"  ì¤‘ì•™ê°’: {df[target_col].median():.2f}")
    print(f"  í‘œì¤€í¸ì°¨: {df[target_col].std():.2f}")
    print(f"  ìµœì†Œ: {df[target_col].min():.2f}")
    print(f"  ìµœëŒ€: {df[target_col].max():.2f}")
    
    # ìœ„í—˜ êµ¬ê°„ ë¶„í¬
    print(f"\nâš ï¸ ìœ„í—˜ êµ¬ê°„ ë¶„í¬:")
    total = len(df)
    print(f"  < 900 (LOW): {(df[target_col] < 900).sum():,}ê°œ ({(df[target_col] < 900).sum()/total*100:.2f}%)")
    print(f"  900-1599 (NORMAL): {((df[target_col] >= 900) & (df[target_col] < 1600)).sum():,}ê°œ")
    print(f"  1600-1699 (CAUTION): {((df[target_col] >= 1600) & (df[target_col] < 1700)).sum():,}ê°œ")
    print(f"  >= 1700 (CRITICAL): {(df[target_col] >= 1700).sum():,}ê°œ ({(df[target_col] >= 1700).sum()/total*100:.2f}%)")
    
    # ê²°ì¸¡ì¹˜ ë¶„ì„
    missing = df.isnull().sum()
    if missing.sum() > 0:
        print(f"\nâš ï¸ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼: {(missing > 0).sum()}ê°œ")
    else:
        print(f"\nâœ… ê²°ì¸¡ì¹˜ ì—†ìŒ")
    
    return {
        'target_mean': df[target_col].mean(),
        'target_std': df[target_col].std(),
        'critical_count': (df[target_col] >= 1700).sum(),
        'critical_ratio': (df[target_col] >= 1700).sum() / total * 100
    }


# ========================================
# 4. ìƒê´€ê´€ê³„ ë¶„ì„
# ========================================
def analyze_correlations(df, target_col='TOTALCNT', top_n=None):
    """íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„ ë¶„ì„"""
    print(f"\nğŸ”— ìƒê´€ê´€ê³„ ë¶„ì„ (íƒ€ê²Ÿ: {target_col})")
    print("-"*60)
    
    # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ
    numeric_df = df.select_dtypes(include=[np.number])
    
    # íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê³„ìˆ˜
    correlations = numeric_df.corr()[target_col].drop(target_col).abs().sort_values(ascending=False)
    
    print(f"\nğŸ“ˆ ìƒê´€ Feature (ì „ì²´ {len(correlations)}ê°œ):")
    display_n = top_n if top_n else len(correlations)
    for i, (col, corr) in enumerate(correlations.head(display_n).items(), 1):
        print(f"  {i:3d}. {col:<55} : {corr:.4f}")
    
    # 0.998 ì´ìƒ (ë°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬)
    leakage_candidates = correlations[correlations > 0.95]
    if len(leakage_candidates) > 0:
        print(f"\nâš ï¸ ë°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬ (ìƒê´€ê³„ìˆ˜ > 0.95):")
        for col, corr in leakage_candidates.items():
            print(f"  - {col}: {corr:.4f}")
    
    return correlations


# ========================================
# 5. Feature Selection ë°©ë²•ë“¤
# ========================================
class FeatureSelector:
    """ë‹¤ì–‘í•œ Feature Selection ë°©ë²• í†µí•©"""
    
    def __init__(self, df, target_col='TOTALCNT', exclude_cols=None):
        self.df = df
        self.target_col = target_col
        self.exclude_cols = exclude_cols or ['CURRTIME', target_col]
        
        # Feature/Target ë¶„ë¦¬
        self.feature_cols = [c for c in df.select_dtypes(include=[np.number]).columns 
                           if c not in self.exclude_cols]
        self.X = df[self.feature_cols].values
        self.y = df[target_col].values
        
        # ê²°ê³¼ ì €ì¥
        self.results = {}
    
    def variance_threshold(self, threshold=0.01):
        """ë¶„ì‚° ê¸°ë°˜ í•„í„°ë§"""
        print("\nğŸ“ 1. Variance Threshold")
        
        # ì •ê·œí™” í›„ ë¶„ì‚° ê³„ì‚°
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(self.X)
        
        selector = VarianceThreshold(threshold=threshold)
        selector.fit(X_scaled)
        
        selected_mask = selector.get_support()
        selected_features = [self.feature_cols[i] for i, sel in enumerate(selected_mask) if sel]
        removed_features = [self.feature_cols[i] for i, sel in enumerate(selected_mask) if not sel]
        
        print(f"  ì„ íƒ: {len(selected_features)}ê°œ, ì œê±°: {len(removed_features)}ê°œ")
        if removed_features:
            print(f"  ì œê±°ëœ Feature: {removed_features[:10]}...")
        
        self.results['variance'] = selected_features
        return selected_features
    
    def correlation_filter(self, corr_threshold=0.3, multicollinearity_threshold=0.95):
        """ìƒê´€ê´€ê³„ ê¸°ë°˜ í•„í„°ë§"""
        print("\nğŸ“Š 2. Correlation Filter")
        
        df_numeric = self.df[self.feature_cols + [self.target_col]]
        
        # íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„
        target_corr = df_numeric.corr()[self.target_col].drop(self.target_col).abs()
        
        # íƒ€ê²Ÿê³¼ ìƒê´€ê´€ê³„ê°€ ë†’ì€ Feature ì„ íƒ
        selected = target_corr[target_corr >= corr_threshold].index.tolist()
        print(f"  íƒ€ê²Ÿ ìƒê´€ >= {corr_threshold}: {len(selected)}ê°œ")
        
        # ë‹¤ì¤‘ê³µì„ ì„± ì œê±°
        if len(selected) > 1:
            corr_matrix = df_numeric[selected].corr().abs()
            upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
            
            to_drop = set()
            for col in upper.columns:
                highly_correlated = upper.index[upper[col] > multicollinearity_threshold].tolist()
                for hc in highly_correlated:
                    # íƒ€ê²Ÿ ìƒê´€ì´ ë‚®ì€ ìª½ ì œê±°
                    if target_corr[col] >= target_corr[hc]:
                        to_drop.add(hc)
                    else:
                        to_drop.add(col)
            
            selected = [f for f in selected if f not in to_drop]
            print(f"  ë‹¤ì¤‘ê³µì„ ì„± ì œê±° í›„: {len(selected)}ê°œ (ì œê±°: {len(to_drop)}ê°œ)")
        
        self.results['correlation'] = selected
        return selected
    
    def mutual_info_selection(self, k=None):
        """Mutual Information ê¸°ë°˜ ì„ íƒ"""
        print("\nğŸ”® 3. Mutual Information")
        
        k = k if k else len(self.feature_cols)
        selector = SelectKBest(score_func=mutual_info_regression, k=k)
        selector.fit(self.X, self.y)
        
        scores = pd.Series(selector.scores_, index=self.feature_cols).sort_values(ascending=False)
        selected = scores.index.tolist()
        
        print(f"  ì „ì²´ {len(selected)}ê°œ ë¶„ì„ ì™„ë£Œ")
        print(f"  ì „ì²´ Feature MI ì ìˆ˜:")
        for i, (col, score) in enumerate(scores.items(), 1):
            print(f"    {i:3d}. {col:<55} : {score:.4f}")
        
        self.results['mutual_info'] = selected
        return selected
    
    def tree_importance(self, n_estimators=100, top_n=None):
        """Tree ê¸°ë°˜ Feature Importance"""
        print("\nğŸŒ² 4. Tree-based Feature Importance")
        
        # RandomForest
        rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=10, 
                                   n_jobs=-1, random_state=42)
        rf.fit(self.X, self.y)
        
        rf_importance = pd.Series(rf.feature_importances_, index=self.feature_cols).sort_values(ascending=False)
        
        # ExtraTrees
        et = ExtraTreesRegressor(n_estimators=n_estimators, max_depth=10,
                                 n_jobs=-1, random_state=42)
        et.fit(self.X, self.y)
        
        et_importance = pd.Series(et.feature_importances_, index=self.feature_cols).sort_values(ascending=False)
        
        # í‰ê·  ì¤‘ìš”ë„
        avg_importance = (rf_importance + et_importance) / 2
        avg_importance = avg_importance.sort_values(ascending=False)
        
        top_n = top_n if top_n else len(avg_importance)
        selected = avg_importance.head(top_n).index.tolist()
        
        print(f"  ì „ì²´ {len(avg_importance)}ê°œ ë¶„ì„ ì™„ë£Œ")
        print(f"  ì „ì²´ Feature ì¤‘ìš”ë„:")
        for i, (col, imp) in enumerate(avg_importance.items(), 1):
            print(f"    {i:3d}. {col:<55} : {imp:.4f}")
        
        self.results['tree_importance'] = selected
        self.results['tree_scores'] = avg_importance
        return selected
    
    def lasso_selection(self, cv=5):
        """LASSO ê¸°ë°˜ Feature Selection"""
        print("\nğŸ“‰ 5. LASSO Selection")
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(self.X)
        
        lasso = LassoCV(cv=cv, random_state=42, n_jobs=-1)
        lasso.fit(X_scaled, self.y)
        
        coef = pd.Series(np.abs(lasso.coef_), index=self.feature_cols).sort_values(ascending=False)
        selected = coef[coef > 0].index.tolist()
        
        print(f"  ì„ íƒ: {len(selected)}ê°œ (ë¹„ì˜ ê³„ìˆ˜)")
        print(f"  ìµœì  alpha: {lasso.alpha_:.4f}")
        print(f"  ì „ì²´ Feature LASSO ê³„ìˆ˜:")
        for i, (col, c) in enumerate(coef.items(), 1):
            marker = "âœ“" if c > 0 else " "
            print(f"    {marker} {i:3d}. {col:<55} : {c:.4f}")
        
        self.results['lasso'] = selected
        return selected
    
    def rfe_selection(self, n_features=None, step=5):
        """Recursive Feature Elimination"""
        print("\nğŸ”„ 6. RFE (Recursive Feature Elimination)")
        
        n_features = n_features if n_features else len(self.feature_cols) // 2
        
        estimator = RandomForestRegressor(n_estimators=50, max_depth=8, 
                                         n_jobs=-1, random_state=42)
        
        rfe = RFE(estimator, n_features_to_select=n_features, step=step)
        rfe.fit(self.X, self.y)
        
        selected = [self.feature_cols[i] for i, sel in enumerate(rfe.support_) if sel]
        ranking = pd.Series(rfe.ranking_, index=self.feature_cols).sort_values()
        
        print(f"  ì„ íƒ: {len(selected)}ê°œ")
        print(f"  ì „ì²´ Feature RFE ìˆœìœ„:")
        for i, (col, rank) in enumerate(ranking.items(), 1):
            marker = "âœ“" if rank == 1 else " "
            print(f"    {marker} {i:3d}. {col:<55} : rank {rank}")
        
        self.results['rfe'] = selected
        return selected
    
    def ensemble_selection(self, min_methods=3):
        """ì—¬ëŸ¬ ë°©ë²•ì˜ êµì§‘í•©/í•©ì§‘í•© ë¶„ì„"""
        print("\nğŸ¯ 7. Ensemble Selection (ë‹¤ì¤‘ ë°©ë²• í†µí•©)")
        print("-"*60)
        
        if len(self.results) < 2:
            print("  âš ï¸ ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°©ë²•ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”")
            return None
        
        # ê° Featureê°€ ëª‡ ê°œì˜ ë°©ë²•ì—ì„œ ì„ íƒëëŠ”ì§€
        all_features = set()
        for method, features in self.results.items():
            if isinstance(features, list):
                all_features.update(features)
        
        feature_counts = {}
        for feat in all_features:
            count = sum(1 for method, features in self.results.items() 
                       if isinstance(features, list) and feat in features)
            feature_counts[feat] = count
        
        feature_counts = pd.Series(feature_counts).sort_values(ascending=False)
        
        # min_methodsê°œ ì´ìƒì—ì„œ ì„ íƒëœ Feature
        consensus = feature_counts[feature_counts >= min_methods].index.tolist()
        
        print(f"\nğŸ“Œ {min_methods}ê°œ ì´ìƒ ë°©ë²•ì—ì„œ ì„ íƒëœ Feature: {len(consensus)}ê°œ")
        print("\n  ì „ì²´ Featureë³„ ì„ íƒ íšŸìˆ˜:")
        for i, (feat, cnt) in enumerate(feature_counts.items(), 1):
            methods = [m for m, f in self.results.items() if isinstance(f, list) and feat in f]
            marker = "âœ“" if cnt >= min_methods else " "
            print(f"    {marker} {i:3d}. {feat:<50} : {cnt}íšŒ ({', '.join(methods)})")
        
        self.results['ensemble'] = consensus
        return consensus
    
    def get_final_features(self, strategy='ensemble', **kwargs):
        """ìµœì¢… Feature Set ë°˜í™˜"""
        if strategy in self.results:
            return self.results[strategy]
        else:
            return self.ensemble_selection(**kwargs)


# ========================================
# 6. Baseline ëª¨ë¸ í‰ê°€
# ========================================
def evaluate_baseline_models(df, feature_cols, target_col='TOTALCNT', pred_offset=10):
    """Baseline ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    print("\nğŸ§ª Baseline ëª¨ë¸ í‰ê°€")
    print("-"*60)
    
    # ì‹œê³„ì—´ ë¶„í•  (ë§ˆì§€ë§‰ 20%ë¥¼ í…ŒìŠ¤íŠ¸)
    train_size = int(len(df) * 0.8)
    
    df_train = df.iloc[:train_size].copy()
    df_test = df.iloc[train_size:].copy()
    
    # íƒ€ê²Ÿ ìƒì„± (pred_offsetë¶„ í›„)
    df_train['target'] = df_train[target_col].shift(-pred_offset)
    df_test['target'] = df_test[target_col].shift(-pred_offset)
    
    df_train = df_train.dropna(subset=['target'])
    df_test = df_test.dropna(subset=['target'])
    
    X_train = df_train[feature_cols].values
    y_train = df_train['target'].values
    X_test = df_test[feature_cols].values
    y_test = df_test['target'].values
    
    print(f"  Train: {len(X_train):,}ê°œ, Test: {len(X_test):,}ê°œ")
    
    results = {}
    
    # 1. Naive Baseline (í˜„ì¬ê°’ = ì˜ˆì¸¡ê°’)
    naive_pred = df_test[target_col].values[:len(y_test)]
    results['Naive (í˜„ì¬ê°’)'] = {
        'MAE': mean_absolute_error(y_test, naive_pred),
        'RMSE': np.sqrt(mean_squared_error(y_test, naive_pred)),
        'R2': r2_score(y_test, naive_pred)
    }
    
    # 2. Ridge Regression
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    ridge = Ridge(alpha=1.0)
    ridge.fit(X_train_scaled, y_train)
    ridge_pred = ridge.predict(X_test_scaled)
    results['Ridge'] = {
        'MAE': mean_absolute_error(y_test, ridge_pred),
        'RMSE': np.sqrt(mean_squared_error(y_test, ridge_pred)),
        'R2': r2_score(y_test, ridge_pred)
    }
    
    # 3. RandomForest
    rf = RandomForestRegressor(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42)
    rf.fit(X_train, y_train)
    rf_pred = rf.predict(X_test)
    results['RandomForest'] = {
        'MAE': mean_absolute_error(y_test, rf_pred),
        'RMSE': np.sqrt(mean_squared_error(y_test, rf_pred)),
        'R2': r2_score(y_test, rf_pred)
    }
    
    # 4. XGBoost
    xgb_model = xgb.XGBRegressor(
        n_estimators=200, max_depth=8, learning_rate=0.05,
        subsample=0.8, colsample_bytree=0.8, n_jobs=-1, random_state=42
    )
    xgb_model.fit(X_train, y_train)
    xgb_pred = xgb_model.predict(X_test)
    results['XGBoost'] = {
        'MAE': mean_absolute_error(y_test, xgb_pred),
        'RMSE': np.sqrt(mean_squared_error(y_test, xgb_pred)),
        'R2': r2_score(y_test, xgb_pred)
    }
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥:")
    print(f"  {'ëª¨ë¸':<20} {'MAE':>10} {'RMSE':>10} {'R2':>10}")
    print("  " + "-"*52)
    for model, metrics in results.items():
        print(f"  {model:<20} {metrics['MAE']:>10.2f} {metrics['RMSE']:>10.2f} {metrics['R2']:>10.4f}")
    
    # 1700+ ê°ì§€ìœ¨ ë¶„ì„
    print("\nğŸ”¥ 1700+ ê°ì§€ìœ¨ (XGBoost ê¸°ì¤€):")
    actual_danger = y_test >= 1700
    pred_danger = xgb_pred >= 1700
    
    if actual_danger.sum() > 0:
        detection_rate = (actual_danger & pred_danger).sum() / actual_danger.sum() * 100
        print(f"  ì‹¤ì œ 1700+ : {actual_danger.sum()}ê°œ")
        print(f"  ê°ì§€ : {(actual_danger & pred_danger).sum()}ê°œ ({detection_rate:.1f}%)")
        
        # ì˜¤íƒ
        false_positive = (pred_danger & ~actual_danger).sum()
        print(f"  ì˜¤íƒ(FP) : {false_positive}ê°œ")
    else:
        print("  í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— 1700+ ì¼€ì´ìŠ¤ ì—†ìŒ")
    
    return results, xgb_model


# ========================================
# 7. ìµœì¢… ì¶”ì²œ Feature Set ìƒì„±
# ========================================
def generate_recommended_features(selector, correlations, top_n=None):
    """ìµœì¢… ì¶”ì²œ Feature Set ìƒì„±"""
    print("\n" + "="*80)
    print("ğŸ† ìµœì¢… ì¶”ì²œ Feature Set")
    print("="*80)
    
    # 1. í•µì‹¬ ì»¬ëŸ¼ (ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜)
    core_features = [
        'TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M10AM14A',
        'M14.QUE.ALL.TRANSPORT4MINOVERCNT', 'M14.QUE.OHT.OHTUTIL',
        'M14.QUE.ALL.CURRENTQCREATED', 'M14.QUE.ALL.CURRENTQCOMPLETED',
        'queue_gap'  # íŒŒìƒ ë³€ìˆ˜
    ]
    
    # 2. ìƒê´€ê´€ê³„ ê¸°ë°˜ ì „ì²´
    corr_all = correlations.index.tolist()
    
    # 3. Ensemble Selection
    ensemble = selector.results.get('ensemble', [])
    
    # 4. Tree Importance
    tree_all = selector.results.get('tree_importance', [])
    
    # í†µí•©
    all_recommended = set(core_features)
    all_recommended.update(corr_all)
    all_recommended.update(ensemble)
    all_recommended.update(tree_all)
    
    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ
    available_cols = set(selector.feature_cols)
    final_features = [f for f in all_recommended if f in available_cols or f in ['queue_gap', 'queue_gap_B']]
    
    # ìƒê´€ê´€ê³„ ìˆœ ì •ë ¬
    final_features = sorted(final_features, 
                           key=lambda x: correlations.get(x, 0) if x in correlations else 0,
                           reverse=True)
    
    if top_n:
        final_features = final_features[:top_n]
    
    print(f"\nğŸ“Œ ì „ì²´ ì¶”ì²œ Feature ({len(final_features)}ê°œ):")
    print("-"*80)
    
    for i, feat in enumerate(final_features, 1):
        corr = correlations.get(feat, 0)
        source = []
        if feat in core_features:
            source.append('Core')
        if feat in corr_all:
            source.append('Corr')
        if feat in ensemble:
            source.append('Ensemble')
        if feat in tree_all:
            source.append('Tree')
        
        print(f"  {i:3d}. {feat:<55} (ìƒê´€:{corr:.4f}, {'/'.join(source)})")
    
    return final_features


# ========================================
# 8. ê²°ê³¼ ì €ì¥
# ========================================
def save_results(selector, correlations, recommended_features, output_dir='feature_selection_results'):
    """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. ìƒê´€ê´€ê³„ ì €ì¥
    correlations.to_csv(f'{output_dir}/correlations.csv', encoding='utf-8-sig')
    
    # 2. ì¶”ì²œ Feature ì €ì¥
    pd.DataFrame({'feature': recommended_features}).to_csv(
        f'{output_dir}/recommended_features.csv', index=False, encoding='utf-8-sig'
    )
    
    # 3. ëª¨ë“  ë°©ë²• ê²°ê³¼ ì €ì¥
    all_results = []
    for method, features in selector.results.items():
        if isinstance(features, list):
            for feat in features:
                all_results.append({'method': method, 'feature': feat})
    
    pd.DataFrame(all_results).to_csv(
        f'{output_dir}/all_method_results.csv', index=False, encoding='utf-8-sig'
    )
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_dir}/")


# ========================================
# Main ì‹¤í–‰
# ========================================
if __name__ == '__main__':
    # 1. ë°ì´í„° ë¡œë”©
    df = load_all_data('*.CSV')
    
    if df is None:
        print("\nâš ï¸ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ì‚¬ìš©ë²•: ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”ì— CSV íŒŒì¼ì„ ë‘ê±°ë‚˜,")
        print("          load_all_data('ê²½ë¡œ/*.CSV')ë¡œ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.")
        exit(1)
    
    # 2. ì „ì²˜ë¦¬
    df = preprocess_data(df)
    
    # 3. EDA
    eda_results = perform_eda(df)
    
    # 4. ìƒê´€ê´€ê³„ ë¶„ì„
    correlations = analyze_correlations(df)
    
    # 5. Feature Selection
    print("\n" + "="*80)
    print("ğŸ” Feature Selection ì‹œì‘")
    print("="*80)
    
    selector = FeatureSelector(df, target_col='TOTALCNT')
    
    # ê° ë°©ë²• ì‹¤í–‰ (ì „ì²´ ë¶„ì„!)
    selector.variance_threshold()
    selector.correlation_filter()
    selector.mutual_info_selection()  # ì „ì²´
    selector.tree_importance()  # ì „ì²´
    selector.lasso_selection()
    
    # RFEëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë ¤ì„œ ì„ íƒì 
    print("\nâ³ RFE ì‹¤í–‰ ì¤‘ (ì‹œê°„ ì†Œìš”)...")
    try:
        selector.rfe_selection()  # ì „ì²´
    except Exception as e:
        print(f"  RFE ìŠ¤í‚µ: {e}")
    
    # Ensemble
    selector.ensemble_selection(min_methods=3)
    
    # 6. ì¶”ì²œ Feature Set (ì „ì²´!)
    recommended = generate_recommended_features(selector, correlations)
    
    # 7. Baseline ëª¨ë¸ í‰ê°€
    print("\n" + "="*80)
    print("ğŸ§ª ì„ íƒëœ Featureë¡œ Baseline ëª¨ë¸ í‰ê°€")
    print("="*80)
    
    # queue_gap ì œì™¸ (ì‹¤ì œ ì»¬ëŸ¼ë§Œ)
    eval_features = [f for f in recommended if f in df.columns]
    
    if len(eval_features) >= 5:
        model_results, best_model = evaluate_baseline_models(df, eval_features)
    else:
        print("  âš ï¸ í‰ê°€í•  Featureê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
    
    # 8. ê²°ê³¼ ì €ì¥
    save_results(selector, correlations, recommended)
    
    # 9. ìµœì¢… ìš”ì•½
    print("\n" + "="*80)
    print("ğŸ“‹ ìµœì¢… ìš”ì•½")
    print("="*80)
    print(f"""
    âœ… ë¶„ì„ ì™„ë£Œ!
    
    ğŸ“Š ë°ì´í„°:
       - ì´ {len(df):,}í–‰, {df.shape[1]}ì»¬ëŸ¼
       - 1700+ ì¼€ì´ìŠ¤: {(df['TOTALCNT'] >= 1700).sum()}ê°œ ({eda_results['critical_ratio']:.2f}%)
    
    ğŸ¯ ì¶”ì²œ Feature: {len(recommended)}ê°œ
       - í•µì‹¬: M14AM14B, M14AM14BSUM, queue_gap, TRANSPORT
    
    ğŸ“ ì €ì¥ëœ íŒŒì¼:
       - feature_selection_results/correlations.csv
       - feature_selection_results/recommended_features.csv
       - feature_selection_results/all_method_results.csv
    
    ğŸš€ ë‹¤ìŒ ë‹¨ê³„:
       1. ì¶”ì²œ Featureë¡œ V9.0 ëª¨ë¸ í•™ìŠµ
       2. 10ë¶„/15ë¶„/25ë¶„ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•
       3. ì‹¤ì‹œê°„ í‰ê°€ ì‹œìŠ¤í…œ ì ìš©
    """)