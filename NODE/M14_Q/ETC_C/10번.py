#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ğŸš€ 10ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ
================================================================================
9ë‹¨ê³„ì—ì„œ ì„ íƒëœ 31ê°œ Featureë¡œ XGBoost ëª¨ë¸ í•™ìŠµ
- í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬
- 1700+ ê°€ì¤‘ì¹˜ í•™ìŠµ
- ëª¨ë¸ ì €ì¥
================================================================================
"""

import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import warnings
import gc

warnings.filterwarnings('ignore')

print("="*80)
print("ğŸš€ 10ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ")
print("   31ê°œ ì„ íƒ Featureë¡œ XGBoost í•™ìŠµ")
print("="*80)

# ==============================================================================
# 1. ì„ íƒëœ Feature ëª©ë¡ (9ë‹¨ê³„ ê²°ê³¼)
# ==============================================================================

SELECTED_FEATURES = [
    'total_last10',
    'M14AM14BSUM_current',
    'total_mean',
    'total_min',
    'SENDFAB_VERTICALQUEUECOUNT_over_280',
    'M10AM14A_over_55',
    'M10AM14A_over_60',
    'M10AM14A_over_50',
    'total_over_1600',
    'OHT_OHTUTIL_over_80',
    'vertical_gold',
    'inflow_surge',
    'M10AM14A_min',
    'gold_normal',
    'gold_strict',
    'triple_check',
    'total_slope',
    'OHT_CURRENTOHTQCNT_over_800',
    'm16_surge',
    'queue_gap_min',
    'cnv_total_high',
    'quad_check',
    'oht_overload',
    'queue_accel_danger',
    'M14AM14BSUM_trend20',
    'SENDFAB_VERTICALQUEUECOUNT_trend10',
    'M14AM14BSUM_trend10',
    'cnv_imbalance',
    'SENDFAB_VERTICALQUEUECOUNT_trend5',
    'OHT_OHTUTIL_trend20',
    'OHT_OHTUTIL_trend10'
]

print(f"\nğŸ“‹ ì„ íƒëœ Feature: {len(SELECTED_FEATURES)}ê°œ")

# ==============================================================================
# 2. ë°ì´í„° ë¡œë“œ
# ==============================================================================

print("\nğŸ“‚ ë°ì´í„° ë¡œë”©...")
INPUT_FILE = 'step8_features_10min.csv'

# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
use_cols = SELECTED_FEATURES + ['target_TOTALCNT', 'is_danger', 'current_TOTALCNT']
df = pd.read_csv(INPUT_FILE, usecols=use_cols)

print(f"  âœ… ë¡œë“œ ì™„ë£Œ: {len(df):,}í–‰ Ã— {len(df.columns)}ì»¬ëŸ¼")

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
df = df.fillna(0)
df = df.replace([np.inf, -np.inf], 0)

# X, y ë¶„ë¦¬
X = df[SELECTED_FEATURES]
y = df['target_TOTALCNT']

print(f"  âœ… X shape: {X.shape}")
print(f"  âœ… y ë²”ìœ„: {y.min():.0f} ~ {y.max():.0f}")

# ==============================================================================
# 3. í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬ (ì‹œê°„ìˆœ)
# ==============================================================================

print("\nğŸ“Š í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬...")

# ì‹œê°„ìˆœ ë¶„ë¦¬ (ë§ˆì§€ë§‰ 20%ë¥¼ ê²€ì¦ìš©)
split_idx = int(len(X) * 0.8)
X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]
y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]

print(f"  í•™ìŠµ: {len(X_train):,}ê°œ")
print(f"  ê²€ì¦: {len(X_val):,}ê°œ")

# 1700+ ìƒ˜í”Œ ìˆ˜
train_danger = (y_train >= 1700).sum()
val_danger = (y_val >= 1700).sum()
print(f"  í•™ìŠµ 1700+: {train_danger:,}ê°œ ({train_danger/len(y_train)*100:.2f}%)")
print(f"  ê²€ì¦ 1700+: {val_danger:,}ê°œ ({val_danger/len(y_val)*100:.2f}%)")

# ==============================================================================
# 4. ê°€ì¤‘ì¹˜ ì„¤ì • (1700+ ê°•ì¡°)
# ==============================================================================

print("\nâš–ï¸ ê°€ì¤‘ì¹˜ ì„¤ì •...")

weights = np.ones(len(y_train))
weights[y_train >= 1500] = 3
weights[y_train >= 1600] = 10
weights[y_train >= 1700] = 30
weights[y_train >= 1800] = 50

print(f"  ì¼ë°˜: 1ë°°")
print(f"  1500+: 3ë°°")
print(f"  1600+: 10ë°°")
print(f"  1700+: 30ë°°")
print(f"  1800+: 50ë°°")

# ==============================================================================
# 5. XGBoost í•™ìŠµ
# ==============================================================================

print("\nğŸš€ XGBoost í•™ìŠµ ì‹œì‘...")

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
try:
    test_model = xgb.XGBRegressor(device='cuda', n_estimators=10)
    test_model.fit(X_train[:100], y_train[:100])
    USE_GPU = True
    print("  âœ… GPU ì‚¬ìš© ê°€ëŠ¥!")
except:
    USE_GPU = False
    print("  âš ï¸ GPU ë¶ˆê°€, CPU ì‚¬ìš©")

# ëª¨ë¸ ì •ì˜
model = xgb.XGBRegressor(
    n_estimators=1000,
    max_depth=8,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=3,
    gamma=0.1,
    reg_alpha=0.1,
    reg_lambda=1.0,
    n_jobs=-1,
    device='cuda' if USE_GPU else 'cpu',
    tree_method='hist',
    random_state=42
)

# í•™ìŠµ
model.fit(
    X_train, y_train,
    sample_weight=weights,
    eval_set=[(X_val, y_val)],
    verbose=100
)

print("  âœ… í•™ìŠµ ì™„ë£Œ!")

# ==============================================================================
# 6. Feature Importance
# ==============================================================================

print("\nğŸ“Š Feature Importance...")

importance = dict(zip(SELECTED_FEATURES, model.feature_importances_))
importance_sorted = sorted(importance.items(), key=lambda x: x[1], reverse=True)

print("\n  TOP 15 Feature:")
for i, (feat, imp) in enumerate(importance_sorted[:15], 1):
    bar = 'â–ˆ' * int(imp * 100)
    print(f"  {i:2d}. {feat[:40]:40s} {imp:.4f} {bar}")

# ==============================================================================
# 7. ê²€ì¦ í‰ê°€
# ==============================================================================

print("\n" + "="*80)
print("ğŸ“ˆ ê²€ì¦ ê²°ê³¼")
print("="*80)

y_pred = model.predict(X_val)

# ê¸°ë³¸ ë©”íŠ¸ë¦­
mae = mean_absolute_error(y_val, y_pred)
rmse = np.sqrt(mean_squared_error(y_val, y_pred))
r2 = r2_score(y_val, y_pred)

print(f"\n  ğŸ“Š ê¸°ë³¸ ë©”íŠ¸ë¦­:")
print(f"     MAE: {mae:.2f}")
print(f"     RMSE: {rmse:.2f}")
print(f"     RÂ²: {r2:.4f}")

# 1700+ ê°ì§€ìœ¨
actual_danger = y_val >= 1700
pred_danger = y_pred >= 1700

tp = ((actual_danger) & (pred_danger)).sum()
fn = ((actual_danger) & (~pred_danger)).sum()
fp = ((~actual_danger) & (pred_danger)).sum()
tn = ((~actual_danger) & (~pred_danger)).sum()

recall = tp / (tp + fn) if (tp + fn) > 0 else 0
precision = tp / (tp + fp) if (tp + fp) > 0 else 0
f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

print(f"\n  ğŸ”¥ 1700+ ê°ì§€ ì„±ëŠ¥:")
print(f"     ì‹¤ì œ 1700+: {actual_danger.sum():,}ê°œ")
print(f"     ì˜ˆì¸¡ 1700+: {pred_danger.sum():,}ê°œ")
print(f"     TP (ì •íƒ): {tp:,}ê°œ")
print(f"     FN (ë¯¸íƒ): {fn:,}ê°œ")
print(f"     FP (ì˜¤íƒ): {fp:,}ê°œ")
print(f"     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"     ê°ì§€ìœ¨ (Recall): {recall*100:.1f}%")
print(f"     ì •ë°€ë„ (Precision): {precision*100:.1f}%")
print(f"     F1 Score: {f1:.4f}")

# êµ¬ê°„ë³„ MAE
print(f"\n  ğŸ“Š êµ¬ê°„ë³„ MAE:")
for low, high in [(0, 1600), (1600, 1700), (1700, 1800), (1800, 9999)]:
    mask = (y_val >= low) & (y_val < high)
    if mask.sum() > 0:
        segment_mae = mean_absolute_error(y_val[mask], y_pred[mask])
        print(f"     {low:4d} ~ {high:4d}: MAE {segment_mae:.2f} ({mask.sum():,}ê°œ)")

# ==============================================================================
# 8. ëª¨ë¸ ì €ì¥
# ==============================================================================

print("\nğŸ’¾ ëª¨ë¸ ì €ì¥...")

# ëª¨ë¸ ì €ì¥
with open('model_v10_31feat.pkl', 'wb') as f:
    pickle.dump(model, f)
print("  âœ… model_v10_31feat.pkl")

# Feature ëª©ë¡ ì €ì¥
with open('model_v10_features.pkl', 'wb') as f:
    pickle.dump(SELECTED_FEATURES, f)
print("  âœ… model_v10_features.pkl")

# Feature Importance ì €ì¥
importance_df = pd.DataFrame(importance_sorted, columns=['feature', 'importance'])
importance_df.to_csv('model_v10_importance.csv', index=False, encoding='utf-8-sig')
print("  âœ… model_v10_importance.csv")

# ==============================================================================
# 9. ê²°ê³¼ ìš”ì•½
# ==============================================================================

print("\n" + "="*80)
print("ğŸ“‹ 10ë‹¨ê³„ ê²°ê³¼ ìš”ì•½")
print("="*80)
print(f"  Feature ìˆ˜: {len(SELECTED_FEATURES)}ê°œ")
print(f"  í•™ìŠµ ìƒ˜í”Œ: {len(X_train):,}ê°œ")
print(f"  ê²€ì¦ ìƒ˜í”Œ: {len(X_val):,}ê°œ")
print(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"  MAE: {mae:.2f}")
print(f"  RÂ²: {r2:.4f}")
print(f"  1700+ ê°ì§€ìœ¨: {recall*100:.1f}%")
print(f"  1700+ ì •ë°€ë„: {precision*100:.1f}%")
print(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"  ëª¨ë¸ íŒŒì¼: model_v10_31feat.pkl")

print("\nâœ… 10ë‹¨ê³„ ì™„ë£Œ!")
print("   â†’ 11ë‹¨ê³„: ë³„ë„ í‰ê°€ ë°ì´í„°ë¡œ ìµœì¢… ê²€ì¦")