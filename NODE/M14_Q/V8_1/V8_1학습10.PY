# -*- coding: utf-8 -*-
"""
280ë¶„ ì‹œí€€ìŠ¤ â†’ 10ë¶„ í›„ TOTALCNT ìµœëŒ€ê°’ ì˜ˆì¸¡
âœ… ì¡°ê¸° ê²½ë³´ Feature í¬í•¨ (85ê°œ)
âœ… 1ë…„ì¹˜ ëŒ€ìš©ëŸ‰ ë°ì´í„° ìµœì í™”
âœ… M14AM14B 450+ êµ¬ê°„ ë°ì´í„° ì¦ê°•
âœ… 1700+ êµ¬ê°„ ì§‘ì¤‘ í•™ìŠµ
"""

import numpy as np
import pandas as pd
import xgboost as xgb
import pickle
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import os
import gc

warnings.filterwarnings('ignore')
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def check_gpu_availability():
    """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    print("\n" + "="*80)
    print("GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸")
    print("="*80)
    
    try:
        test_X = np.random.rand(100, 10)
        test_y = np.random.rand(100)
        
        test_model = xgb.XGBRegressor(
            tree_method='gpu_hist',
            gpu_id=0,
            n_estimators=10
        )
        test_model.fit(test_X, test_y)
        
        print("âœ… GPU ì‚¬ìš© ê°€ëŠ¥! XGBoost GPU ëª¨ë“œë¡œ í•™ìŠµ")
        return 'gpu_hist', 0
        
    except Exception as e:
        print(f"âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€: {str(e)[:100]}")
        print("CPU ëª¨ë“œë¡œ í•™ìŠµ")
        return 'hist', None

def create_features_280min_enhanced(df, start_idx=280):
    """
    280ë¶„ ì‹œí€€ìŠ¤ + ê°•í™”ëœ Feature + ì¡°ê¸° ê²½ë³´ Feature
    
    ì´ 85ê°œ Feature:
    - ê¸°ë³¸ í†µê³„ (4ì»¬ëŸ¼ Ã— 8): 32ê°œ
    - ë¹„ìœ¨: 8ê°œ
    - M14AM14B ì„ê³„ê°’: 8ê°œ
    - M14AM10A ì„ê³„ê°’: 4ê°œ
    - TOTALCNT ì„ê³„ê°’: 8ê°œ
    - í™©ê¸ˆ íŒ¨í„´: 4ê°œ
    - ë³€í™”ìœ¨/ê°€ì†ë„: 8ê°œ
    - ì‹œê°„ëŒ€ë³„ í†µê³„: 8ê°œ
    - ğŸ”¥ ì¡°ê¸° ê²½ë³´: 5ê°œ
    """
    
    print(f"\nFeature ìƒì„± ì¤‘... (280ë¶„ ì‹œí€€ìŠ¤ + ì¡°ê¸° ê²½ë³´)")
    
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required_cols = ['M14AM14B', 'M14AM10A', 'M14AM16', 'TOTALCNT']
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        raise ValueError(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")
    
    print(f"âœ… í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸: {required_cols}")
    
    features_list = []
    labels = []
    
    seq_totalcnt_max_list = []
    seq_totalcnt_min_list = []
    m14b_last_list = []  # M14AM14B ë§ˆì§€ë§‰ ê°’ ì €ì¥
    indices = []
    
    total_sequences = len(df) - start_idx - 10
    print(f"ìƒì„± ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤: {total_sequences:,}ê°œ")
    
    # Feature ìƒì„±
    for i in range(start_idx, len(df) - 10):
        if i % 50000 == 0:
            progress = (i - start_idx) / total_sequences * 100
            print(f"  ì§„í–‰ë¥ : {i-start_idx:,}/{total_sequences:,} ({progress:.1f}%)", end='\r')
        
        # 280ë¶„ ì‹œí€€ìŠ¤ ì¶”ì¶œ
        seq_m14b = df['M14AM14B'].iloc[i-280:i].values
        seq_m10a = df['M14AM10A'].iloc[i-280:i].values
        seq_m16 = df['M14AM16'].iloc[i-280:i].values
        seq_totalcnt = df['TOTALCNT'].iloc[i-280:i].values
        
        features = {}
        
        # ========== M14AM14B ê¸°ë³¸ 8ê°œ ==========
        features['m14b_mean'] = np.mean(seq_m14b)
        features['m14b_std'] = np.std(seq_m14b)
        features['m14b_last_5_mean'] = np.mean(seq_m14b[-5:])
        features['m14b_max'] = np.max(seq_m14b)
        features['m14b_min'] = np.min(seq_m14b)
        features['m14b_slope'] = np.polyfit(np.arange(280), seq_m14b, 1)[0]
        features['m14b_last_10_mean'] = np.mean(seq_m14b[-10:])
        features['m14b_first_10_mean'] = np.mean(seq_m14b[:10])
        
        # ========== M14AM10A ê¸°ë³¸ 8ê°œ ==========
        features['m10a_mean'] = np.mean(seq_m10a)
        features['m10a_std'] = np.std(seq_m10a)
        features['m10a_last_5_mean'] = np.mean(seq_m10a[-5:])
        features['m10a_max'] = np.max(seq_m10a)
        features['m10a_min'] = np.min(seq_m10a)
        features['m10a_slope'] = np.polyfit(np.arange(280), seq_m10a, 1)[0]
        features['m10a_last_10_mean'] = np.mean(seq_m10a[-10:])
        features['m10a_first_10_mean'] = np.mean(seq_m10a[:10])
        
        # ========== M14AM16 ê¸°ë³¸ 8ê°œ ==========
        features['m16_mean'] = np.mean(seq_m16)
        features['m16_std'] = np.std(seq_m16)
        features['m16_last_5_mean'] = np.mean(seq_m16[-5:])
        features['m16_max'] = np.max(seq_m16)
        features['m16_min'] = np.min(seq_m16)
        features['m16_slope'] = np.polyfit(np.arange(280), seq_m16, 1)[0]
        features['m16_last_10_mean'] = np.mean(seq_m16[-10:])
        features['m16_first_10_mean'] = np.mean(seq_m16[:10])
        
        # ========== TOTALCNT ê¸°ë³¸ 8ê°œ ==========
        features['totalcnt_mean'] = np.mean(seq_totalcnt)
        features['totalcnt_std'] = np.std(seq_totalcnt)
        features['totalcnt_last_5_mean'] = np.mean(seq_totalcnt[-5:])
        features['totalcnt_max'] = np.max(seq_totalcnt)
        features['totalcnt_min'] = np.min(seq_totalcnt)
        features['totalcnt_slope'] = np.polyfit(np.arange(280), seq_totalcnt, 1)[0]
        features['totalcnt_last_10_mean'] = np.mean(seq_totalcnt[-10:])
        features['totalcnt_first_10_mean'] = np.mean(seq_totalcnt[:10])
        
        # ========== ë¹„ìœ¨ Feature (8ê°œ) ==========
        features['ratio_m14b_m10a'] = seq_m14b[-1] / (seq_m10a[-1] + 1)
        features['ratio_m14b_m16'] = seq_m14b[-1] / (seq_m16[-1] + 1)
        features['ratio_m10a_m16'] = seq_m10a[-1] / (seq_m16[-1] + 1)
        features['ratio_m14b_m10a_mean'] = np.mean(seq_m14b) / (np.mean(seq_m10a) + 1)
        features['ratio_m14b_m16_mean'] = np.mean(seq_m14b) / (np.mean(seq_m16) + 1)
        features['ratio_m14b_m10a_max'] = np.max(seq_m14b) / (np.max(seq_m10a) + 1)
        features['volatility_m14b'] = np.std(seq_m14b) / (np.mean(seq_m14b) + 1)
        features['volatility_totalcnt'] = np.std(seq_totalcnt) / (np.mean(seq_totalcnt) + 1)
        
        # ========== M14AM14B ì„ê³„ê°’ ì¹´ìš´íŠ¸ (8ê°œ) ==========
        features['m14b_over_250'] = np.sum(seq_m14b > 250)
        features['m14b_over_300'] = np.sum(seq_m14b > 300)
        features['m14b_over_350'] = np.sum(seq_m14b > 350)
        features['m14b_over_400'] = np.sum(seq_m14b > 400)
        features['m14b_over_450'] = np.sum(seq_m14b > 450)
        features['m14b_over_300_last30'] = np.sum(seq_m14b[-30:] > 300)
        features['m14b_over_350_last30'] = np.sum(seq_m14b[-30:] > 350)
        features['m14b_over_400_last30'] = np.sum(seq_m14b[-30:] > 400)
        
        # ========== M14AM10A ì„ê³„ê°’ ì¹´ìš´íŠ¸ (4ê°œ) ==========
        features['m10a_over_70'] = np.sum(seq_m10a > 70)
        features['m10a_over_80'] = np.sum(seq_m10a > 80)
        features['m10a_under_80'] = np.sum(seq_m10a < 80)
        features['m10a_under_70'] = np.sum(seq_m10a < 70)
        
        # ========== TOTALCNT ì„ê³„ê°’ ì¹´ìš´íŠ¸ (8ê°œ) ==========
        features['totalcnt_over_1400'] = np.sum(seq_totalcnt >= 1400)
        features['totalcnt_over_1500'] = np.sum(seq_totalcnt >= 1500)
        features['totalcnt_over_1600'] = np.sum(seq_totalcnt >= 1600)
        features['totalcnt_over_1700'] = np.sum(seq_totalcnt >= 1700)
        features['totalcnt_over_1400_last30'] = np.sum(seq_totalcnt[-30:] >= 1400)
        features['totalcnt_over_1500_last30'] = np.sum(seq_totalcnt[-30:] >= 1500)
        features['totalcnt_over_1600_last30'] = np.sum(seq_totalcnt[-30:] >= 1600)
        features['totalcnt_over_1700_last30'] = np.sum(seq_totalcnt[-30:] >= 1700)
        
        # ========== í™©ê¸ˆ íŒ¨í„´ (4ê°œ) ==========
        features['golden_pattern_300_80'] = 1 if (seq_m14b[-1] > 300 and seq_m10a[-1] < 80) else 0
        features['golden_pattern_350_80'] = 1 if (seq_m14b[-1] > 350 and seq_m10a[-1] < 80) else 0
        features['golden_pattern_400_70'] = 1 if (seq_m14b[-1] > 400 and seq_m10a[-1] < 70) else 0
        features['danger_zone'] = 1 if seq_totalcnt[-1] >= 1700 else 0
        
        # ========== ë³€í™”ìœ¨/ê°€ì†ë„ (8ê°œ) ==========
        features['m14b_change_rate'] = (seq_m14b[-1] - seq_m14b[-30]) / 30 if len(seq_m14b) >= 30 else 0
        features['totalcnt_change_rate'] = (seq_totalcnt[-1] - seq_totalcnt[-30]) / 30 if len(seq_totalcnt) >= 30 else 0
        
        recent_30_m14b = np.mean(seq_m14b[-30:])
        previous_30_m14b = np.mean(seq_m14b[-60:-30]) if len(seq_m14b) >= 60 else np.mean(seq_m14b[-30:])
        features['m14b_acceleration'] = recent_30_m14b - previous_30_m14b
        
        recent_30_totalcnt = np.mean(seq_totalcnt[-30:])
        previous_30_totalcnt = np.mean(seq_totalcnt[-60:-30]) if len(seq_totalcnt) >= 60 else np.mean(seq_totalcnt[-30:])
        features['totalcnt_acceleration'] = recent_30_totalcnt - previous_30_totalcnt
        
        features['m14b_range'] = np.max(seq_m14b) - np.min(seq_m14b)
        features['totalcnt_range'] = np.max(seq_totalcnt) - np.min(seq_totalcnt)
        features['m14b_recent_vs_mean'] = np.mean(seq_m14b[-30:]) / (np.mean(seq_m14b) + 1)
        features['totalcnt_recent_vs_mean'] = np.mean(seq_totalcnt[-30:]) / (np.mean(seq_totalcnt) + 1)
        
        # ========== ì‹œê°„ëŒ€ë³„ í†µê³„ (8ê°œ) ==========
        q1 = seq_totalcnt[:70]
        q2 = seq_totalcnt[70:140]
        q3 = seq_totalcnt[140:210]
        q4 = seq_totalcnt[210:280]
        
        features['totalcnt_q1_mean'] = np.mean(q1)
        features['totalcnt_q2_mean'] = np.mean(q2)
        features['totalcnt_q3_mean'] = np.mean(q3)
        features['totalcnt_q4_mean'] = np.mean(q4)
        features['totalcnt_trend_q1_q2'] = np.mean(q2) - np.mean(q1)
        features['totalcnt_trend_q2_q3'] = np.mean(q3) - np.mean(q2)
        features['totalcnt_trend_q3_q4'] = np.mean(q4) - np.mean(q3)
        features['totalcnt_trend_overall'] = np.mean(q4) - np.mean(q1)
        
        # ========== ğŸ”¥ ì¡°ê¸° ê²½ë³´ Feature (5ê°œ) ==========
        # ë§ˆì§€ë§‰ 10ë¶„ ë¶„ì„
        last_10min = seq_totalcnt[-10:]
        
        features['last_10min_max'] = np.max(last_10min)
        features['last_10min_min'] = np.min(last_10min)
        features['last_10min_mean'] = np.mean(last_10min)
        features['last_10min_rise'] = last_10min[-1] - last_10min[0]  # 10ë¶„ê°„ ìƒìŠ¹í­
        
        # ì¡°ê¸° ê²½ë³´: 1650 ì´ìƒ + 20 ì´ìƒ ìƒìŠ¹
        early_warning = (np.max(last_10min) >= 1650) and ((last_10min[-1] - last_10min[0]) > 20)
        features['early_warning_1650_rising'] = 1 if early_warning else 0
        
        features_list.append(features)
        
        # ë¼ë²¨: 10ë¶„ í›„ TOTALCNT ìµœëŒ€ê°’
        future_totalcnt = df['TOTALCNT'].iloc[i:i+10].values
        labels.append(np.max(future_totalcnt))
        
        # ì‹œí€€ìŠ¤ ì •ë³´ ì €ì¥
        seq_totalcnt_max_list.append(np.max(seq_totalcnt))
        seq_totalcnt_min_list.append(np.min(seq_totalcnt))
        m14b_last_list.append(seq_m14b[-1])
        indices.append(i)
    
    print(f"\nâœ… Feature ìƒì„± ì™„ë£Œ: {len(features_list):,}ê°œ ì‹œí€€ìŠ¤")
    
    X = pd.DataFrame(features_list)
    y = np.array(labels)
    
    seq_info = {
        'seq_max': seq_totalcnt_max_list,
        'seq_min': seq_totalcnt_min_list,
        'm14b_last': m14b_last_list,
        'indices': indices
    }
    
    print(f"Feature ê°œìˆ˜: {X.shape[1]}ê°œ")
    print(f"  - ê¸°ë³¸ í†µê³„ (4ì»¬ëŸ¼ Ã— 8): 32ê°œ")
    print(f"  - ë¹„ìœ¨: 8ê°œ")
    print(f"  - M14AM14B ì„ê³„ê°’: 8ê°œ")
    print(f"  - M14AM10A ì„ê³„ê°’: 4ê°œ")
    print(f"  - TOTALCNT ì„ê³„ê°’: 8ê°œ")
    print(f"  - í™©ê¸ˆ íŒ¨í„´: 4ê°œ")
    print(f"  - ë³€í™”ìœ¨/ê°€ì†ë„: 8ê°œ")
    print(f"  - ì‹œê°„ëŒ€ë³„ í†µê³„: 8ê°œ")
    print(f"  - ğŸ”¥ ì¡°ê¸° ê²½ë³´: 5ê°œ")
    print(f"ë°ì´í„° ë²”ìœ„: ì¸ë±ìŠ¤ {indices[0]:,} ~ {indices[-1]:,}")
    
    return X, y, seq_info

def augment_high_risk_data(X, y, seq_info, augment_factor=10):
    """
    M14AM14B 450+ êµ¬ê°„ê³¼ 1700+ êµ¬ê°„ ë°ì´í„° ì¦ê°•
    
    Parameters:
    - augment_factor: ì¦ê°• ë°°ìˆ˜ (ê¸°ë³¸ 10ë°°)
    """
    print(f"\nğŸ”¥ ê³ ìœ„í—˜ êµ¬ê°„ ë°ì´í„° ì¦ê°• (Ã—{augment_factor})")
    print("-"*40)
    
    m14b_last = np.array(seq_info['m14b_last'])
    
    # M14AM14B 450+ êµ¬ê°„
    high_m14b_mask = m14b_last >= 450
    high_m14b_count = np.sum(high_m14b_mask)
    
    # 1700+ êµ¬ê°„
    high_totalcnt_mask = y >= 1700
    high_totalcnt_count = np.sum(high_totalcnt_mask)
    
    # ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ í•´ë‹¹í•˜ëŠ” êµ¬ê°„
    high_risk_mask = high_m14b_mask | high_totalcnt_mask
    high_risk_count = np.sum(high_risk_mask)
    
    print(f"ì›ë³¸ ë°ì´í„°:")
    print(f"  - M14AM14B 450+: {high_m14b_count:,}ê°œ ({high_m14b_count/len(X)*100:.2f}%)")
    print(f"  - TOTALCNT 1700+: {high_totalcnt_count:,}ê°œ ({high_totalcnt_count/len(X)*100:.2f}%)")
    print(f"  - ê³ ìœ„í—˜ êµ¬ê°„(í•©ê³„): {high_risk_count:,}ê°œ ({high_risk_count/len(X)*100:.2f}%)")
    
    if high_risk_count == 0:
        print("âš ï¸ ê³ ìœ„í—˜ ë°ì´í„° ì—†ìŒ - ì¦ê°• ìŠ¤í‚µ")
        return X, y
    
    # ê³ ìœ„í—˜ ë°ì´í„° ì¶”ì¶œ
    X_high_risk = X[high_risk_mask]
    y_high_risk = y[high_risk_mask]
    
    # ì¦ê°• (augment_factor-1ë²ˆ ë³µì œ)
    X_augmented = X.copy()
    y_augmented = y.copy()
    
    for _ in range(augment_factor - 1):
        X_augmented = pd.concat([X_augmented, X_high_risk], ignore_index=True)
        y_augmented = np.concatenate([y_augmented, y_high_risk])
    
    print(f"\nì¦ê°• í›„ ë°ì´í„°:")
    print(f"  - ì›ë³¸: {len(X):,}ê°œ")
    print(f"  - ì¦ê°• í›„: {len(X_augmented):,}ê°œ")
    print(f"  - ì¦ê°€ëŸ‰: {len(X_augmented) - len(X):,}ê°œ")
    
    # ê³ ìœ„í—˜ ë°ì´í„° ë¹„ìœ¨
    final_high_risk_count = np.sum((X_augmented[X_augmented.columns[0]] >= 0))  # ì „ì²´
    m14b_450_augmented = high_risk_count * augment_factor
    final_ratio = m14b_450_augmented / len(X_augmented) * 100
    print(f"  - ê³ ìœ„í—˜ ë°ì´í„° ë¹„ìœ¨: {final_ratio:.2f}%")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del X_high_risk, y_high_risk
    gc.collect()
    
    return X_augmented, y_augmented

def train_and_evaluate_complete():
    """
    280ë¶„ ì‹œí€€ìŠ¤ â†’ 10ë¶„ í›„ TOTALCNT ì˜ˆì¸¡
    âœ… 1ë…„ì¹˜ ë°ì´í„° ìµœì í™”
    âœ… ì¡°ê¸° ê²½ë³´ Feature
    âœ… ê³ ìœ„í—˜ êµ¬ê°„ ë°ì´í„° ì¦ê°•
    """
    print("="*80)
    print("ğŸš€ XGBoost 280ë¶„ â†’ 10ë¶„ í›„ TOTALCNT ì˜ˆì¸¡ (1ë…„ì¹˜ ë°ì´í„°)")
    print("="*80)
    print("Feature: 85ê°œ (ì¡°ê¸° ê²½ë³´ í¬í•¨)")
    print("ë°ì´í„° ì¦ê°•: M14AM14B 450+ Ã— 10ë°°")
    print("="*80)
    
    # GPU í™•ì¸
    tree_method, gpu_id = check_gpu_availability()
    
    # ===== 1. ë°ì´í„° ë¡œë”© =====
    print("\n[STEP 1] ë°ì´í„° ë¡œë”©")
    print("-"*40)
    
    # CSV íŒŒì¼ ê²½ë¡œ (1ë…„ì¹˜ ë°ì´í„°)
    csv_file = '/mnt/project/V6_6ê²°ê³¼.CSV'
    
    # ë‹¤ë¥¸ ê²½ë¡œ ì‹œë„
    if not os.path.exists(csv_file):
        csv_file = 'V6_6ê²°ê³¼.CSV'
    if not os.path.exists(csv_file):
        csv_file = '/mnt/project/uu.csv'
    if not os.path.exists(csv_file):
        csv_file = 'uu.csv'
    
    print(f"ì‚¬ìš© íŒŒì¼: {csv_file}")
    
    if not os.path.exists(csv_file):
        raise FileNotFoundError(f"âŒ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_file}")
    
    # ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¡œë”©
    print("ë°ì´í„° ë¡œë”© ì¤‘...")
    df = pd.read_csv(csv_file, on_bad_lines='skip')
    print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(df):,}í–‰")
    print(f"ì»¬ëŸ¼: {list(df.columns)}")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
    print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_mb:.1f} MB")
    
    # ===== 2. Feature ìƒì„± =====
    print("\n[STEP 2] Feature ìƒì„± (280ë¶„ ì‹œí€€ìŠ¤ + ì¡°ê¸° ê²½ë³´)")
    print("-"*40)
    
    X, y, seq_info = create_features_280min_enhanced(df)
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del df
    gc.collect()
    
    print(f"\nTOTALCNT ë¶„ì„ (10ë¶„ í›„ ìµœëŒ€ê°’):")
    print(f"  í‰ê· : {y.mean():.2f}")
    print(f"  í‘œì¤€í¸ì°¨: {y.std():.2f}")
    print(f"  ìµœì†Œ: {y.min():.2f}")
    print(f"  ìµœëŒ€: {y.max():.2f}")
    print(f"  ìœ„í—˜(1700+): {np.sum(y >= 1700):,}ê°œ ({np.sum(y >= 1700)/len(y)*100:.2f}%)")
    print(f"  ê·¹ë‹¨(2000+): {np.sum(y >= 2000):,}ê°œ ({np.sum(y >= 2000)/len(y)*100:.2f}%)")
    
    # ì¡°ê¸° ê²½ë³´ í†µê³„
    early_warning_count = X['early_warning_1650_rising'].sum()
    print(f"\nì¡°ê¸° ê²½ë³´ ë°œìƒ: {early_warning_count:,}ê°œ ({early_warning_count/len(X)*100:.2f}%)")
    
    # M14AM14B 450+ í†µê³„
    m14b_last = np.array(seq_info['m14b_last'])
    m14b_450_count = np.sum(m14b_last >= 450)
    print(f"M14AM14B 450+: {m14b_450_count:,}ê°œ ({m14b_450_count/len(X)*100:.2f}%)")
    
    # ===== ê·¹ë‹¨ê°’ ì œê±° =====
    print("\nê·¹ë‹¨ê°’(2000+) í•„í„°ë§...")
    filter_mask = y < 2000
    X_filtered = X[filter_mask].copy()
    y_filtered = y[filter_mask].copy()
    seq_info_filtered = {
        'seq_max': [seq_info['seq_max'][i] for i in range(len(y)) if filter_mask[i]],
        'seq_min': [seq_info['seq_min'][i] for i in range(len(y)) if filter_mask[i]],
        'm14b_last': [seq_info['m14b_last'][i] for i in range(len(y)) if filter_mask[i]],
        'indices': [seq_info['indices'][i] for i in range(len(y)) if filter_mask[i]]
    }
    
    removed_count = len(y) - len(y_filtered)
    print(f"ì œê±°ëœ ë°ì´í„°: {removed_count:,}ê°œ ({removed_count/len(y)*100:.3f}%)")
    print(f"ë‚¨ì€ ë°ì´í„°: {len(y_filtered):,}ê°œ")
    print(f"ìƒˆë¡œìš´ ìµœëŒ€ê°’: {y_filtered.max():.2f}")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del X, y
    gc.collect()
    
    # ===== 3. ê³ ìœ„í—˜ êµ¬ê°„ ë°ì´í„° ì¦ê°• =====
    X_augmented, y_augmented = augment_high_risk_data(
        X_filtered, 
        y_filtered, 
        seq_info_filtered,
        augment_factor=10  # 10ë°° ì¦ê°•
    )
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del X_filtered, y_filtered, seq_info_filtered
    gc.collect()
    
    # ===== 4. Train/Test ë¶„í•  =====
    print("\n[STEP 3] Train/Test ë¶„í• ")
    print("-"*40)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_augmented, y_augmented, test_size=0.2, random_state=42, shuffle=True
    )
    
    print(f"í•™ìŠµ ë°ì´í„°: {X_train.shape[0]:,}ê°œ")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]:,}ê°œ")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del X_augmented, y_augmented
    gc.collect()
    
    # ===== 5. ëª¨ë¸ í•™ìŠµ =====
    print("\n[STEP 4] XGBoost ëª¨ë¸ í•™ìŠµ")
    print("-"*40)
    
    if gpu_id is not None:
        print(f"GPU ëª¨ë“œ: tree_method={tree_method}, gpu_id={gpu_id}")
        model = xgb.XGBRegressor(
            n_estimators=500,
            max_depth=8,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            tree_method=tree_method,
            gpu_id=gpu_id,
            random_state=42
        )
    else:
        print(f"CPU ëª¨ë“œ: tree_method={tree_method}")
        model = xgb.XGBRegressor(
            n_estimators=500,
            max_depth=8,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            tree_method=tree_method,
            random_state=42,
            n_jobs=-1
        )
    
    print("í•™ìŠµ ì‹œì‘...")
    start_time = datetime.now()
    
    model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        verbose=100
    )
    
    elapsed = (datetime.now() - start_time).total_seconds()
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
    
    # ===== 6. ëª¨ë¸ í‰ê°€ =====
    print("\n[STEP 5] ëª¨ë¸ í‰ê°€")
    print("-"*40)
    
    # Train ì„±ëŠ¥
    y_train_pred = model.predict(X_train)
    train_mae = mean_absolute_error(y_train, y_train_pred)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    train_r2 = r2_score(y_train, y_train_pred)
    
    print(f"Train ì„±ëŠ¥:")
    print(f"  MAE:  {train_mae:.2f}")
    print(f"  RMSE: {train_rmse:.2f}")
    print(f"  RÂ²:   {train_r2:.4f}")
    
    # Test ì„±ëŠ¥
    y_test_pred = model.predict(X_test)
    test_mae = mean_absolute_error(y_test, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    test_r2 = r2_score(y_test, y_test_pred)
    
    print(f"\nTest ì„±ëŠ¥:")
    print(f"  MAE:  {test_mae:.2f}")
    print(f"  RMSE: {test_rmse:.2f}")
    print(f"  RÂ²:   {test_r2:.4f}")
    
    # ìœ„í—˜ êµ¬ê°„ ë¶„ì„
    danger_mask_actual = y_test >= 1700
    danger_mask_pred = y_test_pred >= 1650
    
    danger_actual_count = np.sum(danger_mask_actual)
    danger_detected = np.sum(danger_mask_actual & danger_mask_pred)
    
    print(f"\nìœ„í—˜ êµ¬ê°„ (1700+) ë¶„ì„:")
    print(f"  ì‹¤ì œ ìœ„í—˜: {danger_actual_count:,}ê°œ")
    print(f"  ê°ì§€ ì„±ê³µ: {danger_detected:,}ê°œ")
    if danger_actual_count > 0:
        print(f"  ê°ì§€ìœ¨: {danger_detected/danger_actual_count*100:.1f}%")
    
    # ===== 7. ëª¨ë¸ ì €ì¥ =====
    print("\n[STEP 6] ëª¨ë¸ ì €ì¥")
    print("-"*40)
    
    model_filename = 'xgboost_280to10_1year_augmented.pkl'
    with open(model_filename, 'wb') as f:
        pickle.dump(model, f)
    print(f"âœ… ëª¨ë¸ ì €ì¥: {model_filename}")
    
    # ===== 8. Feature ì¤‘ìš”ë„ =====
    print("\n[STEP 7] Feature ì¤‘ìš”ë„ (Top 30)")
    print("-"*40)
    
    feature_importance_full = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(feature_importance_full.head(30).to_string(index=False))
    
    feature_importance_full.to_csv('feature_importance_1year.csv', index=False)
    print(f"\nâœ… Feature ì¤‘ìš”ë„ ì €ì¥: feature_importance_1year.csv")
    
    # ì¡°ê¸° ê²½ë³´ Feature ì¤‘ìš”ë„
    early_warning_features = feature_importance_full[
        feature_importance_full['feature'].str.contains('early_warning|last_10min')
    ]
    
    if len(early_warning_features) > 0:
        print("\nğŸ”¥ ì¡°ê¸° ê²½ë³´ Feature ì¤‘ìš”ë„:")
        print(early_warning_features.to_string(index=False))
    
    # ===== 9. ìµœì¢… ìš”ì•½ =====
    print("\n" + "="*80)
    print("âœ… ìµœì¢… í‰ê°€ ìš”ì•½")
    print("="*80)
    print(f"1. ëª¨ë¸ ì„±ëŠ¥:")
    print(f"   - Test MAE: {test_mae:.2f}")
    print(f"   - Test RÂ²:  {test_r2:.4f}")
    print(f"   - ìœ„í—˜ ê°ì§€ìœ¨: {danger_detected/danger_actual_count*100:.1f}% ({danger_detected}/{danger_actual_count})")
    
    print(f"\n2. ë°ì´í„° ì •ë³´:")
    print(f"   - í•™ìŠµ ë°ì´í„°: {X_train.shape[0]:,}ê°œ")
    print(f"   - Feature ìˆ˜: {X_train.shape[1]}ê°œ (ì¡°ê¸° ê²½ë³´ 5ê°œ í¬í•¨)")
    
    print(f"\n3. ì €ì¥ íŒŒì¼:")
    print(f"   - ëª¨ë¸: {model_filename}")
    print(f"   - Feature ì¤‘ìš”ë„: feature_importance_1year.csv")
    
    print("\n" + "="*80)
    
    return model, feature_importance_full

# ì‹¤í–‰
if __name__ == '__main__':
    model, feature_importance = train_and_evaluate_complete()
    print("\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")