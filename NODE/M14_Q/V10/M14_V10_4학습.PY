# -*- coding: utf-8 -*-
"""
================================================================================
V11 ML ì˜ˆì¸¡ ëª¨ë¸ - í•™ìŠµ ì½”ë“œ
V8.3.1 + V10.3 í†µí•© (XGBoost ì „ìš©)

íŠ¹ì§•:
    - ì‹œí€€ìŠ¤: 280ë¶„ (V8.3.1 ê¸°ì¤€)
    - ì˜ˆì¸¡: 10ë¶„ í›„ ë‹¨ì¼ê°’ (V8.3.1 ê¸°ì¤€)
    - Feature ê·¸ë£¹: target, important, auxiliary (V10.3 êµ¬ì¡°)
    - ëª¨ë¸: XGBoostë§Œ ì‚¬ìš© (LightGBM ì œì™¸)
================================================================================
"""

import os
import pickle
import warnings
import numpy as np
import pandas as pd
import glob
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from xgboost import XGBRegressor

warnings.filterwarnings('ignore')

# ============================================================================
# ì„¤ì •
# ============================================================================
CONFIG = {
    'train_file': 'M14_í•™ìŠµ*.CSV',  # glob íŒ¨í„´
    'model_file': 'models/v11_m14_model.pkl',
    'sequence_length': 280,      # V8.3.1 ê¸°ì¤€
    'prediction_offset': 10,     # V8.3.1 ê¸°ì¤€ (10ë¶„ í›„ ë‹¨ì¼ê°’)
    'limit_value': 1700,
    'target_column': 'TOTALCNT',
}

# Feature ê·¸ë£¹ (V10.3 ë™ì¼)
FEATURE_GROUPS = {
    'target': ['TOTALCNT'],
    'important': [
        'M14.QUE.LOAD.CURRENTLOADQCNT',
        'M14.QUE.LOAD.AVGLOADTIME',
        'M14.QUE.LOAD.AVGLOADTIME1MIN',
        'M14.QUE.LOAD.AVGFOUPLOADTIME',
        'M14.QUE.LOAD.AVGRETICLELOADTIME',
        'M14.QUE.LOAD.CURRENTRETICLELOADQCNT',
        'M14.QUE.ALL.CURRENTQCOMPLETED',
        'M14.QUE.ALL.CURRENTQCREATED',
        'M14.QUE.ALL.TRANSPORT4MINOVERCNT',
        'M14.QUE.ALL.TRANSPORT4MINOVERTIMEAVG',
        'M14.QUE.ALL.TRANSPORT4MINOVERRATIO',
        'M16HUB.QUE.M16TOM14B.CURRENTQCREATED',
        'M16HUB.QUE.M14BTOM16.CURRENTQCREATED',
        'M16HUB.QUE.M14TOM16.CURRENTQCREATED',
        'M16HUB.QUE.M16TOM14.CURRENTQCREATED',
        'M16HUB.QUE.M14TOM16.MESCURRENTQCNT',
        'M16HUB.QUE.M16TOM14.MESCURRENTQCNT',
    ],
    'auxiliary': [
        'M14AM10A', 'M10AM14A', 'M14AM10ASUM',
        'M14AM14B', 'M14BM14A', 'M14AM14BSUM',
        'M14AM16', 'M16M14A', 'M14AM16SUM',
        'M14.QUE.SFAB.SENDQUEUETOTAL',
        'M14.QUE.SFAB.RECEIVEQUEUETOTAL',
        'M14.QUE.SFAB.RETURNQUEUETOTAL',
        'M14.QUE.SFAB.COMPLETEQUEUETOTAL',
        'M14.QUE.OHT.OHTUTIL',
        'M14.QUE.OHT.RTCOHTUTIL',
        'M14.QUE.OHT.CURRENTOHTQCNT',
        'M14.QUE.OHT.CURRENTRETICLEOHTQCNT',
        'M14.QUE.CNV.NORTHCURRENTQCNT',
        'M14.QUE.CNV.SOUTHCURRENTQCNT',
        'M14.QUE.CNV.ALLTONORTHCNVCURRENTQCNT',
        'M14.QUE.CNV.ALLTOSOUTHCNVCURRENTQCNT',
    ],
    # â˜… ì‹ ê·œ ì»¬ëŸ¼ (3ê°œ)
    'pdt_new': [
        'M14.STRATE.N2.STORAGERATIO',
        'M14.PDT.LAYOUT.M14A_M14ATOM14ACNV_CURRENTQCNT',
        'M14.PDT.LAYOUT.HUBROOM_M14TOM16_CURRENTQCNT',
    ]
}

print("=" * 70)
print("ğŸš€ V11 ML ì˜ˆì¸¡ ëª¨ë¸ - í•™ìŠµ ì‹œì‘")
print("   V8.3.1 + V10.3 í†µí•© (XGBoost ì „ìš©)")
print("   ì‹œí€€ìŠ¤: 280ë¶„, ì˜ˆì¸¡: 10ë¶„ í›„")
print("=" * 70)

# ============================================================================
# ë°ì´í„° ë¡œë“œ
# ============================================================================
def load_data(path_pattern):
    print(f"\n[1/6] ë°ì´í„° ë¡œë“œ ì¤‘... ({path_pattern})")
    files = glob.glob(path_pattern)
    
    if not files:
        # ëŒ€ì•ˆ íŒ¨í„´ ì‹œë„
        alt_patterns = ['M14_Q_*.CSV', '*.CSV']
        for alt in alt_patterns:
            files = glob.glob(alt)
            if files:
                print(f"  â†’ ëŒ€ì•ˆ íŒ¨í„´ ì‚¬ìš©: {alt}")
                break
    
    if not files:
        raise ValueError("ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    
    dfs = []
    for f in sorted(files):
        try:
            df = pd.read_csv(f, on_bad_lines='skip', encoding='utf-8')
            print(f"  - {f}: {len(df):,}í–‰")
            dfs.append(df)
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(f, on_bad_lines='skip', encoding='cp949')
                print(f"  - {f} (cp949): {len(df):,}í–‰")
                dfs.append(df)
            except:
                df = pd.read_csv(f, on_bad_lines='skip', encoding='euc-kr')
                print(f"  - {f} (euc-kr): {len(df):,}í–‰")
                dfs.append(df)
    
    return pd.concat(dfs, ignore_index=True)

df = load_data(CONFIG['train_file'])
print(f"  â†’ ì´ ë°ì´í„°: {len(df):,}í–‰")

# CURRTIME íŒŒì‹±
if 'CURRTIME' in df.columns:
    df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), format='%Y%m%d%H%M', errors='coerce')
    df = df.dropna(subset=['CURRTIME'])
    df = df.sort_values('CURRTIME').reset_index(drop=True)

# Feature ê·¸ë£¹ í•„í„°ë§ (ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ)
print("\n[2/6] Feature ê·¸ë£¹ í™•ì¸...")
for group_name in FEATURE_GROUPS:
    original_count = len(FEATURE_GROUPS[group_name])
    FEATURE_GROUPS[group_name] = [f for f in FEATURE_GROUPS[group_name] if f in df.columns]
    print(f"  - {group_name}: {len(FEATURE_GROUPS[group_name])}/{original_count}ê°œ")

# ìˆ«ìí˜• ë³€í™˜
all_cols = []
for group in FEATURE_GROUPS.values():
    all_cols.extend(group)
all_cols = list(set(all_cols))

for col in all_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

# queue_gap íŒŒìƒ ë³€ìˆ˜ ìƒì„±
if 'M14.QUE.ALL.CURRENTQCREATED' in df.columns and 'M14.QUE.ALL.CURRENTQCOMPLETED' in df.columns:
    df['QUEUE_GAP'] = df['M14.QUE.ALL.CURRENTQCREATED'] - df['M14.QUE.ALL.CURRENTQCOMPLETED']
    FEATURE_GROUPS['auxiliary'].append('QUEUE_GAP')
    print(f"  - QUEUE_GAP íŒŒìƒ ë³€ìˆ˜ ì¶”ê°€!")

# ============================================================================
# Feature ìƒì„± í•¨ìˆ˜ (V10.3 ê¸°ë°˜ + V8.3.1 ì‹œí€€ìŠ¤)
# ============================================================================
def create_sequence_features(df, feature_cols, seq_len, idx, limit_val=1700):
    """ì‹œí€€ìŠ¤ ê¸°ë°˜ Feature ìƒì„± (ì»¬ëŸ¼ë‹¹ 20ê°œ)"""
    features = []
    
    for col in feature_cols:
        seq = df[col].iloc[idx - seq_len:idx].values
        current_val = seq[-1]
        
        # ê¸°ë³¸ í†µê³„ (10ê°œ)
        features.extend([
            np.mean(seq),                                           # í‰ê· 
            np.std(seq),                                            # í‘œì¤€í¸ì°¨
            np.min(seq),                                            # ìµœì†Œê°’
            np.max(seq),                                            # ìµœëŒ€ê°’
            current_val,                                            # í˜„ì¬ê°’
            seq[-1] - seq[0],                                       # ì „ì²´ ë³€í™”ëŸ‰
            np.percentile(seq, 25),                                 # 25% ë¶„ìœ„ìˆ˜
            np.percentile(seq, 75),                                 # 75% ë¶„ìœ„ìˆ˜
            np.mean(seq[-10:]) - np.mean(seq[:10]),                # ìµœê·¼ vs ì´ˆê¸° í‰ê·  ì°¨ì´
            np.max(seq[-30:]) if len(seq) >= 30 else np.max(seq),  # ìµœê·¼ 30ë¶„ ìµœëŒ€ê°’
        ])
        
        # ì¶”ì„¸/ë³€í™” (10ê°œ)
        features.extend([
            seq[-1] - seq[-10] if len(seq) >= 10 else 0,           # 10ë¶„ ë³€í™”ëŸ‰
            seq[-1] - seq[-30] if len(seq) >= 30 else 0,           # 30ë¶„ ë³€í™”ëŸ‰
            seq[-1] - seq[-60] if len(seq) >= 60 else 0,           # 60ë¶„ ë³€í™”ëŸ‰
            (seq[-1] - seq[-10]) / 10 if len(seq) >= 10 else 0,    # 10ë¶„ ê¸°ìš¸ê¸°
            (seq[-1] - seq[-30]) / 30 if len(seq) >= 30 else 0,    # 30ë¶„ ê¸°ìš¸ê¸°
            np.max(seq[-10:]) - np.min(seq[-10:]) if len(seq) >= 10 else 0,  # 10ë¶„ ë²”ìœ„
            np.max(seq[-30:]) - np.min(seq[-30:]) if len(seq) >= 30 else 0,  # 30ë¶„ ë²”ìœ„
            limit_val - current_val,                                # ë¦¬ë¯¸íŠ¸ê¹Œì§€ ë‚¨ì€ ê°’
            1 if current_val >= 1500 else 0,                       # 1500 ì´ìƒ í”Œë˜ê·¸
            1 if current_val >= 1600 else 0,                       # 1600 ì´ìƒ í”Œë˜ê·¸
        ])
    
    return features

# ============================================================================
# í•™ìŠµ ë°ì´í„° ìƒì„±
# ============================================================================
print("\n[3/6] í•™ìŠµ ë°ì´í„° ìƒì„± ì¤‘...")

seq_len = CONFIG['sequence_length']
pred_offset = CONFIG['prediction_offset']
limit_val = CONFIG['limit_value']
target_col = CONFIG['target_column']

X_target, X_important, X_auxiliary, X_pdt_new = [], [], [], []
y_values = []

total_samples = len(df) - seq_len - pred_offset
print(f"  â†’ ì˜ˆìƒ ìƒ˜í”Œ ìˆ˜: {total_samples:,}ê°œ")

for idx in range(seq_len, len(df) - pred_offset):
    if idx % 10000 == 0:
        print(f"    ì§„í–‰: {idx:,}/{len(df) - pred_offset:,} ({100*idx/(len(df)-pred_offset):.1f}%)")
    
    # íƒ€ê²Ÿ: 10ë¶„ í›„ TOTALCNT ë‹¨ì¼ê°’ (V8.3.1 ë°©ì‹)
    future_val = df[target_col].iloc[idx + pred_offset]
    y_values.append(future_val)
    
    # Feature ìƒì„±
    X_target.append(create_sequence_features(df, FEATURE_GROUPS['target'], seq_len, idx))
    X_important.append(create_sequence_features(df, FEATURE_GROUPS['important'], seq_len, idx))
    X_auxiliary.append(create_sequence_features(df, FEATURE_GROUPS['auxiliary'], seq_len, idx))
    X_pdt_new.append(create_sequence_features(df, FEATURE_GROUPS['pdt_new'], seq_len, idx))

X_target = np.array(X_target)
X_important = np.array(X_important)
X_auxiliary = np.array(X_auxiliary)
X_pdt_new = np.array(X_pdt_new)
y_values = np.array(y_values)

print(f"\n  â†’ ìƒì„± ì™„ë£Œ:")
print(f"    - ìƒ˜í”Œ ìˆ˜: {len(y_values):,}ê°œ")
print(f"    - Target Features: {X_target.shape[1]}ê°œ")
print(f"    - Important Features: {X_important.shape[1]}ê°œ")
print(f"    - Auxiliary Features: {X_auxiliary.shape[1]}ê°œ")
print(f"    - PDT New Features: {X_pdt_new.shape[1]}ê°œ")
print(f"    - ì´ Features: {X_target.shape[1] + X_important.shape[1] + X_auxiliary.shape[1] + X_pdt_new.shape[1]}ê°œ")

# 1700+ ì¼€ì´ìŠ¤ í†µê³„
danger_count = np.sum(y_values >= limit_val)
print(f"    - 1700+ ì¼€ì´ìŠ¤: {danger_count:,}ê°œ ({100*danger_count/len(y_values):.2f}%)")

# ============================================================================
# ìŠ¤ì¼€ì¼ë§
# ============================================================================
print("\n[4/6] ìŠ¤ì¼€ì¼ë§...")

scalers = {
    'target': StandardScaler(),
    'important': StandardScaler(),
    'auxiliary': StandardScaler(),
    'pdt_new': StandardScaler(),
}

X_target_scaled = scalers['target'].fit_transform(X_target)
X_important_scaled = scalers['important'].fit_transform(X_important)
X_auxiliary_scaled = scalers['auxiliary'].fit_transform(X_auxiliary)
X_pdt_new_scaled = scalers['pdt_new'].fit_transform(X_pdt_new)

# ============================================================================
# ê°€ì¤‘ì¹˜ ì„¤ì • (V8.3.1 ë°©ì‹)
# ============================================================================
print("\n[5/6] ëª¨ë¸ í•™ìŠµ ì¤‘ (XGBoost)...")

# ê°€ì¤‘ì¹˜: ê³ ë¶€í•˜ êµ¬ê°„ ê°•ì¡°
weights = np.ones(len(y_values))
weights[y_values >= 1500] = 3
weights[y_values >= 1600] = 10
weights[y_values >= 1700] = 30

print(f"  â†’ ê°€ì¤‘ì¹˜ ë¶„í¬:")
print(f"    - ê¸°ë³¸(~1500): {np.sum(weights == 1):,}ê°œ")
print(f"    - ì¤‘ê°„(1500~1600): {np.sum(weights == 3):,}ê°œ")
print(f"    - ë†’ìŒ(1600~1700): {np.sum(weights == 10):,}ê°œ")
print(f"    - ìµœê³ (1700+): {np.sum(weights == 30):,}ê°œ")

# ============================================================================
# XGBoost ëª¨ë¸ í•™ìŠµ (LightGBM ì œì™¸!)
# ============================================================================
models = {}

# XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° (V8.3.1 ì°¸ê³ )
xgb_params = {
    'n_estimators': 500,
    'max_depth': 8,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'min_child_weight': 3,
    'gamma': 0.1,
    'reg_alpha': 0.1,
    'reg_lambda': 1.0,
    'random_state': 42,
    'n_jobs': -1,
}

print(f"\n  â†’ XGBoost íŒŒë¼ë¯¸í„°:")
for k, v in xgb_params.items():
    print(f"    - {k}: {v}")

print("\n  - XGB Target í•™ìŠµ...")
models['xgb_target'] = XGBRegressor(**xgb_params)
models['xgb_target'].fit(X_target_scaled, y_values, sample_weight=weights)

print("  - XGB Important í•™ìŠµ...")
models['xgb_important'] = XGBRegressor(**xgb_params)
models['xgb_important'].fit(X_important_scaled, y_values, sample_weight=weights)

print("  - XGB Auxiliary í•™ìŠµ...")
models['xgb_auxiliary'] = XGBRegressor(**xgb_params)
models['xgb_auxiliary'].fit(X_auxiliary_scaled, y_values, sample_weight=weights)

print("  - XGB PDT New í•™ìŠµ...")
models['xgb_pdt_new'] = XGBRegressor(**xgb_params)
models['xgb_pdt_new'].fit(X_pdt_new_scaled, y_values, sample_weight=weights)

# ============================================================================
# ëª¨ë¸ ì €ì¥
# ============================================================================
print("\n[6/6] ëª¨ë¸ ì €ì¥...")

os.makedirs('models', exist_ok=True)

model_data = {
    'models': models,
    'scalers': scalers,
    'feature_indices': {
        'target_end': X_target.shape[1],
        'important_end': X_target.shape[1] + X_important.shape[1],
        'auxiliary_end': X_target.shape[1] + X_important.shape[1] + X_auxiliary.shape[1],
        'pdt_new_end': X_target.shape[1] + X_important.shape[1] + X_auxiliary.shape[1] + X_pdt_new.shape[1],
    },
    'config': CONFIG,
    'feature_groups': FEATURE_GROUPS,
    'xgb_params': xgb_params,
    'training_info': {
        'version': 'V11',
        'train_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'samples': len(y_values),
        'danger_samples': int(danger_count),
        'sequence_length': seq_len,
        'prediction_offset': pred_offset,
    }
}

with open(CONFIG['model_file'], 'wb') as f:
    pickle.dump(model_data, f)

print(f"  â†’ ì €ì¥: {CONFIG['model_file']}")

# ============================================================================
# ì™„ë£Œ
# ============================================================================
print("\n" + "=" * 70)
print("âœ… V11 í•™ìŠµ ì™„ë£Œ!")
print("=" * 70)
print(f"  - ë²„ì „: V11 (V8.3.1 + V10.3 í†µí•©)")
print(f"  - ì‹œí€€ìŠ¤: {seq_len}ë¶„")
print(f"  - ì˜ˆì¸¡: {pred_offset}ë¶„ í›„")
print(f"  - ëª¨ë¸: XGBoost 4ê°œ (target, important, auxiliary, pdt_new)")
print(f"  - ì´ ìƒ˜í”Œ: {len(y_values):,}ê°œ")
print(f"  - 1700+ ìƒ˜í”Œ: {danger_count:,}ê°œ")
print(f"  - ëª¨ë¸ íŒŒì¼: {CONFIG['model_file']}")
print("=" * 70)