# -*- coding: utf-8 -*-
"""
================================================================================
V10_4c ML ì˜ˆì¸¡ ëª¨ë¸ - í‰ê°€ ì½”ë“œ (30ë¶„ ì˜ˆì¸¡)
V10_4b + LGBM ë¶„ë¥˜ê¸° íŠœë‹ (class_weight, threshold ì¡°ì •)
================================================================================
"""

import os
import pickle
import warnings
import gc
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

warnings.filterwarnings('ignore')

# ============================================================================
# ì„¤ì •
# ============================================================================
CONFIG = {
    'model_file': 'models/v10_4c_30min_m14_model.pkl',
    'eval_file': 'M14_í‰ê°€.CSV',
    'output_file': f'V10_4c_30min_í‰ê°€ê²°ê³¼_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
    'sequence_length': 280,
    'prediction_offset': 30,
    'limit_value': 1700,
    'target_column': 'TOTALCNT',
    # â˜… LGBM í™•ë¥  ì„ê³„ê°’ (íŠœë‹ë¨)
    'lgbm_threshold': 0.3,  # ê¸°ì¡´ 0.5 â†’ 0.3ìœ¼ë¡œ ë‚®ì¶¤
}

print("=" * 70)
print("ğŸš€ V10_4c ML ì˜ˆì¸¡ ëª¨ë¸ - í‰ê°€ ì‹œì‘ (30ë¶„ ì˜ˆì¸¡)")
print("   V10_4b + LGBM íŠœë‹ (scale_pos_weight + threshold 0.3)")
print("   ì‹œí€€ìŠ¤: 280ë¶„, ì˜ˆì¸¡: 30ë¶„ í›„")
print("=" * 70)

# ============================================================================
# ëª¨ë¸ ë¡œë“œ
# ============================================================================
print("\n[1/5] ëª¨ë¸ ë¡œë“œ ì¤‘...")

with open(CONFIG['model_file'], 'rb') as f:
    model_data = pickle.load(f)

models = model_data['models']
scalers = model_data['scalers']
FEATURE_GROUPS = model_data['feature_groups']
training_info = model_data.get('training_info', {})

print(f"  - ëª¨ë¸ ë²„ì „: {training_info.get('version', 'V10_4c')}")
print(f"  - í•™ìŠµì¼: {training_info.get('train_date', 'N/A')}")
print(f"  - ëª¨ë¸ ìˆ˜: {len(models)}ê°œ")
print(f"  - í´ë˜ìŠ¤ ë¹„ìœ¨: 1:{training_info.get('class_ratio', 'N/A')}")
print(f"  - íŠœë‹: {training_info.get('tuning', 'N/A')}")

# ============================================================================
# ë°ì´í„° ë¡œë“œ
# ============================================================================
print("\n[2/5] í‰ê°€ ë°ì´í„° ë¡œë“œ ì¤‘...")

try:
    df = pd.read_csv(CONFIG['eval_file'], encoding='utf-8', on_bad_lines='skip')
except:
    try:
        df = pd.read_csv(CONFIG['eval_file'], encoding='cp949', on_bad_lines='skip')
    except:
        df = pd.read_csv(CONFIG['eval_file'], encoding='euc-kr', on_bad_lines='skip')

print(f"  - ì›ë³¸ ë°ì´í„°: {len(df):,}í–‰")

df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), format='%Y%m%d%H%M', errors='coerce')
df = df.dropna(subset=['CURRTIME']).sort_values('CURRTIME').reset_index(drop=True)
print(f"  - íŒŒì‹± í›„ ë°ì´í„°: {len(df):,}í–‰")

# ============================================================================
# íŒŒìƒë³€ìˆ˜ ìƒì„±
# ============================================================================
if 'M14.QUE.ALL.CURRENTQCREATED' in df.columns and 'M14.QUE.ALL.CURRENTQCOMPLETED' in df.columns:
    df['QUEUE_GAP'] = df['M14.QUE.ALL.CURRENTQCREATED'] - df['M14.QUE.ALL.CURRENTQCOMPLETED']
    print("  - QUEUE_GAP íŒŒìƒ ë³€ìˆ˜ ìƒì„±!")

job_cols_exist = all(col in df.columns for col in ['JobPrep_Count', 'Reserved_Count', 'JobEnd_Count'])
if job_cols_exist:
    df['Job_Total'] = df['JobPrep_Count'] + df['Reserved_Count'] + df['JobEnd_Count']
    df['Job_Total_ma10'] = df['Job_Total'].rolling(10, min_periods=1).mean()
    print("  - Job_Total, Job_Total_ma10 íŒŒìƒ ë³€ìˆ˜ ìƒì„±!")

for group_name in FEATURE_GROUPS:
    original = FEATURE_GROUPS[group_name].copy()
    FEATURE_GROUPS[group_name] = [f for f in FEATURE_GROUPS[group_name] if f in df.columns]
    missing = set(original) - set(FEATURE_GROUPS[group_name])
    if missing:
        print(f"  âš  {group_name} ëˆ„ë½: {missing}")

all_cols = []
for group in FEATURE_GROUPS.values():
    all_cols.extend(group)
for col in list(set(all_cols)):
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

# ============================================================================
# Feature ìƒì„± í•¨ìˆ˜
# ============================================================================
def create_sequence_features(df, feature_cols, seq_len, idx, limit_val=1700):
    features = []
    for col in feature_cols:
        seq = df[col].iloc[idx - seq_len:idx].values
        current_val = seq[-1]
        features.extend([
            np.mean(seq), np.std(seq), np.min(seq), np.max(seq), current_val,
            seq[-1] - seq[0],
            np.percentile(seq, 25), np.percentile(seq, 75),
            np.mean(seq[-10:]) - np.mean(seq[:10]),
            np.max(seq[-30:]) if len(seq) >= 30 else np.max(seq),
            seq[-1] - seq[-10] if len(seq) >= 10 else 0,
            seq[-1] - seq[-30] if len(seq) >= 30 else 0,
            seq[-1] - seq[-60] if len(seq) >= 60 else 0,
            (seq[-1] - seq[-10]) / 10 if len(seq) >= 10 else 0,
            (seq[-1] - seq[-30]) / 30 if len(seq) >= 30 else 0,
            np.max(seq[-10:]) - np.min(seq[-10:]) if len(seq) >= 10 else 0,
            np.max(seq[-30:]) - np.min(seq[-30:]) if len(seq) >= 30 else 0,
            limit_val - current_val,
            1 if current_val >= 1500 else 0,
            1 if current_val >= 1600 else 0,
        ])
    return features

# ============================================================================
# í‰ê°€ ìˆ˜í–‰
# ============================================================================
print("\n[3/5] ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")

seq_len = CONFIG['sequence_length']
pred_offset = CONFIG['prediction_offset']
limit_val = CONFIG['limit_value']
target_col = CONFIG['target_column']
lgbm_thresh = CONFIG['lgbm_threshold']

results = []
total = len(df) - seq_len - pred_offset
print(f"  â†’ ì˜ˆìƒ í‰ê°€ ìˆ˜: {total:,}ê°œ")
print(f"  â†’ LGBM í™•ë¥  ì„ê³„ê°’: {lgbm_thresh}")

for idx in range(seq_len, len(df) - pred_offset):
    if (idx - seq_len) % 1000 == 0:
        print(f"    ì§„í–‰: {idx - seq_len:,}/{total:,}")
        gc.collect()
    
    current_time = df['CURRTIME'].iloc[idx - 1]
    current_total = df[target_col].iloc[idx - 1]
    prediction_time = current_time + timedelta(minutes=pred_offset)
    
    future_end = min(idx - 1 + pred_offset, len(df))
    actual_max = df[target_col].iloc[idx - 1:future_end].max()
    
    actual_single_idx = idx - 1 + pred_offset
    actual_single = df[target_col].iloc[actual_single_idx] if actual_single_idx < len(df) else df[target_col].iloc[-1]
    
    # Feature ìƒì„±
    feat_target = create_sequence_features(df, FEATURE_GROUPS['target'], seq_len, idx)
    feat_important = create_sequence_features(df, FEATURE_GROUPS['important'], seq_len, idx)
    feat_auxiliary = create_sequence_features(df, FEATURE_GROUPS['auxiliary'], seq_len, idx)
    
    X_target = scalers['target'].transform([feat_target])
    X_important = scalers['important'].transform([feat_important])
    X_auxiliary = scalers['auxiliary'].transform([feat_auxiliary])
    
    # ============================================
    # XGB íšŒê·€ ì˜ˆì¸¡
    # ============================================
    pred_xgb_target = models['xgb_target'].predict(X_target)[0]
    pred_xgb_important = models['xgb_important'].predict(X_important)[0]
    pred_xgb_auxiliary = models['xgb_auxiliary'].predict(X_auxiliary)[0]
    
    # ============================================
    # LGBM ë¶„ë¥˜ ì˜ˆì¸¡ (íŠœë‹ë¨)
    # ============================================
    prob_lgb_target = models['lgb_target'].predict_proba(X_target)[0][1]
    prob_lgb_important = models['lgb_important'].predict_proba(X_important)[0][1]
    prob_lgb_auxiliary = models['lgb_auxiliary'].predict_proba(X_auxiliary)[0][1]
    
    # â˜… XGB ë¶„ë¥˜ ì˜ˆì¸¡ (ì¶”ê°€ë¨)
    prob_xgb_target_clf = 0
    prob_xgb_important_clf = 0
    if 'xgb_target_clf' in models:
        prob_xgb_target_clf = models['xgb_target_clf'].predict_proba(X_target)[0][1]
    if 'xgb_important_clf' in models:
        prob_xgb_important_clf = models['xgb_important_clf'].predict_proba(X_important)[0][1]
    
    # PDT ëª¨ë¸
    pred_xgb_pdt, prob_lgb_pdt = None, None
    if 'xgb_pdt_new' in models and 'pdt_new' in scalers:
        feat_pdt = create_sequence_features(df, FEATURE_GROUPS.get('pdt_new', []), seq_len, idx)
        if feat_pdt:
            X_pdt = scalers['pdt_new'].transform([feat_pdt])
            pred_xgb_pdt = models['xgb_pdt_new'].predict(X_pdt)[0]
            prob_lgb_pdt = models['lgb_pdt_new'].predict_proba(X_pdt)[0][1]
    
    # Job ëª¨ë¸
    pred_xgb_job, prob_lgb_job = None, None
    if 'xgb_job' in models and 'job_features' in scalers and FEATURE_GROUPS.get('job_features'):
        feat_job = create_sequence_features(df, FEATURE_GROUPS['job_features'], seq_len, idx)
        if feat_job:
            X_job = scalers['job_features'].transform([feat_job])
            pred_xgb_job = models['xgb_job'].predict(X_job)[0]
            prob_lgb_job = models['lgb_job'].predict_proba(X_job)[0][1]
    
    # ============================================
    # íˆ¬í‘œ ì‹œìŠ¤í…œ (â˜… íŠœë‹ëœ ì„ê³„ê°’ ì ìš©)
    # ============================================
    votes = []
    
    # XGB íšŒê·€ >= 1700 íˆ¬í‘œ
    votes.append(1 if pred_xgb_target >= limit_val else 0)
    votes.append(1 if pred_xgb_important >= limit_val else 0)
    votes.append(1 if pred_xgb_auxiliary >= limit_val else 0)
    
    # â˜… LGBM ë¶„ë¥˜ (ë‚®ì€ ì„ê³„ê°’ ì ìš©)
    votes.append(1 if prob_lgb_target >= lgbm_thresh else 0)
    votes.append(1 if prob_lgb_important >= lgbm_thresh else 0)
    votes.append(1 if prob_lgb_auxiliary >= lgbm_thresh else 0)
    
    # â˜… XGB ë¶„ë¥˜ íˆ¬í‘œ (ì¶”ê°€)
    votes.append(1 if prob_xgb_target_clf >= lgbm_thresh else 0)
    votes.append(1 if prob_xgb_important_clf >= lgbm_thresh else 0)
    
    if pred_xgb_pdt is not None:
        votes.append(1 if pred_xgb_pdt >= limit_val else 0)
        votes.append(1 if prob_lgb_pdt >= lgbm_thresh else 0)
    
    if pred_xgb_job is not None:
        votes.append(1 if pred_xgb_job >= limit_val else 0)
        votes.append(1 if prob_lgb_job >= lgbm_thresh else 0)
    
    vote_sum = sum(votes)
    total_votes = len(votes)
    
    # ============================================
    # ìµœì¢… íŒì • ê·œì¹™ (â˜… íŠœë‹)
    # ============================================
    
    # ê¸°ë³¸ ê·œì¹™
    rule1 = vote_sum >= 3
    
    # â˜… LGBM í™•ë¥  ê·œì¹™ (ë‚®ì€ ì„ê³„ê°’)
    rule2 = (prob_lgb_important >= 0.25) and (current_total >= 1500)
    rule3 = (prob_lgb_target >= 0.30) and (current_total >= 1450)
    
    # XGB íšŒê·€ ê·œì¹™
    rule4 = (pred_xgb_important >= 1680) and (current_total >= 1500)
    rule5 = (pred_xgb_target >= 1700)
    rule6 = (pred_xgb_auxiliary >= 1720) and (current_total >= 1550)
    
    # â˜… XGB ë¶„ë¥˜ ê·œì¹™ (ì¶”ê°€)
    rule7 = (prob_xgb_important_clf >= 0.30) and (current_total >= 1450)
    rule8 = (prob_xgb_target_clf >= 0.35) and (current_total >= 1400)
    
    # í˜„ì¬ê°’ ë†’ì„ ë•Œ ë¯¼ê°ë„ ì¦ê°€
    rule9 = (current_total >= 1600) and (vote_sum >= 2)
    rule10 = (current_total >= 1650) and (pred_xgb_target >= 1650)
    
    # â˜… í™•ë¥  í‰ê·  ê·œì¹™
    avg_prob = np.mean([prob_lgb_target, prob_lgb_important, prob_lgb_auxiliary])
    rule11 = (avg_prob >= 0.20) and (current_total >= 1550)
    
    final_pred_danger = 1 if (rule1 or rule2 or rule3 or rule4 or rule5 or 
                               rule6 or rule7 or rule8 or rule9 or rule10 or rule11) else 0
    
    # ì•™ìƒë¸” í‰ê· 
    preds = [pred_xgb_target, pred_xgb_important, pred_xgb_auxiliary]
    if pred_xgb_pdt is not None:
        preds.append(pred_xgb_pdt)
    if pred_xgb_job is not None:
        preds.append(pred_xgb_job)
    ensemble_pred = np.mean(preds)
    
    # â˜… í™•ë¥  í‰ê· 
    probs = [prob_lgb_target, prob_lgb_important, prob_lgb_auxiliary]
    if prob_lgb_pdt is not None:
        probs.append(prob_lgb_pdt)
    if prob_lgb_job is not None:
        probs.append(prob_lgb_job)
    avg_lgbm_prob = np.mean(probs)
    
    # ê²°ê³¼ ì €ì¥
    result = {
        'í˜„ì¬ì‹œê°„': current_time.strftime('%Y-%m-%d %H:%M'),
        'í˜„ì¬TOTALCNT': round(current_total, 2),
        'ì˜ˆì¸¡ì‹œì ': prediction_time.strftime('%Y-%m-%d %H:%M'),
        'ì‹¤ì œê°’30min': round(actual_max, 2),
        'ì‹¤ì œë‹¨ì¼ê°’': round(actual_single, 2),
        'XGB_íƒ€ê²Ÿ': round(pred_xgb_target, 2),
        'XGB_ì¤‘ìš”': round(pred_xgb_important, 2),
        'XGB_ë³´ì¡°': round(pred_xgb_auxiliary, 2),
        'XGB_PDT': round(pred_xgb_pdt, 2) if pred_xgb_pdt else '',
        'XGB_Job': round(pred_xgb_job, 2) if pred_xgb_job else '',
        'LGBM_íƒ€ê²Ÿ_í™•ë¥ ': round(prob_lgb_target, 3),
        'LGBM_ì¤‘ìš”_í™•ë¥ ': round(prob_lgb_important, 3),
        'LGBM_ë³´ì¡°_í™•ë¥ ': round(prob_lgb_auxiliary, 3),
        'LGBM_í‰ê· _í™•ë¥ ': round(avg_lgbm_prob, 3),
        'XGB_íƒ€ê²Ÿ_CLF': round(prob_xgb_target_clf, 3),
        'XGB_ì¤‘ìš”_CLF': round(prob_xgb_important_clf, 3),
        'ì•™ìƒë¸”ì˜ˆì¸¡': round(ensemble_pred, 2),
        f'íˆ¬í‘œ({total_votes}ê°œì¤‘)': vote_sum,
        'ìµœì¢…íŒì •': final_pred_danger,
        'ì‹¤ì œìœ„í—˜(1700+)': 1 if actual_max >= limit_val else 0,
    }
    
    if 'Job_Total_ma10' in df.columns:
        result['Job_Total_ma10'] = round(df['Job_Total_ma10'].iloc[idx - 1], 1)
    
    results.append(result)

print(f"  âœ… ì™„ë£Œ!")

# ============================================================================
# ê²°ê³¼ ì €ì¥
# ============================================================================
print("\n[4/5] ê²°ê³¼ ì €ì¥...")

df_result = pd.DataFrame(results)

# ì˜ˆì¸¡ìƒíƒœ ë¶„ë¥˜
df_result['í˜„ì¬ì‹œê°„_dt'] = pd.to_datetime(df_result['í˜„ì¬ì‹œê°„'])

def get_prediction_status(row, all_df):
    actual = row['ì‹¤ì œìœ„í—˜(1700+)']
    pred = row['ìµœì¢…íŒì •']
    current_time = row['í˜„ì¬ì‹œê°„_dt']
    
    if actual == 1 and pred == 1:
        return 'ì •ìƒì˜ˆì¸¡_TP'
    elif actual == 0 and pred == 0:
        return 'ì •ìƒì˜ˆì¸¡_TN'
    elif actual == 1 and pred == 0:
        time_10min_ago = current_time - timedelta(minutes=10)
        prev_data = all_df[all_df['í˜„ì¬ì‹œê°„_dt'] == time_10min_ago]
        if len(prev_data) > 0 and prev_data['ìµœì¢…íŒì •'].values[0] == 1:
            return 'FN_10ë¶„ì „ì˜ˆì¸¡'
        else:
            return 'FN_ì™„ì „ë†“ì¹¨'
    else:
        time_10min_later = current_time + timedelta(minutes=10)
        later_data = all_df[all_df['í˜„ì¬ì‹œê°„_dt'] == time_10min_later]
        if len(later_data) > 0 and later_data['ì‹¤ì œìœ„í—˜(1700+)'].values[0] == 1:
            return 'FP_10ë¶„í›„ëŒíŒŒ'
        else:
            return 'FP_ì˜ëª»ëœê²½ê³ '

df_result['ì˜ˆì¸¡ìƒíƒœ'] = df_result.apply(lambda row: get_prediction_status(row, df_result), axis=1)
df_result = df_result.drop(columns=['í˜„ì¬ì‹œê°„_dt'])

df_result.to_csv(CONFIG['output_file'], index=False, encoding='utf-8-sig')
print(f"  â†’ ì €ì¥: {CONFIG['output_file']}")

# ============================================================================
# ì„±ëŠ¥ í‰ê°€
# ============================================================================
print("\n[5/5] ì„±ëŠ¥ í‰ê°€...")

print("\n" + "=" * 70)
print("ğŸ“Š V10_4c 30ë¶„ í‰ê°€ í†µê³„ (LGBM íŠœë‹)")
print("=" * 70)

actual_danger = df_result['ì‹¤ì œìœ„í—˜(1700+)'] == 1
pred_danger = df_result['ìµœì¢…íŒì •'] == 1

TP = (actual_danger & pred_danger).sum()
TN = (~actual_danger & ~pred_danger).sum()
FP = (~actual_danger & pred_danger).sum()
FN = (actual_danger & ~pred_danger).sum()

print(f"ì´ ì˜ˆì¸¡: {len(df_result):,}ê°œ")
print(f"ì‹¤ì œ 1700+: {actual_danger.sum()}ê°œ")

print(f"\nğŸ”¥ 1700+ ê°ì§€:")
if actual_danger.sum() > 0:
    recall = TP / actual_danger.sum() * 100
    print(f"  ê°ì§€(TP): {TP}ê°œ ({recall:.1f}%)")
    print(f"  ë¯¸ê°ì§€(FN): {FN}ê°œ ({100-recall:.1f}%)")

print(f"\nâš ï¸ ì˜¤íƒ(FP): {FP}ê°œ")
if pred_danger.sum() > 0:
    precision = TP / pred_danger.sum() * 100
    print(f"ì •ë°€ë„: {precision:.1f}%")

print(f"\n[í˜¼ë™ í–‰ë ¬]")
print(f"  TP: {TP:,}ê±´ | FN: {FN:,}ê±´")
print(f"  FP: {FP:,}ê±´ | TN: {TN:,}ê±´")

# ì˜ˆì¸¡ìƒíƒœë³„ ì§‘ê³„
print(f"\n[ì˜ˆì¸¡ìƒíƒœ ë¶„ë¥˜]")
status_counts = df_result['ì˜ˆì¸¡ìƒíƒœ'].value_counts()
for status, count in status_counts.items():
    print(f"  {status}: {count:,}ê±´")

real_fn = status_counts.get('FN_ì™„ì „ë†“ì¹¨', 0)
real_fp = status_counts.get('FP_ì˜ëª»ëœê²½ê³ ', 0)

print(f"\n[ì‹¤ì§ˆì  ì„±ëŠ¥]")
print(f"  ì‹¤ì§ˆ FN (ì™„ì „ ë†“ì¹¨):    {real_fn:,}ê±´")
print(f"  ì‹¤ì§ˆ FP (ì˜ëª»ëœ ê²½ê³ ):  {real_fp:,}ê±´")

# â˜… LGBM í™•ë¥  ë¶„í¬ í™•ì¸
print(f"\n[LGBM í™•ë¥  ë¶„í¬ (íŠœë‹ íš¨ê³¼)]")
for col in ['LGBM_íƒ€ê²Ÿ_í™•ë¥ ', 'LGBM_ì¤‘ìš”_í™•ë¥ ', 'LGBM_í‰ê· _í™•ë¥ ']:
    if col in df_result.columns:
        vals = pd.to_numeric(df_result[col], errors='coerce')
        print(f"  {col}: í‰ê· ={vals.mean():.3f}, ìµœëŒ€={vals.max():.3f}, >=0.3: {(vals >= 0.3).sum()}ê±´")

# â˜… XGB ë¶„ë¥˜ í™•ë¥  ë¶„í¬
print(f"\n[XGB ë¶„ë¥˜ í™•ë¥  ë¶„í¬]")
for col in ['XGB_íƒ€ê²Ÿ_CLF', 'XGB_ì¤‘ìš”_CLF']:
    if col in df_result.columns:
        vals = pd.to_numeric(df_result[col], errors='coerce')
        print(f"  {col}: í‰ê· ={vals.mean():.3f}, ìµœëŒ€={vals.max():.3f}, >=0.3: {(vals >= 0.3).sum()}ê±´")

if (TP + TN + FP + FN) > 0:
    accuracy = (TP + TN) / (TP + TN + FP + FN) * 100
    print(f"\nì •í™•ë„: {accuracy:.2f}%")

print(f"\nâœ… V10_4c 30ë¶„ í‰ê°€ ì™„ë£Œ! â†’ {CONFIG['output_file']}")
print("=" * 70)