# -*- coding: utf-8 -*-
"""
================================================================================
V10_4 ML ì˜ˆì¸¡ ëª¨ë¸ - í‰ê°€ ì½”ë“œ (30ë¶„ ì˜ˆì¸¡)
V10_3 ë°©ì‹ (XGBoost íšŒê·€ + LightGBM ë¶„ë¥˜ + íˆ¬í‘œ) + 30ë¶„ ì˜ˆì¸¡
================================================================================
"""

import os
import pickle
import warnings
import gc
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

warnings.filterwarnings('ignore')

# ============================================================================
# ì„¤ì •
# ============================================================================
CONFIG = {
    'model_file': 'models/v10_4_30min_m14_model.pkl',
    'eval_file': 'M14_í‰ê°€.CSV',
    'output_file': f'V10_4_30min_í‰ê°€ê²°ê³¼_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
    'sequence_length': 280,
    'prediction_offset': 30,  # 30ë¶„ í›„!
    'limit_value': 1700,
    'target_column': 'TOTALCNT',
}

print("=" * 70)
print("ğŸš€ V10_4 ML ì˜ˆì¸¡ ëª¨ë¸ - í‰ê°€ ì‹œì‘ (30ë¶„ ì˜ˆì¸¡)")
print("   V10_3 ë°©ì‹ (XGB íšŒê·€ + LGBM ë¶„ë¥˜ + íˆ¬í‘œ)")
print("   ì‹œí€€ìŠ¤: 280ë¶„, ì˜ˆì¸¡: 30ë¶„ í›„")
print("=" * 70)

# ============================================================================
# ëª¨ë¸ ë¡œë“œ
# ============================================================================
print("\n[1/5] ëª¨ë¸ ë¡œë“œ ì¤‘...")

with open(CONFIG['model_file'], 'rb') as f:
    model_data = pickle.load(f)

models = model_data['models']
scalers = model_data['scalers']
FEATURE_GROUPS = model_data['feature_groups']
training_info = model_data.get('training_info', {})

print(f"  - ëª¨ë¸ ë²„ì „: {training_info.get('version', 'V10_4')}")
print(f"  - í•™ìŠµì¼: {training_info.get('train_date', 'N/A')}")
print(f"  - ëª¨ë¸ ìˆ˜: {len(models)}ê°œ")

# ============================================================================
# ë°ì´í„° ë¡œë“œ
# ============================================================================
print("\n[2/5] í‰ê°€ ë°ì´í„° ë¡œë“œ ì¤‘...")

try:
    df = pd.read_csv(CONFIG['eval_file'], encoding='utf-8', on_bad_lines='skip')
except:
    try:
        df = pd.read_csv(CONFIG['eval_file'], encoding='cp949', on_bad_lines='skip')
    except:
        df = pd.read_csv(CONFIG['eval_file'], encoding='euc-kr', on_bad_lines='skip')

print(f"  - ì›ë³¸ ë°ì´í„°: {len(df):,}í–‰")

df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), format='%Y%m%d%H%M', errors='coerce')
df = df.dropna(subset=['CURRTIME']).sort_values('CURRTIME').reset_index(drop=True)
print(f"  - íŒŒì‹± í›„ ë°ì´í„°: {len(df):,}í–‰")

# QUEUE_GAP íŒŒìƒ (ë¨¼ì €!)
if 'M14.QUE.ALL.CURRENTQCREATED' in df.columns and 'M14.QUE.ALL.CURRENTQCOMPLETED' in df.columns:
    df['QUEUE_GAP'] = df['M14.QUE.ALL.CURRENTQCREATED'] - df['M14.QUE.ALL.CURRENTQCOMPLETED']
    print("  - QUEUE_GAP íŒŒìƒ ë³€ìˆ˜ ìƒì„±!")

# Feature ê·¸ë£¹ í•„í„°ë§
for group_name in FEATURE_GROUPS:
    original = FEATURE_GROUPS[group_name].copy()
    FEATURE_GROUPS[group_name] = [f for f in FEATURE_GROUPS[group_name] if f in df.columns]
    missing = set(original) - set(FEATURE_GROUPS[group_name])
    if missing:
        print(f"  âš  {group_name} ëˆ„ë½: {missing}")

# ìˆ«ìí˜• ë³€í™˜
all_cols = []
for group in FEATURE_GROUPS.values():
    all_cols.extend(group)
for col in list(set(all_cols)):
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

# ============================================================================
# Feature ìƒì„± í•¨ìˆ˜
# ============================================================================
def create_sequence_features(df, feature_cols, seq_len, idx, limit_val=1700):
    features = []
    for col in feature_cols:
        seq = df[col].iloc[idx - seq_len:idx].values
        current_val = seq[-1]
        features.extend([
            np.mean(seq), np.std(seq), np.min(seq), np.max(seq), current_val,
            seq[-1] - seq[0],
            np.percentile(seq, 25), np.percentile(seq, 75),
            np.mean(seq[-10:]) - np.mean(seq[:10]),
            np.max(seq[-30:]) if len(seq) >= 30 else np.max(seq),
            seq[-1] - seq[-10] if len(seq) >= 10 else 0,
            seq[-1] - seq[-30] if len(seq) >= 30 else 0,
            seq[-1] - seq[-60] if len(seq) >= 60 else 0,
            (seq[-1] - seq[-10]) / 10 if len(seq) >= 10 else 0,
            (seq[-1] - seq[-30]) / 30 if len(seq) >= 30 else 0,
            np.max(seq[-10:]) - np.min(seq[-10:]) if len(seq) >= 10 else 0,
            np.max(seq[-30:]) - np.min(seq[-30:]) if len(seq) >= 30 else 0,
            limit_val - current_val,
            1 if current_val >= 1500 else 0,
            1 if current_val >= 1600 else 0,
        ])
    return features

# ============================================================================
# í‰ê°€ ìˆ˜í–‰
# ============================================================================
print("\n[3/5] ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")

seq_len = CONFIG['sequence_length']
pred_offset = CONFIG['prediction_offset']
limit_val = CONFIG['limit_value']
target_col = CONFIG['target_column']

results = []
total = len(df) - seq_len - pred_offset
print(f"  â†’ ì˜ˆìƒ í‰ê°€ ìˆ˜: {total:,}ê°œ")

for idx in range(seq_len, len(df) - pred_offset):
    if (idx - seq_len) % 1000 == 0:
        print(f"    ì§„í–‰: {idx - seq_len:,}/{total:,}")
        gc.collect()
    
    current_time = df['CURRTIME'].iloc[idx - 1]
    current_total = df[target_col].iloc[idx - 1]
    prediction_time = current_time + timedelta(minutes=pred_offset)
    
    # ì‹¤ì œê°’: 30ë¶„ ë‚´ ìµœëŒ€ê°’
    future_end = min(idx - 1 + pred_offset, len(df))
    actual_max = df[target_col].iloc[idx - 1:future_end].max()
    
    # ì‹¤ì œë‹¨ì¼ê°’: ì •í™•íˆ 30ë¶„ í›„ ê°’ (ì˜ˆì¸¡ ì‹œì ì˜ ì‹¤ì œê°’)
    actual_single_idx = idx - 1 + pred_offset
    if actual_single_idx < len(df):
        actual_single = df[target_col].iloc[actual_single_idx]
    else:
        actual_single = df[target_col].iloc[-1]
    
    # Feature ìƒì„±
    feat_target = create_sequence_features(df, FEATURE_GROUPS['target'], seq_len, idx)
    feat_important = create_sequence_features(df, FEATURE_GROUPS['important'], seq_len, idx)
    feat_auxiliary = create_sequence_features(df, FEATURE_GROUPS['auxiliary'], seq_len, idx)
    
    X_target = scalers['target'].transform([feat_target])
    X_important = scalers['important'].transform([feat_important])
    X_auxiliary = scalers['auxiliary'].transform([feat_auxiliary])
    
    # XGB íšŒê·€ ì˜ˆì¸¡
    pred_xgb_target = models['xgb_target'].predict(X_target)[0]
    pred_xgb_important = models['xgb_important'].predict(X_important)[0]
    pred_xgb_auxiliary = models['xgb_auxiliary'].predict(X_auxiliary)[0]
    
    # LGBM ë¶„ë¥˜ ì˜ˆì¸¡
    pred_lgb_target = models['lgb_target'].predict(X_target)[0]
    pred_lgb_important = models['lgb_important'].predict(X_important)[0]
    pred_lgb_auxiliary = models['lgb_auxiliary'].predict(X_auxiliary)[0]
    
    prob_lgb_target = models['lgb_target'].predict_proba(X_target)[0][1]
    prob_lgb_important = models['lgb_important'].predict_proba(X_important)[0][1]
    prob_lgb_auxiliary = models['lgb_auxiliary'].predict_proba(X_auxiliary)[0][1]
    
    # PDT ëª¨ë¸ (ìˆìœ¼ë©´)
    pred_xgb_pdt, pred_lgb_pdt, prob_lgb_pdt = None, None, None
    if 'xgb_pdt_new' in models and 'pdt_new' in scalers:
        feat_pdt = create_sequence_features(df, FEATURE_GROUPS.get('pdt_new', []), seq_len, idx)
        if feat_pdt:
            X_pdt = scalers['pdt_new'].transform([feat_pdt])
            pred_xgb_pdt = models['xgb_pdt_new'].predict(X_pdt)[0]
            pred_lgb_pdt = models['lgb_pdt_new'].predict(X_pdt)[0]
            prob_lgb_pdt = models['lgb_pdt_new'].predict_proba(X_pdt)[0][1]
    
    # ============================================
    # íˆ¬í‘œ (V10_3 ë°©ì‹)
    # ============================================
    votes = [
        1 if pred_xgb_target >= limit_val else 0,
        1 if pred_xgb_important >= limit_val else 0,
        1 if pred_xgb_auxiliary >= limit_val else 0,
        pred_lgb_target,
        pred_lgb_important,
        pred_lgb_auxiliary,
    ]
    
    if pred_xgb_pdt is not None:
        votes.append(1 if pred_xgb_pdt >= limit_val else 0)
        votes.append(pred_lgb_pdt)
    
    vote_sum = sum(votes)
    total_votes = len(votes)
    
    # ============================================
    # ìµœì¢… íŒì • ê·œì¹™ (V10_3 ë°©ì‹)
    # ============================================
    rule1 = vote_sum >= 3
    rule2 = (prob_lgb_important >= 0.50) and (current_total >= 1450)
    rule3 = (pred_xgb_important >= 1680) and (current_total >= 1500)
    rule4 = (current_total >= 1600) and (vote_sum >= 2)
    rule5 = (pred_xgb_important >= 1700)
    
    final_pred_danger = 1 if (rule1 or rule2 or rule3 or rule4 or rule5) else 0
    
    # ì•™ìƒë¸” í‰ê·  (íšŒê·€ê°’)
    if pred_xgb_pdt is not None:
        ensemble_pred = (pred_xgb_target + pred_xgb_important + pred_xgb_auxiliary + pred_xgb_pdt) / 4
    else:
        ensemble_pred = (pred_xgb_target + pred_xgb_important + pred_xgb_auxiliary) / 3
    
    # ê²°ê³¼ ì €ì¥
    results.append({
        'í˜„ì¬ì‹œê°„': current_time.strftime('%Y-%m-%d %H:%M'),
        'í˜„ì¬TOTALCNT': round(current_total, 2),
        'ì˜ˆì¸¡ì‹œì ': prediction_time.strftime('%Y-%m-%d %H:%M'),
        'ì‹¤ì œê°’10min': round(actual_max, 2),
        'ì‹¤ì œë‹¨ì¼ê°’': round(actual_single, 2),
        'XGB_íƒ€ê²Ÿ': round(pred_xgb_target, 2),
        'XGB_ì¤‘ìš”': round(pred_xgb_important, 2),
        'XGB_ë³´ì¡°': round(pred_xgb_auxiliary, 2),
        'XGB_PDT': round(pred_xgb_pdt, 2) if pred_xgb_pdt else '',
        'LGBM_ì¤‘ìš”_í™•ë¥ ': round(prob_lgb_important, 3),
        'ì•™ìƒë¸”ì˜ˆì¸¡': round(ensemble_pred, 2),
        f'íˆ¬í‘œ({total_votes}ê°œì¤‘)': vote_sum,
        'ìµœì¢…íŒì •': final_pred_danger,
        'ì‹¤ì œìœ„í—˜(1700+)': 1 if actual_max >= limit_val else 0,
    })

print(f"  âœ… ì™„ë£Œ!")

# ============================================================================
# ê²°ê³¼ ì €ì¥
# ============================================================================
print("\n[4/5] ê²°ê³¼ ì €ì¥...")

df_result = pd.DataFrame(results)

# ============================================================================
# ì˜ˆì¸¡ìƒíƒœ ë¶„ë¥˜ (10ë¶„ ì „/í›„ ë¶„ì„)
# ============================================================================
print("\n  â†’ ì˜ˆì¸¡ìƒíƒœ ë¶„ë¥˜ ì¤‘ (10ë¶„ ì „/í›„ ë¶„ì„)...")

df_result['í˜„ì¬ì‹œê°„_dt'] = pd.to_datetime(df_result['í˜„ì¬ì‹œê°„'])

def get_prediction_status(row, all_df):
    actual = row['ì‹¤ì œìœ„í—˜(1700+)']
    pred = row['ìµœì¢…íŒì •']
    current_time = row['í˜„ì¬ì‹œê°„_dt']
    
    if actual == 1 and pred == 1:
        return 'ì •ìƒì˜ˆì¸¡_TP'
    elif actual == 0 and pred == 0:
        return 'ì •ìƒì˜ˆì¸¡_TN'
    elif actual == 1 and pred == 0:
        # 30ë¶„ ì˜ˆì¸¡: 20ë¶„ ì´ë‚´ì— ì˜ˆì¸¡ëœ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
        for minutes_ago in range(1, 21):  # 1~20ë¶„ ì „ í™•ì¸
            time_ago = current_time - timedelta(minutes=minutes_ago)
            prev_data = all_df[all_df['í˜„ì¬ì‹œê°„_dt'] == time_ago]
            if len(prev_data) > 0 and prev_data['ìµœì¢…íŒì •'].values[0] == 1:
                return 'FN_20ë¶„ë‚´ì˜ˆì¸¡'
        return 'FN_ì™„ì „ë†“ì¹¨'
    else:
        # 30ë¶„ ì˜ˆì¸¡: 20ë¶„ ì´ë‚´ì— ì‹¤ì œ ëŒíŒŒê°€ ìˆëŠ”ì§€ í™•ì¸
        for minutes_later in range(1, 21):  # 1~20ë¶„ í›„ í™•ì¸
            time_later = current_time + timedelta(minutes=minutes_later)
            later_data = all_df[all_df['í˜„ì¬ì‹œê°„_dt'] == time_later]
            if len(later_data) > 0 and later_data['ì‹¤ì œìœ„í—˜(1700+)'].values[0] == 1:
                return 'FP_20ë¶„ë‚´ëŒíŒŒ'
        return 'FP_ì˜ëª»ëœê²½ê³ '

df_result['ì˜ˆì¸¡ìƒíƒœ'] = df_result.apply(lambda row: get_prediction_status(row, df_result), axis=1)
df_result = df_result.drop(columns=['í˜„ì¬ì‹œê°„_dt'])

df_result.to_csv(CONFIG['output_file'], index=False, encoding='utf-8-sig')
print(f"  â†’ ì €ì¥: {CONFIG['output_file']}")

# ============================================================================
# ì„±ëŠ¥ í‰ê°€
# ============================================================================
print("\n[5/5] ì„±ëŠ¥ í‰ê°€...")

print("\n" + "=" * 70)
print("ğŸ“Š V10_4 30ë¶„ í‰ê°€ í†µê³„")
print("=" * 70)

actual_danger = df_result['ì‹¤ì œìœ„í—˜(1700+)'] == 1
pred_danger = df_result['ìµœì¢…íŒì •'] == 1

TP = (actual_danger & pred_danger).sum()
TN = (~actual_danger & ~pred_danger).sum()
FP = (~actual_danger & pred_danger).sum()
FN = (actual_danger & ~pred_danger).sum()

print(f"ì´ ì˜ˆì¸¡: {len(df_result):,}ê°œ")
print(f"ì‹¤ì œ 1700+: {actual_danger.sum()}ê°œ")

print(f"\nğŸ”¥ 1700+ ê°ì§€:")
if actual_danger.sum() > 0:
    recall = TP / actual_danger.sum() * 100
    print(f"  ê°ì§€(TP): {TP}ê°œ ({recall:.1f}%)")
    print(f"  ë¯¸ê°ì§€(FN): {FN}ê°œ ({100-recall:.1f}%)")

print(f"\nâš ï¸ ì˜¤íƒ(FP): {FP}ê°œ")
if pred_danger.sum() > 0:
    precision = TP / pred_danger.sum() * 100
    print(f"ì •ë°€ë„: {precision:.1f}%")

print(f"\n[í˜¼ë™ í–‰ë ¬]")
print(f"  TP: {TP:,}ê±´ | FN: {FN:,}ê±´")
print(f"  FP: {FP:,}ê±´ | TN: {TN:,}ê±´")

# ì˜ˆì¸¡ìƒíƒœë³„ ì§‘ê³„
print(f"\n[ì˜ˆì¸¡ìƒíƒœ ë¶„ë¥˜]")
status_counts = df_result['ì˜ˆì¸¡ìƒíƒœ'].value_counts()
for status, count in status_counts.items():
    print(f"  {status}: {count:,}ê±´")

# ì‹¤ì§ˆ FN/FP
real_fn = status_counts.get('FN_ì™„ì „ë†“ì¹¨', 0)
real_fp = status_counts.get('FP_ì˜ëª»ëœê²½ê³ ', 0)
fn_early = status_counts.get('FN_20ë¶„ë‚´ì˜ˆì¸¡', 0)
fp_early = status_counts.get('FP_20ë¶„ë‚´ëŒíŒŒ', 0)

print(f"\n[ì‹¤ì§ˆì  ì„±ëŠ¥ (20ë¶„ ê¸°ì¤€)]")
print(f"  FN 20ë¶„ë‚´ì˜ˆì¸¡ (ì¡°ê¸°ê°ì§€): {fn_early:,}ê±´")
print(f"  FP 20ë¶„ë‚´ëŒíŒŒ (ìœ íš¨ê²½ê³ ): {fp_early:,}ê±´")
print(f"  ì‹¤ì§ˆ FN (ì™„ì „ ë†“ì¹¨):    {real_fn:,}ê±´")
print(f"  ì‹¤ì§ˆ FP (ì˜ëª»ëœ ê²½ê³ ):  {real_fp:,}ê±´")

if (TP + TN + FP + FN) > 0:
    accuracy = (TP + TN) / (TP + TN + FP + FN) * 100
    print(f"\nì •í™•ë„: {accuracy:.2f}%")

print(f"\nâœ… V10_4 í‰ê°€ ì™„ë£Œ! â†’ {CONFIG['output_file']}")
print("=" * 70)