# -*- coding: utf-8 -*-
"""
================================================================================
V11 ML ì˜ˆì¸¡ ëª¨ë¸ - í‰ê°€ ì½”ë“œ
V8.3.1 + V10.3 í†µí•© (XGBoost ì „ìš©)

íŠ¹ì§•:
    - ì‹œí€€ìŠ¤: 280ë¶„ (V8.3.1 ê¸°ì¤€)
    - ì˜ˆì¸¡: 10ë¶„ í›„ ë‹¨ì¼ê°’ (V8.3.1 ê¸°ì¤€)
    - ì¶œë ¥: V8.3.1 í˜•ì‹ (ì¶”ì„¸ì˜ˆì¸¡ í¬í•¨)
    - Feature ê·¸ë£¹: target, important, auxiliary (V10.3 êµ¬ì¡°)
    - ëª¨ë¸: XGBoostë§Œ ì‚¬ìš©
================================================================================
"""

import os
import pickle
import warnings
import gc
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

warnings.filterwarnings('ignore')

# ============================================================================
# ì„¤ì •
# ============================================================================
CONFIG = {
    'model_file': 'models/v11_m14_model.pkl',
    'eval_file': 'M14_í‰ê°€.CSV',  # í‰ê°€ íŒŒì¼ëª… ìˆ˜ì • í•„ìš”
    'output_file': f'V11_í‰ê°€ê²°ê³¼_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
    'sequence_length': 280,
    'prediction_offset': 10,
    'limit_value': 1700,
    'target_column': 'TOTALCNT',
}

print("=" * 70)
print("ğŸš€ V11 ML ì˜ˆì¸¡ ëª¨ë¸ - í‰ê°€ ì‹œì‘")
print("   V8.3.1 + V10.3 í†µí•© (XGBoost ì „ìš©)")
print("   ì‹œí€€ìŠ¤: 280ë¶„, ì˜ˆì¸¡: 10ë¶„ í›„")
print("=" * 70)

# ============================================================================
# ëª¨ë¸ ë¡œë“œ
# ============================================================================
print("\n[1/5] ëª¨ë¸ ë¡œë“œ ì¤‘...")

with open(CONFIG['model_file'], 'rb') as f:
    model_data = pickle.load(f)

models = model_data['models']
scalers = model_data['scalers']
FEATURE_GROUPS = model_data['feature_groups']
training_info = model_data.get('training_info', {})

print(f"  - ëª¨ë¸ ë²„ì „: {training_info.get('version', 'V11')}")
print(f"  - í•™ìŠµì¼: {training_info.get('train_date', 'N/A')}")
print(f"  - í•™ìŠµ ìƒ˜í”Œ: {training_info.get('samples', 'N/A'):,}ê°œ")

# ============================================================================
# ë°ì´í„° ë¡œë“œ
# ============================================================================
print("\n[2/5] í‰ê°€ ë°ì´í„° ë¡œë“œ ì¤‘...")

try:
    df = pd.read_csv(CONFIG['eval_file'], encoding='utf-8', on_bad_lines='skip')
except:
    try:
        df = pd.read_csv(CONFIG['eval_file'], encoding='cp949', on_bad_lines='skip')
    except:
        df = pd.read_csv(CONFIG['eval_file'], encoding='euc-kr', on_bad_lines='skip')

print(f"  - ì›ë³¸ ë°ì´í„°: {len(df):,}í–‰")

# CURRTIME íŒŒì‹±
df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), format='%Y%m%d%H%M', errors='coerce')
df = df.dropna(subset=['CURRTIME'])
df = df.sort_values('CURRTIME').reset_index(drop=True)
print(f"  - íŒŒì‹± í›„ ë°ì´í„°: {len(df):,}í–‰")

# Feature ê·¸ë£¹ í•„í„°ë§
for group_name in FEATURE_GROUPS:
    original = FEATURE_GROUPS[group_name].copy()
    FEATURE_GROUPS[group_name] = [f for f in FEATURE_GROUPS[group_name] if f in df.columns]
    missing = set(original) - set(FEATURE_GROUPS[group_name])
    if missing:
        print(f"  âš  {group_name} ëˆ„ë½ ì»¬ëŸ¼: {missing}")

# ìˆ«ìí˜• ë³€í™˜
all_cols = []
for group in FEATURE_GROUPS.values():
    all_cols.extend(group)
all_cols = list(set(all_cols))

for col in all_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

# queue_gap íŒŒìƒ ë³€ìˆ˜
if 'M14.QUE.ALL.CURRENTQCREATED' in df.columns and 'M14.QUE.ALL.CURRENTQCOMPLETED' in df.columns:
    df['QUEUE_GAP'] = df['M14.QUE.ALL.CURRENTQCREATED'] - df['M14.QUE.ALL.CURRENTQCOMPLETED']
    if 'QUEUE_GAP' not in FEATURE_GROUPS['auxiliary']:
        FEATURE_GROUPS['auxiliary'].append('QUEUE_GAP')

print(f"  - Target ì»¬ëŸ¼: {len(FEATURE_GROUPS['target'])}ê°œ")
print(f"  - Important ì»¬ëŸ¼: {len(FEATURE_GROUPS['important'])}ê°œ")
print(f"  - Auxiliary ì»¬ëŸ¼: {len(FEATURE_GROUPS['auxiliary'])}ê°œ")

# ============================================================================
# Feature ìƒì„± í•¨ìˆ˜
# ============================================================================
def create_sequence_features(df, feature_cols, seq_len, idx, limit_val=1700):
    """ì‹œí€€ìŠ¤ ê¸°ë°˜ Feature ìƒì„±"""
    features = []
    
    for col in feature_cols:
        seq = df[col].iloc[idx - seq_len:idx].values
        current_val = seq[-1]
        
        features.extend([
            np.mean(seq),
            np.std(seq),
            np.min(seq),
            np.max(seq),
            current_val,
            seq[-1] - seq[0],
            np.percentile(seq, 25),
            np.percentile(seq, 75),
            np.mean(seq[-10:]) - np.mean(seq[:10]),
            np.max(seq[-30:]) if len(seq) >= 30 else np.max(seq),
            seq[-1] - seq[-10] if len(seq) >= 10 else 0,
            seq[-1] - seq[-30] if len(seq) >= 30 else 0,
            seq[-1] - seq[-60] if len(seq) >= 60 else 0,
            (seq[-1] - seq[-10]) / 10 if len(seq) >= 10 else 0,
            (seq[-1] - seq[-30]) / 30 if len(seq) >= 30 else 0,
            np.max(seq[-10:]) - np.min(seq[-10:]) if len(seq) >= 10 else 0,
            np.max(seq[-30:]) - np.min(seq[-30:]) if len(seq) >= 30 else 0,
            limit_val - current_val,
            1 if current_val >= 1500 else 0,
            1 if current_val >= 1600 else 0,
        ])
    
    return features

# ============================================================================
# í‰ê°€ ìˆ˜í–‰
# ============================================================================
print("\n[3/5] ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")

seq_len = CONFIG['sequence_length']
pred_offset = CONFIG['prediction_offset']
limit_val = CONFIG['limit_value']
target_col = CONFIG['target_column']

results = []
total = len(df) - seq_len - pred_offset
print(f"  â†’ ì˜ˆìƒ í‰ê°€ ìˆ˜: {total:,}ê°œ")

for idx in range(seq_len, len(df) - pred_offset):
    if (idx - seq_len) % 1000 == 0:
        print(f"    ì§„í–‰: {idx - seq_len:,}/{total:,} ({100*(idx-seq_len)/total:.1f}%)")
        gc.collect()
    
    # í˜„ì¬ ì‹œê°„ ë° ê°’
    current_time = df['CURRTIME'].iloc[idx - 1]
    current_total = df[target_col].iloc[idx - 1]
    prediction_time = current_time + timedelta(minutes=pred_offset)
    
    # ì‹¤ì œê°’ (10ë¶„ í›„)
    actual = df[target_col].iloc[idx + pred_offset - 1]
    
    # ë³´ì¡° ë³€ìˆ˜ ì¶”ì¶œ
    m14b = df['M14AM14B'].iloc[idx - 1] if 'M14AM14B' in df.columns else None
    m14bsum = df['M14AM14BSUM'].iloc[idx - 1] if 'M14AM14BSUM' in df.columns else None
    gap = df['QUEUE_GAP'].iloc[idx - 1] if 'QUEUE_GAP' in df.columns else None
    trans = df['M14.QUE.ALL.TRANSPORT4MINOVERCNT'].iloc[idx - 1] if 'M14.QUE.ALL.TRANSPORT4MINOVERCNT' in df.columns else None
    
    # Feature ìƒì„±
    feat_target = create_sequence_features(df, FEATURE_GROUPS['target'], seq_len, idx)
    feat_important = create_sequence_features(df, FEATURE_GROUPS['important'], seq_len, idx)
    feat_auxiliary = create_sequence_features(df, FEATURE_GROUPS['auxiliary'], seq_len, idx)
    feat_pdt_new = create_sequence_features(df, FEATURE_GROUPS.get('pdt_new', []), seq_len, idx)
    
    # ìŠ¤ì¼€ì¼ë§
    X_target = scalers['target'].transform([feat_target])
    X_important = scalers['important'].transform([feat_important])
    X_auxiliary = scalers['auxiliary'].transform([feat_auxiliary])
    
    # XGBoost ì˜ˆì¸¡
    pred_xgb_target = models['xgb_target'].predict(X_target)[0]
    pred_xgb_important = models['xgb_important'].predict(X_important)[0]
    pred_xgb_auxiliary = models['xgb_auxiliary'].predict(X_auxiliary)[0]
    
    # pdt_new ëª¨ë¸ ì˜ˆì¸¡ (ìˆëŠ” ê²½ìš°)
    if 'xgb_pdt_new' in models and 'pdt_new' in scalers and len(feat_pdt_new) > 0:
        X_pdt_new = scalers['pdt_new'].transform([feat_pdt_new])
        pred_xgb_pdt_new = models['xgb_pdt_new'].predict(X_pdt_new)[0]
        # ì•™ìƒë¸” (4ê°œ í‰ê· )
        final_pred = (pred_xgb_target + pred_xgb_important + pred_xgb_auxiliary + pred_xgb_pdt_new) / 4
    else:
        pred_xgb_pdt_new = None
        # ì•™ìƒë¸” (3ê°œ í‰ê· )
        final_pred = (pred_xgb_target + pred_xgb_important + pred_xgb_auxiliary) / 3
    
    # ê²°ê³¼ ì €ì¥
    results.append({
        'í˜„ì¬ì‹œê°„': current_time.strftime('%Y-%m-%d %H:%M'),
        'í˜„ì¬TOTALCNT': round(current_total, 2),
        'ì˜ˆì¸¡ì‹œì ': prediction_time.strftime('%Y-%m-%d %H:%M'),
        'ì‹¤ì œê°’': round(actual, 2),
        'XGB_íƒ€ê²Ÿ': round(pred_xgb_target, 2),
        'XGB_ì¤‘ìš”': round(pred_xgb_important, 2),
        'XGB_ë³´ì¡°': round(pred_xgb_auxiliary, 2),
        'XGB_PDT': round(pred_xgb_pdt_new, 2) if pred_xgb_pdt_new is not None else '',
        'ìµœì¢…ì˜ˆì¸¡': round(final_pred, 2),
        'ì˜¤ì°¨': round(actual - final_pred, 2),
        'ì ˆëŒ€ì˜¤ì°¨': round(abs(actual - final_pred), 2),
        'M14AM14B': round(m14b, 2) if m14b else '',
        'M14AM14BSUM': round(m14bsum, 2) if m14bsum else '',
        'queue_gap': round(gap, 2) if gap else '',
        'TRANSPORT': round(trans, 2) if trans else '',
        'ì‹¤ì œìœ„í—˜(1700+)': 'O' if actual >= 1700 else '',
        'ì˜ˆì¸¡ìœ„í—˜(1700+)': 'O' if final_pred >= 1700 else ''
    })

print(f"  âœ… ì™„ë£Œ!")

# ============================================================================
# ê²°ê³¼ ì €ì¥
# ============================================================================
print("\n[4/5] ê²°ê³¼ ì €ì¥...")

df_result = pd.DataFrame(results)
df_result.to_csv(CONFIG['output_file'], index=False, encoding='utf-8-sig')
print(f"  â†’ ì €ì¥: {CONFIG['output_file']}")

# ============================================================================
# ì„±ëŠ¥ í‰ê°€
# ============================================================================
print("\n[5/5] ì„±ëŠ¥ í‰ê°€...")

print("\n" + "=" * 70)
print("ğŸ“Š V11 10ë¶„ í‰ê°€ í†µê³„")
print("=" * 70)

print(f"ì´ ì˜ˆì¸¡: {len(df_result):,}ê°œ")
print(f"MAE: {df_result['ì ˆëŒ€ì˜¤ì°¨'].mean():.2f}")

actual_danger = df_result['ì‹¤ì œìœ„í—˜(1700+)'] == 'O'
pred_danger = df_result['ì˜ˆì¸¡ìœ„í—˜(1700+)'] == 'O'
actual_count = actual_danger.sum()
detected = (actual_danger & pred_danger).sum()

print(f"\nğŸ”¥ 1700+ ê°ì§€ (ìµœì¢…ì˜ˆì¸¡ ê¸°ì¤€):")
print(f"  ì‹¤ì œ: {actual_count}ê°œ")

if actual_count > 0:
    recall = detected / actual_count * 100
    print(f"  ê°ì§€(TP): {detected}ê°œ ({recall:.1f}%)")
    
    # ë¯¸ê°ì§€ ë¶„ì„
    fn = actual_count - detected
    print(f"  ë¯¸ê°ì§€(FN): {fn}ê°œ ({fn/actual_count*100:.1f}%)")

# ì˜¤íƒ ë¶„ì„
fp = (pred_danger & ~actual_danger).sum()
print(f"\nâš ï¸ ì˜¤íƒ(FP): {fp}ê°œ")

if pred_danger.sum() > 0:
    precision = detected / pred_danger.sum() * 100
    print(f"ì •ë°€ë„: {precision:.1f}%")

# êµ¬ê°„ë³„ MAE
print(f"\nğŸ“Š êµ¬ê°„ë³„ MAE:")
for low, high in [(0, 1500), (1500, 1600), (1600, 1700), (1700, 2000)]:
    mask = (df_result['ì‹¤ì œê°’'] >= low) & (df_result['ì‹¤ì œê°’'] < high)
    if mask.sum() > 0:
        mae = df_result.loc[mask, 'ì ˆëŒ€ì˜¤ì°¨'].mean()
        print(f"  {low}~{high}: MAE={mae:.2f} ({mask.sum():,}ê°œ)")

# í˜¼ë™ í–‰ë ¬
TP = detected
TN = (~actual_danger & ~pred_danger).sum()
FP = fp
FN = (actual_danger & ~pred_danger).sum()

print(f"\n[í˜¼ë™ í–‰ë ¬]")
print(f"  TP (ì •í™•í•œ ìœ„í—˜ ì˜ˆì¸¡): {TP:,}ê±´")
print(f"  TN (ì •í™•í•œ ì•ˆì „ ì˜ˆì¸¡): {TN:,}ê±´")
print(f"  FP (ì˜¤íƒ):              {FP:,}ê±´")
print(f"  FN (ë¯¸ê°ì§€):            {FN:,}ê±´")

if (TP + TN + FP + FN) > 0:
    accuracy = (TP + TN) / (TP + TN + FP + FN) * 100
    print(f"\nì •í™•ë„: {accuracy:.2f}%")

print(f"\nâœ… V11 10ë¶„ í‰ê°€ ì™„ë£Œ! â†’ {CONFIG['output_file']}")
print("=" * 70)