#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì½”ë”© ì „ìš© LLM ì„œë²„ (v1.0)
- ì½”ë“œ ìƒì„±, ë¦¬ë·°, ë²„ê·¸ ìˆ˜ì •, ì„¤ëª…, ë¦¬íŒ©í† ë§
- SK Hynix ë‚´ë¶€ API ì—°ë™
"""

import os
import re
import requests
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Coding LLM Tool")

# ========================================
# Global Variables
# ========================================
API_TOKEN = None
llm = None  # ë¡œì»¬ LLM (GGUF)

# LLM ëª¨ë“œ: "api" ë˜ëŠ” "local"
LLM_MODE = "api"

# ê°œë°œ/ìš´ì˜ í™˜ê²½ ì„¤ì •
ENV_MODE = "common"

# ìš”ì²­ ì·¨ì†Œ ê´€ë¦¬
import threading
active_requests = {}  # {request_id: threading.Event}
request_lock = threading.Lock()

# ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ (í´ë°±ìš©)
LOCAL_MODEL_PATH = "Qwen3-14B-Q4_K_M.gguf"  # ë˜ëŠ” ë‹¤ë¥¸ GGUF ëª¨ë¸

ENV_CONFIG = {
    "dev": {
        "url": "http://dev.assistant.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Coder-30B-A3B-Instruct",
        "name": "DEV(30B)"
    },
    "prod": {
        "url": "http://summary.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Next-80B-A3B-Instruct",
        "name": "PROD(80B)"
    },
    "common": {
        "url": "http://common.llm.skhynix.com/v1/chat/completions",
        "model": "gpt-oss-20b",
        "name": "COMMON(20B)"
    }
}

API_URL = ENV_CONFIG["common"]["url"]
API_MODEL = ENV_CONFIG["common"]["model"]

# íŒ€ ê¶Œí•œìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸
# - Qwen3-Coder-30B-A3B-Instruct (ì½”ë”© íŠ¹í™”)
# - Qwen3-Next-80B-A3B-Instruct (ë²”ìš© ëŒ€í˜•)
# - Qwen3-VL-30B-A3B-Instruct (ë¹„ì „+ì–¸ì–´)
# - InternVL3_5-241B-A28B (ë¹„ì „)
# 480B ëª¨ë¸ì€ íŒ€ ê¶Œí•œ í•„ìš”!

# ========================================
# ëª¨ë“œë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# ========================================
SYSTEM_PROMPTS = {
    "generate": """ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ê³ í’ˆì§ˆ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ê·œì¹™:
1. ê¹”ë”í•˜ê³  ì½ê¸° ì‰¬ìš´ ì½”ë“œ ì‘ì„±
2. ì ì ˆí•œ ì£¼ì„ í¬í•¨
3. ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨
4. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì¤€ìˆ˜
5. ì½”ë“œ ë¸”ë¡ì€ ```ì–¸ì–´ëª… ìœ¼ë¡œ ê°ì‹¸ê¸°

í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ê³ , ì½”ë“œëŠ” ìš”ì²­ëœ ì–¸ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.""",

    "review": """ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì½”ë“œ ë¦¬ë·°ì–´ì…ë‹ˆë‹¤.
ì œì¶œëœ ì½”ë“œë¥¼ ì² ì €íˆ ê²€í† í•˜ê³  ê°œì„ ì ì„ ì œì•ˆí•©ë‹ˆë‹¤.

ê²€í†  í•­ëª©:
1. ì½”ë“œ í’ˆì§ˆ ë° ê°€ë…ì„±
2. ë²„ê·¸ ë˜ëŠ” ì ì¬ì  ë¬¸ì œ
3. ì„±ëŠ¥ ìµœì í™” ê°€ëŠ¥ì„±
4. ë³´ì•ˆ ì·¨ì•½ì 
5. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì¤€ìˆ˜ ì—¬ë¶€
6. í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ì„±

í•œêµ­ì–´ë¡œ ìƒì„¸íˆ í”¼ë“œë°±í•˜ì„¸ìš”. ì ìˆ˜(1-10)ë„ ë§¤ê²¨ì£¼ì„¸ìš”.""",

    "debug": """ë‹¹ì‹ ì€ ë””ë²„ê¹… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë²„ê·¸ë¥¼ ì°¾ì•„ ìˆ˜ì •í•˜ê³  ì›ì¸ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

ì ‘ê·¼ ë°©ì‹:
1. ì—ëŸ¬ ë©”ì‹œì§€ ë¶„ì„
2. ë²„ê·¸ ì›ì¸ íŒŒì•…
3. ìˆ˜ì •ëœ ì½”ë“œ ì œê³µ
4. ì™œ ë²„ê·¸ê°€ ë°œìƒí–ˆëŠ”ì§€ ì„¤ëª…
5. í–¥í›„ ì˜ˆë°©ë²• ì œì•ˆ

í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ê³ , ìˆ˜ì •ëœ ì½”ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.""",

    "explain": """ë‹¹ì‹ ì€ í”„ë¡œê·¸ë˜ë° êµì‚¬ì…ë‹ˆë‹¤.
ì½”ë“œë¥¼ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…í•©ë‹ˆë‹¤.

ì„¤ëª… í¬í•¨ ì‚¬í•­:
1. ì „ì²´ ì½”ë“œ ëª©ì 
2. ê° ë¶€ë¶„ë³„ ë™ì‘ ì„¤ëª…
3. ì‚¬ìš©ëœ ì•Œê³ ë¦¬ì¦˜/íŒ¨í„´
4. í•µì‹¬ ê°œë… ì„¤ëª…
5. ì‹¤í–‰ íë¦„

ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆê²Œ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.""",

    "refactor": """ë‹¹ì‹ ì€ ë¦¬íŒ©í† ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê¸°ì¡´ ì½”ë“œë¥¼ ë” ì¢‹ì€ ì½”ë“œë¡œ ê°œì„ í•©ë‹ˆë‹¤.

ë¦¬íŒ©í† ë§ ì›ì¹™:
1. ê°€ë…ì„± í–¥ìƒ
2. ì¤‘ë³µ ì œê±° (DRY)
3. ë‹¨ì¼ ì±…ì„ ì›ì¹™
4. ì ì ˆí•œ í•¨ìˆ˜/í´ë˜ìŠ¤ ë¶„ë¦¬
5. ë„¤ì´ë° ê°œì„ 
6. ì„±ëŠ¥ ìµœì í™”

ì›ë³¸ ì½”ë“œì˜ ê¸°ëŠ¥ì€ ìœ ì§€í•˜ë©´ì„œ ê°œì„ ëœ ì½”ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.""",

    "convert": """ë‹¹ì‹ ì€ ë‹¤êµ­ì–´ í”„ë¡œê·¸ë˜ë¨¸ì…ë‹ˆë‹¤.
ì½”ë“œë¥¼ ë‹¤ë¥¸ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

ë³€í™˜ ì›ì¹™:
1. ì›ë³¸ ë¡œì§ ì™„ë²½ ìœ ì§€
2. ëŒ€ìƒ ì–¸ì–´ì˜ ê´€ìš©ì  í‘œí˜„ ì‚¬ìš©
3. ëŒ€ìƒ ì–¸ì–´ì˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì¤€ìˆ˜
4. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª…ì‹œ
5. ì–¸ì–´ë³„ ì°¨ì´ì  ì„¤ëª…

í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ê³ , ë³€í™˜ëœ ì½”ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.""",

    "test": """ë‹¹ì‹ ì€ í…ŒìŠ¤íŠ¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì½”ë“œì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

í…ŒìŠ¤íŠ¸ ì‘ì„± ì›ì¹™:
1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
2. ê²½ê³„ê°’ í…ŒìŠ¤íŠ¸
3. ì˜ˆì™¸ ìƒí™© í…ŒìŠ¤íŠ¸
4. ëª¨í‚¹ì´ í•„ìš”í•œ ê²½ìš° ëª…ì‹œ
5. í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ê³ ë ¤

ì ì ˆí•œ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.""",

    "general": """ë‹¹ì‹ ì€ ì¹œì ˆí•œ í”„ë¡œê·¸ë˜ë° ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì½”ë”© ê´€ë ¨ ëª¨ë“  ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.

ë‹µë³€ ì›ì¹™:
1. ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ
2. ì˜ˆì œ ì½”ë“œ í¬í•¨
3. ê´€ë ¨ ì°¸ê³  ìë£Œ ì–¸ê¸‰
4. ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì„¤ëª…

í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."""
}

# ========================================
# API Token ê´€ë¦¬
# ========================================
def load_api_token():
    """API í† í° ë¡œë“œ"""
    global API_TOKEN
    
    # ì—¬ëŸ¬ ê²½ë¡œì—ì„œ í† í° íŒŒì¼ ì°¾ê¸°
    token_paths = [
        "token.txt",
        "../token.txt",
        os.path.expanduser("~/token.txt")
    ]
    
    for token_path in token_paths:
        if os.path.exists(token_path):
            try:
                with open(token_path, "r", encoding='utf-8') as f:
                    API_TOKEN = f.read().strip()
                if API_TOKEN and "REPLACE" not in API_TOKEN:
                    logger.info(f"âœ… API í† í° ë¡œë“œ ì™„ë£Œ: {token_path}")
                    return True
                else:
                    logger.warning(f"âš ï¸ í† í° íŒŒì¼ì´ ê¸°ë³¸ê°’: {token_path}")
            except Exception as e:
                logger.error(f"âŒ í† í° ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    logger.warning("âš ï¸ í† í° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    return False


# ========================================
# ë¡œì»¬ LLM ë¡œë“œ (GGUF)
# ========================================
def load_local_llm():
    """ë¡œì»¬ GGUF ëª¨ë¸ ë¡œë“œ"""
    global llm
    
    if not os.path.exists(LOCAL_MODEL_PATH):
        logger.warning(f"âš ï¸ ë¡œì»¬ ëª¨ë¸ ì—†ìŒ: {LOCAL_MODEL_PATH}")
        return False
    
    try:
        from llama_cpp import Llama
        logger.info(f"ğŸ“¦ ë¡œì»¬ LLM ë¡œë”© ì¤‘: {LOCAL_MODEL_PATH}")
        llm = Llama(
            model_path=LOCAL_MODEL_PATH,
            n_ctx=32768,
            n_gpu_layers=-1,  # GPU ì „ì²´ ì‚¬ìš©
            verbose=False
        )
        logger.info("âœ… ë¡œì»¬ LLM ë¡œë“œ ì™„ë£Œ!")
        return True
    except ImportError:
        logger.warning("âš ï¸ llama-cpp-python ë¯¸ì„¤ì¹˜. pip install llama-cpp-python")
        return False
    except Exception as e:
        logger.error(f"âŒ ë¡œì»¬ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


# ========================================
# ë¡œì»¬ LLM í˜¸ì¶œ
# ========================================
def call_local_llm(prompt: str, system_prompt: str = "", max_tokens: int = 2000) -> dict:
    """ë¡œì»¬ GGUF ëª¨ë¸ í˜¸ì¶œ"""
    global llm
    
    if llm is None:
        return {"success": False, "error": "ë¡œì»¬ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    
    try:
        # ChatML í˜•ì‹ (Qwen3ìš©)
        if not system_prompt:
            system_prompt = "ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ í”„ë¡œê·¸ë˜ë¨¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."
        
        formatted_prompt = f"""<|im_start|>system
{system_prompt}
<|im_end|>
<|im_start|>user
{prompt}
<|im_end|>
<|im_start|>assistant
"""
        
        logger.info("ğŸ–¥ï¸ ë¡œì»¬ LLM í˜¸ì¶œ ì¤‘...")
        response = llm(
            formatted_prompt,
            max_tokens=max_tokens,
            temperature=0.3,
            stop=["<|im_end|>", "\n\n\n"]
        )
        
        content = response['choices'][0]['text'].strip()
        
        # <think> íƒœê·¸ ì œê±°
        content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
        content = re.sub(r'<think>.*', '', content, flags=re.DOTALL)
        content = content.strip()
        
        return {"success": True, "content": content}
        
    except Exception as e:
        logger.error(f"âŒ ë¡œì»¬ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": f"ë¡œì»¬ LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"}


# ========================================
# LLM API í˜¸ì¶œ
# ========================================
def call_llm_api(prompt: str, system_prompt: str = "", max_tokens: int = 4000, request_id: str = None) -> dict:
    """LLM API í˜¸ì¶œ"""
    global API_TOKEN
    
    if not API_TOKEN:
        return {"success": False, "error": "API í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. token.txtë¥¼ í™•ì¸í•˜ì„¸ìš”."}
    
    headers = {
        "Authorization": f"Bearer {API_TOKEN}",
        "Content-Type": "application/json"
    }
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    data = {
        "model": API_MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": 0.3
    }
    
    # ì·¨ì†Œ ì´ë²¤íŠ¸ ë“±ë¡
    cancel_event = None
    if request_id:
        cancel_event = threading.Event()
        with request_lock:
            active_requests[request_id] = cancel_event
    
    for attempt in range(2):
        try:
            # ì·¨ì†Œ í™•ì¸
            if cancel_event and cancel_event.is_set():
                return {"success": False, "error": "ìš”ì²­ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.", "cancelled": True}
            
            logger.info(f"ğŸš€ API í˜¸ì¶œ ì¤‘... (ì‹œë„ {attempt + 1}/2)")
            
            # ì§§ì€ timeoutìœ¼ë¡œ ì—¬ëŸ¬ë²ˆ ì²´í¬í•˜ë©´ì„œ ìš”ì²­
            session = requests.Session()
            response = session.post(API_URL, headers=headers, json=data, timeout=300)
            
            # ì·¨ì†Œ í™•ì¸
            if cancel_event and cancel_event.is_set():
                return {"success": False, "error": "ìš”ì²­ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.", "cancelled": True}
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                # <think> íƒœê·¸ ì œê±° (Qwen3 íŠ¹ì„±)
                content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
                content = content.strip()
                
                return {"success": True, "content": content}
            else:
                logger.error(f"âŒ API ì˜¤ë¥˜: {response.status_code}")
                return {"success": False, "error": f"API ì˜¤ë¥˜: {response.status_code}\n{response.text}"}
                
        except requests.exceptions.Timeout:
            logger.warning(f"â±ï¸ API íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}/2)")
            if attempt == 1:
                return {"success": False, "error": "API ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (5ë¶„). ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”."}
        except Exception as e:
            logger.error(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": f"API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"}
        finally:
            # ìš”ì²­ ì™„ë£Œ ì‹œ ì •ë¦¬
            if request_id:
                with request_lock:
                    active_requests.pop(request_id, None)
    
    return {"success": False, "error": "API í˜¸ì¶œ ì‹¤íŒ¨"}


# ========================================
# í†µí•© LLM í˜¸ì¶œ (API â†’ Local í´ë°±)
# ========================================
def call_llm(prompt: str, system_prompt: str = "", max_tokens: int = 4000, request_id: str = None) -> dict:
    """LLM í˜¸ì¶œ (ëª¨ë“œì— ë”°ë¼ API ë˜ëŠ” Local ì‚¬ìš©, í´ë°± ì§€ì›)"""
    global LLM_MODE
    
    if LLM_MODE == "api":
        # API ëª¨ë“œ
        result = call_llm_api(prompt, system_prompt, max_tokens, request_id)
        
        # API ì‹¤íŒ¨ ì‹œ ë¡œì»¬ë¡œ í´ë°± (ì·¨ì†Œëœ ê²½ìš° ì œì™¸)
        if not result["success"] and not result.get("cancelled") and llm is not None:
            logger.info("âš ï¸ API ì‹¤íŒ¨ â†’ ë¡œì»¬ LLM í´ë°±")
            result = call_local_llm(prompt, system_prompt, min(max_tokens, 2000))
            if result["success"]:
                result["fallback"] = True  # í´ë°± ì‚¬ìš© í‘œì‹œ
        
        return result
    else:
        # ë¡œì»¬ ëª¨ë“œ
        if llm is None:
            return {"success": False, "error": "ë¡œì»¬ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        return call_local_llm(prompt, system_prompt, min(max_tokens, 2000))


# ========================================
# Pydantic Models
# ========================================
class CodingQuery(BaseModel):
    question: str
    mode: str = "general"
    language: str = "python"
    code: Optional[str] = ""
    request_id: Optional[str] = None


class ConvertRequest(BaseModel):
    code: str
    from_lang: str
    to_lang: str


# ========================================
# FastAPI Startup
# ========================================
@app.on_event("startup")
async def startup():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global LLM_MODE
    
    # API í† í° ë¡œë“œ
    if load_api_token():
        LLM_MODE = "api"
        logger.info("âœ… API ëª¨ë“œë¡œ ì‹œì‘")
    else:
        LLM_MODE = "local"
        logger.info("âš ï¸ API í† í° ì—†ìŒ â†’ ë¡œì»¬ ëª¨ë“œ ì‹œë„")
    
    # ë¡œì»¬ LLM ë¡œë“œ (ë°±ì—…ìš©)
    if load_local_llm():
        logger.info("âœ… ë¡œì»¬ LLM ì¤€ë¹„ ì™„ë£Œ (í´ë°± ê°€ëŠ¥)")
        if not API_TOKEN:
            LLM_MODE = "local"
    else:
        logger.info("â„¹ï¸ ë¡œì»¬ LLM ì—†ìŒ (API ì „ìš© ëª¨ë“œ)")
        if not API_TOKEN:
            logger.warning("âš ï¸ API í† í°ë„ ì—†ê³  ë¡œì»¬ LLMë„ ì—†ìŒ!")
    
    logger.info(f"ğŸš€ ì½”ë”© LLM ì„œë²„ ì‹œì‘! ëª¨ë“œ: {LLM_MODE}, í™˜ê²½: {ENV_MODE}")


# ========================================
# API Endpoints
# ========================================
@app.get("/")
async def home():
    """ë©”ì¸ í˜ì´ì§€"""
    return FileResponse("index_coding.html")


@app.get("/api/status")
async def api_status():
    """API ìƒíƒœ í™•ì¸"""
    return {
        "llm_mode": LLM_MODE,
        "api_available": API_TOKEN is not None,
        "local_available": llm is not None,
        "env": ENV_MODE,
        "model": API_MODEL if LLM_MODE == "api" else LOCAL_MODEL_PATH,
        "env_name": ENV_CONFIG[ENV_MODE]["name"]
    }


@app.post("/api/set_llm_mode")
async def set_llm_mode(data: dict):
    """LLM ëª¨ë“œ ì „í™˜ (api/local)"""
    global LLM_MODE
    
    new_mode = data.get("mode", "api")
    
    if new_mode == "local" and llm is None:
        return {"success": False, "message": "ë¡œì»¬ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    if new_mode == "api" and API_TOKEN is None:
        return {"success": False, "message": "API í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    
    LLM_MODE = new_mode
    logger.info(f"ğŸ”„ LLM ëª¨ë“œ ë³€ê²½: {new_mode}")
    
    return {
        "success": True,
        "mode": LLM_MODE,
        "message": f"{new_mode.upper()} ëª¨ë“œë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
    }


@app.post("/api/reload_token")
async def reload_token():
    """í† í° ë¦¬ë¡œë“œ"""
    success = load_api_token()
    return {
        "success": success,
        "message": "í† í° ë¦¬ë¡œë“œ ì™„ë£Œ" if success else "í† í° ë¦¬ë¡œë“œ ì‹¤íŒ¨"
    }


@app.post("/api/set_env")
async def set_env(data: dict):
    """í™˜ê²½ ì „í™˜"""
    global ENV_MODE, API_URL, API_MODEL
    
    new_env = data.get("env", "dev")
    if new_env not in ENV_CONFIG:
        return {"success": False, "message": "ì˜ëª»ëœ í™˜ê²½ì…ë‹ˆë‹¤."}
    
    ENV_MODE = new_env
    API_URL = ENV_CONFIG[new_env]["url"]
    API_MODEL = ENV_CONFIG[new_env]["model"]
    
    logger.info(f"ğŸ”„ í™˜ê²½ ì „í™˜: {new_env} ({API_MODEL})")
    return {
        "success": True,
        "env": ENV_MODE,
        "model": API_MODEL,
        "name": ENV_CONFIG[new_env]["name"]
    }


@app.post("/api/cancel")
async def cancel_request(data: dict):
    """ìš”ì²­ ì·¨ì†Œ"""
    request_id = data.get("request_id")
    
    if not request_id:
        return {"success": False, "message": "request_idê°€ í•„ìš”í•©ë‹ˆë‹¤."}
    
    with request_lock:
        if request_id in active_requests:
            active_requests[request_id].set()  # ì·¨ì†Œ ì‹ í˜¸
            logger.info(f"â¹ ìš”ì²­ ì·¨ì†Œ: {request_id}")
            return {"success": True, "message": "ìš”ì²­ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."}
        else:
            return {"success": False, "message": "í•´ë‹¹ ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}


@app.post("/api/ask")
async def ask(query: CodingQuery):
    """ë©”ì¸ ì§ˆë¬¸ ì²˜ë¦¬"""
    mode = query.mode
    question = query.question.strip()
    code = query.code.strip() if query.code else ""
    language = query.language
    request_id = query.request_id
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
    system_prompt = SYSTEM_PROMPTS.get(mode, SYSTEM_PROMPTS["general"])
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    if mode == "generate":
        prompt = f"ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” {language} ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:\n\n{question}"
    elif mode in ["review", "debug", "explain", "refactor"]:
        if not code:
            return {"success": False, "answer": "âŒ ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!"}
        prompt = f"ë‹¤ìŒ {language} ì½”ë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n```{language}\n{code}\n```\n\n{question if question else ''}"
    elif mode == "test":
        if not code:
            return {"success": False, "answer": "âŒ í…ŒìŠ¤íŠ¸í•  ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!"}
        prompt = f"ë‹¤ìŒ {language} ì½”ë“œì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:\n\n```{language}\n{code}\n```\n\n{question if question else ''}"
    else:
        prompt = question
        if code:
            prompt += f"\n\n```{language}\n{code}\n```"
    
    # LLM í˜¸ì¶œ (API â†’ GGUF í´ë°±)
    result = call_llm(prompt, system_prompt, request_id=request_id)
    
    if result["success"]:
        return {"success": True, "answer": result["content"]}
    else:
        return {"success": False, "answer": f"âŒ {result['error']}"}


@app.post("/api/convert")
async def convert_code(request: ConvertRequest):
    """ì½”ë“œ ì–¸ì–´ ë³€í™˜"""
    system_prompt = SYSTEM_PROMPTS["convert"]
    prompt = f"""ë‹¤ìŒ {request.from_lang} ì½”ë“œë¥¼ {request.to_lang}ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”:

```{request.from_lang}
{request.code}
```

ë³€í™˜ ì‹œ {request.to_lang}ì˜ ê´€ìš©ì ì¸ í‘œí˜„ê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ë”°ë¼ì£¼ì„¸ìš”."""
    
    result = call_llm(prompt, system_prompt)
    
    if result["success"]:
        return {"success": True, "answer": result["content"]}
    else:
        return {"success": False, "answer": f"âŒ {result['error']}"}


@app.post("/api/quick")
async def quick_action(data: dict):
    """ë¹ ë¥¸ ì‘ì—… (ì£¼ì„ ì¶”ê°€, ë³€ìˆ˜ëª… ê°œì„  ë“±)"""
    action = data.get("action", "")
    code = data.get("code", "")
    language = data.get("language", "python")
    
    if not code:
        return {"success": False, "answer": "âŒ ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!"}
    
    action_prompts = {
        "add_comments": "ì´ ì½”ë“œì— í•œêµ­ì–´ ì£¼ì„ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”. ê° í•¨ìˆ˜ì™€ ì¤‘ìš”í•œ ë¡œì§ì— ì„¤ëª…ì„ ë‹¬ì•„ì£¼ì„¸ìš”.",
        "improve_names": "ì´ ì½”ë“œì˜ ë³€ìˆ˜ëª…ê³¼ í•¨ìˆ˜ëª…ì„ ë” ëª…í™•í•˜ê³  ì˜ë¯¸ìˆê²Œ ê°œì„ í•´ì£¼ì„¸ìš”.",
        "optimize": "ì´ ì½”ë“œì˜ ì„±ëŠ¥ì„ ìµœì í™”í•´ì£¼ì„¸ìš”. ë¶ˆí•„ìš”í•œ ì—°ì‚°ì„ ì¤„ì´ê³  íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.",
        "simplify": "ì´ ì½”ë“œë¥¼ ë” ê°„ê²°í•˜ê³  ì½ê¸° ì‰½ê²Œ ë‹¨ìˆœí™”í•´ì£¼ì„¸ìš”.",
        "type_hints": "ì´ Python ì½”ë“œì— íƒ€ì… íŒíŠ¸ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.",
        "docstring": "ì´ ì½”ë“œì˜ ëª¨ë“  í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ì— docstringì„ ì¶”ê°€í•´ì£¼ì„¸ìš”."
    }
    
    if action not in action_prompts:
        return {"success": False, "answer": "âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—…ì…ë‹ˆë‹¤."}
    
    prompt = f"""{action_prompts[action]}

```{language}
{code}
```"""
    
    result = call_llm(prompt, SYSTEM_PROMPTS["refactor"])
    
    if result["success"]:
        return {"success": True, "answer": result["content"]}
    else:
        return {"success": False, "answer": f"âŒ {result['error']}"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)