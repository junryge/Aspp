#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì½”ë”© LLM + Self-Correction í†µí•© ì„œë²„ (v2.1)
- ëª¨ë“  ëª¨ë“œì—ì„œ Self-Correction ì˜µì…˜ ì§€ì›
- ì½”ë“œ ìƒì„±/ë¦¬ë·°/ë””ë²„ê·¸/ì„¤ëª…/ë¦¬íŒ©í† ë§/í…ŒìŠ¤íŠ¸/ì¼ë°˜ + SC
"""

import os
import re
import json
import uuid
import time
import requests
import tempfile
import threading
from typing import TypedDict, Literal, Optional
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse
from pydantic import BaseModel
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# --- ì—¬ê¸° ì¶”ê°€ ---
try:
    from coding_agent import get_agent_router
    logger.info("âœ… ë§ˆê¸°(main1_First) ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œë¨")
except ImportError as e:
    logger.warning(f"âš ï¸ ë§ˆê¸°(main1_First) ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    get_agent_router = None
# ----------------
# aider-chat ë¸Œë¦¿ì§€ ëª¨ë“ˆ
try:
    from aider_bridge import get_aider_router
    logger.info("âœ… aider-chat bridge ëª¨ë“ˆ ë¡œë“œë¨")
except ImportError as e:
    logger.warning(f"âš ï¸ aider-chat bridge ë¡œë“œ ì‹¤íŒ¨: {e}")
    get_aider_router = None
# ----------------
app = FastAPI(title="Coding LLM + Self-Correction v3.0")
# --- ì—¬ê¸° ì¶”ê°€ ---
if get_agent_router:
    app.include_router(get_agent_router())
    logger.info("âœ… ë§ˆê¸°(main1_First) ë¼ìš°í„° ë“±ë¡ ì™„ë£Œ")
# aider ë¼ìš°í„° ë“±ë¡
if get_aider_router:
    app.include_router(get_aider_router())
    logger.info("âœ… aider ë¼ìš°í„° ë“±ë¡ ì™„ë£Œ")
# ----------------
# PC ë¹„ì„œ ëª¨ë“ˆ import
try:
    from pc_assistant import router as assistant_router, init_assistant
    app.include_router(assistant_router)
    logger.info("âœ… PC ë¹„ì„œ ëª¨ë“ˆ ë¡œë“œë¨")
except ImportError as e:
    logger.warning(f"âš ï¸ PC ë¹„ì„œ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")

# ========================================
# Global Variables
# ========================================
API_TOKEN = None
LLM_MODE = "api"  # "api" or "local"
ENV_MODE = "common"

active_requests = {}
request_lock = threading.Lock()

# GGUF ë¡œì»¬ ëª¨ë¸ ì„¤ì • (pc_assistant.pyì˜ AVAILABLE_MODELSì™€ ë™ê¸°í™”)
try:
    from pc_assistant import GGUF_MODEL_PATH as _pa_path, AVAILABLE_MODELS as _pa_models, CURRENT_LOCAL_MODEL as _pa_current
    GGUF_MODEL_PATH = _pa_path
    AVAILABLE_GGUF_MODELS = _pa_models
    CURRENT_GGUF_MODEL = _pa_current
except:
    GGUF_MODEL_PATH = "Qwen3-14B-Q4_K_M.gguf"
    AVAILABLE_GGUF_MODELS = {}
    CURRENT_GGUF_MODEL = ""
LOCAL_LLM = None  # llama-cpp ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤

ENV_CONFIG = {
    "dev": {
        "url": "http://dev.assistant.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Coder-30B-A3B-Instruct",
        "name": "DEV(30B)"
    },
    "prod": {
        "url": "http://summary.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Next-80B-A3B-Instruct",
        "name": "PROD(80B)"
    },
    "common": {
        "url": "http://common.llm.skhynix.com/v1/chat/completions",
        "model": "gpt-oss-20b",
        "name": "COMMON(20B)"
    },
    "local": {
        "url": "",
        "model": "Qwen3-14B-Q4_K_M",
        "name": "LOCAL(14B-GGUF)"
    }
}

API_URL = ENV_CONFIG["common"]["url"]
API_MODEL = ENV_CONFIG["common"]["model"]

# ========================================
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# ========================================
SYSTEM_PROMPTS = {
    "generate": """ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ê³ í’ˆì§ˆ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
ê·œì¹™: ê¹”ë”í•œ ì½”ë“œ, ì ì ˆí•œ ì£¼ì„, ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨.
ì½”ë“œ ë¸”ë¡ì€ ```ì–¸ì–´ëª… ìœ¼ë¡œ ê°ì‹¸ê¸°. í•œêµ­ì–´ë¡œ ì„¤ëª….""",

    "review": """ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì½”ë“œ ë¦¬ë·°ì–´ì…ë‹ˆë‹¤.
ê²€í†  í•­ëª©: ì½”ë“œ í’ˆì§ˆ, ë²„ê·¸, ì„±ëŠ¥, ë³´ì•ˆ, ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤.
í•œêµ­ì–´ë¡œ ìƒì„¸íˆ í”¼ë“œë°±í•˜ê³  ì ìˆ˜(1-10)ë„ ë§¤ê²¨ì£¼ì„¸ìš”.""",

    "debug": """ë‹¹ì‹ ì€ ë””ë²„ê¹… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì—ëŸ¬ ë¶„ì„ â†’ ë²„ê·¸ ì›ì¸ íŒŒì•… â†’ ìˆ˜ì • ì½”ë“œ â†’ ì„¤ëª… â†’ ì˜ˆë°©ë²•.
í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ê³  ìˆ˜ì •ëœ ì½”ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.""",

    "explain": """ë‹¹ì‹ ì€ í”„ë¡œê·¸ë˜ë° êµì‚¬ì…ë‹ˆë‹¤.
ì½”ë“œ ëª©ì , ë¶€ë¶„ë³„ ë™ì‘, ì•Œê³ ë¦¬ì¦˜, ì‹¤í–‰ íë¦„ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆê²Œ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.""",

    "refactor": """ë‹¹ì‹ ì€ ë¦¬íŒ©í† ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê°€ë…ì„± í–¥ìƒ, ì¤‘ë³µ ì œê±°, ë‹¨ì¼ ì±…ì„, ë„¤ì´ë° ê°œì„ , ì„±ëŠ¥ ìµœì í™”.
ì›ë³¸ ê¸°ëŠ¥ ìœ ì§€í•˜ë©´ì„œ ê°œì„ ëœ ì½”ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.""",

    "convert": """ë‹¹ì‹ ì€ ë‹¤êµ­ì–´ í”„ë¡œê·¸ë˜ë¨¸ì…ë‹ˆë‹¤.
ì›ë³¸ ë¡œì§ ìœ ì§€, ëŒ€ìƒ ì–¸ì–´ ê´€ìš©ì  í‘œí˜„, ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì¤€ìˆ˜.
í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ê³  ë³€í™˜ëœ ì½”ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.""",

    "test": """ë‹¹ì‹ ì€ í…ŒìŠ¤íŠ¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¨ìœ„ í…ŒìŠ¤íŠ¸, ê²½ê³„ê°’, ì˜ˆì™¸ ìƒí™© í…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
ì ì ˆí•œ í”„ë ˆì„ì›Œí¬ ì‚¬ìš©.""",

    "general": """ë‹¹ì‹ ì€ ì¹œì ˆí•œ í”„ë¡œê·¸ë˜ë° ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì •ë³´, ì˜ˆì œ ì½”ë“œ í¬í•¨. í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ.""",
}

# Self-Correction ê²€í† ìš© í”„ë¡¬í”„íŠ¸
SC_REVIEW_PROMPT = """ë‹¹ì‹ ì€ ì—„ê²©í•œ í’ˆì§ˆ ê²€í† ìì…ë‹ˆë‹¤.
ìƒì„±ëœ ë‹µë³€ì„ ê²€í† í•˜ê³  ë¬¸ì œì ì„ ì°¾ì•„ì£¼ì„¸ìš”.

ê²€í†  ê¸°ì¤€:
1. ì§ˆë¬¸/ìš”ì²­ì— ì •í™•íˆ ë‹µë³€í–ˆëŠ”ê°€?
2. ì½”ë“œê°€ ìˆë‹¤ë©´ ë¬¸ë²• ì˜¤ë¥˜ë‚˜ ë²„ê·¸ê°€ ì—†ëŠ”ê°€?
3. ë…¼ë¦¬ì  ì˜¤ë¥˜ë‚˜ ëª¨ìˆœì´ ìˆëŠ”ê°€?
4. ì„¤ëª…ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
5. ëˆ„ë½ëœ ì¤‘ìš” ì •ë³´ê°€ ìˆëŠ”ê°€?

ì¶œë ¥ í˜•ì‹: ì²« ì¤„ì— PASS ë˜ëŠ” FAIL, ì´í›„ ìƒì„¸ í”¼ë“œë°±"""


# ========================================
# í† í° ë¡œë“œ
# ========================================
def load_api_token():
    global API_TOKEN
    paths = ["token.txt", "../token.txt", os.path.expanduser("~/token.txt")]
    for p in paths:
        if os.path.exists(p):
            try:
                with open(p, "r", encoding='utf-8') as f:
                    API_TOKEN = f.read().strip()
                if API_TOKEN and "REPLACE" not in API_TOKEN:
                    logger.info(f"âœ… í† í° ë¡œë“œ: {p}")
                    return True
            except Exception as e:
                logger.error(f"âŒ í† í° ë¡œë“œ ì‹¤íŒ¨: {e}")
    return False


# ========================================
# GGUF ë¡œì»¬ ëª¨ë¸ ë¡œë“œ
# ========================================
def load_local_model():
    global LOCAL_LLM
    try:
        from llama_cpp import Llama

        if not os.path.exists(GGUF_MODEL_PATH):
            logger.error(f"âŒ GGUF íŒŒì¼ ì—†ìŒ: {GGUF_MODEL_PATH}")
            return False

        # GPU ì§€ì› í™•ì¸
        try:
            from llama_cpp import llama_supports_gpu_offload
            gpu_ok = llama_supports_gpu_offload()
            logger.info(f"ğŸ”§ llama-cpp GPU ì˜¤í”„ë¡œë“œ ì§€ì›: {gpu_ok}")
        except Exception:
            gpu_ok = False
            logger.warning("âš ï¸ GPU ì˜¤í”„ë¡œë“œ ì§€ì› í™•ì¸ ë¶ˆê°€")

        # ëª¨ë¸ë³„ gpu_layers / ctx ì„¤ì •
        gpu_layers = 35
        n_ctx = 8192
        if CURRENT_GGUF_MODEL and CURRENT_GGUF_MODEL in AVAILABLE_GGUF_MODELS:
            cfg = AVAILABLE_GGUF_MODELS[CURRENT_GGUF_MODEL]
            gpu_layers = cfg.get("gpu_layers", 35)
            n_ctx = cfg.get("ctx", 8192)

        logger.info(f"ğŸ”„ GGUF ëª¨ë¸ ë¡œë”©: {CURRENT_GGUF_MODEL} | path={GGUF_MODEL_PATH}")
        logger.info(f"   â®‘ n_gpu_layers={gpu_layers}, n_ctx={n_ctx}, GPUì§€ì›={gpu_ok}")

        LOCAL_LLM = Llama(
            model_path=GGUF_MODEL_PATH,
            n_ctx=n_ctx,
            n_threads=8,
            n_gpu_layers=gpu_layers,
            n_batch=512,
            verbose=False
        )
        logger.info(f"âœ… GGUF ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (n_gpu_layers={gpu_layers})")
        return True
    except ImportError:
        logger.error("âŒ llama-cpp-python ë¯¸ì„¤ì¹˜. pip install llama-cpp-python")
        return False
    except Exception as e:
        logger.error(f"âŒ GGUF ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


# ========================================
# ë¡œì»¬ LLM í˜¸ì¶œ (GGUF)
# ========================================
def call_local_llm(prompt: str, system_prompt: str = "", max_tokens: int = 4000) -> dict:
    global LOCAL_LLM

    if LOCAL_LLM is None:
        return {"success": False, "error": "ë¡œì»¬ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}

    # Qwen3 ì±„íŒ… í¬ë§·
    full_prompt = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{prompt}<|im_end|>
<|im_start|>assistant
"""

    try:
        logger.info(f"ğŸ§  GGUF ì¶”ë¡  ì‹œì‘ (prompt {len(full_prompt)}ì, max_tokens={max_tokens})")
        t0 = time.time()

        output = LOCAL_LLM(
            full_prompt,
            max_tokens=max_tokens,
            temperature=0.3,
            top_p=0.9,
            stop=["<|im_end|>", "<|im_start|>"],
            echo=False
        )

        elapsed = time.time() - t0
        content = output["choices"][0]["text"].strip()
        # <think> íƒœê·¸ ì œê±°
        content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
        logger.info(f"âœ… GGUF ì¶”ë¡  ì™„ë£Œ: {elapsed:.1f}ì´ˆ, ì‘ë‹µ {len(content)}ì")
        return {"success": True, "content": content}
    except Exception as e:
        logger.error(f"âŒ GGUF ì¶”ë¡  ì˜¤ë¥˜: {e}")
        return {"success": False, "error": str(e)}


# ========================================
# LLM API í˜¸ì¶œ
# ========================================
def call_llm_api(prompt: str, system_prompt: str = "", max_tokens: int = 4000) -> dict:
    global API_TOKEN

    if not API_TOKEN:
        return {"success": False, "error": "API í† í° ì—†ìŒ"}

    headers = {
        "Authorization": f"Bearer {API_TOKEN}",
        "Content-Type": "application/json"
    }

    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    data = {
        "model": API_MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": 0.3
    }

    try:
        response = requests.post(API_URL, headers=headers, json=data, timeout=300)

        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
            return {"success": True, "content": content}
        else:
            return {"success": False, "error": f"API ì˜¤ë¥˜: {response.status_code}"}
    except Exception as e:
        return {"success": False, "error": str(e)}


# ========================================
# LLM í˜¸ì¶œ (API ë˜ëŠ” ë¡œì»¬ ìë™ ì„ íƒ)
# ========================================
def call_llm(prompt: str, system_prompt: str = "", max_tokens: int = 4000) -> dict:
    """LLM_MODEì— ë”°ë¼ API ë˜ëŠ” ë¡œì»¬ ëª¨ë¸ í˜¸ì¶œ"""
    if LLM_MODE == "local":
        return call_local_llm(prompt, system_prompt, max_tokens)
    else:
        return call_llm_api(prompt, system_prompt, max_tokens)


# ========================================
# Self-Correction ë¡œì§
# ========================================
def run_self_correction(prompt: str, system_prompt: str, max_retries: int = 3) -> dict:
    """Self-Correction ë£¨í”„ ì‹¤í–‰"""
    
    answer = ""
    review = ""
    is_valid = False
    attempt = 0
    
    for attempt in range(1, max_retries + 1):
        logger.info(f"ğŸ”„ [SC] ì‹œë„ {attempt}/{max_retries}")
        
        # 1. ë‹µë³€ ìƒì„±
        if attempt == 1:
            gen_prompt = prompt
        else:
            # ì¬ì‹œë„: ì´ì „ í”¼ë“œë°± ë°˜ì˜
            gen_prompt = f"{prompt}\n\n[ì´ì „ ê²€í†  í”¼ë“œë°±]\n{review}\n\nìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ë‹¤ì‹œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        
        result = call_llm(gen_prompt, system_prompt)
        if not result["success"]:
            return {"success": False, "error": result["error"], "retry_count": attempt}
        
        answer = result["content"]
        
        # 2. ë‹µë³€ ê²€í† 
        review_prompt = f"[ì›ë³¸ ìš”ì²­]\n{prompt}\n\n[ìƒì„±ëœ ë‹µë³€]\n{answer}"
        review_result = call_llm(review_prompt, SC_REVIEW_PROMPT, max_tokens=500)
        
        if not review_result["success"]:
            # ê²€í†  ì‹¤íŒ¨í•´ë„ ë‹µë³€ì€ ë°˜í™˜
            return {
                "success": True,
                "answer": answer,
                "retry_count": attempt,
                "is_valid": False,
                "review": "ê²€í†  ì‹¤íŒ¨"
            }
        
        review = review_result["content"]
        is_valid = review.strip().upper().startswith("PASS")
        
        logger.info(f"   ê²°ê³¼: {'âœ… PASS' if is_valid else 'âŒ FAIL'}")
        
        if is_valid:
            break
    
    return {
        "success": True,
        "answer": answer,
        "retry_count": attempt,
        "is_valid": is_valid,
        "review": review
    }


# ========================================
# í”„ë¡¬í”„íŠ¸ ë¹Œë”
# ========================================
def build_prompt(mode: str, question: str, code: str, language: str) -> str:
    """ëª¨ë“œì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    if mode == "generate":
        return f"ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” {language} ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:\n\n{question}"
    
    elif mode in ["review", "debug", "explain", "refactor"]:
        prompt = f"ë‹¤ìŒ {language} ì½”ë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n```{language}\n{code}\n```"
        if question:
            prompt += f"\n\n{question}"
        return prompt
    
    elif mode == "test":
        prompt = f"ë‹¤ìŒ {language} ì½”ë“œì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:\n\n```{language}\n{code}\n```"
        if question:
            prompt += f"\n\n{question}"
        return prompt
    
    else:  # general
        prompt = question
        if code:
            prompt += f"\n\n```{language}\n{code}\n```"
        return prompt


# ========================================
# Pydantic Models
# ========================================
class CodingQuery(BaseModel):
    question: str
    mode: str = "general"
    language: str = "python"
    code: Optional[str] = ""
    use_sc: bool = False  # â˜… Self-Correction ì‚¬ìš© ì—¬ë¶€
    request_id: Optional[str] = None


class ConvertRequest(BaseModel):
    code: str
    from_lang: str
    to_lang: str
    use_sc: bool = False


class QuickRequest(BaseModel):
    action: str
    code: str
    language: str = "python"
    use_sc: bool = False


# ========================================
# Startup
# ========================================
@app.on_event("startup")
async def startup():
    global LLM_MODE, ENV_MODE, API_MODEL

    # 1. API í† í° ë¨¼ì € ì‹œë„
    if load_api_token():
        LLM_MODE = "api"
        logger.info("âœ… API ëª¨ë“œë¡œ ì‹œì‘")
    # 2. í† í° ì—†ìœ¼ë©´ ë¡œì»¬ GGUF ëª¨ë¸ ì‹œë„
    elif load_local_model():
        LLM_MODE = "local"
        ENV_MODE = "local"
        API_MODEL = ENV_CONFIG["local"]["model"]
        logger.info("âœ… ë¡œì»¬ GGUF ëª¨ë“œë¡œ ì‹œì‘")
    else:
        logger.warning("âš ï¸ API í† í°ë„ ì—†ê³  ë¡œì»¬ ëª¨ë¸ë„ ë¡œë“œ ì‹¤íŒ¨")

    # PC ë¹„ì„œ ì´ˆê¸°í™”
    try:
        init_assistant()
        logger.info("âœ… PC ë¹„ì„œ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ PC ë¹„ì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    logger.info(f"ğŸš€ ì„œë²„ ì‹œì‘: {ENV_MODE} ({LLM_MODE} ëª¨ë“œ)")


# ========================================
# API Endpoints
# ========================================
# HTML íŒŒì¼ ê²½ë¡œ
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
HTML_FILE = os.path.join(BASE_DIR, "INDEX.HTML")

@app.get("/")
async def home():
    return FileResponse(HTML_FILE)

@app.get("/magi.png")
async def magi_icon():
    return FileResponse(os.path.join(os.path.dirname(os.path.abspath(__file__)), "magi.png"), media_type="image/png")

@app.get("/magi_f.png")
async def magi_f_icon():
    return FileResponse(os.path.join(os.path.dirname(os.path.abspath(__file__)), "magi_f.png"), media_type="image/png")


@app.get("/api/status")
async def api_status():
    return {
        "llm_mode": LLM_MODE,
        "api_available": API_TOKEN is not None,
        "local_available": LOCAL_LLM is not None,
        "env": ENV_MODE,
        "model": API_MODEL,
        "env_name": ENV_CONFIG[ENV_MODE]["name"],
        "self_correction": True,
        "aider_available": get_aider_router is not None,
        "available_envs": list(ENV_CONFIG.keys())
    }


@app.post("/api/set_env")
async def set_env(data: dict):
    global ENV_MODE, API_URL, API_MODEL, LLM_MODE
    new_env = data.get("env", "common")
    if new_env not in ENV_CONFIG:
        return {"success": False, "message": "ì˜ëª»ëœ í™˜ê²½"}

    # ë¡œì»¬ ëª¨ë“œ ì „í™˜
    if new_env == "local":
        if LOCAL_LLM is None:
            # ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹œë„
            if not load_local_model():
                return {"success": False, "message": "ë¡œì»¬ GGUF ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨"}
        LLM_MODE = "local"
    else:
        # API ëª¨ë“œ ì „í™˜
        if not API_TOKEN:
            return {"success": False, "message": "API í† í°ì´ ì—†ìŠµë‹ˆë‹¤. ë¡œì»¬ ëª¨ë“œë§Œ ì‚¬ìš© ê°€ëŠ¥"}
        LLM_MODE = "api"

    ENV_MODE = new_env
    API_URL = ENV_CONFIG[new_env]["url"]
    API_MODEL = ENV_CONFIG[new_env]["model"]
    return {"success": True, "env": ENV_MODE, "model": API_MODEL, "name": ENV_CONFIG[new_env]["name"], "llm_mode": LLM_MODE}


@app.post("/api/reload_token")
async def reload_token():
    success = load_api_token()
    return {"success": success, "message": "í† í° ë¦¬ë¡œë“œ ì™„ë£Œ" if success else "í† í° ë¦¬ë¡œë“œ ì‹¤íŒ¨"}


@app.get("/api/local_models")
async def get_local_models():
    """ë¡œì»¬ GGUF ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    models = []
    for key, cfg in AVAILABLE_GGUF_MODELS.items():
        exists = os.path.exists(cfg.get("path", ""))
        models.append({
            "key": key,
            "name": cfg.get("name", key),
            "desc": cfg.get("desc", ""),
            "available": exists,
            "current": key == CURRENT_GGUF_MODEL
        })
    return {"success": True, "models": models, "current": CURRENT_GGUF_MODEL}


@app.post("/api/switch_local_model")
async def switch_local_model(data: dict):
    """ë¡œì»¬ GGUF ëª¨ë¸ ì „í™˜"""
    global LOCAL_LLM, GGUF_MODEL_PATH, CURRENT_GGUF_MODEL
    model_key = data.get("model_key", "")

    if model_key not in AVAILABLE_GGUF_MODELS:
        return {"success": False, "message": f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_key}"}

    cfg = AVAILABLE_GGUF_MODELS[model_key]
    if not os.path.exists(cfg["path"]):
        return {"success": False, "message": f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {cfg['name']}"}

    # ê¸°ì¡´ ëª¨ë¸ í•´ì œ
    if LOCAL_LLM is not None:
        del LOCAL_LLM
        LOCAL_LLM = None
        import gc; gc.collect()

    # ìƒˆ ëª¨ë¸ ë¡œë“œ
    GGUF_MODEL_PATH = cfg["path"]
    CURRENT_GGUF_MODEL = model_key
    ENV_CONFIG["local"]["model"] = cfg["name"]
    ENV_CONFIG["local"]["name"] = f"LOCAL({cfg['name']})"

    # ëª¨ë“œ ìƒê´€ì—†ì´ ëª¨ë¸ ë¡œë“œ (aiderê°€ /v1/chat/completionsë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
    if not load_local_model():
        return {"success": False, "message": "ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨"}

    return {
        "success": True,
        "model_key": model_key,
        "model_name": cfg["name"],
        "message": f"{cfg['name']} ì „í™˜ ì™„ë£Œ"
    }


# ========================================
# OpenAI í˜¸í™˜ API (aider + GGUF ì—°ë™ìš©)
# ========================================
@app.post("/v1/chat/completions")
async def openai_chat_completions(data: dict):
    """OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸ - aiderê°€ GGUF ëª¨ë¸ í˜¸ì¶œí•  ë•Œ ì‚¬ìš© (stream ì§€ì›)"""
    import asyncio

    # LOCAL_LLMì´ Noneì´ë©´ ìë™ ë¡œë“œ ì‹œë„
    if LOCAL_LLM is None:
        logger.info("âš¡ /v1/chat/completions: LOCAL_LLMì´ None â†’ ìë™ ë¡œë“œ ì‹œë„")
        loaded = await asyncio.get_event_loop().run_in_executor(None, load_local_model)
        if not loaded or LOCAL_LLM is None:
            return JSONResponse(status_code=503, content={"error": {"message": f"ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (path={GGUF_MODEL_PATH})", "type": "server_error"}})

    messages = data.get("messages", [])
    max_tokens = data.get("max_tokens", 4000)
    is_stream = data.get("stream", False)

    # messages â†’ í”„ë¡¬í”„íŠ¸ ë³€í™˜
    system_prompt = ""
    user_prompt = ""
    for msg in messages:
        role = msg.get("role", "")
        content = msg.get("content", "")
        if role == "system":
            system_prompt = content
        elif role == "user":
            user_prompt = content
        elif role == "assistant":
            user_prompt += f"\n\nassistant: {content}"

    # LLM í˜¸ì¶œ (think íƒœê·¸ ì œê±°)
    def _llm_call(prompt, sys_prompt, max_tok):
        if LOCAL_LLM is None:
            return {"success": False, "error": "ë¡œì»¬ ëª¨ë¸ ë¯¸ë¡œë“œ"}
        full = (f"<|im_start|>system\n{sys_prompt}<|im_end|>\n"
                f"<|im_start|>user\n{prompt} /no_think<|im_end|>\n"
                f"<|im_start|>assistant\n")
        logger.info(f"ğŸ§  GGUF ì¶”ë¡  ì‹œì‘ [v1] (prompt {len(full)}ì, max_tokens={max_tok})")
        t0 = time.time()
        output = LOCAL_LLM(full, max_tokens=max_tok, temperature=0.3, top_p=0.9,
                           stop=["<|im_end|>", "<|im_start|>"], echo=False)
        elapsed = time.time() - t0
        raw_text = output["choices"][0]["text"].strip()
        usage = output.get("usage", {})
        # <think>...</think> ì œê±°
        cleaned = re.sub(r'<think>.*?</think>', '', raw_text, flags=re.DOTALL).strip()
        if not cleaned and '</think>' in raw_text:
            cleaned = raw_text.split('</think>', 1)[-1].strip()
        if not cleaned:
            cleaned = raw_text
        logger.info(f"âœ… GGUF ì¶”ë¡  ì™„ë£Œ [v1]: {elapsed:.1f}ì´ˆ, raw={len(raw_text)}ì â†’ cleaned={len(cleaned)}ì")
        return {"success": True, "content": cleaned, "usage": usage}

    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(
        None, lambda: _llm_call(user_prompt, system_prompt, max_tokens)
    )

    if not result.get("success"):
        return JSONResponse(status_code=500, content={"error": {"message": result.get("error", "LLM í˜¸ì¶œ ì‹¤íŒ¨")}})

    usage = result.get("usage", {})
    content = result["content"]
    p_tokens = usage.get("prompt_tokens", 0) or max(1, len(user_prompt) // 4)
    c_tokens = usage.get("completion_tokens", 0) or max(1, len(content) // 4)
    chat_id = f"chatcmpl-local-{uuid.uuid4().hex[:8]}"
    model_id = CURRENT_GGUF_MODEL or "local-gguf"

    # â˜… stream ëª¨ë“œ: SSE í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ (aiderê°€ stream:trueë¡œ ìš”ì²­)
    if is_stream:
        def stream_generator():
            # 1. contentë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì „ì†¡
            chunk_size = 20  # í† í° ë‹¨ìœ„ê°€ ì•„ë‹ˆë¼ ë¬¸ì ë‹¨ìœ„
            for i in range(0, len(content), chunk_size):
                chunk = content[i:i+chunk_size]
                chunk_data = {
                    "id": chat_id,
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": model_id,
                    "choices": [{"index": 0, "delta": {"content": chunk}, "finish_reason": None}]
                }
                yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"

            # 2. finish ì²­í¬
            finish_data = {
                "id": chat_id,
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": model_id,
                "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
                "usage": {"prompt_tokens": p_tokens, "completion_tokens": c_tokens, "total_tokens": p_tokens + c_tokens}
            }
            yield f"data: {json.dumps(finish_data, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"

        return StreamingResponse(stream_generator(), media_type="text/event-stream")

    # ì¼ë°˜ ëª¨ë“œ: JSON ì‘ë‹µ
    return {
        "id": chat_id,
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model_id,
        "choices": [{
            "index": 0,
            "message": {"role": "assistant", "content": content},
            "finish_reason": "stop"
        }],
        "usage": {"prompt_tokens": p_tokens, "completion_tokens": c_tokens, "total_tokens": p_tokens + c_tokens}
    }


@app.get("/v1/models")
async def openai_models():
    """OpenAI í˜¸í™˜ ëª¨ë¸ ëª©ë¡ - aider ì—°ê²° í™•ì¸ìš©"""
    models = []
    if CURRENT_GGUF_MODEL and CURRENT_GGUF_MODEL in AVAILABLE_GGUF_MODELS:
        models.append({
            "id": CURRENT_GGUF_MODEL,
            "object": "model",
            "owned_by": "local"
        })
    return {"object": "list", "data": models}


@app.post("/api/ask")
async def ask(query: CodingQuery):
    """ë©”ì¸ ì§ˆë¬¸ ì²˜ë¦¬ - Self-Correction ì˜µì…˜ í¬í•¨"""
    mode = query.mode
    question = query.question.strip()
    code = query.code.strip() if query.code else ""
    language = query.language
    use_sc = query.use_sc
    
    # ì½”ë“œ í•„ìš”í•œ ëª¨ë“œ ì²´í¬
    if mode in ["review", "debug", "explain", "refactor", "test"] and not code:
        return {"success": False, "answer": "âŒ ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!"}
    
    if mode == "generate" and not question:
        return {"success": False, "answer": "âŒ ìš”ì²­ì‚¬í•­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!"}
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = build_prompt(mode, question, code, language)
    system_prompt = SYSTEM_PROMPTS.get(mode, SYSTEM_PROMPTS["general"])
    
    # â˜… Self-Correction ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
    if use_sc:
        logger.info(f"ğŸ”„ Self-Correction ëª¨ë“œ: {mode}")
        result = run_self_correction(prompt, system_prompt)
        
        if result["success"]:
            return {
                "success": True,
                "answer": result["answer"],
                "use_sc": True,
                "retry_count": result["retry_count"],
                "is_valid": result["is_valid"],
                "review": result.get("review", "")
            }
        else:
            return {"success": False, "answer": f"âŒ {result['error']}"}
    else:
        # ì¼ë°˜ ëª¨ë“œ (1íšŒ í˜¸ì¶œ)
        result = call_llm(prompt, system_prompt)
        
        if result["success"]:
            return {"success": True, "answer": result["content"], "use_sc": False}
        return {"success": False, "answer": f"âŒ {result['error']}"}


@app.post("/api/convert")
async def convert_code(request: ConvertRequest):
    """ì½”ë“œ ì–¸ì–´ ë³€í™˜"""
    system_prompt = SYSTEM_PROMPTS["convert"]
    prompt = f"""ë‹¤ìŒ {request.from_lang} ì½”ë“œë¥¼ {request.to_lang}ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”:

```{request.from_lang}
{request.code}
```

ë³€í™˜ ì‹œ {request.to_lang}ì˜ ê´€ìš©ì ì¸ í‘œí˜„ê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ë”°ë¼ì£¼ì„¸ìš”."""
    
    if request.use_sc:
        result = run_self_correction(prompt, system_prompt)
        if result["success"]:
            return {
                "success": True,
                "answer": result["answer"],
                "use_sc": True,
                "retry_count": result["retry_count"],
                "is_valid": result["is_valid"]
            }
        return {"success": False, "answer": f"âŒ {result['error']}"}
    else:
        result = call_llm(prompt, system_prompt)
        if result["success"]:
            return {"success": True, "answer": result["content"], "use_sc": False}
        return {"success": False, "answer": f"âŒ {result['error']}"}


@app.post("/api/quick")
async def quick_action(request: QuickRequest):
    """ë¹ ë¥¸ ì‘ì—…"""
    action = request.action
    code = request.code
    language = request.language
    use_sc = request.use_sc
    
    if not code:
        return {"success": False, "answer": "âŒ ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!"}
    
    action_prompts = {
        "add_comments": "ì´ ì½”ë“œì— í•œêµ­ì–´ ì£¼ì„ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”. ê° í•¨ìˆ˜ì™€ ì¤‘ìš”í•œ ë¡œì§ì— ì„¤ëª…ì„ ë‹¬ì•„ì£¼ì„¸ìš”.",
        "improve_names": "ì´ ì½”ë“œì˜ ë³€ìˆ˜ëª…ê³¼ í•¨ìˆ˜ëª…ì„ ë” ëª…í™•í•˜ê³  ì˜ë¯¸ìˆê²Œ ê°œì„ í•´ì£¼ì„¸ìš”.",
        "optimize": "ì´ ì½”ë“œì˜ ì„±ëŠ¥ì„ ìµœì í™”í•´ì£¼ì„¸ìš”. ë¶ˆí•„ìš”í•œ ì—°ì‚°ì„ ì¤„ì´ê³  íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.",
        "simplify": "ì´ ì½”ë“œë¥¼ ë” ê°„ê²°í•˜ê³  ì½ê¸° ì‰½ê²Œ ë‹¨ìˆœí™”í•´ì£¼ì„¸ìš”.",
        "type_hints": "ì´ Python ì½”ë“œì— íƒ€ì… íŒíŠ¸ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.",
        "docstring": "ì´ ì½”ë“œì˜ ëª¨ë“  í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ì— docstringì„ ì¶”ê°€í•´ì£¼ì„¸ìš”."
    }
    
    if action not in action_prompts:
        return {"success": False, "answer": "âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—…ì…ë‹ˆë‹¤."}
    
    prompt = f"{action_prompts[action]}\n\n```{language}\n{code}\n```"
    system_prompt = SYSTEM_PROMPTS["refactor"]
    
    if use_sc:
        result = run_self_correction(prompt, system_prompt)
        if result["success"]:
            return {
                "success": True,
                "answer": result["answer"],
                "use_sc": True,
                "retry_count": result["retry_count"],
                "is_valid": result["is_valid"]
            }
        return {"success": False, "answer": f"âŒ {result['error']}"}
    else:
        result = call_llm(prompt, system_prompt)
        if result["success"]:
            return {"success": True, "answer": result["content"], "use_sc": False}
        return {"success": False, "answer": f"âŒ {result['error']}"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=10002)