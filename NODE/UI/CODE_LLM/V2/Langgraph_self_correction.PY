#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LangGraph Self-Correction ë£¨í”„ (v1.0)
- SK Hynix ë‚´ë¶€ 20B API (gpt-oss-20b) ì‚¬ìš©
- ë‹µë³€ ìƒì„± â†’ ê²€í†  â†’ ì¬ìƒì„± ë£¨í”„
- ì—‘ì…€ ë°ì´í„° ì²˜ë¦¬ ë„êµ¬ í¬í•¨
"""

import os
import re
import json
import requests
from typing import TypedDict, Annotated, Literal
from dataclasses import dataclass

# LangGraph ì„í¬íŠ¸
try:
    from langgraph.graph import StateGraph, END
    from langgraph.graph.message import add_messages
except ImportError:
    print("âŒ langgraph ë¯¸ì„¤ì¹˜. ì‹¤í–‰: pip install langgraph langchain-core")
    exit(1)

# ========================================
# ì„¤ì •
# ========================================
API_CONFIG = {
    "url": "http://common.llm.skhynix.com/v1/chat/completions",
    "model": "gpt-oss-20b",
    "name": "COMMON(20B)"
}

# í† í° ë¡œë“œ
def load_token():
    paths = ["token.txt", "../token.txt", os.path.expanduser("~/token.txt")]
    for p in paths:
        if os.path.exists(p):
            with open(p, "r") as f:
                token = f.read().strip()
                if token and "REPLACE" not in token:
                    return token
    return None

API_TOKEN = load_token()

# ========================================
# ìƒíƒœ ì •ì˜
# ========================================
class AgentState(TypedDict):
    """ì—ì´ì „íŠ¸ ìƒíƒœ"""
    question: str           # ì›ë³¸ ì§ˆë¬¸
    context: str            # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì—‘ì…€ ë°ì´í„° ë“±)
    answer: str             # ìƒì„±ëœ ë‹µë³€
    review: str             # ê²€í†  ê²°ê³¼
    is_valid: bool          # ë‹µë³€ ìœ íš¨ì„±
    retry_count: int        # ì¬ì‹œë„ íšŸìˆ˜
    final_answer: str       # ìµœì¢… ë‹µë³€


# ========================================
# LLM í˜¸ì¶œ í•¨ìˆ˜
# ========================================
def call_llm(prompt: str, system_prompt: str = "", max_tokens: int = 2000) -> str:
    """20B API í˜¸ì¶œ"""
    if not API_TOKEN:
        return "[ERROR] API í† í°ì´ ì—†ìŠµë‹ˆë‹¤."
    
    headers = {
        "Authorization": f"Bearer {API_TOKEN}",
        "Content-Type": "application/json"
    }
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    data = {
        "model": API_CONFIG["model"],
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": 0.3
    }
    
    try:
        response = requests.post(API_CONFIG["url"], headers=headers, json=data, timeout=120)
        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            # <think> íƒœê·¸ ì œê±°
            content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
            return content.strip()
        else:
            return f"[ERROR] API ì˜¤ë¥˜: {response.status_code}"
    except Exception as e:
        return f"[ERROR] {str(e)}"


# ========================================
# ë„êµ¬: ì—‘ì…€ ë°ì´í„° ì²˜ë¦¬
# ========================================
def read_excel_data(file_path: str, sheet_name: str = None) -> dict:
    """ì—‘ì…€ íŒŒì¼ ì½ê¸° ë„êµ¬"""
    try:
        import pandas as pd
        
        if not os.path.exists(file_path):
            return {"success": False, "error": f"íŒŒì¼ ì—†ìŒ: {file_path}"}
        
        if sheet_name:
            df = pd.read_excel(file_path, sheet_name=sheet_name)
        else:
            df = pd.read_excel(file_path)
        
        return {
            "success": True,
            "columns": df.columns.tolist(),
            "rows": len(df),
            "preview": df.head(10).to_dict('records'),
            "summary": df.describe().to_dict() if df.select_dtypes(include='number').columns.any() else {}
        }
    except Exception as e:
        return {"success": False, "error": str(e)}


def analyze_excel_column(file_path: str, column: str) -> dict:
    """ì—‘ì…€ íŠ¹ì • ì»¬ëŸ¼ ë¶„ì„"""
    try:
        import pandas as pd
        df = pd.read_excel(file_path)
        
        if column not in df.columns:
            return {"success": False, "error": f"ì»¬ëŸ¼ ì—†ìŒ: {column}"}
        
        col_data = df[column]
        result = {
            "success": True,
            "column": column,
            "dtype": str(col_data.dtype),
            "non_null": int(col_data.count()),
            "null_count": int(col_data.isna().sum())
        }
        
        if col_data.dtype in ['int64', 'float64']:
            result.update({
                "min": float(col_data.min()),
                "max": float(col_data.max()),
                "mean": float(col_data.mean()),
                "std": float(col_data.std())
            })
        else:
            result["unique_values"] = col_data.nunique()
            result["top_values"] = col_data.value_counts().head(5).to_dict()
        
        return result
    except Exception as e:
        return {"success": False, "error": str(e)}


# ========================================
# ê·¸ë˜í”„ ë…¸ë“œë“¤
# ========================================
def generate_answer(state: AgentState) -> AgentState:
    """1ë‹¨ê³„: ë‹µë³€ ìƒì„±"""
    print(f"\nğŸ”„ [ìƒì„±] ë‹µë³€ ìƒì„± ì¤‘... (ì‹œë„ {state['retry_count'] + 1})")
    
    system_prompt = """ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ê·œì¹™:
1. ë°ì´í„°ê°€ ì œê³µë˜ë©´ ë°˜ë“œì‹œ ì°¸ì¡°í•˜ì—¬ ë‹µë³€
2. ìˆ«ìë‚˜ í†µê³„ëŠ” ì •í™•í•˜ê²Œ ì¸ìš©
3. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ëª…ì‹œ
4. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€"""

    # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í¬í•¨
    prompt = f"ì§ˆë¬¸: {state['question']}"
    if state.get('context'):
        prompt = f"[ë°ì´í„°]\n{state['context']}\n\n{prompt}"
    
    # ì¬ì‹œë„ì¸ ê²½ìš° ì´ì „ í”¼ë“œë°± í¬í•¨
    if state['retry_count'] > 0 and state.get('review'):
        prompt += f"\n\n[ì´ì „ ê²€í†  í”¼ë“œë°±]\n{state['review']}\n\nìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ë‹¤ì‹œ ë‹µë³€í•´ì£¼ì„¸ìš”."
    
    answer = call_llm(prompt, system_prompt)
    
    return {
        **state,
        "answer": answer,
        "retry_count": state['retry_count'] + 1
    }


def review_answer(state: AgentState) -> AgentState:
    """2ë‹¨ê³„: ë‹µë³€ ê²€í† """
    print("ğŸ” [ê²€í† ] ë‹µë³€ ê²€í†  ì¤‘...")
    
    system_prompt = """ë‹¹ì‹ ì€ ì—„ê²©í•œ í’ˆì§ˆ ê²€í† ìì…ë‹ˆë‹¤.
ìƒì„±ëœ ë‹µë³€ì„ ê²€í† í•˜ê³  ë¬¸ì œì ì„ ì°¾ì•„ì£¼ì„¸ìš”.

ê²€í†  ê¸°ì¤€:
1. ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í–ˆëŠ”ê°€?
2. ì œê³µëœ ë°ì´í„°ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©í–ˆëŠ”ê°€?
3. ë…¼ë¦¬ì  ì˜¤ë¥˜ë‚˜ ëª¨ìˆœì´ ìˆëŠ”ê°€?
4. ìˆ«ì/í†µê³„ê°€ ì •í™•í•œê°€?

ì¶œë ¥ í˜•ì‹:
- ì²« ì¤„: PASS ë˜ëŠ” FAIL
- ì´í›„: ìƒì„¸ í”¼ë“œë°±"""

    prompt = f"""[ì›ë³¸ ì§ˆë¬¸]
{state['question']}

[ìƒì„±ëœ ë‹µë³€]
{state['answer']}
"""
    if state.get('context'):
        prompt = f"[ì°¸ì¡° ë°ì´í„°]\n{state['context']}\n\n{prompt}"
    
    review = call_llm(prompt, system_prompt, max_tokens=500)
    
    # PASS/FAIL íŒì •
    is_valid = review.strip().upper().startswith("PASS")
    
    print(f"   ê²°ê³¼: {'âœ… PASS' if is_valid else 'âŒ FAIL'}")
    
    return {
        **state,
        "review": review,
        "is_valid": is_valid
    }


def finalize_answer(state: AgentState) -> AgentState:
    """3ë‹¨ê³„: ìµœì¢… ë‹µë³€ í™•ì •"""
    print("âœ… [ì™„ë£Œ] ìµœì¢… ë‹µë³€ í™•ì •")
    
    return {
        **state,
        "final_answer": state['answer']
    }


def should_retry(state: AgentState) -> Literal["retry", "finalize"]:
    """ì¡°ê±´ë¶€ ë¶„ê¸°: ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •"""
    # ìµœëŒ€ 3íšŒ ì‹œë„
    if state['retry_count'] >= 3:
        print("âš ï¸  ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬")
        return "finalize"
    
    # ê²€í†  í†µê³¼í•˜ë©´ ì¢…ë£Œ
    if state['is_valid']:
        return "finalize"
    
    # ê²€í†  ì‹¤íŒ¨í•˜ë©´ ì¬ì‹œë„
    return "retry"


# ========================================
# ê·¸ë˜í”„ êµ¬ì„±
# ========================================
def build_graph():
    """LangGraph êµ¬ì„±"""
    graph = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    graph.add_node("generate", generate_answer)
    graph.add_node("review", review_answer)
    graph.add_node("finalize", finalize_answer)
    
    # ì—£ì§€ ì—°ê²°
    graph.set_entry_point("generate")
    graph.add_edge("generate", "review")
    
    # ì¡°ê±´ë¶€ ì—£ì§€ (ê²€í†  ê²°ê³¼ì— ë”°ë¼ ë¶„ê¸°)
    graph.add_conditional_edges(
        "review",
        should_retry,
        {
            "retry": "generate",    # ì¬ì‹œë„
            "finalize": "finalize"  # ì™„ë£Œ
        }
    )
    
    graph.add_edge("finalize", END)
    
    return graph.compile()


# ========================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ========================================
def run_self_correction(question: str, context: str = "") -> dict:
    """Self-Correction ë£¨í”„ ì‹¤í–‰"""
    print("=" * 50)
    print("ğŸš€ LangGraph Self-Correction ì‹œì‘")
    print(f"ğŸ“ ì§ˆë¬¸: {question[:100]}...")
    print("=" * 50)
    
    # ê·¸ë˜í”„ ë¹Œë“œ
    app = build_graph()
    
    # ì´ˆê¸° ìƒíƒœ
    initial_state = {
        "question": question,
        "context": context,
        "answer": "",
        "review": "",
        "is_valid": False,
        "retry_count": 0,
        "final_answer": ""
    }
    
    # ì‹¤í–‰
    result = app.invoke(initial_state)
    
    print("\n" + "=" * 50)
    print("ğŸ¯ ìµœì¢… ê²°ê³¼")
    print("=" * 50)
    print(f"ì¬ì‹œë„ íšŸìˆ˜: {result['retry_count']}")
    print(f"ê²€í†  í†µê³¼: {'âœ…' if result['is_valid'] else 'âŒ'}")
    
    return {
        "question": result['question'],
        "final_answer": result['final_answer'],
        "retry_count": result['retry_count'],
        "is_valid": result['is_valid'],
        "last_review": result['review']
    }


# ========================================
# ì—‘ì…€ ë°ì´í„° + Self-Correction í†µí•©
# ========================================
def analyze_excel_with_correction(file_path: str, question: str) -> dict:
    """ì—‘ì…€ ë°ì´í„° ë¶„ì„ + Self-Correction"""
    print(f"\nğŸ“Š ì—‘ì…€ íŒŒì¼ ë¡œë”©: {file_path}")
    
    # ì—‘ì…€ ë°ì´í„° ì½ê¸°
    excel_data = read_excel_data(file_path)
    
    if not excel_data['success']:
        return {"success": False, "error": excel_data['error']}
    
    # ë°ì´í„° ìš”ì•½ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    context = f"""ì—‘ì…€ ë°ì´í„° ì •ë³´:
- ì»¬ëŸ¼: {excel_data['columns']}
- í–‰ ìˆ˜: {excel_data['rows']}
- ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 10í–‰):
{json.dumps(excel_data['preview'], ensure_ascii=False, indent=2)}
"""
    
    if excel_data.get('summary'):
        context += f"\n- ìˆ˜ì¹˜ ì»¬ëŸ¼ í†µê³„:\n{json.dumps(excel_data['summary'], ensure_ascii=False, indent=2)}"
    
    # Self-Correction ë£¨í”„ ì‹¤í–‰
    result = run_self_correction(question, context)
    result['excel_info'] = {
        "columns": excel_data['columns'],
        "rows": excel_data['rows']
    }
    
    return result


# ========================================
# FastAPI í†µí•©ìš© í´ë˜ìŠ¤
# ========================================
class SelfCorrectionAgent:
    """FastAPI í†µí•©ìš© ì—ì´ì „íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.graph = build_graph()
    
    def ask(self, question: str, context: str = "") -> dict:
        """ì§ˆë¬¸ ì²˜ë¦¬"""
        initial_state = {
            "question": question,
            "context": context,
            "answer": "",
            "review": "",
            "is_valid": False,
            "retry_count": 0,
            "final_answer": ""
        }
        
        result = self.graph.invoke(initial_state)
        
        return {
            "success": True,
            "answer": result['final_answer'],
            "retry_count": result['retry_count'],
            "is_valid": result['is_valid']
        }
    
    def ask_with_excel(self, file_path: str, question: str) -> dict:
        """ì—‘ì…€ ë°ì´í„°ì™€ í•¨ê»˜ ì§ˆë¬¸"""
        excel_data = read_excel_data(file_path)
        
        if not excel_data['success']:
            return {"success": False, "error": excel_data['error']}
        
        context = f"ì—‘ì…€ ì»¬ëŸ¼: {excel_data['columns']}\n"
        context += f"í–‰ ìˆ˜: {excel_data['rows']}\n"
        context += f"ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\n{json.dumps(excel_data['preview'], ensure_ascii=False)}"
        
        return self.ask(question, context)


# ========================================
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# ========================================
if __name__ == "__main__":
    print("ğŸ”§ LangGraph Self-Correction í…ŒìŠ¤íŠ¸")
    print(f"ğŸ“¡ API: {API_CONFIG['name']} ({API_CONFIG['model']})")
    print(f"ğŸ”‘ í† í°: {'âœ… ë¡œë“œë¨' if API_TOKEN else 'âŒ ì—†ìŒ'}")
    
    if not API_TOKEN:
        print("\nâš ï¸ token.txt íŒŒì¼ì— API í† í°ì„ ì €ì¥í•˜ì„¸ìš”.")
        exit(1)
    
    # í…ŒìŠ¤íŠ¸ 1: ì¼ë°˜ ì§ˆë¬¸
    print("\n" + "=" * 60)
    print("í…ŒìŠ¤íŠ¸ 1: ì¼ë°˜ ì§ˆë¬¸")
    result = run_self_correction(
        "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì„ ì„¤ëª…í•˜ê³ , ê°ê° ì–¸ì œ ì‚¬ìš©í•˜ë©´ ì¢‹ì€ì§€ ì•Œë ¤ì¤˜."
    )
    print(f"\nğŸ“ ìµœì¢… ë‹µë³€:\n{result['final_answer']}")
    
    # í…ŒìŠ¤íŠ¸ 2: ë°ì´í„° ë¶„ì„ ì§ˆë¬¸ (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
    print("\n" + "=" * 60)
    print("í…ŒìŠ¤íŠ¸ 2: ë°ì´í„° ë¶„ì„ ì§ˆë¬¸")
    sample_context = """
ë§¤ì¶œ ë°ì´í„°:
- 1ì›”: 1,200ë§Œì›
- 2ì›”: 1,450ë§Œì›  
- 3ì›”: 1,100ë§Œì›
- 4ì›”: 1,680ë§Œì›
- 5ì›”: 1,320ë§Œì›
"""
    result2 = run_self_correction(
        "ì´ ë§¤ì¶œ ë°ì´í„°ì—ì„œ ê°€ì¥ ë†’ì€ ë‹¬ê³¼ ë‚®ì€ ë‹¬ì„ ì°¾ê³ , ì „ì²´ í‰ê· ê³¼ ì¶”ì„¸ë¥¼ ë¶„ì„í•´ì¤˜.",
        sample_context
    )
    print(f"\nğŸ“ ìµœì¢… ë‹µë³€:\n{result2['final_answer']}")