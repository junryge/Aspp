#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì½”ë”© LLM + Self-Correction í†µí•© ì„œë²„ (v2.1)
- ëª¨ë“  ëª¨ë“œì—ì„œ Self-Correction ì˜µì…˜ ì§€ì›
- ì½”ë“œ ìƒì„±/ë¦¬ë·°/ë””ë²„ê·¸/ì„¤ëª…/ë¦¬íŒ©í† ë§/í…ŒìŠ¤íŠ¸/ì¼ë°˜ + SC
"""

import os
import re
import json
import requests
import tempfile
import threading
from typing import TypedDict, Literal, Optional
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Coding LLM + Self-Correction v2.1")

# ========================================
# Global Variables
# ========================================
API_TOKEN = None
LLM_MODE = "api"
ENV_MODE = "common"

active_requests = {}
request_lock = threading.Lock()

ENV_CONFIG = {
    "dev": {
        "url": "http://dev.assistant.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Coder-30B-A3B-Instruct",
        "name": "DEV(30B)"
    },
    "prod": {
        "url": "http://summary.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Next-80B-A3B-Instruct",
        "name": "PROD(80B)"
    },
    "common": {
        "url": "http://common.llm.skhynix.com/v1/chat/completions",
        "model": "gpt-oss-20b",
        "name": "COMMON(20B)"
    }
}

API_URL = ENV_CONFIG["common"]["url"]
API_MODEL = ENV_CONFIG["common"]["model"]

# ========================================
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# ========================================
SYSTEM_PROMPTS = {
    "generate": """ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ê³ í’ˆì§ˆ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
ê·œì¹™: ê¹”ë”í•œ ì½”ë“œ, ì ì ˆí•œ ì£¼ì„, ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨.
ì½”ë“œ ë¸”ë¡ì€ ```ì–¸ì–´ëª… ìœ¼ë¡œ ê°ì‹¸ê¸°. í•œêµ­ì–´ë¡œ ì„¤ëª….""",

    "review": """ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì½”ë“œ ë¦¬ë·°ì–´ì…ë‹ˆë‹¤.
ê²€í†  í•­ëª©: ì½”ë“œ í’ˆì§ˆ, ë²„ê·¸, ì„±ëŠ¥, ë³´ì•ˆ, ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤.
í•œêµ­ì–´ë¡œ ìƒì„¸íˆ í”¼ë“œë°±í•˜ê³  ì ìˆ˜(1-10)ë„ ë§¤ê²¨ì£¼ì„¸ìš”.""",

    "debug": """ë‹¹ì‹ ì€ ë””ë²„ê¹… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì—ëŸ¬ ë¶„ì„ â†’ ë²„ê·¸ ì›ì¸ íŒŒì•… â†’ ìˆ˜ì • ì½”ë“œ â†’ ì„¤ëª… â†’ ì˜ˆë°©ë²•.
í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ê³  ìˆ˜ì •ëœ ì½”ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.""",

    "explain": """ë‹¹ì‹ ì€ í”„ë¡œê·¸ë˜ë° êµì‚¬ì…ë‹ˆë‹¤.
ì½”ë“œ ëª©ì , ë¶€ë¶„ë³„ ë™ì‘, ì•Œê³ ë¦¬ì¦˜, ì‹¤í–‰ íë¦„ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆê²Œ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.""",

    "refactor": """ë‹¹ì‹ ì€ ë¦¬íŒ©í† ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê°€ë…ì„± í–¥ìƒ, ì¤‘ë³µ ì œê±°, ë‹¨ì¼ ì±…ì„, ë„¤ì´ë° ê°œì„ , ì„±ëŠ¥ ìµœì í™”.
ì›ë³¸ ê¸°ëŠ¥ ìœ ì§€í•˜ë©´ì„œ ê°œì„ ëœ ì½”ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.""",

    "convert": """ë‹¹ì‹ ì€ ë‹¤êµ­ì–´ í”„ë¡œê·¸ë˜ë¨¸ì…ë‹ˆë‹¤.
ì›ë³¸ ë¡œì§ ìœ ì§€, ëŒ€ìƒ ì–¸ì–´ ê´€ìš©ì  í‘œí˜„, ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì¤€ìˆ˜.
í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ê³  ë³€í™˜ëœ ì½”ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.""",

    "test": """ë‹¹ì‹ ì€ í…ŒìŠ¤íŠ¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¨ìœ„ í…ŒìŠ¤íŠ¸, ê²½ê³„ê°’, ì˜ˆì™¸ ìƒí™© í…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
ì ì ˆí•œ í”„ë ˆì„ì›Œí¬ ì‚¬ìš©.""",

    "general": """ë‹¹ì‹ ì€ ì¹œì ˆí•œ í”„ë¡œê·¸ë˜ë° ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì •ë³´, ì˜ˆì œ ì½”ë“œ í¬í•¨. í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ.""",
}

# Self-Correction ê²€í† ìš© í”„ë¡¬í”„íŠ¸
SC_REVIEW_PROMPT = """ë‹¹ì‹ ì€ ì—„ê²©í•œ í’ˆì§ˆ ê²€í† ìì…ë‹ˆë‹¤.
ìƒì„±ëœ ë‹µë³€ì„ ê²€í† í•˜ê³  ë¬¸ì œì ì„ ì°¾ì•„ì£¼ì„¸ìš”.

ê²€í†  ê¸°ì¤€:
1. ì§ˆë¬¸/ìš”ì²­ì— ì •í™•íˆ ë‹µë³€í–ˆëŠ”ê°€?
2. ì½”ë“œê°€ ìˆë‹¤ë©´ ë¬¸ë²• ì˜¤ë¥˜ë‚˜ ë²„ê·¸ê°€ ì—†ëŠ”ê°€?
3. ë…¼ë¦¬ì  ì˜¤ë¥˜ë‚˜ ëª¨ìˆœì´ ìˆëŠ”ê°€?
4. ì„¤ëª…ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
5. ëˆ„ë½ëœ ì¤‘ìš” ì •ë³´ê°€ ìˆëŠ”ê°€?

ì¶œë ¥ í˜•ì‹: ì²« ì¤„ì— PASS ë˜ëŠ” FAIL, ì´í›„ ìƒì„¸ í”¼ë“œë°±"""


# ========================================
# í† í° ë¡œë“œ
# ========================================
def load_api_token():
    global API_TOKEN
    paths = ["token.txt", "../token.txt", os.path.expanduser("~/token.txt")]
    for p in paths:
        if os.path.exists(p):
            try:
                with open(p, "r", encoding='utf-8') as f:
                    API_TOKEN = f.read().strip()
                if API_TOKEN and "REPLACE" not in API_TOKEN:
                    logger.info(f"âœ… í† í° ë¡œë“œ: {p}")
                    return True
            except Exception as e:
                logger.error(f"âŒ í† í° ë¡œë“œ ì‹¤íŒ¨: {e}")
    return False


# ========================================
# LLM API í˜¸ì¶œ
# ========================================
def call_llm_api(prompt: str, system_prompt: str = "", max_tokens: int = 4000) -> dict:
    global API_TOKEN
    
    if not API_TOKEN:
        return {"success": False, "error": "API í† í° ì—†ìŒ"}
    
    headers = {
        "Authorization": f"Bearer {API_TOKEN}",
        "Content-Type": "application/json"
    }
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    data = {
        "model": API_MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": 0.3
    }
    
    try:
        response = requests.post(API_URL, headers=headers, json=data, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
            return {"success": True, "content": content}
        else:
            return {"success": False, "error": f"API ì˜¤ë¥˜: {response.status_code}"}
    except Exception as e:
        return {"success": False, "error": str(e)}


# ========================================
# Self-Correction ë¡œì§
# ========================================
def run_self_correction(prompt: str, system_prompt: str, max_retries: int = 3) -> dict:
    """Self-Correction ë£¨í”„ ì‹¤í–‰"""
    
    answer = ""
    review = ""
    is_valid = False
    attempt = 0
    
    for attempt in range(1, max_retries + 1):
        logger.info(f"ğŸ”„ [SC] ì‹œë„ {attempt}/{max_retries}")
        
        # 1. ë‹µë³€ ìƒì„±
        if attempt == 1:
            gen_prompt = prompt
        else:
            # ì¬ì‹œë„: ì´ì „ í”¼ë“œë°± ë°˜ì˜
            gen_prompt = f"{prompt}\n\n[ì´ì „ ê²€í†  í”¼ë“œë°±]\n{review}\n\nìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ë‹¤ì‹œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        
        result = call_llm_api(gen_prompt, system_prompt)
        if not result["success"]:
            return {"success": False, "error": result["error"], "retry_count": attempt}
        
        answer = result["content"]
        
        # 2. ë‹µë³€ ê²€í† 
        review_prompt = f"[ì›ë³¸ ìš”ì²­]\n{prompt}\n\n[ìƒì„±ëœ ë‹µë³€]\n{answer}"
        review_result = call_llm_api(review_prompt, SC_REVIEW_PROMPT, max_tokens=500)
        
        if not review_result["success"]:
            # ê²€í†  ì‹¤íŒ¨í•´ë„ ë‹µë³€ì€ ë°˜í™˜
            return {
                "success": True,
                "answer": answer,
                "retry_count": attempt,
                "is_valid": False,
                "review": "ê²€í†  ì‹¤íŒ¨"
            }
        
        review = review_result["content"]
        is_valid = review.strip().upper().startswith("PASS")
        
        logger.info(f"   ê²°ê³¼: {'âœ… PASS' if is_valid else 'âŒ FAIL'}")
        
        if is_valid:
            break
    
    return {
        "success": True,
        "answer": answer,
        "retry_count": attempt,
        "is_valid": is_valid,
        "review": review
    }


# ========================================
# í”„ë¡¬í”„íŠ¸ ë¹Œë”
# ========================================
def build_prompt(mode: str, question: str, code: str, language: str) -> str:
    """ëª¨ë“œì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    if mode == "generate":
        return f"ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” {language} ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:\n\n{question}"
    
    elif mode in ["review", "debug", "explain", "refactor"]:
        prompt = f"ë‹¤ìŒ {language} ì½”ë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n```{language}\n{code}\n```"
        if question:
            prompt += f"\n\n{question}"
        return prompt
    
    elif mode == "test":
        prompt = f"ë‹¤ìŒ {language} ì½”ë“œì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:\n\n```{language}\n{code}\n```"
        if question:
            prompt += f"\n\n{question}"
        return prompt
    
    else:  # general
        prompt = question
        if code:
            prompt += f"\n\n```{language}\n{code}\n```"
        return prompt


# ========================================
# Pydantic Models
# ========================================
class CodingQuery(BaseModel):
    question: str
    mode: str = "general"
    language: str = "python"
    code: Optional[str] = ""
    use_sc: bool = False  # â˜… Self-Correction ì‚¬ìš© ì—¬ë¶€
    request_id: Optional[str] = None


class ConvertRequest(BaseModel):
    code: str
    from_lang: str
    to_lang: str
    use_sc: bool = False


class QuickRequest(BaseModel):
    action: str
    code: str
    language: str = "python"
    use_sc: bool = False


# ========================================
# Startup
# ========================================
@app.on_event("startup")
async def startup():
    global LLM_MODE
    if load_api_token():
        LLM_MODE = "api"
        logger.info("âœ… API ëª¨ë“œ")
    else:
        logger.warning("âš ï¸ í† í° ì—†ìŒ")
    
    logger.info(f"ğŸš€ ì„œë²„ ì‹œì‘: {ENV_MODE}")


# ========================================
# API Endpoints
# ========================================
@app.get("/")
async def home():
    return FileResponse("index_coding_v2.html")


@app.get("/api/status")
async def api_status():
    return {
        "llm_mode": LLM_MODE,
        "api_available": API_TOKEN is not None,
        "env": ENV_MODE,
        "model": API_MODEL,
        "env_name": ENV_CONFIG[ENV_MODE]["name"],
        "self_correction": True
    }


@app.post("/api/set_env")
async def set_env(data: dict):
    global ENV_MODE, API_URL, API_MODEL
    new_env = data.get("env", "common")
    if new_env not in ENV_CONFIG:
        return {"success": False, "message": "ì˜ëª»ëœ í™˜ê²½"}
    ENV_MODE = new_env
    API_URL = ENV_CONFIG[new_env]["url"]
    API_MODEL = ENV_CONFIG[new_env]["model"]
    return {"success": True, "env": ENV_MODE, "model": API_MODEL, "name": ENV_CONFIG[new_env]["name"]}


@app.post("/api/reload_token")
async def reload_token():
    success = load_api_token()
    return {"success": success, "message": "í† í° ë¦¬ë¡œë“œ ì™„ë£Œ" if success else "í† í° ë¦¬ë¡œë“œ ì‹¤íŒ¨"}


@app.post("/api/ask")
async def ask(query: CodingQuery):
    """ë©”ì¸ ì§ˆë¬¸ ì²˜ë¦¬ - Self-Correction ì˜µì…˜ í¬í•¨"""
    mode = query.mode
    question = query.question.strip()
    code = query.code.strip() if query.code else ""
    language = query.language
    use_sc = query.use_sc
    
    # ì½”ë“œ í•„ìš”í•œ ëª¨ë“œ ì²´í¬
    if mode in ["review", "debug", "explain", "refactor", "test"] and not code:
        return {"success": False, "answer": "âŒ ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!"}
    
    if mode == "generate" and not question:
        return {"success": False, "answer": "âŒ ìš”ì²­ì‚¬í•­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!"}
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = build_prompt(mode, question, code, language)
    system_prompt = SYSTEM_PROMPTS.get(mode, SYSTEM_PROMPTS["general"])
    
    # â˜… Self-Correction ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
    if use_sc:
        logger.info(f"ğŸ”„ Self-Correction ëª¨ë“œ: {mode}")
        result = run_self_correction(prompt, system_prompt)
        
        if result["success"]:
            return {
                "success": True,
                "answer": result["answer"],
                "use_sc": True,
                "retry_count": result["retry_count"],
                "is_valid": result["is_valid"],
                "review": result.get("review", "")
            }
        else:
            return {"success": False, "answer": f"âŒ {result['error']}"}
    else:
        # ì¼ë°˜ ëª¨ë“œ (1íšŒ í˜¸ì¶œ)
        result = call_llm_api(prompt, system_prompt)
        
        if result["success"]:
            return {"success": True, "answer": result["content"], "use_sc": False}
        return {"success": False, "answer": f"âŒ {result['error']}"}


@app.post("/api/convert")
async def convert_code(request: ConvertRequest):
    """ì½”ë“œ ì–¸ì–´ ë³€í™˜"""
    system_prompt = SYSTEM_PROMPTS["convert"]
    prompt = f"""ë‹¤ìŒ {request.from_lang} ì½”ë“œë¥¼ {request.to_lang}ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”:

```{request.from_lang}
{request.code}
```

ë³€í™˜ ì‹œ {request.to_lang}ì˜ ê´€ìš©ì ì¸ í‘œí˜„ê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ë”°ë¼ì£¼ì„¸ìš”."""
    
    if request.use_sc:
        result = run_self_correction(prompt, system_prompt)
        if result["success"]:
            return {
                "success": True,
                "answer": result["answer"],
                "use_sc": True,
                "retry_count": result["retry_count"],
                "is_valid": result["is_valid"]
            }
        return {"success": False, "answer": f"âŒ {result['error']}"}
    else:
        result = call_llm_api(prompt, system_prompt)
        if result["success"]:
            return {"success": True, "answer": result["content"], "use_sc": False}
        return {"success": False, "answer": f"âŒ {result['error']}"}


@app.post("/api/quick")
async def quick_action(request: QuickRequest):
    """ë¹ ë¥¸ ì‘ì—…"""
    action = request.action
    code = request.code
    language = request.language
    use_sc = request.use_sc
    
    if not code:
        return {"success": False, "answer": "âŒ ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!"}
    
    action_prompts = {
        "add_comments": "ì´ ì½”ë“œì— í•œêµ­ì–´ ì£¼ì„ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”. ê° í•¨ìˆ˜ì™€ ì¤‘ìš”í•œ ë¡œì§ì— ì„¤ëª…ì„ ë‹¬ì•„ì£¼ì„¸ìš”.",
        "improve_names": "ì´ ì½”ë“œì˜ ë³€ìˆ˜ëª…ê³¼ í•¨ìˆ˜ëª…ì„ ë” ëª…í™•í•˜ê³  ì˜ë¯¸ìˆê²Œ ê°œì„ í•´ì£¼ì„¸ìš”.",
        "optimize": "ì´ ì½”ë“œì˜ ì„±ëŠ¥ì„ ìµœì í™”í•´ì£¼ì„¸ìš”. ë¶ˆí•„ìš”í•œ ì—°ì‚°ì„ ì¤„ì´ê³  íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.",
        "simplify": "ì´ ì½”ë“œë¥¼ ë” ê°„ê²°í•˜ê³  ì½ê¸° ì‰½ê²Œ ë‹¨ìˆœí™”í•´ì£¼ì„¸ìš”.",
        "type_hints": "ì´ Python ì½”ë“œì— íƒ€ì… íŒíŠ¸ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.",
        "docstring": "ì´ ì½”ë“œì˜ ëª¨ë“  í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ì— docstringì„ ì¶”ê°€í•´ì£¼ì„¸ìš”."
    }
    
    if action not in action_prompts:
        return {"success": False, "answer": "âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—…ì…ë‹ˆë‹¤."}
    
    prompt = f"{action_prompts[action]}\n\n```{language}\n{code}\n```"
    system_prompt = SYSTEM_PROMPTS["refactor"]
    
    if use_sc:
        result = run_self_correction(prompt, system_prompt)
        if result["success"]:
            return {
                "success": True,
                "answer": result["answer"],
                "use_sc": True,
                "retry_count": result["retry_count"],
                "is_valid": result["is_valid"]
            }
        return {"success": False, "answer": f"âŒ {result['error']}"}
    else:
        result = call_llm_api(prompt, system_prompt)
        if result["success"]:
            return {"success": True, "answer": result["content"], "use_sc": False}
        return {"success": False, "answer": f"âŒ {result['error']}"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)