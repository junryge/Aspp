#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì½”ë”© LLM + Self-Correction í†µí•© ì„œë²„ (v2.0)
- ê¸°ì¡´ ì½”ë“œ ìƒì„±/ë¦¬ë·°/ë””ë²„ê·¸ ê¸°ëŠ¥
- LangGraph Self-Correction ë£¨í”„ ì¶”ê°€
- ì—‘ì…€ ë°ì´í„° ì²˜ë¦¬ ë„êµ¬ í¬í•¨
"""

import os
import re
import json
import requests
import tempfile
import threading
from typing import TypedDict, Literal, Optional
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LangGraph ì„í¬íŠ¸
try:
    from langgraph.graph import StateGraph, END
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False
    logger.warning("âš ï¸ langgraph ë¯¸ì„¤ì¹˜. Self-Correction ë¹„í™œì„±í™”")

app = FastAPI(title="Coding LLM + Self-Correction")

# ========================================
# Global Variables
# ========================================
API_TOKEN = None
llm = None

LLM_MODE = "api"
ENV_MODE = "common"

active_requests = {}
request_lock = threading.Lock()

LOCAL_MODEL_PATH = "Qwen3-14B-Q4_K_M.gguf"

ENV_CONFIG = {
    "dev": {
        "url": "http://dev.assistant.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Coder-30B-A3B-Instruct",
        "name": "DEV(30B)"
    },
    "prod": {
        "url": "http://summary.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Next-80B-A3B-Instruct",
        "name": "PROD(80B)"
    },
    "common": {
        "url": "http://common.llm.skhynix.com/v1/chat/completions",
        "model": "gpt-oss-20b",
        "name": "COMMON(20B)"
    }
}

API_URL = ENV_CONFIG["common"]["url"]
API_MODEL = ENV_CONFIG["common"]["model"]

# ========================================
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# ========================================
SYSTEM_PROMPTS = {
    "generate": """ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ê³ í’ˆì§ˆ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
ê·œì¹™: ê¹”ë”í•œ ì½”ë“œ, ì ì ˆí•œ ì£¼ì„, ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨.
ì½”ë“œ ë¸”ë¡ì€ ```ì–¸ì–´ëª… ìœ¼ë¡œ ê°ì‹¸ê¸°. í•œêµ­ì–´ë¡œ ì„¤ëª….""",

    "review": """ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì½”ë“œ ë¦¬ë·°ì–´ì…ë‹ˆë‹¤.
ê²€í†  í•­ëª©: ì½”ë“œ í’ˆì§ˆ, ë²„ê·¸, ì„±ëŠ¥, ë³´ì•ˆ, ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤.
í•œêµ­ì–´ë¡œ ìƒì„¸íˆ í”¼ë“œë°±í•˜ê³  ì ìˆ˜(1-10)ë„ ë§¤ê²¨ì£¼ì„¸ìš”.""",

    "debug": """ë‹¹ì‹ ì€ ë””ë²„ê¹… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì—ëŸ¬ ë¶„ì„ â†’ ë²„ê·¸ ì›ì¸ íŒŒì•… â†’ ìˆ˜ì • ì½”ë“œ â†’ ì„¤ëª… â†’ ì˜ˆë°©ë²•.
í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ê³  ìˆ˜ì •ëœ ì½”ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.""",

    "explain": """ë‹¹ì‹ ì€ í”„ë¡œê·¸ë˜ë° êµì‚¬ì…ë‹ˆë‹¤.
ì½”ë“œ ëª©ì , ë¶€ë¶„ë³„ ë™ì‘, ì•Œê³ ë¦¬ì¦˜, ì‹¤í–‰ íë¦„ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆê²Œ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.""",

    "refactor": """ë‹¹ì‹ ì€ ë¦¬íŒ©í† ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê°€ë…ì„± í–¥ìƒ, ì¤‘ë³µ ì œê±°, ë‹¨ì¼ ì±…ì„, ë„¤ì´ë° ê°œì„ , ì„±ëŠ¥ ìµœì í™”.
ì›ë³¸ ê¸°ëŠ¥ ìœ ì§€í•˜ë©´ì„œ ê°œì„ ëœ ì½”ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.""",

    "convert": """ë‹¹ì‹ ì€ ë‹¤êµ­ì–´ í”„ë¡œê·¸ë˜ë¨¸ì…ë‹ˆë‹¤.
ì›ë³¸ ë¡œì§ ìœ ì§€, ëŒ€ìƒ ì–¸ì–´ ê´€ìš©ì  í‘œí˜„, ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì¤€ìˆ˜.
í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ê³  ë³€í™˜ëœ ì½”ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.""",

    "test": """ë‹¹ì‹ ì€ í…ŒìŠ¤íŠ¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¨ìœ„ í…ŒìŠ¤íŠ¸, ê²½ê³„ê°’, ì˜ˆì™¸ ìƒí™© í…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
ì ì ˆí•œ í”„ë ˆì„ì›Œí¬ ì‚¬ìš©.""",

    "general": """ë‹¹ì‹ ì€ ì¹œì ˆí•œ í”„ë¡œê·¸ë˜ë° ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì •ë³´, ì˜ˆì œ ì½”ë“œ í¬í•¨. í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ.""",
    
    # Self-Correctionìš© í”„ë¡¬í”„íŠ¸
    "sc_generate": """ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
ê·œì¹™:
1. ë°ì´í„°ê°€ ì œê³µë˜ë©´ ë°˜ë“œì‹œ ì°¸ì¡°í•˜ì—¬ ë‹µë³€
2. ìˆ«ìë‚˜ í†µê³„ëŠ” ì •í™•í•˜ê²Œ ì¸ìš©
3. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ëª…ì‹œ
4. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€""",

    "sc_review": """ë‹¹ì‹ ì€ ì—„ê²©í•œ í’ˆì§ˆ ê²€í† ìì…ë‹ˆë‹¤.
ìƒì„±ëœ ë‹µë³€ì„ ê²€í† í•˜ê³  ë¬¸ì œì ì„ ì°¾ì•„ì£¼ì„¸ìš”.
ê²€í†  ê¸°ì¤€:
1. ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í–ˆëŠ”ê°€?
2. ì œê³µëœ ë°ì´í„°ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©í–ˆëŠ”ê°€?
3. ë…¼ë¦¬ì  ì˜¤ë¥˜ë‚˜ ëª¨ìˆœì´ ìˆëŠ”ê°€?
4. ìˆ«ì/í†µê³„ê°€ ì •í™•í•œê°€?
ì¶œë ¥ í˜•ì‹: ì²« ì¤„ì— PASS ë˜ëŠ” FAIL, ì´í›„ ìƒì„¸ í”¼ë“œë°±"""
}


# ========================================
# í† í° ë¡œë“œ
# ========================================
def load_api_token():
    global API_TOKEN
    paths = ["token.txt", "../token.txt", os.path.expanduser("~/token.txt")]
    for p in paths:
        if os.path.exists(p):
            try:
                with open(p, "r", encoding='utf-8') as f:
                    API_TOKEN = f.read().strip()
                if API_TOKEN and "REPLACE" not in API_TOKEN:
                    logger.info(f"âœ… í† í° ë¡œë“œ: {p}")
                    return True
            except Exception as e:
                logger.error(f"âŒ í† í° ë¡œë“œ ì‹¤íŒ¨: {e}")
    return False


# ========================================
# LLM API í˜¸ì¶œ
# ========================================
def call_llm_api(prompt: str, system_prompt: str = "", max_tokens: int = 4000, request_id: str = None) -> dict:
    global API_TOKEN
    
    if not API_TOKEN:
        return {"success": False, "error": "API í† í° ì—†ìŒ"}
    
    headers = {
        "Authorization": f"Bearer {API_TOKEN}",
        "Content-Type": "application/json"
    }
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    data = {
        "model": API_MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": 0.3
    }
    
    cancel_event = None
    if request_id:
        cancel_event = threading.Event()
        with request_lock:
            active_requests[request_id] = cancel_event
    
    try:
        if cancel_event and cancel_event.is_set():
            return {"success": False, "error": "ìš”ì²­ ì·¨ì†Œë¨", "cancelled": True}
        
        response = requests.post(API_URL, headers=headers, json=data, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
            return {"success": True, "content": content}
        else:
            return {"success": False, "error": f"API ì˜¤ë¥˜: {response.status_code}"}
    except Exception as e:
        return {"success": False, "error": str(e)}
    finally:
        if request_id:
            with request_lock:
                active_requests.pop(request_id, None)


def call_llm(prompt: str, system_prompt: str = "", max_tokens: int = 4000, request_id: str = None) -> dict:
    return call_llm_api(prompt, system_prompt, max_tokens, request_id)


# ========================================
# Self-Correction ìƒíƒœ ë° ê·¸ë˜í”„
# ========================================
if LANGGRAPH_AVAILABLE:
    class AgentState(TypedDict):
        question: str
        context: str
        answer: str
        review: str
        is_valid: bool
        retry_count: int
        final_answer: str

    def sc_generate_answer(state: AgentState) -> AgentState:
        """ë‹µë³€ ìƒì„±"""
        logger.info(f"ğŸ”„ [ìƒì„±] ì‹œë„ {state['retry_count'] + 1}")
        
        prompt = f"ì§ˆë¬¸: {state['question']}"
        if state.get('context'):
            prompt = f"[ë°ì´í„°]\n{state['context']}\n\n{prompt}"
        if state['retry_count'] > 0 and state.get('review'):
            prompt += f"\n\n[ì´ì „ í”¼ë“œë°±]\n{state['review']}\n\nìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ë‹¤ì‹œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        
        result = call_llm(prompt, SYSTEM_PROMPTS["sc_generate"], 2000)
        answer = result.get("content", result.get("error", "ì˜¤ë¥˜"))
        
        return {**state, "answer": answer, "retry_count": state['retry_count'] + 1}

    def sc_review_answer(state: AgentState) -> AgentState:
        """ë‹µë³€ ê²€í† """
        logger.info("ğŸ” [ê²€í† ]")
        
        prompt = f"[ì›ë³¸ ì§ˆë¬¸]\n{state['question']}\n\n[ìƒì„±ëœ ë‹µë³€]\n{state['answer']}"
        if state.get('context'):
            prompt = f"[ì°¸ì¡° ë°ì´í„°]\n{state['context']}\n\n{prompt}"
        
        result = call_llm(prompt, SYSTEM_PROMPTS["sc_review"], 500)
        review = result.get("content", "ê²€í†  ì‹¤íŒ¨")
        is_valid = review.strip().upper().startswith("PASS")
        
        logger.info(f"   ê²°ê³¼: {'âœ… PASS' if is_valid else 'âŒ FAIL'}")
        return {**state, "review": review, "is_valid": is_valid}

    def sc_finalize(state: AgentState) -> AgentState:
        """ìµœì¢… í™•ì •"""
        logger.info("âœ… [ì™„ë£Œ]")
        return {**state, "final_answer": state['answer']}

    def sc_should_retry(state: AgentState) -> Literal["retry", "finalize"]:
        if state['retry_count'] >= 3:
            return "finalize"
        return "finalize" if state['is_valid'] else "retry"

    def build_sc_graph():
        graph = StateGraph(AgentState)
        graph.add_node("generate", sc_generate_answer)
        graph.add_node("review", sc_review_answer)
        graph.add_node("finalize", sc_finalize)
        graph.set_entry_point("generate")
        graph.add_edge("generate", "review")
        graph.add_conditional_edges("review", sc_should_retry, {"retry": "generate", "finalize": "finalize"})
        graph.add_edge("finalize", END)
        return graph.compile()

    SC_GRAPH = build_sc_graph()
else:
    SC_GRAPH = None


# ========================================
# ì—‘ì…€ ë„êµ¬
# ========================================
def read_excel_data(file_path: str) -> dict:
    try:
        import pandas as pd
        if not os.path.exists(file_path):
            return {"success": False, "error": f"íŒŒì¼ ì—†ìŒ: {file_path}"}
        df = pd.read_excel(file_path)
        return {
            "success": True,
            "columns": df.columns.tolist(),
            "rows": len(df),
            "preview": df.head(10).to_dict('records'),
            "summary": df.describe().to_dict() if df.select_dtypes(include='number').columns.any() else {}
        }
    except Exception as e:
        return {"success": False, "error": str(e)}


# ========================================
# Pydantic Models
# ========================================
class CodingQuery(BaseModel):
    question: str
    mode: str = "general"
    language: str = "python"
    code: Optional[str] = ""
    request_id: Optional[str] = None

class SCQuery(BaseModel):
    question: str
    context: Optional[str] = ""


# ========================================
# Startup
# ========================================
@app.on_event("startup")
async def startup():
    global LLM_MODE
    if load_api_token():
        LLM_MODE = "api"
        logger.info("âœ… API ëª¨ë“œ")
    else:
        logger.warning("âš ï¸ í† í° ì—†ìŒ")
    
    logger.info(f"ğŸš€ ì„œë²„ ì‹œì‘: {ENV_MODE}, SC={'âœ…' if LANGGRAPH_AVAILABLE else 'âŒ'}")


# ========================================
# API Endpoints
# ========================================
@app.get("/")
async def home():
    return FileResponse("index_coding.html")


@app.get("/api/status")
async def api_status():
    return {
        "llm_mode": LLM_MODE,
        "api_available": API_TOKEN is not None,
        "env": ENV_MODE,
        "model": API_MODEL,
        "env_name": ENV_CONFIG[ENV_MODE]["name"],
        "self_correction": LANGGRAPH_AVAILABLE
    }


@app.post("/api/set_env")
async def set_env(data: dict):
    global ENV_MODE, API_URL, API_MODEL
    new_env = data.get("env", "common")
    if new_env not in ENV_CONFIG:
        return {"success": False, "message": "ì˜ëª»ëœ í™˜ê²½"}
    ENV_MODE = new_env
    API_URL = ENV_CONFIG[new_env]["url"]
    API_MODEL = ENV_CONFIG[new_env]["model"]
    return {"success": True, "env": ENV_MODE, "model": API_MODEL, "name": ENV_CONFIG[new_env]["name"]}


@app.post("/api/ask")
async def ask(query: CodingQuery):
    """ê¸°ì¡´ ì½”ë”© ì§ˆë¬¸ ì²˜ë¦¬"""
    mode = query.mode
    question = query.question.strip()
    code = query.code.strip() if query.code else ""
    language = query.language
    request_id = query.request_id
    
    system_prompt = SYSTEM_PROMPTS.get(mode, SYSTEM_PROMPTS["general"])
    
    if mode == "generate":
        prompt = f"ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” {language} ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:\n\n{question}"
    elif mode in ["review", "debug", "explain", "refactor"]:
        if not code:
            return {"success": False, "answer": "âŒ ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!"}
        prompt = f"ë‹¤ìŒ {language} ì½”ë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n```{language}\n{code}\n```\n\n{question if question else ''}"
    elif mode == "test":
        if not code:
            return {"success": False, "answer": "âŒ í…ŒìŠ¤íŠ¸í•  ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!"}
        prompt = f"ë‹¤ìŒ {language} ì½”ë“œì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:\n\n```{language}\n{code}\n```"
    else:
        prompt = question
        if code:
            prompt += f"\n\n```{language}\n{code}\n```"
    
    result = call_llm(prompt, system_prompt, request_id=request_id)
    
    if result["success"]:
        return {"success": True, "answer": result["content"]}
    return {"success": False, "answer": f"âŒ {result['error']}"}


# ========================================
# Self-Correction Endpoints
# ========================================
@app.post("/api/sc/ask")
async def sc_ask(query: SCQuery):
    """Self-Correction ë£¨í”„ë¡œ ì§ˆë¬¸ ì²˜ë¦¬"""
    if not LANGGRAPH_AVAILABLE:
        return {"success": False, "error": "langgraph ë¯¸ì„¤ì¹˜. pip install langgraph"}
    
    initial_state = {
        "question": query.question,
        "context": query.context or "",
        "answer": "",
        "review": "",
        "is_valid": False,
        "retry_count": 0,
        "final_answer": ""
    }
    
    result = SC_GRAPH.invoke(initial_state)
    
    return {
        "success": True,
        "answer": result['final_answer'],
        "retry_count": result['retry_count'],
        "is_valid": result['is_valid'],
        "review": result['review']
    }


@app.post("/api/sc/ask_excel")
async def sc_ask_excel(
    file: UploadFile = File(...),
    question: str = Form(...)
):
    """ì—‘ì…€ + Self-Correction"""
    if not LANGGRAPH_AVAILABLE:
        return {"success": False, "error": "langgraph ë¯¸ì„¤ì¹˜"}
    
    suffix = os.path.splitext(file.filename)[1]
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        content = await file.read()
        tmp.write(content)
        tmp_path = tmp.name
    
    try:
        excel_data = read_excel_data(tmp_path)
        if not excel_data['success']:
            return {"success": False, "error": excel_data['error']}
        
        context = f"ì—‘ì…€ ì»¬ëŸ¼: {excel_data['columns']}\n"
        context += f"í–‰ ìˆ˜: {excel_data['rows']}\n"
        context += f"ë°ì´í„°:\n{json.dumps(excel_data['preview'], ensure_ascii=False, indent=2)}"
        
        initial_state = {
            "question": question,
            "context": context,
            "answer": "",
            "review": "",
            "is_valid": False,
            "retry_count": 0,
            "final_answer": ""
        }
        
        result = SC_GRAPH.invoke(initial_state)
        
        return {
            "success": True,
            "answer": result['final_answer'],
            "retry_count": result['retry_count'],
            "is_valid": result['is_valid'],
            "excel_info": {"columns": excel_data['columns'], "rows": excel_data['rows']}
        }
    finally:
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)


@app.post("/api/sc/excel_read")
async def sc_excel_read(file: UploadFile = File(...)):
    """ì—‘ì…€ ë¯¸ë¦¬ë³´ê¸°"""
    suffix = os.path.splitext(file.filename)[1]
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        content = await file.read()
        tmp.write(content)
        tmp_path = tmp.name
    
    try:
        return read_excel_data(tmp_path)
    finally:
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)