#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
knowledge_section_patch.py
pc_assistant.pyì— ì„¹ì…˜ ë‹¨ìœ„ ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ì„ ì¶”ê°€í•˜ëŠ” íŒ¨ì¹˜

=== í˜„ì¬ ë¬¸ì œ ===
"ì•™ìƒë¸” 5ê°œ ê·œì¹™ ì•Œë ¤ì¤˜" â†’ search_knowledge() â†’ íŒŒì¼ëª… ë§¤ì¹­ â†’ read_knowledge() â†’ MD íŒŒì¼ ì „ì²´(30000ì)ë¥¼ LLMì— ë„˜ê¹€
â†’ í† í° ë‚­ë¹„ + ì—‰ëš±í•œ ë‚´ìš© í¬í•¨ + LLMì´ í•µì‹¬ì„ ë†“ì¹  ìˆ˜ ìˆìŒ

=== í•´ê²° ===
search_knowledge_section() â†’ ## í—¤ë” ê¸°ì¤€ ì„¹ì…˜ íŒŒì‹± â†’ ê´€ë ¨ ì„¹ì…˜ë§Œ ë°˜í™˜ (2000~5000ì)
â†’ ì •í™•í•œ ë‹µë³€ + í† í° ì ˆì•½ + ë¹ ë¥¸ ì‘ë‹µ

=== ì ìš© ë°©ë²• ===
1. ì´ íŒŒì¼ì˜ í•¨ìˆ˜ë“¤ì„ pc_assistant.pyì— ë³µì‚¬
2. process_chat()ì˜ ìë™ ì§€ì‹ê²€ìƒ‰ ë¶€ë¶„ì—ì„œ search_knowledge() ëŒ€ì‹  search_knowledge_section() í˜¸ì¶œ
3. read_knowledge() ëŒ€ì‹  ì„¹ì…˜ì˜ contentë¥¼ ì§ì ‘ ì‚¬ìš©

ìƒì„¸í•œ ì ìš© ìœ„ì¹˜ëŠ” ì•„ë˜ ì£¼ì„ ì°¸ê³ 
"""

import os
import re
import logging
from typing import List, Dict, Optional

logger = logging.getLogger("KBSection")


# ============================================================
# [1] MD ì„¹ì…˜ íŒŒì‹± í•¨ìˆ˜ - pc_assistant.py 782ë²ˆì¤„ ê·¼ì²˜ì— ì¶”ê°€
# ============================================================

def parse_md_sections(filepath: str) -> List[dict]:
    """
    MD íŒŒì¼ì„ ## (H2) í—¤ë” ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜ ë¶„ë¦¬
    H2 ì„¹ì…˜ì€ í•˜ìœ„ H3~H6ê¹Œì§€ ëª¨ë‘ í¬í•¨!
    
    "## 11. ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ ì‚°ì¶œ ë¡œì§" ê²€ìƒ‰í•˜ë©´
    â†’ 11-1, 11-2, 11-3 ì„œë¸Œì„¹ì…˜ ë‚´ìš©ê¹Œì§€ ì „ë¶€ í¬í•¨
    
    Returns:
        [{"header": "7. ì•™ìƒë¸” ëª¨ë¸ 5ê°œ ê·œì¹™ (ìƒì„¸)", 
          "level": 2, 
          "content": "ì„¹ì…˜ ì „ì²´ ë‚´ìš© (í•˜ìœ„ í¬í•¨)...",
          "sub_headers": ["7-1. íˆ¬í‘œ êµ¬ì„±", "7-2. ìµœì¢… íŒì • 5ê°œ ê·œì¹™"],
          "keywords": ["ì•™ìƒë¸”", "ê·œì¹™", "íˆ¬í‘œ", ...]}, ...]
    """
    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
    except:
        return []
    
    # === 1ë‹¨ê³„: H2 ê¸°ì¤€ìœ¼ë¡œ í° ë¸”ë¡ ë¶„ë¦¬ ===
    h2_sections = []
    current_lines = []
    current_header = ""
    current_sub_headers = []
    
    for line in lines:
        header_match = re.match(r'^(#{1,6})\s+(.+)', line.strip())
        
        if header_match:
            level = len(header_match.group(1))
            header_text = header_match.group(2).strip()
            
            if level <= 2:  # H1 ë˜ëŠ” H2 â†’ ìƒˆ ë¸”ë¡ ì‹œì‘
                # ì´ì „ ë¸”ë¡ ì €ì¥
                if current_lines:
                    content = ''.join(current_lines).strip()
                    if len(content) > 20:
                        h2_sections.append({
                            "header": current_header,
                            "level": 2,
                            "content": content,
                            "sub_headers": current_sub_headers[:],
                            "keywords": _extract_section_keywords(content, current_header),
                        })
                
                current_header = header_text
                current_lines = [line]
                current_sub_headers = []
            else:
                # H3~H6 â†’ í˜„ì¬ H2 ë¸”ë¡ì— í¬í•¨
                current_lines.append(line)
                current_sub_headers.append(header_text)
        else:
            current_lines.append(line)
    
    # ë§ˆì§€ë§‰ ë¸”ë¡
    if current_lines:
        content = ''.join(current_lines).strip()
        if len(content) > 20:
            h2_sections.append({
                "header": current_header,
                "level": 2,
                "content": content,
                "sub_headers": current_sub_headers[:],
                "keywords": _extract_section_keywords(content, current_header),
            })
    
    # === 2ë‹¨ê³„: H3 ì„œë¸Œì„¹ì…˜ë„ ë…ë¦½ í•­ëª©ìœ¼ë¡œ ì¶”ê°€ (ì„¸ë°€ ê²€ìƒ‰ìš©) ===
    all_sections = list(h2_sections)  # H2 ë¸”ë¡ ë³µì‚¬
    
    for line_text in ''.join(lines).split('\n'):
        pass  # H3ëŠ” ì´ë¯¸ H2ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë³„ë„ ì¶”ê°€ ì•ˆ í•¨
    
    # H3 ì„œë¸Œì„¹ì…˜ë„ ë…ë¦½ì ìœ¼ë¡œ ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ ì¶”ê°€
    current_h3_lines = []
    current_h3_header = ""
    parent_h2 = ""
    
    for line in lines:
        header_match = re.match(r'^(#{1,6})\s+(.+)', line.strip())
        if header_match:
            level = len(header_match.group(1))
            header_text = header_match.group(2).strip()
            
            if level == 2:
                # H3 ë¸”ë¡ ì €ì¥
                if current_h3_lines and current_h3_header:
                    content = ''.join(current_h3_lines).strip()
                    if len(content) > 30:
                        all_sections.append({
                            "header": current_h3_header,
                            "level": 3,
                            "content": content,
                            "sub_headers": [],
                            "keywords": _extract_section_keywords(content, current_h3_header),
                            "parent": parent_h2,
                        })
                parent_h2 = header_text
                current_h3_lines = []
                current_h3_header = ""
            elif level == 3:
                # ì´ì „ H3 ì €ì¥
                if current_h3_lines and current_h3_header:
                    content = ''.join(current_h3_lines).strip()
                    if len(content) > 30:
                        all_sections.append({
                            "header": current_h3_header,
                            "level": 3,
                            "content": content,
                            "sub_headers": [],
                            "keywords": _extract_section_keywords(content, current_h3_header),
                            "parent": parent_h2,
                        })
                current_h3_header = header_text
                current_h3_lines = [line]
            else:
                current_h3_lines.append(line)
        else:
            current_h3_lines.append(line)
    
    # ë§ˆì§€ë§‰ H3
    if current_h3_lines and current_h3_header:
        content = ''.join(current_h3_lines).strip()
        if len(content) > 30:
            all_sections.append({
                "header": current_h3_header,
                "level": 3,
                "content": content,
                "sub_headers": [],
                "keywords": _extract_section_keywords(content, current_h3_header),
                "parent": parent_h2,
            })
    
    return all_sections


def _extract_section_keywords(content: str, header: str) -> List[str]:
    """ì„¹ì…˜ì—ì„œ ê²€ìƒ‰ìš© í‚¤ì›Œë“œ ì¶”ì¶œ"""
    keywords = set()
    
    # í—¤ë”ì—ì„œ (ë²ˆí˜¸ ì œê±° í›„)
    clean_header = re.sub(r'^\d+[-.]?\d*\.?\s*', '', header)
    keywords.add(clean_header.lower())
    for word in re.split(r'[\s/()ï¼ˆï¼‰\[\]]+', clean_header):
        if len(word) >= 2:
            keywords.add(word.lower())
    
    # ë³¼ë“œ í…ìŠ¤íŠ¸ (**text**)
    for m in re.finditer(r'\*\*([^*]+)\*\*', content):
        keywords.add(m.group(1).lower().strip())
    
    # ì¸ë¼ì¸ ì½”ë“œ (`code`)
    for m in re.finditer(r'`([^`]{2,40})`', content):
        keywords.add(m.group(1).lower())
    
    # í•œêµ­ì–´ ëª…ì‚¬ (2~8ê¸€ì)
    stop_words = {'ìˆìœ¼ë©´', 'ì—†ìœ¼ë©´', 'ì•„ë‹ˆë©´', 'ì…ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ë©ë‹ˆë‹¤', 'ì—ì„œëŠ”', 'ìœ¼ë¡œì˜', 'ì´ë¯€ë¡œ', 'ë•Œë¬¸ì—'}
    for m in re.finditer(r'[ê°€-í£]{2,8}', content):
        word = m.group()
        if word not in stop_words:
            keywords.add(word)
    
    return list(keywords)[:80]


# ============================================================
# [2] ì„¹ì…˜ ë‹¨ìœ„ ê²€ìƒ‰ í•¨ìˆ˜ - search_knowledge() ë°”ë¡œ ì•„ë˜ì— ì¶”ê°€
# ============================================================

def search_knowledge_section(keyword: str, knowledge_dir: str, top_k: int = 3) -> List[dict]:
    """
    ì§€ì‹ë² ì´ìŠ¤ì—ì„œ í‚¤ì›Œë“œë¡œ **ì„¹ì…˜ ë‹¨ìœ„** ê²€ìƒ‰
    
    ê¸°ì¡´ search_knowledge()ëŠ” íŒŒì¼ ë‹¨ìœ„ â†’ ì´ í•¨ìˆ˜ëŠ” ì„¹ì…˜ ë‹¨ìœ„
    
    Args:
        keyword: "ì•™ìƒë¸” ëª¨ë¸ 5ê°œ ê·œì¹™" ë˜ëŠ” "ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ ì‚°ì¶œ ë¡œì§"
        knowledge_dir: KNOWLEDGE_DIR ê²½ë¡œ
        top_k: ìƒìœ„ Nê°œ ì„¹ì…˜ ë°˜í™˜
    
    Returns:
        [{"filename": "V10_4.md", 
          "header": "7. ì•™ìƒë¸” ëª¨ë¸ 5ê°œ ê·œì¹™ (ìƒì„¸)",
          "content": "í•´ë‹¹ ì„¹ì…˜ ì „ì²´ í…ìŠ¤íŠ¸",
          "score": 350,
          "matched": ["ì•™ìƒë¸”", "ê·œì¹™", "íˆ¬í‘œ"]}, ...]
    """
    # ì¿¼ë¦¬ í† í°í™” (ì¡°ì‚¬ ì œê±°)
    cleaned = re.sub(
        r'(ì„|ë¥¼|ì´|ê°€|ì€|ëŠ”|ì˜|ì—|ì—ì„œ|ë¡œ|ìœ¼ë¡œ|ë¶€í„°|ê¹Œì§€|ë„|ë§Œ|ì—ëŒ€í•´|ì¢€|ì¤˜|í•´ì¤˜|ì•Œë ¤ì¤˜|ì„¤ëª…í•´|ë­ì•¼|ì–´ë–»ê²Œ|ì— ëŒ€í•´)',
        ' ', keyword
    )
    
    tokens = []
    # í•œêµ­ì–´ í† í°
    for m in re.finditer(r'[ê°€-í£]{2,}', cleaned):
        tokens.append(m.group().lower())
    # ì˜ì–´/ìˆ«ì í† í°
    for m in re.finditer(r'[a-zA-Z0-9_]{2,}', cleaned):
        tokens.append(m.group().lower())
    
    if not tokens:
        tokens = [keyword.strip().lower()]
    
    logger.info(f"ğŸ” ì„¹ì…˜ê²€ìƒ‰: '{keyword}' â†’ í† í°: {tokens}")
    
    results = []
    
    try:
        for f in os.listdir(knowledge_dir):
            if not f.endswith(('.md', '.txt')):
                continue
            
            filepath = os.path.join(knowledge_dir, f)
            sections = parse_md_sections(filepath)
            
            for section in sections:
                score, matched = _score_section(section, tokens, keyword)
                if score >= 30:  # ìµœì†Œ ì ìˆ˜
                    results.append({
                        "filename": f,
                        "header": section["header"],
                        "level": section["level"],
                        "content": section["content"],
                        "score": score,
                        "matched": matched,
                    })
    except Exception as e:
        logger.error(f"ì„¹ì…˜ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    
    # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ
    results.sort(key=lambda x: x["score"], reverse=True)
    
    if results:
        logger.info(f"âœ… ì„¹ì…˜ê²€ìƒ‰ ê²°ê³¼ {len(results)}ê°œ (ìƒìœ„: {results[0]['header'][:30]}... ì ìˆ˜:{results[0]['score']})")
    
    return results[:top_k]


def _score_section(section: dict, tokens: List[str], original_query: str) -> tuple:
    """ì„¹ì…˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
    score = 0
    matched = []
    
    header_lower = section["header"].lower()
    content_lower = section["content"].lower()
    keywords_set = set(k.lower() for k in section.get("keywords", []))
    
    for token in tokens:
        t = token.lower()
        
        # í—¤ë” ë§¤ì¹­ (+100) - ê°€ì¥ ì¤‘ìš”!
        if t in header_lower:
            score += 100
            matched.append(token)
        
        # í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ë§¤ì¹­ (+40)
        if t in keywords_set:
            score += 40
            if token not in matched:
                matched.append(token)
        
        # ë³¸ë¬¸ ë¹ˆë„ (+15 * count, max 60)
        count = content_lower.count(t)
        if count > 0:
            score += min(count * 15, 60)
            if token not in matched:
                matched.append(token)
    
    # ì›ë³¸ ì¿¼ë¦¬ êµ¬ì ˆ ë§¤ì¹­ ë³´ë„ˆìŠ¤ (+80)
    q = original_query.lower()
    if len(q) >= 4 and q in content_lower:
        score += 80
    
    # H2 ì„¹ì…˜ ìš°ëŒ€ (ìƒìœ„ ì„¹ì…˜ì¼ìˆ˜ë¡ ì¤‘ìš”)
    if section.get("level") == 2:
        score = int(score * 1.3)
    elif section.get("level", 0) >= 4:
        score = int(score * 0.7)
    
    return score, matched


# ============================================================
# [3] process_chat() ìë™ ì§€ì‹ê²€ìƒ‰ ë¶€ë¶„ êµì²´ ì½”ë“œ
# ============================================================
"""
â˜…â˜…â˜… pc_assistant.py ìˆ˜ì • í¬ì¸íŠ¸ â˜…â˜…â˜…

í˜„ì¬ ì½”ë“œ (1547~1640ë²ˆì¤„ ê·¼ì²˜):

    # â˜…â˜…â˜… LLMì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šì€ ê²½ìš° â†’ ìë™ ì§€ì‹ë² ì´ìŠ¤ íƒìƒ‰ â˜…â˜…â˜…
    ...
    auto_results = search_knowledge(clean_msg)    # â† íŒŒì¼ ë‹¨ìœ„ ê²€ìƒ‰
    ...
    doc_content = read_knowledge(filename)         # â† íŒŒì¼ ì „ì²´ ì½ê¸°
    ...

ì´ê²ƒì„ ì•„ë˜ë¡œ êµì²´:
"""

def auto_knowledge_search_section(user_message: str, knowledge_dir: str) -> Optional[str]:
    """
    process_chat() ë‚´ë¶€ì—ì„œ í˜¸ì¶œí•  ìë™ ì§€ì‹ë² ì´ìŠ¤ ì„¹ì…˜ ê²€ìƒ‰
    
    ê¸°ì¡´ auto_results = search_knowledge(clean_msg) ë¸”ë¡ì„ ì´ í•¨ìˆ˜ë¡œ êµì²´
    
    Returns:
        None: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â†’ ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ fallback
        str: LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ (ì„¹ì…˜ ë‚´ìš©ë§Œ)
    """
    # ì¡°ì‚¬/ì–´ë¯¸ ì œê±°
    clean_msg = re.sub(
        r'(ì•Œë ¤ì¤˜|ì„¤ëª…í•´ì¤˜|ë­ì•¼|ë­ì—ìš”|í•´ì¤˜|í• ë˜|ì— ëŒ€í•´|ì—ëŒ€í•´|ì¢€|ì¤˜|ìš”|ëŠ”|ì€|ì´|ê°€|ì„|ë¥¼|ì˜|ë¡œ|ìœ¼ë¡œ|ì—ì„œ|ë¶€í„°|ê¹Œì§€|ì´ë‘|ë‘|í•˜ê³ |ê·¸ë¦¬ê³ |ë˜ëŠ”|ì´ë‚˜|ë‚˜|ì´ë“ )',
        '', user_message.lower()
    ).strip()
    
    if not clean_msg:
        clean_msg = user_message.lower()
    
    # ì„¹ì…˜ ë‹¨ìœ„ ê²€ìƒ‰
    section_results = search_knowledge_section(clean_msg, knowledge_dir, top_k=3)
    
    if not section_results:
        return None
    
    # ìƒìœ„ ì„¹ì…˜ë“¤ì„ í•©ì³ì„œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    MAX_CONTEXT = 12000
    context_parts = []
    total_len = 0
    doc_names = []
    
    top_score = section_results[0]["score"]
    
    for r in section_results:
        # 1ìœ„ì™€ ì ìˆ˜ ì°¨ì´ 50% ì´ìƒì´ë©´ ì œì™¸
        if r["score"] < top_score * 0.4:
            break
        
        remaining = MAX_CONTEXT - total_len
        if remaining < 500:
            break
        
        content = r["content"]
        if len(content) > remaining:
            content = content[:remaining] + "\n... (ì„¹ì…˜ ì¼ë¶€ ìƒëµ)"
        
        header_info = f"ğŸ“„ [{r['filename']}] {r['header']}"
        context_parts.append(f"{header_info}\n{content}")
        doc_names.append(f"{r['filename']}#{r['header']}")
        total_len += len(content)
    
    if not context_parts:
        return None
    
    combined = "\n\n---\n\n".join(context_parts)
    logger.info(f"ğŸ“š ì„¹ì…˜ ì»¨í…ìŠ¤íŠ¸: {len(doc_names)}ê°œ ì„¹ì…˜, {total_len}ì")
    
    return combined


# ============================================================
# [4] ì‹¤ì œ ì ìš© ì˜ˆì‹œ (process_chat ë‚´ë¶€)
# ============================================================
"""
â˜…â˜…â˜… pc_assistant.pyì˜ process_chat() í•¨ìˆ˜ ë‚´ì—ì„œ êµì²´í•  ë¶€ë¶„ â˜…â˜…â˜…

=== ê¸°ì¡´ ì½”ë“œ (1570~1640ë²ˆì¤„ ê·¼ì²˜) ===

    if kb_has_files and not is_pc_cmd and not is_greeting and not is_short:
        logger.info(f"ğŸ”„ ìë™ ì§€ì‹ë² ì´ìŠ¤ íƒìƒ‰: '{user_message}'")
        clean_msg = re.sub(r'(ì•Œë ¤ì¤˜|ì„¤ëª…í•´ì¤˜|...)', '', msg_lower).strip()
        if not clean_msg:
            clean_msg = msg_lower
        auto_results = search_knowledge(clean_msg)       # â† ì—¬ê¸°ë¥¼ ë³€ê²½
        if auto_results:
            ...
            for i, res_item in enumerate(auto_results):
                filename = res_item["filename"]
                doc_content = read_knowledge(filename)    # â† íŒŒì¼ ì „ì²´ ì½ê¸°
                ...

=== ìƒˆ ì½”ë“œ ===

    if kb_has_files and not is_pc_cmd and not is_greeting and not is_short:
        logger.info(f"ğŸ”„ ìë™ ì§€ì‹ë² ì´ìŠ¤ íƒìƒ‰(ì„¹ì…˜): '{user_message}'")
        
        # â˜… ì„¹ì…˜ ë‹¨ìœ„ ê²€ìƒ‰ìœ¼ë¡œ êµì²´
        section_context = auto_knowledge_search_section(user_message, KNOWLEDGE_DIR)
        
        if section_context:
            follow_up_prompt = f\"\"\"[ì‚¬ìš©ì ì§ˆë¬¸]
{user_message}

[ì°¸ê³  ë¬¸ì„œ ì„¹ì…˜]
{section_context}

ìœ„ ì„¹ì…˜ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ë‹µë³€í•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.\"\"\"

            follow_up_system = \"\"\"ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ì´ì ê¸°ìˆ  ë¬¸ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ë‹µë³€ í˜•ì‹]
**ğŸ“‹ í•µì‹¬ ìš”ì•½**
ì§ˆë¬¸ì— ëŒ€í•œ í•µì‹¬ ë‹µë³€ì„ 2~3ì¤„ë¡œ ìš”ì•½

**ğŸ“ ìƒì„¸ ë‚´ìš©**
ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ë‚´ìš©ì„ ì¶©ë¶„íˆ ìì„¸í•˜ê²Œ ì •ë¦¬

[ë‹µë³€ ê·œì¹™]
1. ë¬¸ì„œ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  ì¶©ë¶„íˆ ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
2. ë§ˆí¬ë‹¤ìš´ í‘œ(| --- |) ì‚¬ìš© ê¸ˆì§€. "- í•­ëª©: ê°’" í˜•íƒœë¡œ ë‚˜ì—´í•˜ì„¸ìš”.
3. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
4. ì ˆëŒ€ JSONì„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.\"\"\"

            result2 = call_llm(follow_up_prompt, follow_up_system, max_tokens=6000)
            if result2["success"]:
                content = result2["content"].strip()
                if content and not extract_tool_json(content):
                    return content
        
        # ì„¹ì…˜ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ íŒŒì¼ ë‹¨ìœ„ë¡œ fallback
        # (ê¸°ì¡´ search_knowledge ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€)


â˜…â˜…â˜… ë˜í•œ tool_name == "search_knowledge" ì²˜ë¦¬ ë¶€ë¶„ (1398ë²ˆì¤„)ë„ ë™ì¼í•˜ê²Œ êµì²´ ê°€ëŠ¥ â˜…â˜…â˜…

search_resultsê°€ ëŒì•„ì™”ì„ ë•Œ, read_knowledge(filename)ìœ¼ë¡œ íŒŒì¼ ì „ì²´ë¥¼ ì½ëŠ” ëŒ€ì‹ :
section_results = search_knowledge_section(user_message, KNOWLEDGE_DIR)
ë¡œ êµì²´í•˜ë©´ ì„¹ì…˜ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
"""


# ============================================================
# [5] í…ŒìŠ¤íŠ¸: ì´ íŒŒì¼ ë‹¨ë… ì‹¤í–‰ìœ¼ë¡œ ê²€ì¦
# ============================================================
if __name__ == "__main__":
    # V10_4 ì§€ì¹¨ì„œ MDê°€ ìˆëŠ” ê²½ë¡œë¡œ ë³€ê²½
    TEST_DIR = "./knowledge"
    
    test_queries = [
        "ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ ì‚°ì¶œ ë¡œì§",
        "ì˜ˆì¸¡ê°’ í™œìš© í™©ê¸ˆ íŒ¨í„´", 
        "ì•™ìƒë¸” ëª¨ë¸ 5ê°œ ê·œì¹™",
        "Feature ì—”ì§€ë‹ˆì–´ë§",
        "ì˜ˆì¸¡ìƒíƒœ 6ë¶„ë¥˜",
    ]
    
    print("=" * 70)
    print("ğŸ“š ì„¹ì…˜ ë‹¨ìœ„ ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    for q in test_queries:
        print(f"\n{'â”€' * 50}")
        print(f"ğŸ” ì§ˆë¬¸: {q}")
        print(f"{'â”€' * 50}")
        
        results = search_knowledge_section(q, TEST_DIR, top_k=2)
        
        if not results:
            print("  âŒ ê²°ê³¼ ì—†ìŒ")
            continue
        
        for i, r in enumerate(results, 1):
            print(f"\n  [{i}] ì ìˆ˜: {r['score']} | ë§¤ì¹­: {r['matched']}")
            print(f"      íŒŒì¼: {r['filename']}")
            print(f"      ì„¹ì…˜: {r['header']}")
            print(f"      ë‚´ìš©: {r['content'][:200]}...")
            print(f"      ê¸¸ì´: {len(r['content'])}ì")