#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
knowledge_search.py
ì„¹ì…˜ ë‹¨ìœ„ MD ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ëª¨ë“ˆ

ê¸°ëŠ¥:
1. MD íŒŒì¼ì„ ## í—¤ë” ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜ ë¶„ë¦¬
2. ì„¹ì…˜ë³„ í‚¤ì›Œë“œ ì¸ë±ì‹±
3. ìì—°ì–´ ì¿¼ë¦¬ â†’ ê´€ë ¨ ì„¹ì…˜ ê²€ìƒ‰ (í¼ì§€ ë§¤ì¹­)
4. LLM ì—°ë™ ì‹œ ê²€ìƒ‰ëœ ì„¹ì…˜ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬

ì‚¬ìš©ë²•:
    from knowledge_search import KnowledgeBase
    kb = KnowledgeBase("./knowledge")
    results = kb.search("ì•™ìƒë¸” ëª¨ë¸ 5ê°œ ê·œì¹™")
"""

import os
import re
import json
import logging
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, field, asdict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("KnowledgeSearch")


# ========================================
# ë°ì´í„° í´ë˜ìŠ¤
# ========================================
@dataclass
class Section:
    """MD íŒŒì¼ì˜ í•œ ì„¹ì…˜"""
    file: str           # ì›ë³¸ íŒŒì¼ëª…
    header: str         # ì„¹ì…˜ ì œëª© (## 11. ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ...)
    level: int          # í—¤ë” ë ˆë²¨ (1=H1, 2=H2, 3=H3...)
    content: str        # ì„¹ì…˜ ë³¸ë¬¸ (í—¤ë” í¬í•¨)
    line_start: int     # ì‹œì‘ ë¼ì¸
    line_end: int       # ë ë¼ì¸
    keywords: List[str] = field(default_factory=list)  # ì¶”ì¶œëœ í‚¤ì›Œë“œ
    parent_header: str = ""  # ìƒìœ„ ì„¹ì…˜ ì œëª©

    def to_dict(self):
        return {
            "file": self.file,
            "header": self.header,
            "level": self.level,
            "content": self.content[:500] + ("..." if len(self.content) > 500 else ""),
            "full_content": self.content,
            "line_start": self.line_start,
            "line_end": self.line_end,
            "keywords": self.keywords,
            "parent_header": self.parent_header,
        }


@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼"""
    section: Section
    score: float        # ê´€ë ¨ì„± ì ìˆ˜
    matched_terms: List[str]  # ë§¤ì¹­ëœ í‚¤ì›Œë“œë“¤

    def to_dict(self):
        return {
            "file": self.section.file,
            "header": self.section.header,
            "level": self.section.level,
            "content": self.section.content,
            "score": self.score,
            "matched_terms": self.matched_terms,
            "parent_header": self.section.parent_header,
            "lines": f"{self.section.line_start}-{self.section.line_end}",
        }


# ========================================
# MD íŒŒì„œ
# ========================================
class MDParser:
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì„¹ì…˜ ë‹¨ìœ„ë¡œ íŒŒì‹±"""

    @staticmethod
    def parse_file(filepath: str) -> List[Section]:
        """MD íŒŒì¼ì„ ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        filename = os.path.basename(filepath)

        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
        except Exception as e:
            logger.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {filepath} - {e}")
            return []

        sections = []
        current_section_lines = []
        current_header = ""
        current_level = 0
        current_start = 1
        parent_headers = {}  # level â†’ header ë§¤í•‘

        for i, line in enumerate(lines, 1):
            # í—¤ë” ê°ì§€ (# ~ ######)
            header_match = re.match(r'^(#{1,6})\s+(.+)', line.strip())

            if header_match:
                # ì´ì „ ì„¹ì…˜ ì €ì¥
                if current_section_lines:
                    content = ''.join(current_section_lines).strip()
                    if content and len(content) > 10:  # ë„ˆë¬´ ì§§ì€ ì„¹ì…˜ ì œì™¸
                        section = Section(
                            file=filename,
                            header=current_header or filename,
                            level=current_level,
                            content=content,
                            line_start=current_start,
                            line_end=i - 1,
                            keywords=MDParser._extract_keywords(content),
                            parent_header=parent_headers.get(current_level - 1, ""),
                        )
                        sections.append(section)

                # ìƒˆ ì„¹ì…˜ ì‹œì‘
                level = len(header_match.group(1))
                header = header_match.group(2).strip()
                parent_headers[level] = header

                current_header = header
                current_level = level
                current_section_lines = [line]
                current_start = i
            else:
                current_section_lines.append(line)

        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
        if current_section_lines:
            content = ''.join(current_section_lines).strip()
            if content and len(content) > 10:
                section = Section(
                    file=filename,
                    header=current_header or filename,
                    level=current_level,
                    content=content,
                    line_start=current_start,
                    line_end=len(lines),
                    keywords=MDParser._extract_keywords(content),
                    parent_header=parent_headers.get(current_level - 1, ""),
                )
                sections.append(section)

        logger.info(f"ğŸ“„ {filename}: {len(sections)}ê°œ ì„¹ì…˜ íŒŒì‹±ë¨")
        return sections

    @staticmethod
    def _extract_keywords(content: str) -> List[str]:
        """ì„¹ì…˜ ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = set()

        # 1. í—¤ë”ì—ì„œ í‚¤ì›Œë“œ (## ë’¤ì˜ í…ìŠ¤íŠ¸)
        for match in re.finditer(r'^#{1,6}\s+(.+)', content, re.MULTILINE):
            header_text = match.group(1).strip()
            # ë²ˆí˜¸ ì œê±° (7-1. â†’ ì œê±°)
            header_text = re.sub(r'^\d+[-.]?\d*\.?\s*', '', header_text)
            keywords.add(header_text.lower())
            # ê°œë³„ ë‹¨ì–´ë„ ì¶”ê°€
            for word in re.split(r'[\s/()ï¼ˆï¼‰\[\]]+', header_text):
                if len(word) >= 2:
                    keywords.add(word.lower())

        # 2. ë³¼ë“œ í…ìŠ¤íŠ¸ (**text**)
        for match in re.finditer(r'\*\*([^*]+)\*\*', content):
            keywords.add(match.group(1).lower().strip())

        # 3. ì½”ë“œ ë¸”ë¡ ë‚´ ì£¼ìš” ë³€ìˆ˜ëª…
        for match in re.finditer(r'`([^`]+)`', content):
            code = match.group(1).strip()
            if len(code) >= 2 and len(code) <= 50:
                keywords.add(code.lower())

        # 4. í…Œì´ë¸” í—¤ë” (| í‚¤ | ê°’ |)
        for match in re.finditer(r'\|\s*\*?\*?([^|*]+)\*?\*?\s*\|', content):
            cell = match.group(1).strip()
            if len(cell) >= 2 and cell not in ('---', 'í•­ëª©', 'ê°’', 'ì„¤ëª…', '#'):
                keywords.add(cell.lower())

        # 5. í•œêµ­ì–´ í•µì‹¬ ëª…ì‚¬ (2ê¸€ì ì´ìƒ)
        for match in re.finditer(r'[ê°€-í£]{2,10}', content):
            word = match.group()
            # ì¡°ì‚¬/ì–´ë¯¸ ì œì™¸
            if word not in ('ìˆìœ¼ë©´', 'ì—†ìœ¼ë©´', 'ì•„ë‹ˆë©´', 'ì…ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ë©ë‹ˆë‹¤',
                           'ì—ì„œëŠ”', 'ì—ì„œì˜', 'ìœ¼ë¡œì˜', 'ì´ë¯€ë¡œ', 'ë•Œë¬¸ì—', 'ê·¸ë˜ì„œ'):
                keywords.add(word.lower())

        return list(keywords)[:100]  # ìµœëŒ€ 100ê°œ


# ========================================
# ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì—”ì§„
# ========================================
class KnowledgeBase:
    """ì„¹ì…˜ ë‹¨ìœ„ ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰"""

    def __init__(self, knowledge_dir: str):
        self.knowledge_dir = knowledge_dir
        self.sections: List[Section] = []
        self.files: List[str] = []
        self._index()

    def _index(self):
        """ì§€ì‹ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  MD/TXT íŒŒì¼ ì¸ë±ì‹±"""
        self.sections = []
        self.files = []

        if not os.path.exists(self.knowledge_dir):
            os.makedirs(self.knowledge_dir, exist_ok=True)
            logger.warning(f"ğŸ“‚ ì§€ì‹ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ ìƒì„±: {self.knowledge_dir}")
            return

        for f in sorted(os.listdir(self.knowledge_dir)):
            if f.endswith(('.md', '.txt')):
                filepath = os.path.join(self.knowledge_dir, f)
                self.files.append(f)
                sections = MDParser.parse_file(filepath)
                self.sections.extend(sections)

        logger.info(f"ğŸ“š ì§€ì‹ë² ì´ìŠ¤ ì¸ë±ì‹± ì™„ë£Œ: {len(self.files)}ê°œ íŒŒì¼, {len(self.sections)}ê°œ ì„¹ì…˜")

    def refresh(self):
        """ì¸ë±ìŠ¤ ê°±ì‹ """
        self._index()

    def search(self, query: str, top_k: int = 5, min_score: int = 10) -> List[SearchResult]:
        """
        ìì—°ì–´ ì¿¼ë¦¬ë¡œ ê´€ë ¨ ì„¹ì…˜ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ì–´ (ì˜ˆ: "ì•™ìƒë¸” ëª¨ë¸ 5ê°œ ê·œì¹™")
            top_k: ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜
            min_score: ìµœì†Œ ì ìˆ˜ (ì´í•˜ ì œì™¸)

        Returns:
            SearchResult ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ)
        """
        if not self.sections:
            return []

        # ì¿¼ë¦¬ í† í°í™”
        query_tokens = self._tokenize_query(query)
        if not query_tokens:
            return []

        logger.info(f"ğŸ” ê²€ìƒ‰: '{query}' â†’ í† í°: {query_tokens}")

        results = []
        for section in self.sections:
            score, matched = self._score_section(section, query_tokens, query)
            if score >= min_score:
                results.append(SearchResult(
                    section=section,
                    score=score,
                    matched_terms=matched,
                ))

        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        results.sort(key=lambda r: r.score, reverse=True)
        return results[:top_k]

    def search_by_topic(self, topics: List[str], top_k_per_topic: int = 3) -> Dict[str, List[SearchResult]]:
        """
        ì—¬ëŸ¬ í† í”½ì„ í•œ ë²ˆì— ê²€ìƒ‰

        Args:
            topics: ["ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ", "í™©ê¸ˆ íŒ¨í„´", "ì•™ìƒë¸” 5ê°œ ê·œì¹™"]
            top_k_per_topic: í† í”½ë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜

        Returns:
            {"í† í”½": [SearchResult, ...], ...}
        """
        results = {}
        for topic in topics:
            results[topic] = self.search(topic, top_k=top_k_per_topic)
        return results

    def get_context_for_llm(self, query: str, max_tokens: int = 8000) -> str:
        """
        LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ ìƒì„±

        ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•©ì³ì„œ LLM í”„ë¡¬í”„íŠ¸ì— ë„£ì„ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë°˜í™˜
        """
        results = self.search(query, top_k=5)
        if not results:
            return ""

        context_parts = []
        total_len = 0

        for r in results:
            section_text = f"ğŸ“„ [{r.section.file}] {r.section.header}\n{r.section.content}"
            if total_len + len(section_text) > max_tokens * 4:  # ëŒ€ëµì  í† í° ì¶”ì •
                break
            context_parts.append(section_text)
            total_len += len(section_text)

        return "\n\n---\n\n".join(context_parts)

    def list_files(self) -> List[Dict]:
        """íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        result = []
        for f in self.files:
            filepath = os.path.join(self.knowledge_dir, f)
            size = os.path.getsize(filepath)
            sections = [s for s in self.sections if s.file == f]
            result.append({
                "filename": f,
                "size": f"{size:,}B",
                "sections": len(sections),
                "headers": [s.header for s in sections],
            })
        return result

    def list_all_sections(self) -> List[Dict]:
        """ëª¨ë“  ì„¹ì…˜ ëª©ì°¨ ë°˜í™˜"""
        return [
            {
                "file": s.file,
                "header": s.header,
                "level": s.level,
                "parent": s.parent_header,
                "content_length": len(s.content),
            }
            for s in self.sections
        ]

    def get_section_by_header(self, header_keyword: str) -> Optional[Section]:
        """í—¤ë” í‚¤ì›Œë“œë¡œ ì •í™•í•œ ì„¹ì…˜ ê°€ì ¸ì˜¤ê¸°"""
        keyword_lower = header_keyword.lower()
        for s in self.sections:
            if keyword_lower in s.header.lower():
                return s
        return None

    # ========================================
    # ë‚´ë¶€ ë©”ì„œë“œ
    # ========================================
    def _tokenize_query(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ë¥¼ ê²€ìƒ‰ í† í°ìœ¼ë¡œ ë¶„ë¦¬"""
        # ì¡°ì‚¬/ì–´ë¯¸ ì œê±°
        cleaned = re.sub(
            r'(ì„|ë¥¼|ì´|ê°€|ì€|ëŠ”|ì˜|ì—|ì—ì„œ|ë¡œ|ìœ¼ë¡œ|ë¶€í„°|ê¹Œì§€|ë„|ë§Œ|ì—ëŒ€í•´|ì— ëŒ€í•´|ì¢€|ì¤˜|í•´ì¤˜|ì•Œë ¤ì¤˜|ì„¤ëª…í•´|ë­ì•¼|ì–´ë–»ê²Œ)',
            ' ', query
        )

        tokens = []
        # ì˜ì–´ + ìˆ«ì í† í°
        for match in re.finditer(r'[a-zA-Z0-9_]+', cleaned):
            word = match.group().lower()
            if len(word) >= 2:
                tokens.append(word)

        # í•œêµ­ì–´ í† í° (2ê¸€ì ì´ìƒ)
        for match in re.finditer(r'[ê°€-í£]{2,}', cleaned):
            word = match.group()
            tokens.append(word)

        # ì›ë³¸ ì¿¼ë¦¬ì˜ í•µì‹¬ êµ¬ì ˆë„ ì¶”ê°€ (ì—°ì†ëœ í•œê¸€)
        for match in re.finditer(r'[ê°€-í£]{3,}', query):
            word = match.group()
            if word not in tokens:
                tokens.append(word)

        return list(set(tokens))

    def _score_section(self, section: Section, query_tokens: List[str], original_query: str) -> Tuple[float, List[str]]:
        """ì„¹ì…˜ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        matched = []

        header_lower = section.header.lower()
        content_lower = section.content.lower()
        keywords_set = set(k.lower() for k in section.keywords)

        for token in query_tokens:
            token_lower = token.lower()

            # 1. í—¤ë” ì •í™• ë§¤ì¹­ (+100)
            if token_lower in header_lower:
                score += 100
                matched.append(f"í—¤ë”:{token}")

            # 2. í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ë§¤ì¹­ (+50)
            elif token_lower in keywords_set:
                score += 50
                matched.append(f"í‚¤ì›Œë“œ:{token}")

            # 3. ë³¸ë¬¸ í¬í•¨ (+20 * ë¹ˆë„, ìµœëŒ€ 60)
            count = content_lower.count(token_lower)
            if count > 0:
                score += min(count * 20, 60)
                if f"ë³¸ë¬¸:{token}" not in matched:
                    matched.append(f"ë³¸ë¬¸:{token}({count}íšŒ)")

            # 4. ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ (3ê¸€ì ì´ìƒ, +10)
            if len(token) >= 3:
                for kw in keywords_set:
                    if token_lower in kw or kw in token_lower:
                        score += 10
                        break

        # 5. ì›ë³¸ ì¿¼ë¦¬ ì—°ì† ë§¤ì¹­ ë³´ë„ˆìŠ¤ (+50)
        # "ì•™ìƒë¸” ëª¨ë¸ 5ê°œ ê·œì¹™" â†’ ì´ êµ¬ì ˆì´ í†µì§¸ë¡œ ìˆìœ¼ë©´ ë³´ë„ˆìŠ¤
        query_lower = original_query.lower()
        if len(query_lower) >= 4 and query_lower in content_lower:
            score += 50
            matched.append(f"êµ¬ì ˆë§¤ì¹­:{original_query[:20]}")

        # 6. í—¤ë” ë ˆë²¨ ê°€ì¤‘ì¹˜ (H2 > H3 > H4)
        if section.level == 2:
            score *= 1.2  # H2 ì„¹ì…˜ ìš°ëŒ€
        elif section.level >= 4:
            score *= 0.8  # í•˜ìœ„ ì„¹ì…˜ ê°ì 

        return score, matched


# ========================================
# FastAPI ë¼ìš°í„° (ê¸°ì¡´ pc_assistantì— ì¶”ê°€ ê°€ëŠ¥)
# ========================================
def create_kb_router(knowledge_dir: str) -> "APIRouter":
    """ì§€ì‹ë² ì´ìŠ¤ API ë¼ìš°í„° ìƒì„±"""
    from fastapi import APIRouter
    from pydantic import BaseModel

    kb_router = APIRouter(prefix="/kb", tags=["knowledge-base"])
    kb = KnowledgeBase(knowledge_dir)

    class SearchRequest(BaseModel):
        query: str
        top_k: int = 5

    class MultiSearchRequest(BaseModel):
        topics: List[str]
        top_k_per_topic: int = 3

    @kb_router.get("/files")
    async def list_files():
        return {"success": True, "files": kb.list_files()}

    @kb_router.get("/sections")
    async def list_sections():
        return {"success": True, "sections": kb.list_all_sections()}

    @kb_router.post("/search")
    async def search(req: SearchRequest):
        results = kb.search(req.query, top_k=req.top_k)
        return {
            "success": True,
            "query": req.query,
            "count": len(results),
            "results": [r.to_dict() for r in results],
        }

    @kb_router.post("/multi_search")
    async def multi_search(req: MultiSearchRequest):
        all_results = kb.search_by_topic(req.topics, req.top_k_per_topic)
        return {
            "success": True,
            "results": {
                topic: [r.to_dict() for r in results]
                for topic, results in all_results.items()
            },
        }

    @kb_router.post("/context")
    async def get_context(req: SearchRequest):
        """LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜"""
        context = kb.get_context_for_llm(req.query)
        return {"success": True, "context": context}

    @kb_router.post("/refresh")
    async def refresh():
        kb.refresh()
        return {
            "success": True,
            "files": len(kb.files),
            "sections": len(kb.sections),
        }

    return kb_router


# ========================================
# í…ŒìŠ¤íŠ¸
# ========================================
if __name__ == "__main__":
    import sys

    # í…ŒìŠ¤íŠ¸ìš©
    test_dir = "./knowledge"
    if len(sys.argv) > 1:
        test_dir = sys.argv[1]

    kb = KnowledgeBase(test_dir)

    print(f"\nğŸ“š íŒŒì¼: {len(kb.files)}ê°œ, ì„¹ì…˜: {len(kb.sections)}ê°œ\n")

    # íŒŒì¼ ëª©ë¡
    for f in kb.list_files():
        print(f"  ğŸ“„ {f['filename']} ({f['size']}, {f['sections']}ê°œ ì„¹ì…˜)")

    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    test_queries = [
        "ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ ì‚°ì¶œ ë¡œì§",
        "ì˜ˆì¸¡ê°’ í™œìš© í™©ê¸ˆ íŒ¨í„´",
        "ì•™ìƒë¸” ëª¨ë¸ 5ê°œ ê·œì¹™",
    ]

    for q in test_queries:
        print(f"\n{'='*60}")
        print(f"ğŸ” ê²€ìƒ‰: {q}")
        print(f"{'='*60}")
        results = kb.search(q, top_k=3)
        for i, r in enumerate(results, 1):
            print(f"\n  [{i}] ì ìˆ˜: {r.score:.0f} | {r.section.file}")
            print(f"      í—¤ë”: {r.section.header}")
            print(f"      ë§¤ì¹­: {', '.join(r.matched_terms)}")
            print(f"      ë¯¸ë¦¬ë³´ê¸°: {r.section.content[:150]}...")