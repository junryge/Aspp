#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
pc_assistant.py
PC ê°œì¸ë¹„ì„œ AI (Moltbot ìŠ¤íƒ€ì¼ Tool Calling) v0.2
- ìŠ¤í¬ë¦°ìƒ·: ì „ìš© í´ë” ì €ì¥ + ì›¹ ì¸ë¼ì¸ í‘œì‹œ
- íŒŒì¼ íƒìƒ‰ê¸°/ë©”ëª¨ì¥ ì‹¤í–‰ ì œê±°
"""

import os
import re
import json
import subprocess
import platform
import psutil
import tempfile
import datetime
import webbrowser
import fnmatch
import requests
import pandas as pd
from typing import Optional, List
from fastapi import FastAPI, APIRouter, UploadFile, File
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("PCAssistant")

router = APIRouter(prefix="/assistant", tags=["assistant"])
app = FastAPI(title="ì§í‰ ëª°íŠ¸ë´‡ ê°ë§ˆë²„ì „ VER 0.2")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ========================================
# Global Configuration
# ========================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# â˜… ë¡œì»¬ GGUF ëª¨ë¸ ì„¤ì • (ìƒì„± íŒŒë¼ë¯¸í„° í¬í•¨)
AVAILABLE_MODELS = {
    "qwen3-14b": {
        "path": os.path.join(BASE_DIR, "Qwen3-14B-Q4_K_M.gguf"),
        "name": "Qwen3-14B (Q4_K_M)",
        "desc": "í•œê¸€ ìµœì í™” â­ì¶”ì²œ",
        "ctx": 8192,
        "gpu_layers": 50,
        "chat_format": "chatml",
        "repeat_penalty": 1.1,
        "temperature": 0.7,
        "korean_support": True
    },
    "mistral-nemo": {
        "path": os.path.join(BASE_DIR, "Mistral-Nemo-12B.Q6_K.gguf"),
        "name": "Mistral-Nemo-12B (Q6_K)",
        "desc": "í•œê¸€ ë³´í†µ, ì½”ë”© ê°•ì ",
        "ctx": 8192,
        "gpu_layers": 50,
        "chat_format": "llama2",
        "repeat_penalty": 1.2,
        "temperature": 0.6,
        "korean_support": True
    },
    "phi3-mini": {
        "path": os.path.join(BASE_DIR, "Phi-3-mini-4k-instruct-Q5_K_M.gguf"),
        "name": "Phi-3-Mini (Q5_K_M)",
        "desc": "âš ï¸í•œê¸€ ë¶ˆì•ˆì •, ì˜ì–´ ì „ìš©",
        "ctx": 4096,
        "gpu_layers": 35,
        "chat_format": "phi3",
        "repeat_penalty": 1.15,
        "temperature": 0.7,
        "korean_support": False
    }
}
CURRENT_LOCAL_MODEL = "qwen3-14b"
GGUF_MODEL_PATH = AVAILABLE_MODELS[CURRENT_LOCAL_MODEL]["path"]

LOCAL_LLM = None
CHAT_HISTORY = []
HISTORY_FILE = os.path.join(BASE_DIR, "chat_history.json")

# â˜… í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
TOKEN_USAGE = {
    "prompt_tokens": 0,
    "completion_tokens": 0,
    "total_tokens": 0,
    "call_count": 0
}

# â˜… ìŠ¤í¬ë¦°ìƒ· ì „ìš© í´ë”
SCREENSHOT_DIR = os.path.join(BASE_DIR, "screenshots")
os.makedirs(SCREENSHOT_DIR, exist_ok=True)

# â˜… ì§€ì‹ë² ì´ìŠ¤(MD ë¬¸ì„œ) í´ë”
KNOWLEDGE_DIR = os.path.join(BASE_DIR, "knowledge")
os.makedirs(KNOWLEDGE_DIR, exist_ok=True)

# â˜… ê³¼ê±°ì§€ì‹ ë³´ê´€ í´ë”
KNOWLEDGE_ARCHIVE_DIR = os.path.join(BASE_DIR, "knowledge_archive")
os.makedirs(KNOWLEDGE_ARCHIVE_DIR, exist_ok=True)

# â˜… LLM ìƒì„± íŒŒë¼ë¯¸í„° (UIì—ì„œ ì¡°ì ˆ ê°€ëŠ¥)
LLM_PARAMS = {
    "temperature": 0.7,
    "repeat_penalty": 1.1,
    "max_tokens": 4096
}

LLM_MODE = "local"
API_TOKEN = None

ENV_CONFIG = {
    "dev": {
        "url": "http://dev.assistant.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Coder-30B-A3B-Instruct",
        "name": "DEV(30B)"
    },
    "prod": {
        "url": "http://summary.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Next-80B-A3B-Instruct",
        "name": "PROD(80B)"
    },
    "common": {
        "url": "http://common.llm.skhynix.com/v1/chat/completions",
        "model": "gpt-oss-20b",
        "name": "COMMON(20B)"
    }
}
CURRENT_ENV = "common"
API_URL = ENV_CONFIG["common"]["url"]
API_MODEL = ENV_CONFIG["common"]["model"]

# ========================================
# System Prompt
# ========================================
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ 'ì§í‰ ëª°íŠ¸ë´‡ ê°ë§ˆë²„ì „ VER 0.2'ì´ë¼ëŠ” PC ê°œì¸ë¹„ì„œ AIì…ë‹ˆë‹¤.

â˜…â˜…â˜… ìµœìš°ì„  ê·œì¹™: ì§€ì‹ë² ì´ìŠ¤ ìš°ì„  ê²€ìƒ‰ â˜…â˜…â˜…
ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ ë°˜ë“œì‹œ ì´ ìˆœì„œë¥¼ ë”°ë¥´ì„¸ìš”:

1ë‹¨ê³„: ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ (í•„ìˆ˜)
- ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”
- ë°˜ë“œì‹œ search_knowledgeë¡œ ë¨¼ì € ê²€ìƒ‰í•˜ì„¸ìš”
- ì˜ˆ: {"tool": "search_knowledge", "keyword": "ì¶”ì¶œí•œí‚¤ì›Œë“œ"}

2ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
- ê´€ë ¨ ë¬¸ì„œê°€ ìˆìœ¼ë©´ â†’ ê·¸ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
- ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ â†’ 3ë‹¨ê³„ë¡œ

3ë‹¨ê³„: ì¼ë°˜ ëŒ€í™”ë¡œ ë‹µë³€
- ì§€ì‹ë² ì´ìŠ¤ì— ì—†ëŠ” ë‚´ìš©ì€ "ì§€ì‹ë² ì´ìŠ¤ì— ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë¨¼ì € ë§í•˜ê³ 
- ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ê±°ë‚˜, ëª¨ë¥´ë©´ "ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"ë¼ê³  ì†”ì§íˆ ë‹µë³€

â˜… ì ˆëŒ€ í•˜ì§€ ë§ ê²ƒ:
- ì›¹ê²€ìƒ‰(google_search) ì„ì˜ë¡œ ì‚¬ìš© ê¸ˆì§€ (ì‚¬ìš©ìê°€ "ê²€ìƒ‰í•´ì¤˜"ë¼ê³  í•  ë•Œë§Œ)
- íŒŒì¼ê²€ìƒ‰(search_files) ì„ì˜ë¡œ ì‚¬ìš© ê¸ˆì§€ (ì‚¬ìš©ìê°€ "íŒŒì¼ ì°¾ì•„ì¤˜"ë¼ê³  í•  ë•Œë§Œ)
- ì‹œìŠ¤í…œì •ë³´(get_system_info) ì„ì˜ë¡œ ì‚¬ìš© ê¸ˆì§€ (ì‚¬ìš©ìê°€ "ì‹œìŠ¤í…œ ì •ë³´"ë¼ê³  í•  ë•Œë§Œ)
- ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì—†ì´ ë°”ë¡œ ë‹¤ë¥¸ ë„êµ¬ ì‚¬ìš© ê¸ˆì§€

[ë„êµ¬ í˜¸ì¶œ í˜•ì‹]
JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ë¶™ì´ì§€ ë§ˆì„¸ìš”.

[ì§€ì‹ë² ì´ìŠ¤ ë„êµ¬] - í•­ìƒ ë¨¼ì € ì‚¬ìš©
- ì§€ì‹ê²€ìƒ‰: {"tool": "search_knowledge", "keyword": "í‚¤ì›Œë“œ"}
- ì§€ì‹ëª©ë¡: {"tool": "list_knowledge"}
- ì§€ì‹ì½ê¸°: {"tool": "read_knowledge", "filename": "íŒŒì¼ëª….md"}

[PC ë„êµ¬] - ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•  ë•Œë§Œ
- ì‹œìŠ¤í…œì •ë³´: {"tool": "get_system_info"} â† "ì‹œìŠ¤í…œ ì •ë³´ ì•Œë ¤ì¤˜"
- ìŠ¤í¬ë¦°ìƒ·: {"tool": "screenshot"} â† "ìŠ¤í¬ë¦°ìƒ· ì°ì–´ì¤˜"
- í˜„ì¬ì‹œê°„: {"tool": "get_time"} â† "ì§€ê¸ˆ ëª‡ì‹œì•¼"
- í”„ë¡œê·¸ë¨ì‹¤í–‰: {"tool": "run_program", "program": "notepad"} â† "ë©”ëª¨ì¥ ì‹¤í–‰í•´ì¤˜"
- íŒŒì¼ê²€ìƒ‰: {"tool": "search_files", "keyword": "ë¬¸ì„œ", "path": "C:/"} â† "íŒŒì¼ ì°¾ì•„ì¤˜"
- ì›¹ì—´ê¸°/ê²€ìƒ‰: {"tool": "google_search", "query": "ê²€ìƒ‰ì–´"} â† "ê²€ìƒ‰í•´ì¤˜"
- ìµœì‹ ë‰´ìŠ¤: {"tool": "latest_news"} â† "ë‰´ìŠ¤ ë³´ì—¬ì¤˜"

ì¼ë°˜ ëŒ€í™”ëŠ” í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”."""


# ========================================
# LLM Functions
# ========================================
def load_local_model(model_key: str = None):
    """ë¡œì»¬ GGUF ëª¨ë¸ ë¡œë“œ (model_keyë¡œ ëª¨ë¸ ì„ íƒ)"""
    global LOCAL_LLM, CURRENT_LOCAL_MODEL, GGUF_MODEL_PATH

    if model_key is None:
        model_key = CURRENT_LOCAL_MODEL

    if model_key not in AVAILABLE_MODELS:
        logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_key}")
        return None

    model_config = AVAILABLE_MODELS[model_key]
    model_path = model_config["path"]

    if not os.path.exists(model_path):
        logger.error(f"GGUF íŒŒì¼ ì—†ìŒ: {model_path}")
        return None

    try:
        from llama_cpp import Llama
        logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_config['name']}...")
        llm = Llama(
            model_path=model_path,
            n_ctx=model_config.get("ctx", 8192),
            n_threads=8,
            n_gpu_layers=model_config.get("gpu_layers", 50),
            n_batch=512,
            verbose=False
        )
        CURRENT_LOCAL_MODEL = model_key
        GGUF_MODEL_PATH = model_path
        logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_config['name']}")
        return llm
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def get_available_local_models() -> List[dict]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë¡œì»¬ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    models = []
    for key, config in AVAILABLE_MODELS.items():
        exists = os.path.exists(config["path"])
        models.append({
            "key": key,
            "name": config["name"],
            "desc": config.get("desc", ""),
            "available": exists,
            "current": key == CURRENT_LOCAL_MODEL,
            "korean_support": config.get("korean_support", True)
        })
    return models


def load_api_token():
    global API_TOKEN
    paths = [
        os.path.join(BASE_DIR, "token.txt"),
        os.path.join(BASE_DIR, "api_token.txt"),
        "token.txt",
        "../token.txt",
        os.path.expanduser("~/token.txt")
    ]
    for p in paths:
        if os.path.exists(p):
            try:
                with open(p, 'r', encoding='utf-8') as f:
                    API_TOKEN = f.read().strip()
                if API_TOKEN and "REPLACE" not in API_TOKEN:
                    logger.info(f"âœ… API í† í° ë¡œë“œ: {p}")
                    return True
            except Exception as e:
                logger.error(f"âŒ í† í° ë¡œë“œ ì‹¤íŒ¨: {e}")
    logger.warning("âš ï¸ API í† í° íŒŒì¼ ì—†ìŒ")
    return False


def call_local_llm(prompt: str, system_prompt: str = "", max_tokens: int = 4096) -> dict:
    global LOCAL_LLM, CURRENT_LOCAL_MODEL, LLM_PARAMS
    if LOCAL_LLM is None:
        return {"success": False, "error": "ë¡œì»¬ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}

    # â˜… UIì—ì„œ ì„¤ì •í•œ íŒŒë¼ë¯¸í„° ì‚¬ìš© (LLM_PARAMS ìš°ì„ )
    temperature = LLM_PARAMS.get("temperature", 0.7)
    repeat_penalty = LLM_PARAMS.get("repeat_penalty", 1.1)
    actual_max_tokens = LLM_PARAMS.get("max_tokens", max_tokens)

    # â˜… ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ í˜•ì‹
    if CURRENT_LOCAL_MODEL == "mistral-nemo":
        # Mistral-Nemo: ChatML í˜•ì‹ ì‚¬ìš© (ê³µì‹ ê¶Œì¥)
        full_prompt = f"""<s>[INST] <<SYS>>
{system_prompt}
<</SYS>>

{prompt} [/INST]"""
        stop_tokens = ["</s>", "[INST]"]
    elif CURRENT_LOCAL_MODEL == "phi3-mini":
        # Phi-3: ì˜ì–´ ìœ„ì£¼ ëª¨ë¸ (í•œê¸€ ì¶œë ¥ ë¶ˆì•ˆì •)
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì˜ì–´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
        full_prompt = f"""<|system|>
You are a helpful PC assistant. Respond in Korean. Follow JSON tool format strictly.
{system_prompt}<|end|>
<|user|>
{prompt}<|end|>
<|assistant|>"""
        stop_tokens = ["<|end|>", "<|user|>", "<|system|>"]
    else:
        # Qwen3 (ê¸°ë³¸): ChatML í˜•ì‹ - í•œê¸€ ìµœì í™”
        full_prompt = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{prompt}<|im_end|>
<|im_start|>assistant
"""
        stop_tokens = ["<|im_end|>", "<|im_start|>"]

    try:
        output = LOCAL_LLM(
            full_prompt,
            max_tokens=actual_max_tokens,
            temperature=temperature,
            repeat_penalty=repeat_penalty,  # â˜… ë°˜ë³µ ì–µì œ
            stop=stop_tokens,
            echo=False
        )
        content = output["choices"][0]["text"].strip()
        content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
        return {"success": True, "content": content}
    except Exception as e:
        return {"success": False, "error": str(e)}


def call_api_llm(prompt: str, system_prompt: str = "", max_tokens: int = 4096) -> dict:
    global API_TOKEN
    if not API_TOKEN:
        return {"success": False, "error": "API í† í° ì—†ìŒ"}

    headers = {
        "Authorization": f"Bearer {API_TOKEN}",
        "Content-Type": "application/json"
    }
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    data = {
        "model": API_MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": 0.3
    }
    try:
        response = requests.post(API_URL, headers=headers, json=data, timeout=300)
        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
            
            # â˜… í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì 
            usage = result.get("usage", {})
            if usage:
                TOKEN_USAGE["prompt_tokens"] += usage.get("prompt_tokens", 0)
                TOKEN_USAGE["completion_tokens"] += usage.get("completion_tokens", 0)
                TOKEN_USAGE["total_tokens"] += usage.get("total_tokens", 0)
                TOKEN_USAGE["call_count"] += 1
                logger.info(f"ğŸ“Š í† í°: +{usage.get('total_tokens', 0)} (ëˆ„ì : {TOKEN_USAGE['total_tokens']})")
            
            return {"success": True, "content": content}
        else:
            return {"success": False, "error": f"API ì˜¤ë¥˜: {response.status_code}"}
    except Exception as e:
        return {"success": False, "error": str(e)}


def call_llm(prompt: str, system_prompt: str = "", max_tokens: int = 4096) -> dict:
    if LLM_MODE == "local":
        return call_local_llm(prompt, system_prompt, max_tokens)
    else:
        return call_api_llm(prompt, system_prompt, max_tokens)


# ========================================
# Tool Functions
# ========================================
def search_files(keyword: str, path: str = "C:/", limit: int = 50) -> List[dict]:
    results = []
    logger.info(f"íŒŒì¼ ê²€ìƒ‰: '{keyword}' in '{path}'")
    try:
        for root, dirs, files in os.walk(path):
            for name in files + dirs:
                if keyword.lower() in name.lower():
                    full_path = os.path.join(root, name)
                    is_dir = os.path.isdir(full_path)
                    try:
                        size = os.path.getsize(full_path) if not is_dir else 0
                        size_str = f"{size / (1024**3):.2f}GB" if size > 1024**3 else f"{size / (1024**2):.1f}MB" if size > 1024**2 else f"{size}B"
                    except:
                        size_str = "?"
                    results.append({
                        "name": name, "path": full_path,
                        "type": "í´ë”" if is_dir else "íŒŒì¼", "size": size_str
                    })
                    if len(results) >= limit:
                        return results
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    return results


def search_content(keyword: str, path: str = "C:/", limit: int = 30) -> List[dict]:
    results = []
    extensions = ['.txt', '.py', '.md', '.json', '.html', '.css', '.js', '.csv', '.log']
    try:
        for root, dirs, files in os.walk(path):
            for name in files:
                ext = os.path.splitext(name)[1].lower()
                if ext in extensions:
                    full_path = os.path.join(root, name)
                    try:
                        with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read(50000)
                            if keyword.lower() in content.lower():
                                idx = content.lower().find(keyword.lower())
                                snippet = content[max(0, idx-30):min(len(content), idx+70)].replace('\n', ' ')
                                results.append({"name": name, "path": full_path, "snippet": f"...{snippet}..."})
                                if len(results) >= limit:
                                    return results
                    except:
                        continue
    except Exception as e:
        logger.error(f"ë‚´ìš© ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    return results


def get_system_info() -> dict:
    drives = []
    for p in psutil.disk_partitions():
        try:
            usage = psutil.disk_usage(p.mountpoint)
            drives.append({"drive": p.device, "total": f"{usage.total / (1024**3):.1f}GB", "used": f"{usage.percent}%"})
        except:
            pass
    return {
        "os": f"{platform.system()} {platform.release()}",
        "cpu": f"{psutil.cpu_count()}ì½”ì–´, {psutil.cpu_percent()}%",
        "memory": f"{psutil.virtual_memory().total // (1024**3)}GB, {psutil.virtual_memory().percent}%",
        "drives": drives
    }


def list_directory(path: str) -> List[dict]:
    items = []
    try:
        for name in os.listdir(path)[:50]:
            full_path = os.path.join(path, name)
            is_dir = os.path.isdir(full_path)
            try:
                size = os.path.getsize(full_path) if not is_dir else 0
                modified = datetime.datetime.fromtimestamp(os.path.getmtime(full_path)).strftime("%Y-%m-%d %H:%M")
            except:
                size = 0
                modified = "?"
            items.append({"name": name, "type": "í´ë”" if is_dir else "íŒŒì¼", "size": f"{size:,}" if not is_dir else "-", "modified": modified})
    except Exception as e:
        return [{"error": str(e)}]
    return items


def read_file(path: str, max_chars: int = 5000) -> str:
    try:
        with open(path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read(max_chars)
            if len(content) == max_chars:
                content += "\n... (íŒŒì¼ì´ ë„ˆë¬´ ì»¤ì„œ ì¼ë¶€ë§Œ í‘œì‹œ)"
            return content
    except Exception as e:
        return f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}"


def run_program(program: str) -> str:
    try:
        subprocess.Popen(program, shell=True)
        return f"'{program}' ì‹¤í–‰ë¨"
    except Exception as e:
        return f"ì‹¤í–‰ ì˜¤ë¥˜: {e}"


def kill_program(name: str) -> str:
    try:
        killed = 0
        for proc in psutil.process_iter(['name']):
            if name.lower() in proc.info['name'].lower():
                proc.kill()
                killed += 1
        return f"{killed}ê°œ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œë¨"
    except Exception as e:
        return f"ì¢…ë£Œ ì˜¤ë¥˜: {e}"


def open_web(url: str) -> str:
    if not url.startswith('http'):
        url = 'https://' + url
    webbrowser.open(url)
    return f"'{url}' ì—´ë¦¼"


def google_search(query: str) -> str:
    url = f"https://www.google.com/search?q={query}"
    webbrowser.open(url)
    return f"'{query}' ê²€ìƒ‰ ì¤‘..."


def get_time() -> str:
    now = datetime.datetime.now()
    return f"{now.strftime('%Yë…„ %mì›” %dì¼ %A %Hì‹œ %Më¶„ %Sì´ˆ')}"


# â˜… ìŠ¤í¬ë¦°ìƒ·: ì „ìš© í´ë” ì €ì¥ + URL ë°˜í™˜
def take_screenshot() -> dict:
    """ìŠ¤í¬ë¦°ìƒ· ì°ê³  ì „ìš© í´ë”ì— ì €ì¥, ì›¹ í‘œì‹œìš© URL ë°˜í™˜"""
    try:
        from PIL import ImageGrab
        filename = f"screenshot_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        filepath = os.path.join(SCREENSHOT_DIR, filename)
        img = ImageGrab.grab()
        img.save(filepath)
        logger.info(f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {filepath}")
        # ì›¹ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•œ URL ë°˜í™˜
        return {
            "success": True,
            "filename": filename,
            "path": filepath,
            "url": f"/assistant/screenshots/{filename}"
        }
    except ImportError:
        return {"success": False, "error": "PIL(Pillow) ë¯¸ì„¤ì¹˜. pip install Pillow"}
    except Exception as e:
        return {"success": False, "error": str(e)}


# â˜… ìµœì‹ ë‰´ìŠ¤: ë…ë¦½ ë¸Œë¼ìš°ì € ì°½ ì—´ê¸° â†’ ìŠ¤í¬ë¦°ìƒ· â†’ ê·¸ ì°½ë§Œ ë‹«ê¸°
def latest_news() -> dict:
    """êµ¬ê¸€ë‰´ìŠ¤ë¥¼ ë…ë¦½ ë¸Œë¼ìš°ì €ë¡œ ì—´ê³ , ìŠ¤í¬ë¦°ìƒ· ì°ê³ , ê·¸ ì°½ë§Œ ë‹«ê¸°"""
    import time
    import shutil
    
    news_proc = None
    temp_profile = None
    
    try:
        news_url = "https://news.google.com/home?hl=ko&gl=KR&ceid=KR:ko"
        
        # ì„ì‹œ í”„ë¡œí•„ í´ë” (ë…ë¦½ Chrome ì¸ìŠ¤í„´ìŠ¤ìš©)
        temp_profile = os.path.join(tempfile.gettempdir(), "chrome_news_temp")
        
        # 1. Chrome ì°¾ê¸°
        chrome_paths = [
            r"C:\Program Files\Google\Chrome\Application\chrome.exe",
            r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe",
            os.path.expanduser(r"~\AppData\Local\Google\Chrome\Application\chrome.exe"),
        ]
        
        chrome_exe = None
        for p in chrome_paths:
            if os.path.exists(p):
                chrome_exe = p
                break
        
        if chrome_exe:
            # ë…ë¦½ Chrome ì¸ìŠ¤í„´ìŠ¤ (ê¸°ì¡´ Chromeê³¼ ë³„ê°œ, ì „ì²´í™”ë©´)
            news_proc = subprocess.Popen([
                chrome_exe,
                f"--user-data-dir={temp_profile}",
                "--no-first-run",
                "--no-default-browser-check",
                "--start-maximized",
                "--disable-extensions",
                "--disable-sync",
                "--disable-translate",
                news_url
            ])
            logger.info(f"ğŸ“° êµ¬ê¸€ë‰´ìŠ¤ ë…ë¦½ ì°½ ì—´ê¸° (PID: {news_proc.pid})")
        else:
            webbrowser.open(news_url)
            logger.info("ğŸ“° êµ¬ê¸€ë‰´ìŠ¤ ì—´ê¸° (ê¸°ë³¸ ë¸Œë¼ìš°ì €)")
        
        # 2. ì´ˆê¸° ë¡œë”© ëŒ€ê¸° (ì„ì‹œ í”„ë¡œí•„ ì²« ì‹¤í–‰ì€ ëŠë¦¼)
        time.sleep(3)
        
        # 2.5. ê°•ì œ ì „ì²´í™”ë©´ (ì„ì‹œ í”„ë¡œí•„ì€ ìµœëŒ€í™” ë¬´ì‹œí•  ìˆ˜ ìˆìŒ)
        try:
            import ctypes
            import ctypes.wintypes
            
            # ê°€ì¥ ì•ì— ìˆëŠ” Chrome ì°½ ì°¾ì•„ì„œ ìµœëŒ€í™”
            user32 = ctypes.windll.user32
            hwnd = user32.GetForegroundWindow()
            if hwnd:
                SW_MAXIMIZE = 3
                user32.ShowWindow(hwnd, SW_MAXIMIZE)
                logger.info(f"ğŸ”² ë‰´ìŠ¤ ì°½ ìµœëŒ€í™” ì™„ë£Œ (hwnd: {hwnd})")
        except Exception as e:
            logger.warning(f"âš ï¸ ìµœëŒ€í™” ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
        
        # 3. ë‰´ìŠ¤ í˜ì´ì§€ ì™„ì „íˆ ë¡œë”©ë  ë•Œê¹Œì§€ ì¶©ë¶„íˆ ëŒ€ê¸°
        logger.info("â³ ë‰´ìŠ¤ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... (8ì´ˆ)")
        time.sleep(8)
        
        # 3. ìŠ¤í¬ë¦°ìƒ· ì°ê¸°
        from PIL import ImageGrab
        filename = f"news_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        filepath = os.path.join(SCREENSHOT_DIR, filename)
        img = ImageGrab.grab()
        img.save(filepath)
        logger.info(f"ğŸ“¸ ë‰´ìŠ¤ ìŠ¤í¬ë¦°ìƒ·: {filepath}")
        
        # 4. ë…ë¦½ Chromeë§Œ ì¢…ë£Œ
        time.sleep(0.5)
        if news_proc and news_proc.poll() is None:
            # ìì‹ í”„ë¡œì„¸ìŠ¤ í¬í•¨ ì „ì²´ ì¢…ë£Œ
            try:
                parent = psutil.Process(news_proc.pid)
                for child in parent.children(recursive=True):
                    child.terminate()
                parent.terminate()
                logger.info(f"ğŸ”’ ë‰´ìŠ¤ ì°½ ë‹«ê¸° ì™„ë£Œ (PID: {news_proc.pid})")
            except psutil.NoSuchProcess:
                pass
        
        # 5. ì„ì‹œ í”„ë¡œí•„ ì •ë¦¬ (ë°±ê·¸ë¼ìš´ë“œ)
        try:
            if temp_profile and os.path.exists(temp_profile):
                shutil.rmtree(temp_profile, ignore_errors=True)
        except:
            pass
        
        return {
            "success": True,
            "filename": filename,
            "path": filepath,
            "url": f"/assistant/screenshots/{filename}"
        }
    except Exception as e:
        # ì—ëŸ¬ ì‹œì—ë„ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
        if news_proc and news_proc.poll() is None:
            try:
                news_proc.terminate()
            except:
                pass
        return {"success": False, "error": str(e)}


# â˜… í”„ë¡œì„¸ìŠ¤ ëª©ë¡ ì¡°íšŒ
def list_processes(sort_by: str = "memory", limit: int = 30) -> List[dict]:
    """ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ëª©ë¡ ë°˜í™˜"""
    processes = []
    try:
        for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_info', 'status']):
            try:
                info = proc.info
                mem = info.get('memory_info')
                mem_mb = mem.rss / (1024 * 1024) if mem else 0
                processes.append({
                    "pid": info['pid'],
                    "name": info['name'],
                    "cpu": proc.cpu_percent(interval=0),
                    "memory_mb": round(mem_mb, 1),
                    "status": info.get('status', '?')
                })
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue

        # ì •ë ¬
        if sort_by == "cpu":
            processes.sort(key=lambda x: x['cpu'], reverse=True)
        else:
            processes.sort(key=lambda x: x['memory_mb'], reverse=True)

        return processes[:limit]
    except Exception as e:
        logger.error(f"í”„ë¡œì„¸ìŠ¤ ëª©ë¡ ì˜¤ë¥˜: {e}")
        return [{"error": str(e)}]


# â˜… ì§€ì‹ë² ì´ìŠ¤ í•¨ìˆ˜ë“¤
def list_knowledge() -> List[dict]:
    """ì§€ì‹ë² ì´ìŠ¤ íŒŒì¼ ëª©ë¡"""
    files = []
    try:
        for f in sorted(os.listdir(KNOWLEDGE_DIR)):
            if f.endswith(('.md', '.txt')):
                filepath = os.path.join(KNOWLEDGE_DIR, f)
                size = os.path.getsize(filepath)
                modified = datetime.datetime.fromtimestamp(os.path.getmtime(filepath)).strftime("%Y-%m-%d %H:%M")
                files.append({"filename": f, "size": f"{size:,}B", "modified": modified})
    except Exception as e:
        logger.error(f"ì§€ì‹ ëª©ë¡ ì˜¤ë¥˜: {e}")
    return files


def normalize_keyword(keyword: str) -> List[str]:
    """í‚¤ì›Œë“œë¥¼ ì—¬ëŸ¬ ë³€í˜•ìœ¼ë¡œ í™•ì¥ (ì–¸ë”ìŠ¤ì½”ì–´, ê³µë°±, í•˜ì´í”ˆ ë“±)"""
    keyword = keyword.strip().lower()
    variants = [keyword]

    # ì–¸ë”ìŠ¤ì½”ì–´ <-> ê³µë°± <-> í•˜ì´í”ˆ ë³€í™˜
    if '_' in keyword:
        variants.append(keyword.replace('_', ' '))
        variants.append(keyword.replace('_', '-'))
        variants.append(keyword.replace('_', ''))
    if ' ' in keyword:
        variants.append(keyword.replace(' ', '_'))
        variants.append(keyword.replace(' ', '-'))
        variants.append(keyword.replace(' ', ''))
    if '-' in keyword:
        variants.append(keyword.replace('-', '_'))
        variants.append(keyword.replace('-', ' '))
        variants.append(keyword.replace('-', ''))

    # ëŒ€ì†Œë¬¸ì ë³€í˜• ì¶”ê°€
    variants.append(keyword.upper())
    variants.append(keyword.title())

    return list(set(variants))


def calculate_relevance_score(filename: str, content: str, keyword: str, variants: List[str]) -> int:
    """ë¬¸ì„œì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ê´€ë ¨ì„± ë†’ìŒ)"""
    score = 0
    filename_lower = filename.lower()
    content_lower = content.lower()

    for variant in variants:
        v_lower = variant.lower()

        # íŒŒì¼ëª…ì— í‚¤ì›Œë“œ í¬í•¨ (+50ì )
        if v_lower in filename_lower:
            score += 50
            # íŒŒì¼ëª…ì´ í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ë©´ ì¶”ê°€ ì ìˆ˜
            if filename_lower.startswith(v_lower):
                score += 30

        # ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ë“±ì¥ íšŸìˆ˜ (ìµœëŒ€ 100ì )
        count = content_lower.count(v_lower)
        score += min(count * 5, 100)

        # ì œëª©/í—¤ë”ì— í‚¤ì›Œë“œ ìˆìœ¼ë©´ ì¶”ê°€ ì ìˆ˜
        lines = content.split('\n')[:20]  # ìƒìœ„ 20ì¤„ë§Œ í™•ì¸
        for line in lines:
            if line.startswith('#') and v_lower in line.lower():
                score += 40
                break

    return score


def search_knowledge(keyword: str) -> List[dict]:
    """ì§€ì‹ë² ì´ìŠ¤ì—ì„œ í‚¤ì›Œë“œë¡œ íŒŒì¼ ê²€ìƒ‰ (ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ ì •ë ¬)"""
    results = []
    variants = normalize_keyword(keyword)
    logger.info(f"ğŸ” ì§€ì‹ê²€ìƒ‰: '{keyword}' â†’ ë³€í˜•: {variants[:5]}")

    try:
        for f in os.listdir(KNOWLEDGE_DIR):
            if not f.endswith(('.md', '.txt')):
                continue
            filepath = os.path.join(KNOWLEDGE_DIR, f)

            try:
                with open(filepath, 'r', encoding='utf-8', errors='ignore') as fh:
                    content = fh.read()
            except:
                continue

            # ë§¤ì¹­ ì—¬ë¶€ í™•ì¸ (ëª¨ë“  ë³€í˜•ì— ëŒ€í•´)
            matched = False
            snippet = ""
            for variant in variants:
                v_lower = variant.lower()
                if v_lower in f.lower() or v_lower in content.lower():
                    matched = True
                    # ìŠ¤ë‹ˆí« ì¶”ì¶œ
                    idx = content.lower().find(v_lower)
                    if idx >= 0:
                        snippet = content[max(0, idx-50):min(len(content), idx+100)].replace('\n', ' ').strip()
                    break

            if matched:
                score = calculate_relevance_score(f, content, keyword, variants)
                results.append({
                    "filename": f,
                    "snippet": f"...{snippet}..." if snippet else "(íŒŒì¼ëª… ë§¤ì¹­)",
                    "score": score
                })

        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬ (ë†’ì€ ìˆœ)
        results.sort(key=lambda x: x.get("score", 0), reverse=True)
        logger.info(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ (ìƒìœ„: {[r['filename'] for r in results[:3]]})")

    except Exception as e:
        logger.error(f"ì§€ì‹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    return results


def read_knowledge(filename: str) -> str:
    """ì§€ì‹ë² ì´ìŠ¤ MD íŒŒì¼ ì½ê¸°"""
    filepath = os.path.join(KNOWLEDGE_DIR, filename)
    if not os.path.exists(filepath):
        for f in os.listdir(KNOWLEDGE_DIR):
            if filename.lower() in f.lower():
                filepath = os.path.join(KNOWLEDGE_DIR, f)
                break
        else:
            return f"âŒ '{filename}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read(30000)
            if len(content) == 30000:
                content += "\n\n... (ë¬¸ì„œê°€ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ í‘œì‹œ)"
            return content
    except Exception as e:
        return f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}"

    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read(30000)
            if len(content) == 30000:
                content += "\n\n... (ë¬¸ì„œê°€ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ í‘œì‹œ)"
            return content
    except Exception as e:
        return f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}"


def analyze_data(path: str) -> str:
    try:
        ext = os.path.splitext(path)[1].lower()
        if ext == '.csv':
            df = pd.read_csv(path, encoding='utf-8', errors='ignore')
        elif ext in ['.xlsx', '.xls']:
            df = pd.read_excel(path)
        else:
            return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {ext}"
        result = []
        result.append(f"íŒŒì¼: {os.path.basename(path)}")
        result.append(f"í¬ê¸°: {len(df):,}í–‰ x {len(df.columns)}ì—´")
        result.append(f"ì»¬ëŸ¼: {', '.join(df.columns.tolist()[:20])}")
        numeric_cols = df.select_dtypes(include=['number']).columns
        if len(numeric_cols) > 0:
            stats = df[numeric_cols].describe().to_string()
            result.append(f"í†µê³„:\n{stats}")
        result.append(f"ìƒ˜í”Œ:\n{df.head(5).to_string()}")
        return "\n".join(result)
    except Exception as e:
        return f"ë¶„ì„ ì˜¤ë¥˜: {e}"


# ========================================
# Tool ì‹¤í–‰ê¸°
# ========================================
def execute_tool(tool_data: dict) -> str:
    tool_name = tool_data.get("tool")

    if tool_name == "search_files":
        results = search_files(tool_data.get("keyword", ""), tool_data.get("path", "C:/"))
        return json.dumps(results[:20], ensure_ascii=False, indent=2)

    elif tool_name == "search_content":
        results = search_content(tool_data.get("keyword", ""), tool_data.get("path", "C:/"))
        return json.dumps(results[:10], ensure_ascii=False, indent=2)

    elif tool_name == "get_system_info":
        return json.dumps(get_system_info(), ensure_ascii=False, indent=2)

    elif tool_name == "list_directory":
        results = list_directory(tool_data.get("path", "C:/"))
        return json.dumps(results, ensure_ascii=False, indent=2)

    elif tool_name == "read_file":
        return read_file(tool_data.get("path", ""))

    elif tool_name == "run_program":
        return run_program(tool_data.get("program", ""))

    elif tool_name == "kill_program":
        return kill_program(tool_data.get("name", ""))

    elif tool_name == "open_web":
        return open_web(tool_data.get("url", ""))

    elif tool_name == "google_search":
        return google_search(tool_data.get("query", ""))

    elif tool_name == "get_time":
        return get_time()

    # â˜… ìŠ¤í¬ë¦°ìƒ· - JSON ë°˜í™˜
    elif tool_name == "screenshot":
        result = take_screenshot()
        return json.dumps(result, ensure_ascii=False)

    # â˜… ìµœì‹ ë‰´ìŠ¤ - êµ¬ê¸€ë‰´ìŠ¤ ì—´ê³  ìŠ¤í¬ë¦°ìƒ· ì°ê³  ë‹«ê¸°
    elif tool_name == "latest_news":
        result = latest_news()
        return json.dumps(result, ensure_ascii=False)

    elif tool_name == "analyze_data":
        return analyze_data(tool_data.get("path", ""))

    # â˜… ì§€ì‹ë² ì´ìŠ¤ ë„êµ¬ë“¤
    elif tool_name == "list_knowledge":
        results = list_knowledge()
        return json.dumps(results, ensure_ascii=False, indent=2)

    elif tool_name == "search_knowledge":
        results = search_knowledge(tool_data.get("keyword", ""))
        return json.dumps(results, ensure_ascii=False, indent=2)

    elif tool_name == "read_knowledge":
        return read_knowledge(tool_data.get("filename", ""))

    # â˜… í”„ë¡œì„¸ìŠ¤ ëª©ë¡
    elif tool_name == "list_processes":
        results = list_processes(tool_data.get("sort_by", "memory"), tool_data.get("limit", 30))
        return json.dumps(results, ensure_ascii=False, indent=2)

    return "ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬"


# ========================================
# JSON ê°ì§€
# ========================================
def extract_tool_json(text: str) -> Optional[dict]:
    # íŒ¨í„´ 1: ```json ì½”ë“œë¸”ë¡
    match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', text, re.DOTALL)
    if match:
        try:
            data = json.loads(match.group(1))
            if "tool" in data:
                return data
        except json.JSONDecodeError:
            pass

    # íŒ¨í„´ 2: ì¸ë¼ì¸ {"tool": "..."}
    match = re.search(r'(\{[^{}]*"tool"\s*:\s*"[^"]+?"[^{}]*\})', text, re.DOTALL)
    if match:
        try:
            data = json.loads(match.group(1))
            if "tool" in data:
                return data
        except json.JSONDecodeError:
            pass

    # íŒ¨í„´ 3: ì „ì²´ê°€ JSON
    stripped = text.strip()
    if stripped.startswith('{') and stripped.endswith('}'):
        try:
            data = json.loads(stripped)
            if "tool" in data:
                return data
        except json.JSONDecodeError:
            pass

    # íŒ¨í„´ 4: ë©€í‹°ë¼ì¸ JSON
    match = re.search(r'\{\s*"tool"\s*:.*?\}', text, re.DOTALL)
    if match:
        try:
            json_str = re.sub(r'[\n\r\t]', ' ', match.group(0))
            json_str = re.sub(r'\s+', ' ', json_str)
            data = json.loads(json_str)
            if "tool" in data:
                return data
        except json.JSONDecodeError:
            pass

    return None


# ========================================
# ========================================
# Chat Processing
# ========================================
def get_recent_context(max_turns: int = 4) -> str:
    """ìµœê·¼ ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜ (ë§¥ë½ ìœ ì§€ìš©)"""
    if not CHAT_HISTORY:
        return ""

    recent = CHAT_HISTORY[-(max_turns * 2):]  # user + assistant ìŒ
    context_lines = []
    for msg in recent:
        role = "ì‚¬ìš©ì" if msg["role"] == "user" else "ë¹„ì„œ"
        content = msg["content"][:500]  # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        context_lines.append(f"[{role}]: {content}")

    return "\n".join(context_lines)


def process_chat(user_message: str) -> str:
    global LOCAL_LLM, LLM_MODE

    if LLM_MODE == "local" and LOCAL_LLM is None:
        return "âŒ ë¡œì»¬ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    if LLM_MODE != "local" and not API_TOKEN:
        return "âŒ API í† í°ì´ ì—†ìŠµë‹ˆë‹¤."

    try:
        # â˜… ëŒ€í™” ë§¥ë½ ì¶”ê°€
        recent_context = get_recent_context(max_turns=3)
        if recent_context:
            context_prompt = f"""[ì´ì „ ëŒ€í™”]
{recent_context}

[í˜„ì¬ ì§ˆë¬¸]
{user_message}"""
        else:
            context_prompt = user_message

        result = call_llm(context_prompt, SYSTEM_PROMPT)
        if not result["success"]:
            return f"âŒ LLM ì˜¤ë¥˜: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"

        text = result["content"]
        logger.info(f"ğŸ“ LLM ì‘ë‹µ: {text[:200]}")

        tool_data = extract_tool_json(text)

        if tool_data:
            try:
                if "keyword" in tool_data:
                    kw = tool_data["keyword"].replace("*", "").replace(".", "").strip()
                    if not kw:
                        return "âŒ ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
                    tool_data["keyword"] = kw

                logger.info(f"ğŸ”§ ë„êµ¬ ì‹¤í–‰: {tool_data}")
                tool_result = execute_tool(tool_data)
                logger.info(f"ğŸ“Š ë„êµ¬ ê²°ê³¼: {tool_result[:300]}")

                tool_name = tool_data.get("tool")

                # â˜… ìŠ¤í¬ë¦°ìƒ·: ì§ì ‘ í¬ë§·íŒ… (LLM 2ì°¨ í˜¸ì¶œ ë¶ˆí•„ìš”)
                if tool_name == "screenshot":
                    try:
                        sc_data = json.loads(tool_result)
                        if sc_data.get("success"):
                            return f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ·ì„ ì°ì—ˆìŠµë‹ˆë‹¤!\n\n![ìŠ¤í¬ë¦°ìƒ·]({sc_data['url']})\n\nì €ì¥ ìœ„ì¹˜: `{sc_data['path']}`"
                        else:
                            return f"âŒ ìŠ¤í¬ë¦°ìƒ· ì‹¤íŒ¨: {sc_data.get('error', '?')}"
                    except:
                        return f"âŒ ìŠ¤í¬ë¦°ìƒ· ì²˜ë¦¬ ì˜¤ë¥˜"

                # â˜… ìµœì‹ ë‰´ìŠ¤: ì§ì ‘ í¬ë§·íŒ…
                if tool_name == "latest_news":
                    try:
                        news_data = json.loads(tool_result)
                        if news_data.get("success"):
                            return f"ğŸ“° **ìµœì‹  ë‰´ìŠ¤** (êµ¬ê¸€ë‰´ìŠ¤)\n\n![ë‰´ìŠ¤]({news_data['url']})\n\në¸Œë¼ìš°ì €ë¥¼ ë‹«ì•˜ìŠµë‹ˆë‹¤."
                        else:
                            return f"âŒ ë‰´ìŠ¤ í™•ì¸ ì‹¤íŒ¨: {news_data.get('error', '?')}"
                    except:
                        return f"âŒ ë‰´ìŠ¤ ì²˜ë¦¬ ì˜¤ë¥˜"

                # ========================================
                # â˜… ì§€ì‹ë² ì´ìŠ¤ í•¸ë“¤ëŸ¬ (3ê°€ì§€ êµ¬ì¡°)
                # ========================================

                # 1) read_knowledge â†’ "ì´ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í•´ë¼" + ê¸°ìˆ  ë¬¸ì„œ ì „ë¬¸ê°€
                if tool_name == "read_knowledge":
                    if tool_result.startswith("âŒ"):
                        return tool_result
                    
                    # ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ ìë¥´ê¸°
                    doc_content = tool_result if len(tool_result) <= 12000 else tool_result[:12000] + "\n\n... (ì´í•˜ ìƒëµ)"
                    
                    follow_up_prompt = f"""[ì‚¬ìš©ì ì§ˆë¬¸]
{user_message}

[ì°¸ê³  ë¬¸ì„œ]
{doc_content}

ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ë‹µë³€í•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”."""

                    follow_up_system = """ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ì´ì ê¸°ìˆ  ë¬¸ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ë‹µë³€ í˜•ì‹]
**ğŸ“‹ í•µì‹¬ ìš”ì•½**
ì§ˆë¬¸ì— ëŒ€í•œ í•µì‹¬ ë‹µë³€ì„ 2~3ì¤„ë¡œ ìš”ì•½

**ğŸ“ ìƒì„¸ ë‚´ìš©**
ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ë‚´ìš©ì„ ì¶©ë¶„íˆ ìì„¸í•˜ê²Œ ì •ë¦¬:
- ì£¼ìš” ê¸°ëŠ¥/ëª©ì 
- êµ¬ì„± ìš”ì†Œ ë° ê´€ê³„
- ë™ì‘ ë°©ì‹/íë¦„
- ì¤‘ìš”í•œ ì„¤ì •ì´ë‚˜ íŒŒë¼ë¯¸í„°
- ì£¼ì˜ì‚¬í•­ì´ë‚˜ íŠ¹ì´ì‚¬í•­

[ë‹µë³€ ê·œì¹™]
1. ë¬¸ì„œ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  **ì¶©ë¶„íˆ ìƒì„¸í•˜ê²Œ** ë‹µë³€í•˜ì„¸ìš”.
2. ìƒì„¸ ë‚´ìš©ì€ ìµœì†Œ 10ì¤„ ì´ìƒ ì‘ì„±í•˜ì„¸ìš”. ë¬¸ì„œì— ìˆëŠ” ì¤‘ìš” ì •ë³´ëŠ” ë¹ ëœ¨ë¦¬ì§€ ë§ˆì„¸ìš”.
3. ì†ŒìŠ¤ì½”ë“œ ì›ë³¸ì€ ë³´ì—¬ì£¼ì§€ ë§ê³ , ì½”ë“œì˜ ê¸°ëŠ¥/ì—­í• /ë™ì‘ì„ ì„¤ëª…í•˜ì„¸ìš”.
4. ë§ˆí¬ë‹¤ìš´ í‘œ(| --- |) ì‚¬ìš© ê¸ˆì§€. "- í•­ëª©: ê°’" í˜•íƒœë¡œ ë‚˜ì—´í•˜ì„¸ìš”.
5. ## ### ëŒ€ì œëª© í—¤ë” ëŒ€ì‹  **ë³¼ë“œ**ì™€ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
6. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
7. ì ˆëŒ€ JSONì„ ì¶œë ¥í•˜ê±°ë‚˜ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”."""

                    result2 = call_llm(follow_up_prompt, follow_up_system, max_tokens=6000)
                    if result2["success"]:
                        content = result2["content"].strip()
                        logger.info(f"ğŸ“ ì§€ì‹ì½ê¸° 2ì°¨ ì‘ë‹µ: {content[:200] if content else '(ë¹ˆ ì‘ë‹µ)'}")
                        if content and not extract_tool_json(content):
                            return content
                    # fallback: ë¬¸ì„œ ë‚´ìš© ì§ì ‘ ë°˜í™˜
                    logger.info("âš ï¸ 2ì°¨ LLM ì‘ë‹µ ì—†ìŒ â†’ ë¬¸ì„œ ì§ì ‘ ë°˜í™˜")
                    return f"ğŸ“„ **ë¬¸ì„œ ë‚´ìš©:**\n\n{doc_content[:5000]}"

                # 2) search_knowledge â†’ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë§Œ ì‚¬ìš©
                if tool_name == "search_knowledge":
                    try:
                        search_results = json.loads(tool_result)
                        if not search_results:
                            return "ğŸ” ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§€ì‹ë² ì´ìŠ¤ì— ë¬¸ì„œë¥¼ ë¨¼ì € ë“±ë¡í•´ì£¼ì„¸ìš”."

                        # â˜… ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ ì„ íƒ
                        # 1ìœ„ ë¬¸ì„œì™€ ì ìˆ˜ ì°¨ì´ê°€ 50% ì´ìƒì´ë©´ 1ìœ„ë§Œ ì‚¬ìš©
                        MAX_TOTAL_LENGTH = 15000
                        merged_docs = []
                        total_length = 0
                        doc_names = []

                        top_score = search_results[0].get("score", 100)

                        for i, result in enumerate(search_results):
                            filename = result["filename"]
                            score = result.get("score", 0)

                            # 1ìœ„ ë¬¸ì„œì™€ ì ìˆ˜ ì°¨ì´ê°€ 50% ì´ìƒì´ë©´ ì œì™¸
                            if i > 0 and score < top_score * 0.5:
                                logger.info(f"â­ï¸ ì ìˆ˜ ë‚®ì•„ ì œì™¸: {filename} (ì ìˆ˜: {score}, 1ìœ„: {top_score})")
                                break

                            doc_content = read_knowledge(filename)

                            if doc_content.startswith("âŒ"):
                                continue

                            # ë‚¨ì€ ê³µê°„ì— ë§ê²Œ ìë¥´ê¸°
                            remaining = MAX_TOTAL_LENGTH - total_length
                            if remaining <= 1000:
                                break

                            if len(doc_content) > remaining:
                                doc_content = doc_content[:remaining] + "\n\n... (ë¬¸ì„œ ì¼ë¶€ ìƒëµ)"

                            merged_docs.append(f"ğŸ“„ **[{filename}]**\n{doc_content}")
                            doc_names.append(filename)
                            total_length += len(doc_content)

                        if not merged_docs:
                            return "ğŸ” ë¬¸ì„œë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                        combined_content = "\n\n---\n\n".join(merged_docs)
                        doc_list = ", ".join(doc_names)
                        logger.info(f"ğŸ“š ì°¸ì¡° ë¬¸ì„œ: {doc_list} (ì´ {total_length}ì)")

                        follow_up_prompt = f"""[ì‚¬ìš©ì ì§ˆë¬¸]
{user_message}

[ì°¸ê³  ë¬¸ì„œ {len(doc_names)}ê°œ: {doc_list}]
{combined_content}

ìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”.
ì—¬ëŸ¬ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¢…í•©í•´ì„œ ë‹µë³€í•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”."""

                        follow_up_system = """ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ì´ì ê¸°ìˆ  ë¬¸ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ë‹µë³€ í˜•ì‹]
**ğŸ“‹ í•µì‹¬ ìš”ì•½**
ì§ˆë¬¸ì— ëŒ€í•œ í•µì‹¬ ë‹µë³€ì„ 2~3ì¤„ë¡œ ìš”ì•½

**ğŸ“ ìƒì„¸ ë‚´ìš©**
ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ë‚´ìš©ì„ ì¶©ë¶„íˆ ìì„¸í•˜ê²Œ ì •ë¦¬:
- ì£¼ìš” ê¸°ëŠ¥/ëª©ì 
- êµ¬ì„± ìš”ì†Œ ë° ê´€ê³„
- ë™ì‘ ë°©ì‹/íë¦„
- ì¤‘ìš”í•œ ì„¤ì •ì´ë‚˜ íŒŒë¼ë¯¸í„°
- ì£¼ì˜ì‚¬í•­ì´ë‚˜ íŠ¹ì´ì‚¬í•­

[ë‹µë³€ ê·œì¹™]
1. ë¬¸ì„œ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  **ì¶©ë¶„íˆ ìƒì„¸í•˜ê²Œ** ë‹µë³€í•˜ì„¸ìš”.
2. ìƒì„¸ ë‚´ìš©ì€ ìµœì†Œ 10ì¤„ ì´ìƒ ì‘ì„±í•˜ì„¸ìš”. ë¬¸ì„œì— ìˆëŠ” ì¤‘ìš” ì •ë³´ëŠ” ë¹ ëœ¨ë¦¬ì§€ ë§ˆì„¸ìš”.
3. ì†ŒìŠ¤ì½”ë“œ ì›ë³¸ì€ ë³´ì—¬ì£¼ì§€ ë§ê³ , ì½”ë“œì˜ ê¸°ëŠ¥/ì—­í• /ë™ì‘ì„ ì„¤ëª…í•˜ì„¸ìš”.
4. ë§ˆí¬ë‹¤ìš´ í‘œ(| --- |) ì‚¬ìš© ê¸ˆì§€. "- í•­ëª©: ê°’" í˜•íƒœë¡œ ë‚˜ì—´í•˜ì„¸ìš”.
5. ## ### ëŒ€ì œëª© í—¤ë” ëŒ€ì‹  **ë³¼ë“œ**ì™€ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
6. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
7. ì ˆëŒ€ JSONì„ ì¶œë ¥í•˜ê±°ë‚˜ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”."""

                        result2 = call_llm(follow_up_prompt, follow_up_system, max_tokens=6000)
                        if result2["success"]:
                            content = result2["content"].strip()
                            logger.info(f"ğŸ“ ì§€ì‹ê²€ìƒ‰ 2ì°¨ ì‘ë‹µ: {content[:200] if content else '(ë¹ˆ ì‘ë‹µ)'}")
                            if content and not extract_tool_json(content):
                                # ì°¸ì¡° ë¬¸ì„œ ëª©ë¡ ì¶”ê°€
                                source_info = f"\n\n---\nğŸ“š **ì°¸ì¡° ë¬¸ì„œ**: {doc_list}"
                                return content + source_info
                        # fallback: ë¬¸ì„œ ë‚´ìš© ì§ì ‘ ë°˜í™˜
                        logger.info("âš ï¸ 2ì°¨ LLM ì‘ë‹µ ì—†ìŒ â†’ ë¬¸ì„œ ì§ì ‘ ë°˜í™˜")
                        return f"ğŸ“„ **ì°¸ì¡° ë¬¸ì„œ:**\n\n{combined_content[:5000]}"
                    except Exception as e:
                        logger.error(f"ì§€ì‹ê²€ìƒ‰ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        pass

                # 3) list_knowledge â†’ LLM í˜¸ì¶œ ì—†ì´ ì§ì ‘ í¬ë§·íŒ… (API ë‚­ë¹„ ë°©ì§€)
                if tool_name == "list_knowledge":
                    try:
                        files = json.loads(tool_result)
                        if not files:
                            return "ğŸ“­ ì§€ì‹ë² ì´ìŠ¤ì— ë“±ë¡ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\n\nğŸ“š ì§€ì‹ë² ì´ìŠ¤ ë²„íŠ¼ìœ¼ë¡œ MD/TXT íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”."
                        lines = [f"## ğŸ“š ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ({len(files)}ê°œ)\n"]
                        for f in files:
                            lines.append(f"- ğŸ“„ **{f['filename']}** ({f['size']}, {f['modified']})")
                        lines.append(f"\nğŸ’¡ ë¬¸ì„œ ë‚´ìš©ì´ ê¶ê¸ˆí•˜ë©´ íŒŒì¼ëª…ìœ¼ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”. (ì˜ˆ: \"HID_INOUT ì•Œë ¤ì¤˜\")")
                        return "\n".join(lines)
                    except:
                        pass

                # ê¸°íƒ€ ë„êµ¬: 2ì°¨ LLMìœ¼ë¡œ í•´ì„
                follow_up_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {user_message}

ë„êµ¬ ì‹¤í–‰ ê²°ê³¼:
{tool_result}

ìœ„ ê²°ê³¼ë¥¼ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ í•œêµ­ì–´ë¡œ ì •ë¦¬í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
- JSON ì›ë³¸ì„ ë³´ì—¬ì£¼ì§€ ë§ê³  í•µì‹¬ë§Œ ì •ë¦¬
- ë„êµ¬ë¥¼ ë‹¤ì‹œ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš” (JSON ì¶œë ¥ ê¸ˆì§€)
- ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ"""

                follow_up_system = """ë‹¹ì‹ ì€ PC ê°œì¸ë¹„ì„œì…ë‹ˆë‹¤.
ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
ì ˆëŒ€ JSONì„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. ìì—°ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""

                result2 = call_llm(follow_up_prompt, follow_up_system)
                if result2["success"]:
                    response = result2["content"]
                    if extract_tool_json(response):
                        return format_tool_result_fallback(tool_data, tool_result)
                    return response
                else:
                    return format_tool_result_fallback(tool_data, tool_result)

            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                return "âŒ ëª…ë ¹ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        return text

    except Exception as e:
        logger.error(f"âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return f"âŒ ì˜¤ë¥˜: {e}"


# Fallback í¬ë§·í„°
def format_tool_result_fallback(tool_data: dict, tool_result: str) -> str:
    tool_name = tool_data.get("tool", "")
    try:
        if tool_name == "get_system_info":
            info = json.loads(tool_result)
            lines = ["## ğŸ’» ì‹œìŠ¤í…œ ì •ë³´", f"- **OS**: {info.get('os', '?')}", f"- **CPU**: {info.get('cpu', '?')}", f"- **ë©”ëª¨ë¦¬**: {info.get('memory', '?')}"]
            for d in info.get('drives', []):
                lines.append(f"- **{d['drive']}**: {d['total']} (ì‚¬ìš©ë¥  {d['used']})")
            return "\n".join(lines)

        elif tool_name == "get_time":
            return f"ğŸ• í˜„ì¬ ì‹œê°„: {tool_result}"

        elif tool_name in ["search_files", "search_content"]:
            results = json.loads(tool_result)
            if not results:
                return f"ğŸ” '{tool_data.get('keyword', '')}' ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            lines = [f"ğŸ” ê²€ìƒ‰ ê²°ê³¼: **{len(results)}ê°œ** ë°œê²¬\n"]
            for r in results[:10]:
                if "snippet" in r:
                    lines.append(f"- ğŸ“„ `{r['name']}` â†’ {r['snippet']}")
                else:
                    lines.append(f"- {'ğŸ“' if r.get('type') == 'í´ë”' else 'ğŸ“„'} `{r['name']}` ({r.get('size', '?')})")
            return "\n".join(lines)

        elif tool_name == "list_directory":
            items = json.loads(tool_result)
            lines = [f"ğŸ“‚ `{tool_data.get('path', '')}` ë‚´ìš©:\n"]
            for item in items[:20]:
                icon = "ğŸ“" if item.get("type") == "í´ë”" else "ğŸ“„"
                lines.append(f"- {icon} `{item['name']}` ({item.get('size', '-')})")
            return "\n".join(lines)

        elif tool_name == "read_file":
            return f"ğŸ“„ **íŒŒì¼ ë‚´ìš©:**\n```\n{tool_result}\n```"

        elif tool_name in ["run_program", "kill_program", "open_web", "google_search"]:
            return f"âœ… {tool_result}"

        elif tool_name == "list_processes":
            procs = json.loads(tool_result)
            if not procs or "error" in procs[0]:
                return "âŒ í”„ë¡œì„¸ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            lines = [f"## ğŸ“‹ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ (ìƒìœ„ {len(procs)}ê°œ)\n"]
            lines.append("| ì´ë¦„ | PID | ë©”ëª¨ë¦¬(MB) | CPU% | ìƒíƒœ |")
            lines.append("|------|-----|-----------|------|------|")
            for p in procs:
                lines.append(f"| {p['name']} | {p['pid']} | {p['memory_mb']} | {p['cpu']}% | {p['status']} |")
            return "\n".join(lines)

        elif tool_name == "analyze_data":
            return f"ğŸ“Š **ë°ì´í„° ë¶„ì„:**\n```\n{tool_result}\n```"

    except Exception as e:
        logger.error(f"í¬ë§·íŒ… ì˜¤ë¥˜: {e}")

    return f"ğŸ“‹ **ê²°ê³¼:**\n```\n{tool_result}\n```"


# ========================================
# ëŒ€í™” ê¸°ë¡
# ========================================
def save_history():
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(CHAT_HISTORY[-100:], f, ensure_ascii=False, indent=2)
    except:
        pass

def load_history():
    global CHAT_HISTORY
    try:
        if os.path.exists(HISTORY_FILE):
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                CHAT_HISTORY = json.load(f)
    except:
        CHAT_HISTORY = []


# ========================================
# API Models
# ========================================
class ChatRequest(BaseModel):
    message: str

class SearchRequest(BaseModel):
    keyword: str
    path: str = "C:/"
    file_content: bool = False

class EnvRequest(BaseModel):
    env: str

class ModelRequest(BaseModel):
    model_key: str


# ========================================
# Endpoints
# ========================================
def init_assistant():
    global LOCAL_LLM, LLM_MODE
    load_history()
    if load_api_token():
        LLM_MODE = "api"
        logger.info("âœ… ë¹„ì„œ: API ëª¨ë“œ")
    else:
        LOCAL_LLM = load_local_model()
        if LOCAL_LLM:
            LLM_MODE = "local"
            logger.info("âœ… ë¹„ì„œ: LOCAL ëª¨ë“œ")


@router.get("/")
async def assistant_home():
    return FileResponse(os.path.join(BASE_DIR, "assistant_ui.html"))


# â˜… ìŠ¤í¬ë¦°ìƒ· ì´ë¯¸ì§€ ì„œë¹™
@router.get("/screenshots/{filename}")
async def serve_screenshot(filename: str):
    filepath = os.path.join(SCREENSHOT_DIR, filename)
    if os.path.exists(filepath):
        return FileResponse(filepath, media_type="image/png")
    return {"error": "íŒŒì¼ ì—†ìŒ"}


# â˜… ìŠ¤í¬ë¦°ìƒ· ëª©ë¡
@router.get("/api/screenshots")
async def list_screenshots():
    files = []
    if os.path.exists(SCREENSHOT_DIR):
        for f in sorted(os.listdir(SCREENSHOT_DIR), reverse=True)[:20]:
            if f.endswith('.png'):
                filepath = os.path.join(SCREENSHOT_DIR, f)
                size = os.path.getsize(filepath)
                files.append({
                    "filename": f,
                    "url": f"/assistant/screenshots/{f}",
                    "size": f"{size / 1024:.0f}KB",
                    "time": datetime.datetime.fromtimestamp(os.path.getmtime(filepath)).strftime("%Y-%m-%d %H:%M:%S")
                })
    return {"screenshots": files}


@router.get("/api/status")
async def assistant_status():
    # í˜„ì¬ ë¡œì»¬ ëª¨ë¸ ì •ë³´
    current_model_name = "LOCAL"
    if LLM_MODE == "local" and CURRENT_LOCAL_MODEL in AVAILABLE_MODELS:
        current_model_name = AVAILABLE_MODELS[CURRENT_LOCAL_MODEL]["name"]
    elif LLM_MODE != "local":
        current_model_name = ENV_CONFIG.get(CURRENT_ENV, {}).get("name", "API")

    return {
        "mode": LLM_MODE,
        "env": CURRENT_ENV if LLM_MODE != "local" else "local",
        "model_loaded": LOCAL_LLM is not None if LLM_MODE == "local" else API_TOKEN is not None,
        "model_name": current_model_name,
        "current_local_model": CURRENT_LOCAL_MODEL,
        "system": get_system_info(),
        "history_count": len(CHAT_HISTORY),
        "token_usage": TOKEN_USAGE
    }


# â˜… ë¡œì»¬ ëª¨ë¸ ëª©ë¡ API
@router.get("/api/models")
async def list_local_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë¡œì»¬ GGUF ëª¨ë¸ ëª©ë¡"""
    return {
        "success": True,
        "models": get_available_local_models(),
        "current": CURRENT_LOCAL_MODEL
    }


# â˜… ë¡œì»¬ ëª¨ë¸ ë³€ê²½ API
@router.post("/api/models/switch")
async def switch_local_model(request: ModelRequest):
    """ë¡œì»¬ GGUF ëª¨ë¸ ë³€ê²½"""
    global LOCAL_LLM, LLM_MODE

    model_key = request.model_key

    if model_key not in AVAILABLE_MODELS:
        return {"success": False, "error": f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_key}"}

    if not os.path.exists(AVAILABLE_MODELS[model_key]["path"]):
        return {"success": False, "error": f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {AVAILABLE_MODELS[model_key]['name']}"}

    # ê¸°ì¡´ ëª¨ë¸ í•´ì œ
    if LOCAL_LLM is not None:
        del LOCAL_LLM
        LOCAL_LLM = None
        import gc
        gc.collect()

    # ìƒˆ ëª¨ë¸ ë¡œë“œ
    LOCAL_LLM = load_local_model(model_key)
    if LOCAL_LLM:
        LLM_MODE = "local"
        return {
            "success": True,
            "model_key": model_key,
            "model_name": AVAILABLE_MODELS[model_key]["name"]
        }
    else:
        return {"success": False, "error": "ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨"}


# â˜… í† í° ì‚¬ìš©ëŸ‰ API
@router.get("/api/tokens")
async def assistant_tokens():
    return {
        "success": True,
        "prompt_tokens": TOKEN_USAGE["prompt_tokens"],
        "completion_tokens": TOKEN_USAGE["completion_tokens"],
        "total_tokens": TOKEN_USAGE["total_tokens"],
        "call_count": TOKEN_USAGE["call_count"]
    }


@router.post("/api/tokens/reset")
async def assistant_reset_tokens():
    TOKEN_USAGE["prompt_tokens"] = 0
    TOKEN_USAGE["completion_tokens"] = 0
    TOKEN_USAGE["total_tokens"] = 0
    TOKEN_USAGE["call_count"] = 0
    return {"success": True, "message": "í† í° ì¹´ìš´í„° ì´ˆê¸°í™”ë¨"}


# â˜… íŒŒë¼ë¯¸í„° ì¡°íšŒ API
@router.get("/api/params")
async def get_params():
    return {"success": True, "params": LLM_PARAMS}


# â˜… íŒŒë¼ë¯¸í„° ë³€ê²½ API
@router.post("/api/params")
async def set_params(request: dict):
    global LLM_PARAMS
    if "temperature" in request:
        LLM_PARAMS["temperature"] = float(request["temperature"])
    if "repeat_penalty" in request:
        LLM_PARAMS["repeat_penalty"] = float(request["repeat_penalty"])
    if "max_tokens" in request:
        LLM_PARAMS["max_tokens"] = int(request["max_tokens"])
    logger.info(f"âš™ï¸ íŒŒë¼ë¯¸í„° ë³€ê²½: {LLM_PARAMS}")
    return {"success": True, "params": LLM_PARAMS}


@router.post("/api/set_env")
async def assistant_set_env(request: EnvRequest):
    global LLM_MODE, LOCAL_LLM, CURRENT_ENV, API_URL, API_MODEL
    env = request.env.lower()

    if env == "local":
        if LOCAL_LLM is None:
            LOCAL_LLM = load_local_model()
        if LOCAL_LLM:
            LLM_MODE = "local"
            return {"success": True, "env": "local", "name": "LOCAL(14B-GGUF)"}
        return {"success": False, "error": "ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨"}

    elif env in ENV_CONFIG:
        if not API_TOKEN and not load_api_token():
            return {"success": False, "error": "API í† í° ì—†ìŒ"}
        LLM_MODE = "api"
        CURRENT_ENV = env
        API_URL = ENV_CONFIG[env]["url"]
        API_MODEL = ENV_CONFIG[env]["model"]
        return {"success": True, "env": env, "name": ENV_CONFIG[env]["name"]}

    return {"success": False, "error": f"ì•Œ ìˆ˜ ì—†ëŠ” í™˜ê²½: {env}"}


@router.post("/api/chat")
async def assistant_chat(request: ChatRequest):
    user_msg = request.message.strip()
    CHAT_HISTORY.append({"role": "user", "content": user_msg, "time": datetime.datetime.now().isoformat()})
    response = process_chat(user_msg)
    CHAT_HISTORY.append({"role": "assistant", "content": response, "time": datetime.datetime.now().isoformat()})
    save_history()
    return {"success": True, "response": response}


@router.post("/api/search")
async def assistant_search(request: SearchRequest):
    if request.file_content:
        results = search_content(request.keyword, request.path)
    else:
        results = search_files(request.keyword, request.path)
    return {"success": True, "results": results, "count": len(results)}


@router.get("/api/drives")
async def assistant_drives():
    drives = []
    for p in psutil.disk_partitions():
        drives.append({"device": p.device, "mountpoint": p.mountpoint})
    return {"success": True, "drives": drives}


@router.get("/api/history")
async def assistant_get_history():
    return {"history": CHAT_HISTORY[-50:]}


@router.delete("/api/history")
async def assistant_clear_history():
    global CHAT_HISTORY
    CHAT_HISTORY = []
    save_history()
    return {"success": True}


# â˜… ì§€ì‹ë² ì´ìŠ¤ API
@router.get("/api/knowledge")
async def api_list_knowledge():
    """ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ëª©ë¡"""
    files = list_knowledge()
    return {"success": True, "files": files, "count": len(files)}


@router.post("/api/knowledge/upload")
async def api_upload_knowledge(file: UploadFile = File(...)):
    """MD/TXT íŒŒì¼ ì—…ë¡œë“œ"""
    if not file.filename.lower().endswith(('.md', '.txt')):
        return {"success": False, "error": "md ë˜ëŠ” txt íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."}
    try:
        filepath = os.path.join(KNOWLEDGE_DIR, file.filename)
        content = await file.read()
        with open(filepath, 'wb') as f:
            f.write(content)
        return {"success": True, "filename": file.filename, "size": len(content)}
    except Exception as e:
        return {"success": False, "error": str(e)}


@router.delete("/api/knowledge/{filename}")
async def api_delete_knowledge(filename: str):
    """ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ì‚­ì œ"""
    filepath = os.path.join(KNOWLEDGE_DIR, filename)
    if os.path.exists(filepath):
        os.remove(filepath)
        return {"success": True, "message": f"'{filename}' ì‚­ì œë¨"}
    return {"success": False, "error": "íŒŒì¼ ì—†ìŒ"}


@router.get("/api/knowledge/download/{filename}")
async def api_download_knowledge(filename: str):
    """ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ"""
    filepath = os.path.join(KNOWLEDGE_DIR, filename)
    if os.path.exists(filepath):
        return FileResponse(filepath, filename=filename, media_type="application/octet-stream")
    return JSONResponse(status_code=404, content={"error": "íŒŒì¼ ì—†ìŒ"})


# ========================================
# ê³¼ê±°ì§€ì‹ ë³´ê´€ì†Œ API
# ========================================
@router.get("/api/knowledge/archive")
async def api_list_archive():
    """ê³¼ê±°ì§€ì‹ ë¬¸ì„œ ëª©ë¡"""
    files = []
    try:
        for f in sorted(os.listdir(KNOWLEDGE_ARCHIVE_DIR)):
            if f.lower().endswith(('.md', '.txt')):
                filepath = os.path.join(KNOWLEDGE_ARCHIVE_DIR, f)
                size = os.path.getsize(filepath)
                modified = datetime.datetime.fromtimestamp(os.path.getmtime(filepath)).strftime("%Y-%m-%d %H:%M")
                size_str = f"{size / 1024:.1f}KB" if size > 1024 else f"{size}B"
                files.append({"filename": f, "size": size_str, "modified": modified})
    except Exception as e:
        logger.error(f"ê³¼ê±°ì§€ì‹ ëª©ë¡ ì˜¤ë¥˜: {e}")
    return {"success": True, "files": files, "count": len(files)}


@router.post("/api/knowledge/archive/{filename}")
async def api_archive_knowledge(filename: str):
    """ì§€ì‹ë² ì´ìŠ¤ â†’ ê³¼ê±°ì§€ì‹ìœ¼ë¡œ ì´ë™"""
    import shutil
    src = os.path.join(KNOWLEDGE_DIR, filename)
    dst = os.path.join(KNOWLEDGE_ARCHIVE_DIR, filename)
    if not os.path.exists(src):
        return {"success": False, "error": f"'{filename}' íŒŒì¼ ì—†ìŒ"}
    try:
        shutil.move(src, dst)
        return {"success": True, "message": f"'{filename}' â†’ ê³¼ê±°ì§€ì‹ìœ¼ë¡œ ì´ë™ë¨"}
    except Exception as e:
        return {"success": False, "error": str(e)}


@router.post("/api/knowledge/restore/{filename}")
async def api_restore_knowledge(filename: str):
    """ê³¼ê±°ì§€ì‹ â†’ ì§€ì‹ë² ì´ìŠ¤ë¡œ ë³µì›"""
    import shutil
    src = os.path.join(KNOWLEDGE_ARCHIVE_DIR, filename)
    dst = os.path.join(KNOWLEDGE_DIR, filename)
    if not os.path.exists(src):
        return {"success": False, "error": f"'{filename}' íŒŒì¼ ì—†ìŒ"}
    try:
        shutil.move(src, dst)
        return {"success": True, "message": f"'{filename}' â†’ ì§€ì‹ë² ì´ìŠ¤ë¡œ ë³µì›ë¨"}
    except Exception as e:
        return {"success": False, "error": str(e)}


@router.delete("/api/knowledge/archive/{filename}")
async def api_delete_archive(filename: str):
    """ê³¼ê±°ì§€ì‹ ë¬¸ì„œ ì™„ì „ ì‚­ì œ"""
    filepath = os.path.join(KNOWLEDGE_ARCHIVE_DIR, filename)
    if os.path.exists(filepath):
        os.remove(filepath)
        return {"success": True, "message": f"'{filename}' ì™„ì „ ì‚­ì œë¨"}
    return {"success": False, "error": "íŒŒì¼ ì—†ìŒ"}


if __name__ == "__main__":
    import uvicorn
    app.include_router(router)

    @app.on_event("startup")
    async def standalone_startup():
        init_assistant()

    uvicorn.run(app, host="0.0.0.0", port=10003)