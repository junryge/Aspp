#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì½”ë”© LLM + Self-Correction í†µí•© ì„œë²„ (v2.1)
- ëª¨ë“  ëª¨ë“œì—ì„œ Self-Correction ì˜µì…˜ ì§€ì›
- ì½”ë“œ ìƒì„±/ë¦¬ë·°/ë””ë²„ê·¸/ì„¤ëª…/ë¦¬íŒ©í† ë§/í…ŒìŠ¤íŠ¸/ì¼ë°˜ + SC
"""

import os
import re
import json
import requests
import tempfile
import threading
from typing import TypedDict, Literal, Optional
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# --- ì—¬ê¸° ì¶”ê°€ ---
try:
    from coding_agent import get_agent_router
    logger.info("âœ… ë§ˆê¸°(main1_First) ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œë¨")
except ImportError as e:
    logger.warning(f"âš ï¸ ë§ˆê¸°(main1_First) ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    get_agent_router = None
# ----------------
app = FastAPI(title="Coding LLM + Self-Correction v2.1")
# --- ì—¬ê¸° ì¶”ê°€ ---
if get_agent_router:
    app.include_router(get_agent_router())
    logger.info("âœ… ë§ˆê¸°(main1_First) ë¼ìš°í„° ë“±ë¡ ì™„ë£Œ")
# ----------------
# PC ë¹„ì„œ ëª¨ë“ˆ import
try:
    from pc_assistant import router as assistant_router, init_assistant
    app.include_router(assistant_router)
    logger.info("âœ… PC ë¹„ì„œ ëª¨ë“ˆ ë¡œë“œë¨")
except ImportError as e:
    logger.warning(f"âš ï¸ PC ë¹„ì„œ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")

# ========================================
# Global Variables
# ========================================
API_TOKEN = None
LLM_MODE = "api"  # "api" or "local"
ENV_MODE = "common"

active_requests = {}
request_lock = threading.Lock()

# GGUF ë¡œì»¬ ëª¨ë¸ ì„¤ì • (pc_assistant.pyì˜ AVAILABLE_MODELSì™€ ë™ê¸°í™”)
try:
    from pc_assistant import GGUF_MODEL_PATH as _pa_path
    GGUF_MODEL_PATH = _pa_path
except:
    GGUF_MODEL_PATH = "Qwen3-14B-Q4_K_M.gguf"
LOCAL_LLM = None  # llama-cpp ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤

ENV_CONFIG = {
    "dev": {
        "url": "http://dev.assistant.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Coder-30B-A3B-Instruct",
        "name": "DEV(30B)"
    },
    "prod": {
        "url": "http://summary.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Next-80B-A3B-Instruct",
        "name": "PROD(80B)"
    },
    "common": {
        "url": "http://common.llm.skhynix.com/v1/chat/completions",
        "model": "gpt-oss-20b",
        "name": "COMMON(20B)"
    },
    "local": {
        "url": "",
        "model": "Qwen3-14B-Q4_K_M",
        "name": "LOCAL(14B-GGUF)"
    }
}

API_URL = ENV_CONFIG["common"]["url"]
API_MODEL = ENV_CONFIG["common"]["model"]

# ========================================
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# ========================================
SYSTEM_PROMPTS = {
    "generate": """ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ê³ í’ˆì§ˆ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
ê·œì¹™: ê¹”ë”í•œ ì½”ë“œ, ì ì ˆí•œ ì£¼ì„, ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨.
ì½”ë“œ ë¸”ë¡ì€ ```ì–¸ì–´ëª… ìœ¼ë¡œ ê°ì‹¸ê¸°. í•œêµ­ì–´ë¡œ ì„¤ëª….""",

    "review": """ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì½”ë“œ ë¦¬ë·°ì–´ì…ë‹ˆë‹¤.
ê²€í†  í•­ëª©: ì½”ë“œ í’ˆì§ˆ, ë²„ê·¸, ì„±ëŠ¥, ë³´ì•ˆ, ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤.
í•œêµ­ì–´ë¡œ ìƒì„¸íˆ í”¼ë“œë°±í•˜ê³  ì ìˆ˜(1-10)ë„ ë§¤ê²¨ì£¼ì„¸ìš”.""",

    "debug": """ë‹¹ì‹ ì€ ë””ë²„ê¹… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì—ëŸ¬ ë¶„ì„ â†’ ë²„ê·¸ ì›ì¸ íŒŒì•… â†’ ìˆ˜ì • ì½”ë“œ â†’ ì„¤ëª… â†’ ì˜ˆë°©ë²•.
í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ê³  ìˆ˜ì •ëœ ì½”ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.""",

    "explain": """ë‹¹ì‹ ì€ í”„ë¡œê·¸ë˜ë° êµì‚¬ì…ë‹ˆë‹¤.
ì½”ë“œ ëª©ì , ë¶€ë¶„ë³„ ë™ì‘, ì•Œê³ ë¦¬ì¦˜, ì‹¤í–‰ íë¦„ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆê²Œ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.""",

    "refactor": """ë‹¹ì‹ ì€ ë¦¬íŒ©í† ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê°€ë…ì„± í–¥ìƒ, ì¤‘ë³µ ì œê±°, ë‹¨ì¼ ì±…ì„, ë„¤ì´ë° ê°œì„ , ì„±ëŠ¥ ìµœì í™”.
ì›ë³¸ ê¸°ëŠ¥ ìœ ì§€í•˜ë©´ì„œ ê°œì„ ëœ ì½”ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.""",

    "convert": """ë‹¹ì‹ ì€ ë‹¤êµ­ì–´ í”„ë¡œê·¸ë˜ë¨¸ì…ë‹ˆë‹¤.
ì›ë³¸ ë¡œì§ ìœ ì§€, ëŒ€ìƒ ì–¸ì–´ ê´€ìš©ì  í‘œí˜„, ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì¤€ìˆ˜.
í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ê³  ë³€í™˜ëœ ì½”ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.""",

    "test": """ë‹¹ì‹ ì€ í…ŒìŠ¤íŠ¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¨ìœ„ í…ŒìŠ¤íŠ¸, ê²½ê³„ê°’, ì˜ˆì™¸ ìƒí™© í…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
ì ì ˆí•œ í”„ë ˆì„ì›Œí¬ ì‚¬ìš©.""",

    "general": """ë‹¹ì‹ ì€ ì¹œì ˆí•œ í”„ë¡œê·¸ë˜ë° ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì •ë³´, ì˜ˆì œ ì½”ë“œ í¬í•¨. í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ.""",
}

# Self-Correction ê²€í† ìš© í”„ë¡¬í”„íŠ¸
SC_REVIEW_PROMPT = """ë‹¹ì‹ ì€ ì—„ê²©í•œ í’ˆì§ˆ ê²€í† ìì…ë‹ˆë‹¤.
ìƒì„±ëœ ë‹µë³€ì„ ê²€í† í•˜ê³  ë¬¸ì œì ì„ ì°¾ì•„ì£¼ì„¸ìš”.

ê²€í†  ê¸°ì¤€:
1. ì§ˆë¬¸/ìš”ì²­ì— ì •í™•íˆ ë‹µë³€í–ˆëŠ”ê°€?
2. ì½”ë“œê°€ ìˆë‹¤ë©´ ë¬¸ë²• ì˜¤ë¥˜ë‚˜ ë²„ê·¸ê°€ ì—†ëŠ”ê°€?
3. ë…¼ë¦¬ì  ì˜¤ë¥˜ë‚˜ ëª¨ìˆœì´ ìˆëŠ”ê°€?
4. ì„¤ëª…ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
5. ëˆ„ë½ëœ ì¤‘ìš” ì •ë³´ê°€ ìˆëŠ”ê°€?

ì¶œë ¥ í˜•ì‹: ì²« ì¤„ì— PASS ë˜ëŠ” FAIL, ì´í›„ ìƒì„¸ í”¼ë“œë°±"""


# ========================================
# í† í° ë¡œë“œ
# ========================================
def load_api_token():
    global API_TOKEN
    paths = ["token.txt", "../token.txt", os.path.expanduser("~/token.txt")]
    for p in paths:
        if os.path.exists(p):
            try:
                with open(p, "r", encoding='utf-8') as f:
                    API_TOKEN = f.read().strip()
                if API_TOKEN and "REPLACE" not in API_TOKEN:
                    logger.info(f"âœ… í† í° ë¡œë“œ: {p}")
                    return True
            except Exception as e:
                logger.error(f"âŒ í† í° ë¡œë“œ ì‹¤íŒ¨: {e}")
    return False


# ========================================
# GGUF ë¡œì»¬ ëª¨ë¸ ë¡œë“œ
# ========================================
def load_local_model():
    global LOCAL_LLM
    try:
        from llama_cpp import Llama

        if not os.path.exists(GGUF_MODEL_PATH):
            logger.error(f"âŒ GGUF íŒŒì¼ ì—†ìŒ: {GGUF_MODEL_PATH}")
            return False

        # pc_assistantì˜ ëª¨ë¸ ì„¤ì •ì—ì„œ gpu_layers ê°€ì ¸ì˜¤ê¸°
        gpu_layers = 35  # ê¸°ë³¸ê°’ (GPU+CPU ë³‘ë ¬)
        try:
            from pc_assistant import AVAILABLE_MODELS, CURRENT_LOCAL_MODEL
            gpu_layers = AVAILABLE_MODELS.get(CURRENT_LOCAL_MODEL, {}).get("gpu_layers", 35)
        except:
            pass

        logger.info(f"ğŸ”„ GGUF ëª¨ë¸ ë¡œë”© ì¤‘... (GPU layers={gpu_layers}, RTX 3060 12GB + RAM)")
        LOCAL_LLM = Llama(
            model_path=GGUF_MODEL_PATH,
            n_ctx=8192,
            n_threads=8,
            n_gpu_layers=gpu_layers,
            n_batch=512,
            verbose=False
        )
        logger.info("âœ… GGUF ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        return True
    except ImportError:
        logger.error("âŒ llama-cpp-python ë¯¸ì„¤ì¹˜. pip install llama-cpp-python")
        return False
    except Exception as e:
        logger.error(f"âŒ GGUF ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


# ========================================
# ë¡œì»¬ LLM í˜¸ì¶œ (GGUF)
# ========================================
def call_local_llm(prompt: str, system_prompt: str = "", max_tokens: int = 4000) -> dict:
    global LOCAL_LLM

    if LOCAL_LLM is None:
        return {"success": False, "error": "ë¡œì»¬ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}

    # Qwen3 ì±„íŒ… í¬ë§·
    full_prompt = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{prompt}<|im_end|>
<|im_start|>assistant
"""

    try:
        output = LOCAL_LLM(
            full_prompt,
            max_tokens=max_tokens,
            temperature=0.3,
            top_p=0.9,
            stop=["<|im_end|>", "<|im_start|>"],
            echo=False
        )

        content = output["choices"][0]["text"].strip()
        # <think> íƒœê·¸ ì œê±°
        content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
        return {"success": True, "content": content}
    except Exception as e:
        return {"success": False, "error": str(e)}


# ========================================
# LLM API í˜¸ì¶œ
# ========================================
def call_llm_api(prompt: str, system_prompt: str = "", max_tokens: int = 4000) -> dict:
    global API_TOKEN

    if not API_TOKEN:
        return {"success": False, "error": "API í† í° ì—†ìŒ"}

    headers = {
        "Authorization": f"Bearer {API_TOKEN}",
        "Content-Type": "application/json"
    }

    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    data = {
        "model": API_MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": 0.3
    }

    try:
        response = requests.post(API_URL, headers=headers, json=data, timeout=300)

        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
            return {"success": True, "content": content}
        else:
            return {"success": False, "error": f"API ì˜¤ë¥˜: {response.status_code}"}
    except Exception as e:
        return {"success": False, "error": str(e)}


# ========================================
# LLM í˜¸ì¶œ (API ë˜ëŠ” ë¡œì»¬ ìë™ ì„ íƒ)
# ========================================
def call_llm(prompt: str, system_prompt: str = "", max_tokens: int = 4000) -> dict:
    """LLM_MODEì— ë”°ë¼ API ë˜ëŠ” ë¡œì»¬ ëª¨ë¸ í˜¸ì¶œ"""
    if LLM_MODE == "local":
        return call_local_llm(prompt, system_prompt, max_tokens)
    else:
        return call_llm_api(prompt, system_prompt, max_tokens)


# ========================================
# Self-Correction ë¡œì§
# ========================================
def run_self_correction(prompt: str, system_prompt: str, max_retries: int = 3) -> dict:
    """Self-Correction ë£¨í”„ ì‹¤í–‰"""
    
    answer = ""
    review = ""
    is_valid = False
    attempt = 0
    
    for attempt in range(1, max_retries + 1):
        logger.info(f"ğŸ”„ [SC] ì‹œë„ {attempt}/{max_retries}")
        
        # 1. ë‹µë³€ ìƒì„±
        if attempt == 1:
            gen_prompt = prompt
        else:
            # ì¬ì‹œë„: ì´ì „ í”¼ë“œë°± ë°˜ì˜
            gen_prompt = f"{prompt}\n\n[ì´ì „ ê²€í†  í”¼ë“œë°±]\n{review}\n\nìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ë‹¤ì‹œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        
        result = call_llm(gen_prompt, system_prompt)
        if not result["success"]:
            return {"success": False, "error": result["error"], "retry_count": attempt}
        
        answer = result["content"]
        
        # 2. ë‹µë³€ ê²€í† 
        review_prompt = f"[ì›ë³¸ ìš”ì²­]\n{prompt}\n\n[ìƒì„±ëœ ë‹µë³€]\n{answer}"
        review_result = call_llm(review_prompt, SC_REVIEW_PROMPT, max_tokens=500)
        
        if not review_result["success"]:
            # ê²€í†  ì‹¤íŒ¨í•´ë„ ë‹µë³€ì€ ë°˜í™˜
            return {
                "success": True,
                "answer": answer,
                "retry_count": attempt,
                "is_valid": False,
                "review": "ê²€í†  ì‹¤íŒ¨"
            }
        
        review = review_result["content"]
        is_valid = review.strip().upper().startswith("PASS")
        
        logger.info(f"   ê²°ê³¼: {'âœ… PASS' if is_valid else 'âŒ FAIL'}")
        
        if is_valid:
            break
    
    return {
        "success": True,
        "answer": answer,
        "retry_count": attempt,
        "is_valid": is_valid,
        "review": review
    }


# ========================================
# í”„ë¡¬í”„íŠ¸ ë¹Œë”
# ========================================
def build_prompt(mode: str, question: str, code: str, language: str) -> str:
    """ëª¨ë“œì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    if mode == "generate":
        return f"ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” {language} ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:\n\n{question}"
    
    elif mode in ["review", "debug", "explain", "refactor"]:
        prompt = f"ë‹¤ìŒ {language} ì½”ë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n```{language}\n{code}\n```"
        if question:
            prompt += f"\n\n{question}"
        return prompt
    
    elif mode == "test":
        prompt = f"ë‹¤ìŒ {language} ì½”ë“œì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:\n\n```{language}\n{code}\n```"
        if question:
            prompt += f"\n\n{question}"
        return prompt
    
    else:  # general
        prompt = question
        if code:
            prompt += f"\n\n```{language}\n{code}\n```"
        return prompt


# ========================================
# Pydantic Models
# ========================================
class CodingQuery(BaseModel):
    question: str
    mode: str = "general"
    language: str = "python"
    code: Optional[str] = ""
    use_sc: bool = False  # â˜… Self-Correction ì‚¬ìš© ì—¬ë¶€
    request_id: Optional[str] = None


class ConvertRequest(BaseModel):
    code: str
    from_lang: str
    to_lang: str
    use_sc: bool = False


class QuickRequest(BaseModel):
    action: str
    code: str
    language: str = "python"
    use_sc: bool = False


# ========================================
# Startup
# ========================================
@app.on_event("startup")
async def startup():
    global LLM_MODE, ENV_MODE, API_MODEL

    # 1. API í† í° ë¨¼ì € ì‹œë„
    if load_api_token():
        LLM_MODE = "api"
        logger.info("âœ… API ëª¨ë“œë¡œ ì‹œì‘")
    # 2. í† í° ì—†ìœ¼ë©´ ë¡œì»¬ GGUF ëª¨ë¸ ì‹œë„
    elif load_local_model():
        LLM_MODE = "local"
        ENV_MODE = "local"
        API_MODEL = ENV_CONFIG["local"]["model"]
        logger.info("âœ… ë¡œì»¬ GGUF ëª¨ë“œë¡œ ì‹œì‘")
    else:
        logger.warning("âš ï¸ API í† í°ë„ ì—†ê³  ë¡œì»¬ ëª¨ë¸ë„ ë¡œë“œ ì‹¤íŒ¨")

    # PC ë¹„ì„œ ì´ˆê¸°í™”
    try:
        init_assistant()
        logger.info("âœ… PC ë¹„ì„œ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ PC ë¹„ì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    logger.info(f"ğŸš€ ì„œë²„ ì‹œì‘: {ENV_MODE} ({LLM_MODE} ëª¨ë“œ)")


# ========================================
# API Endpoints
# ========================================
# HTML íŒŒì¼ ê²½ë¡œ
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
HTML_FILE = os.path.join(BASE_DIR, "INDEX.HTML")

@app.get("/")
async def home():
    return FileResponse(HTML_FILE)

@app.get("/magi.png")
async def magi_icon():
    return FileResponse(os.path.join(os.path.dirname(os.path.abspath(__file__)), "magi.png"), media_type="image/png")

@app.get("/magi_f.png")
async def magi_f_icon():
    return FileResponse(os.path.join(os.path.dirname(os.path.abspath(__file__)), "magi_f.png"), media_type="image/png")


@app.get("/api/status")
async def api_status():
    return {
        "llm_mode": LLM_MODE,
        "api_available": API_TOKEN is not None,
        "local_available": LOCAL_LLM is not None,
        "env": ENV_MODE,
        "model": API_MODEL,
        "env_name": ENV_CONFIG[ENV_MODE]["name"],
        "self_correction": True,
        "available_envs": list(ENV_CONFIG.keys())
    }


@app.post("/api/set_env")
async def set_env(data: dict):
    global ENV_MODE, API_URL, API_MODEL, LLM_MODE
    new_env = data.get("env", "common")
    if new_env not in ENV_CONFIG:
        return {"success": False, "message": "ì˜ëª»ëœ í™˜ê²½"}

    # ë¡œì»¬ ëª¨ë“œ ì „í™˜
    if new_env == "local":
        if LOCAL_LLM is None:
            # ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹œë„
            if not load_local_model():
                return {"success": False, "message": "ë¡œì»¬ GGUF ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨"}
        LLM_MODE = "local"
    else:
        # API ëª¨ë“œ ì „í™˜
        if not API_TOKEN:
            return {"success": False, "message": "API í† í°ì´ ì—†ìŠµë‹ˆë‹¤. ë¡œì»¬ ëª¨ë“œë§Œ ì‚¬ìš© ê°€ëŠ¥"}
        LLM_MODE = "api"

    ENV_MODE = new_env
    API_URL = ENV_CONFIG[new_env]["url"]
    API_MODEL = ENV_CONFIG[new_env]["model"]
    return {"success": True, "env": ENV_MODE, "model": API_MODEL, "name": ENV_CONFIG[new_env]["name"], "llm_mode": LLM_MODE}


@app.post("/api/reload_token")
async def reload_token():
    success = load_api_token()
    return {"success": success, "message": "í† í° ë¦¬ë¡œë“œ ì™„ë£Œ" if success else "í† í° ë¦¬ë¡œë“œ ì‹¤íŒ¨"}


@app.post("/api/ask")
async def ask(query: CodingQuery):
    """ë©”ì¸ ì§ˆë¬¸ ì²˜ë¦¬ - Self-Correction ì˜µì…˜ í¬í•¨"""
    mode = query.mode
    question = query.question.strip()
    code = query.code.strip() if query.code else ""
    language = query.language
    use_sc = query.use_sc
    
    # ì½”ë“œ í•„ìš”í•œ ëª¨ë“œ ì²´í¬
    if mode in ["review", "debug", "explain", "refactor", "test"] and not code:
        return {"success": False, "answer": "âŒ ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!"}
    
    if mode == "generate" and not question:
        return {"success": False, "answer": "âŒ ìš”ì²­ì‚¬í•­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!"}
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = build_prompt(mode, question, code, language)
    system_prompt = SYSTEM_PROMPTS.get(mode, SYSTEM_PROMPTS["general"])
    
    # â˜… Self-Correction ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
    if use_sc:
        logger.info(f"ğŸ”„ Self-Correction ëª¨ë“œ: {mode}")
        result = run_self_correction(prompt, system_prompt)
        
        if result["success"]:
            return {
                "success": True,
                "answer": result["answer"],
                "use_sc": True,
                "retry_count": result["retry_count"],
                "is_valid": result["is_valid"],
                "review": result.get("review", "")
            }
        else:
            return {"success": False, "answer": f"âŒ {result['error']}"}
    else:
        # ì¼ë°˜ ëª¨ë“œ (1íšŒ í˜¸ì¶œ)
        result = call_llm(prompt, system_prompt)
        
        if result["success"]:
            return {"success": True, "answer": result["content"], "use_sc": False}
        return {"success": False, "answer": f"âŒ {result['error']}"}


@app.post("/api/convert")
async def convert_code(request: ConvertRequest):
    """ì½”ë“œ ì–¸ì–´ ë³€í™˜"""
    system_prompt = SYSTEM_PROMPTS["convert"]
    prompt = f"""ë‹¤ìŒ {request.from_lang} ì½”ë“œë¥¼ {request.to_lang}ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”:

```{request.from_lang}
{request.code}
```

ë³€í™˜ ì‹œ {request.to_lang}ì˜ ê´€ìš©ì ì¸ í‘œí˜„ê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ë”°ë¼ì£¼ì„¸ìš”."""
    
    if request.use_sc:
        result = run_self_correction(prompt, system_prompt)
        if result["success"]:
            return {
                "success": True,
                "answer": result["answer"],
                "use_sc": True,
                "retry_count": result["retry_count"],
                "is_valid": result["is_valid"]
            }
        return {"success": False, "answer": f"âŒ {result['error']}"}
    else:
        result = call_llm(prompt, system_prompt)
        if result["success"]:
            return {"success": True, "answer": result["content"], "use_sc": False}
        return {"success": False, "answer": f"âŒ {result['error']}"}


@app.post("/api/quick")
async def quick_action(request: QuickRequest):
    """ë¹ ë¥¸ ì‘ì—…"""
    action = request.action
    code = request.code
    language = request.language
    use_sc = request.use_sc
    
    if not code:
        return {"success": False, "answer": "âŒ ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!"}
    
    action_prompts = {
        "add_comments": "ì´ ì½”ë“œì— í•œêµ­ì–´ ì£¼ì„ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”. ê° í•¨ìˆ˜ì™€ ì¤‘ìš”í•œ ë¡œì§ì— ì„¤ëª…ì„ ë‹¬ì•„ì£¼ì„¸ìš”.",
        "improve_names": "ì´ ì½”ë“œì˜ ë³€ìˆ˜ëª…ê³¼ í•¨ìˆ˜ëª…ì„ ë” ëª…í™•í•˜ê³  ì˜ë¯¸ìˆê²Œ ê°œì„ í•´ì£¼ì„¸ìš”.",
        "optimize": "ì´ ì½”ë“œì˜ ì„±ëŠ¥ì„ ìµœì í™”í•´ì£¼ì„¸ìš”. ë¶ˆí•„ìš”í•œ ì—°ì‚°ì„ ì¤„ì´ê³  íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.",
        "simplify": "ì´ ì½”ë“œë¥¼ ë” ê°„ê²°í•˜ê³  ì½ê¸° ì‰½ê²Œ ë‹¨ìˆœí™”í•´ì£¼ì„¸ìš”.",
        "type_hints": "ì´ Python ì½”ë“œì— íƒ€ì… íŒíŠ¸ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.",
        "docstring": "ì´ ì½”ë“œì˜ ëª¨ë“  í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ì— docstringì„ ì¶”ê°€í•´ì£¼ì„¸ìš”."
    }
    
    if action not in action_prompts:
        return {"success": False, "answer": "âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—…ì…ë‹ˆë‹¤."}
    
    prompt = f"{action_prompts[action]}\n\n```{language}\n{code}\n```"
    system_prompt = SYSTEM_PROMPTS["refactor"]
    
    if use_sc:
        result = run_self_correction(prompt, system_prompt)
        if result["success"]:
            return {
                "success": True,
                "answer": result["answer"],
                "use_sc": True,
                "retry_count": result["retry_count"],
                "is_valid": result["is_valid"]
            }
        return {"success": False, "answer": f"âŒ {result['error']}"}
    else:
        result = call_llm(prompt, system_prompt)
        if result["success"]:
            return {"success": True, "answer": result["content"], "use_sc": False}
        return {"success": False, "answer": f"âŒ {result['error']}"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=10002)