#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
pc_assistant.py
MAGI (main1_First AI) - PC ë¹„ì„œ + ì§€ì‹ê¸°ë°˜ AI v0.2
- ìŠ¤í¬ë¦°ìƒ·: ì „ìš© í´ë” ì €ì¥ + ì›¹ ì¸ë¼ì¸ í‘œì‹œ
- íŒŒì¼ íƒìƒ‰ê¸°/ë©”ëª¨ì¥ ì‹¤í–‰ ì œê±°
"""

import os
import re
import json
import base64
import subprocess
import platform
import psutil
import tempfile
import datetime
import time
import webbrowser
import fnmatch
import requests
import shutil
import pandas as pd
from typing import Optional, List
import asyncio
from fastapi import FastAPI, APIRouter, UploadFile, File
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import logging
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("PCAssistant")

router = APIRouter(prefix="/assistant", tags=["assistant"])
app = FastAPI(title="MAGI (main1_First AI)")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ========================================
# Global Configuration
# ========================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# â˜… ë¡œì»¬ GGUF ëª¨ë¸ ì„¤ì • (ìƒì„± íŒŒë¼ë¯¸í„° í¬í•¨)
AVAILABLE_MODELS = {
    "qwen3-14b": {
        "path": os.path.join(BASE_DIR, "Qwen3-14B-Q4_K_M.gguf"),
        "name": "Qwen3-14B (Q4_K_M)",
        "desc": "í•œê¸€ ìµœì í™” â­ì¶”ì²œ (GPU+CPU)",
        "ctx": 8192,
        "gpu_layers": 35,
        "chat_format": "chatml",
        "repeat_penalty": 1.1,
        "temperature": 0.7,
        "korean_support": True
    },
    "qwen3-8b": {
        "path": os.path.join(BASE_DIR, "Qwen3-8B-Q6_K.gguf"),
        "name": "Qwen3-8B (Q6_K)",
        "desc": "ê²½ëŸ‰ ëª¨ë¸, ë¹ ë¥¸ ì‘ë‹µ (í’€ GPU)",
        "ctx": 4096,
        "gpu_layers": 50,
        "chat_format": "chatml",
        "repeat_penalty": 1.1,
        "temperature": 0.7,
        "korean_support": True
    },
    "gemma3-12b": {
        "path": os.path.join(BASE_DIR, "gemma-3-12b-it-q4_k_m.gguf"),
        "name": "Gemma3-12B (Q4_K_M)",
        "desc": "Google 12B, ê· í˜•ì¡íŒ ì„±ëŠ¥ (í’€ GPU)",
        "ctx": 4096,
        "gpu_layers": 50,
        "chat_format": "gemma",
        "repeat_penalty": 1.1,
        "temperature": 0.7,
        "korean_support": True
    },
    "oh-dcft-claude": {
        "path": os.path.join(BASE_DIR, "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q8_0.gguf"),
        "name": "OH-DCFT Claude-Sonnet (Q8_0)",
        "desc": "Claude ìŠ¤íƒ€ì¼ ì‘ë‹µ, ê³ í’ˆì§ˆ (GPU+CPU)",
        "ctx": 8192,
        "gpu_layers": 35,
        "chat_format": "chatml",
        "repeat_penalty": 1.1,
        "temperature": 0.7,
        "korean_support": True
    },
    "qwen25-7b": {
        "path": os.path.join(BASE_DIR, "Qwen2.5-7B-Instruct-Q8_0.gguf"),
        "name": "Qwen2.5-7B (Q8_0)",
        "desc": "Qwen2.5 ê²½ëŸ‰ ê³ ì •ë°€, ë¹ ë¥¸ ì‘ë‹µ (í’€ GPU)",
        "ctx": 4096,
        "gpu_layers": 50,
        "chat_format": "chatml",
        "repeat_penalty": 1.1,
        "temperature": 0.7,
        "korean_support": True
    },
    "llama-korean-8b": {
        "path": os.path.join(BASE_DIR, "Llama-3.1-Korean-8B-Instruct.Q8_0.gguf"),
        "name": "Llama-3.1-Korean-8B (Q8_0)",
        "desc": "í•œêµ­ì–´ íŠ¹í™” Llama, í•œê¸€ ìµœì í™” (í’€ GPU)",
        "ctx": 8192,
        "gpu_layers": 50,
        "chat_format": "llama-3",
        "repeat_penalty": 1.1,
        "temperature": 0.7,
        "korean_support": True
    },
}
CURRENT_LOCAL_MODEL = "qwen3-8b"
GGUF_MODEL_PATH = AVAILABLE_MODELS[CURRENT_LOCAL_MODEL]["path"]

LOCAL_LLM = None
CHAT_HISTORY = []
HISTORY_FILE = os.path.join(BASE_DIR, "chat_history.json")
HISTORY_MAX = 500  # â˜… íˆìŠ¤í† ë¦¬ ìµœëŒ€ ê±´ìˆ˜ (ì„¤ì • ê°€ëŠ¥)

# â˜… ì„¸ì…˜ ê´€ë¦¬
CHAT_SESSIONS_DIR = os.path.join(BASE_DIR, "chat_sessions")
os.makedirs(CHAT_SESSIONS_DIR, exist_ok=True)
CURRENT_SESSION_ID = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

# â˜… í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
TOKEN_USAGE = {
    "prompt_tokens": 0,
    "completion_tokens": 0,
    "total_tokens": 0,
    "call_count": 0
}

# â˜… ìŠ¤í¬ë¦°ìƒ· ì „ìš© í´ë”
SCREENSHOT_DIR = os.path.join(BASE_DIR, "screenshots")
os.makedirs(SCREENSHOT_DIR, exist_ok=True)

# â˜… ìŠ¤í¬ë¦°ìƒ· ë¹„ë°€ë²ˆí˜¸ ì¸ì¦
SCREENSHOT_PASSWORD = "1234"
screenshot_authenticated = False  # ì¸ì¦ ìƒíƒœ (1íšŒìš©)

# â˜… ë¦¬ì†ŒìŠ¤ í´ë” (HTML êµ¬ì„±ë„ ë“± ì •ì  íŒŒì¼)
RESOURCES_DIR = os.path.join(BASE_DIR, "resources")
os.makedirs(RESOURCES_DIR, exist_ok=True)

# â˜… ì§€ì‹ë² ì´ìŠ¤(MD ë¬¸ì„œ) í´ë”
KNOWLEDGE_DIR = os.path.join(BASE_DIR, "knowledge")
os.makedirs(KNOWLEDGE_DIR, exist_ok=True)

# â˜… ê³¼ê±°ì§€ì‹ ë³´ê´€ í´ë”
KNOWLEDGE_ARCHIVE_DIR = os.path.join(BASE_DIR, "knowledge_archive")
os.makedirs(KNOWLEDGE_ARCHIVE_DIR, exist_ok=True)

# â˜… TF-IDF + BM25 ê²€ìƒ‰ ì¸ë±ìŠ¤ (ì§€ì‹ë² ì´ìŠ¤)
TFIDF_INDEX = {
    "vectorizer": None,
    "matrix": None,
    "bm25": None,  # â˜… BM25 ì¸ë±ìŠ¤ ì¶”ê°€
    "tokenized_docs": [],  # â˜… BM25ìš© í† í°í™”ëœ ë¬¸ì„œ
    "filenames": [],
    "contents": [],
    "built_at": None
}

# â˜… í•œêµ­ì–´ ì¡°ì‚¬/ì–´ë¯¸ ë¶„ë¦¬ íŒ¨í„´ (í™•ì¥)
_KOREAN_PARTICLE_PATTERN = re.compile(
    r'(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì˜|ì—|ì—ì„œ|ë¡œ|ìœ¼ë¡œ|ë„|ì™€|ê³¼|ë‘|ì´ë‘|ë¶€í„°|ê¹Œì§€|ë§Œ|ë¼ê³ |ì´ë¼ê³ |ì—ê²Œ|í•œí…Œ|ê»˜|ë³´ë‹¤|ì²˜ëŸ¼|ê°™ì´|ë§ˆë‹¤|ëŒ€ë¡œ|ë°–ì—|ì¡°ì°¨|ë¿)'
    r'$'
)

def korean_tokenize(text: str) -> List[str]:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„ë¦¬ (ì¡°ì‚¬ ë¶„ë¦¬, ë³µí•©ëª…ì‚¬ ë¶„ë¦¬)"""
    tokens = []
    # ì˜ì–´/ìˆ«ì í† í°
    for match in re.finditer(r'[a-zA-Z0-9_]+', text):
        word = match.group().lower()
        if len(word) >= 2:
            tokens.append(word)

    # í•œêµ­ì–´ í† í° (2ê¸€ì ì´ìƒ)
    for match in re.finditer(r'[ê°€-í£]{2,}', text):
        word = match.group()
        tokens.append(word)
        # ì¡°ì‚¬ ë¶„ë¦¬ ì‹œë„
        stripped = _KOREAN_PARTICLE_PATTERN.sub('', word)
        if stripped and stripped != word and len(stripped) >= 2:
            tokens.append(stripped)

    return list(set(tokens))


def build_tfidf_index():
    """ì§€ì‹ë² ì´ìŠ¤ íŒŒì¼ë“¤ë¡œ TF-IDF + BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
    filenames = []
    contents = []
    for f in sorted(os.listdir(KNOWLEDGE_DIR)):
        if not f.endswith(('.md', '.txt')):
            continue
        filepath = os.path.join(KNOWLEDGE_DIR, f)
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as fh:
                text = fh.read()
                if text.strip():
                    filenames.append(f)
                    contents.append(text)
        except (OSError, PermissionError):
            continue

    if not contents:
        TFIDF_INDEX["vectorizer"] = None
        TFIDF_INDEX["matrix"] = None
        TFIDF_INDEX["bm25"] = None
        TFIDF_INDEX["tokenized_docs"] = []
        TFIDF_INDEX["filenames"] = []
        TFIDF_INDEX["contents"] = []
        TFIDF_INDEX["built_at"] = None
        return

    vectorizer = TfidfVectorizer(
        analyzer='char_wb',
        ngram_range=(2, 4),
        max_features=10000,
        sublinear_tf=True
    )
    matrix = vectorizer.fit_transform(contents)

    # â˜… BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
    bm25_instance = None
    tokenized_docs = []
    try:
        from rank_bm25 import BM25Okapi
        tokenized_docs = [korean_tokenize(doc) for doc in contents]
        bm25_instance = BM25Okapi(tokenized_docs)
        logger.info(f"ğŸ“Š BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(tokenized_docs)}ê°œ ë¬¸ì„œ")
    except ImportError:
        logger.warning("âš ï¸ rank_bm25 ë¯¸ì„¤ì¹˜ â†’ TF-IDFë§Œ ì‚¬ìš©")
    except Exception as e:
        logger.warning(f"âš ï¸ BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")

    TFIDF_INDEX["vectorizer"] = vectorizer
    TFIDF_INDEX["matrix"] = matrix
    TFIDF_INDEX["bm25"] = bm25_instance
    TFIDF_INDEX["tokenized_docs"] = tokenized_docs
    TFIDF_INDEX["filenames"] = filenames
    TFIDF_INDEX["contents"] = contents
    TFIDF_INDEX["built_at"] = datetime.datetime.now().isoformat()
    logger.info(f"ğŸ“Š TF-IDF ì¸ë±ìŠ¤ êµ¬ì¶•: {len(filenames)}ê°œ ë¬¸ì„œ, {matrix.shape[1]}ê°œ íŠ¹ì„±")


def tfidf_search(query: str, top_k: int = 5) -> List[dict]:
    """TF-IDF + BM25 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì ìˆ˜ ê²°í•©)"""
    if TFIDF_INDEX["vectorizer"] is None or TFIDF_INDEX["matrix"] is None:
        build_tfidf_index()

    if TFIDF_INDEX["vectorizer"] is None:
        return []

    num_docs = len(TFIDF_INDEX["filenames"])

    # â˜… TF-IDF ìŠ¤ì½”ì–´ (0~1 ì •ê·œí™”)
    query_vec = TFIDF_INDEX["vectorizer"].transform([query])
    tfidf_scores = cosine_similarity(query_vec, TFIDF_INDEX["matrix"]).flatten()

    # â˜… BM25 ìŠ¤ì½”ì–´ (ìˆìœ¼ë©´ ê²°í•©)
    bm25_scores = np.zeros(num_docs)
    if TFIDF_INDEX["bm25"] is not None:
        try:
            query_tokens = korean_tokenize(query)
            raw_bm25 = TFIDF_INDEX["bm25"].get_scores(query_tokens)
            # 0~1ë¡œ ì •ê·œí™”
            max_bm25 = max(raw_bm25) if max(raw_bm25) > 0 else 1.0
            bm25_scores = np.array(raw_bm25) / max_bm25
        except Exception as e:
            logger.warning(f"âš ï¸ BM25 ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

    # â˜… í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: TF-IDF 40% + BM25 60% (BM25ê°€ í‚¤ì›Œë“œ ë§¤ì¹­ì— ë” íš¨ê³¼ì )
    if TFIDF_INDEX["bm25"] is not None:
        combined_scores = tfidf_scores * 0.4 + bm25_scores * 0.6
    else:
        combined_scores = tfidf_scores

    top_indices = np.argsort(combined_scores)[::-1][:top_k]
    results = []
    for idx in top_indices:
        score = float(combined_scores[idx])
        if score < 0.01:
            continue
        content = TFIDF_INDEX["contents"][idx]
        # ìŠ¤ë‹ˆí«: ì¿¼ë¦¬ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¶€ë¶„ ì¶”ì¶œ
        snippet = ""
        query_tokens_for_snippet = korean_tokenize(query)
        content_lower = content.lower()
        for token in query_tokens_for_snippet:
            pos = content_lower.find(token.lower())
            if pos >= 0:
                snippet = content[max(0, pos-30):min(len(content), pos+100)].replace('\n', ' ').strip()
                break
        if not snippet and len(content) > 0:
            snippet = content[:100].replace('\n', ' ').strip()

        search_method = "tfidf+bm25" if TFIDF_INDEX["bm25"] is not None else "tfidf"
        results.append({
            "filename": TFIDF_INDEX["filenames"][idx],
            "snippet": f"...{snippet}..." if snippet else "",
            "score": round(score * 100, 1),
            "method": search_method
        })

    return results


# â˜… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ / ë„ë©”ì¸ ì§€ì‹ TXT íŒŒì¼
SYSTEM_PROMPT_FILE = os.path.join(BASE_DIR, "system_prompt.txt")
DOMAIN_KNOWLEDGE_FILE = os.path.join(BASE_DIR, "domain_knowledge.txt")

# â˜… LLM ìƒì„± íŒŒë¼ë¯¸í„° (UIì—ì„œ ì¡°ì ˆ ê°€ëŠ¥)
LLM_PARAMS = {
    "temperature": 0.7,
    "repeat_penalty": 1.1,
    "max_tokens": 4096,
    "task_auto_mode": False,   # íƒœìŠ¤í¬ë³„ ìë™ ë¶„ë¥˜ ON/OFF
}

# â˜… íƒœìŠ¤í¬ë³„ íŒŒë¼ë¯¸í„° í”„ë¡œí•„ (ë‹µë³€ í’ˆì§ˆ ìµœì í™”)
TASK_PARAM_PROFILES = {
    "knowledge_qa": {"temperature": 0.2, "top_p": 0.8, "max_tokens": 2048},
    "general_chat": {"temperature": 0.5, "top_p": 0.9, "max_tokens": 2048},
    "tool_call":    {"temperature": 0.1, "top_p": 0.7, "max_tokens": 1024},
}

def classify_task_type(user_message: str) -> str:
    """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ íƒœìŠ¤í¬ ìœ í˜• ë¶„ë¥˜"""
    msg = user_message.lower().strip()

    # tool_call: PC ì œì–´ ê´€ë ¨ í‚¤ì›Œë“œ
    tool_keywords = ["í”„ë¡œê·¸ë¨", "ì‹¤í–‰", "ì¢…ë£Œ", "í”„ë¡œì„¸ìŠ¤", "ìŠ¤í¬ë¦°ìƒ·", "ìº¡ì²˜", "í™”ë©´",
                     "ì‹œìŠ¤í…œ", "cpu", "ë©”ëª¨ë¦¬", "ë””ìŠ¤í¬", "ëª‡ì‹œ", "ì‹œê°„", "ë‚ ì§œ",
                     "íŒŒì¼ ì°¾", "íŒŒì¼ ê²€ìƒ‰", "ê²€ìƒ‰í•´", "êµ¬ê¸€", "ë‰´ìŠ¤", "í´ë”", "ë””ë ‰í† ë¦¬"]
    if any(kw in msg for kw in tool_keywords):
        return "tool_call"

    # knowledge_qa: ì§€ì‹/ë¬¸ì„œ ê´€ë ¨ í‚¤ì›Œë“œ ë˜ëŠ” êµ¬ì²´ì  ì§ˆë¬¸
    knowledge_keywords = ["ë¬¸ì„œ", "ì•„í‚¤í…ì²˜", "ì„¤ê³„", "êµ¬ì¡°", "ëª¨ë¸", "ì˜ˆì¸¡", "ì»¬ëŸ¼",
                          "ìŠ¤í™", "ì‚¬ì–‘", "ê°€ì´ë“œ", "ë§¤ë‰´ì–¼", "ê·œì¹™", "ì •ì±…", "í”„ë¡œì íŠ¸",
                          "ì•Œë ¤ì¤˜", "ì„¤ëª…í•´", "ë­ì•¼", "ì–´ë–»ê²Œ", "ì™œ", "ë¬´ì—‡",
                          "ì´ì•¼ê¸°í•´", "ì— ëŒ€í•´", "ì—ëŒ€í•´", "ê´€ë ¨", "ë‚´ìš©", "ì •ë¦¬í•´",
                          "ìš”ì•½í•´", "ë³€ê²½", "ìˆ˜ì •", "ì—…ë°ì´íŠ¸", "íˆìŠ¤í† ë¦¬"]
    if any(kw in msg for kw in knowledge_keywords):
        return "knowledge_qa"

    # ì˜ë¬¸/ì–¸ë”ìŠ¤ì½”ì–´ í¬í•¨ í‚¤ì›Œë“œ (íŒŒì¼ëª…, ëª¨ë“ˆëª… ë“±) â†’ knowledge_qa
    if re.search(r'[A-Za-z_]{3,}', user_message):
        return "knowledge_qa"

    return "general_chat"

def get_task_params(task_type: str) -> dict:
    """íƒœìŠ¤í¬ ìœ í˜•ì— ë§ëŠ” LLM íŒŒë¼ë¯¸í„° ë°˜í™˜ (ìë™ ë¶„ë¥˜ ëª¨ë“œì‹œ í”„ë¡œí•„ ì‚¬ìš©)"""
    if LLM_PARAMS.get("task_auto_mode"):
        # ìë™ ë¶„ë¥˜ ON â†’ íƒœìŠ¤í¬ë³„ í”„ë¡œí•„ ì‚¬ìš©
        profile = TASK_PARAM_PROFILES.get(task_type, TASK_PARAM_PROFILES["general_chat"])
        return {
            "temperature": profile["temperature"],
            "top_p": profile["top_p"],
            "max_tokens": profile["max_tokens"],
            "repeat_penalty": LLM_PARAMS.get("repeat_penalty", 1.1),
        }
    else:
        # ìë™ ë¶„ë¥˜ OFF â†’ UI ìˆ˜ë™ ì„¤ì • ì‚¬ìš©
        return {
            "temperature": LLM_PARAMS.get("temperature", 0.7),
            "top_p": 0.9,
            "max_tokens": LLM_PARAMS.get("max_tokens", 4096),
            "repeat_penalty": LLM_PARAMS.get("repeat_penalty", 1.1),
        }

LLM_MODE = "local"
API_TOKEN = None

ENV_CONFIG = {
    "dev": {
        "url": "http://dev.assistant.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Coder-30B-A3B-Instruct",
        "name": "DEV(30B)"
    },
    "prod": {
        "url": "http://summary.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Next-80B-A3B-Instruct",
        "name": "PROD(80B)"
    },
    "common": {
        "url": "http://common.llm.skhynix.com/v1/chat/completions",
        "model": "gpt-oss-20b",
        "name": "COMMON(20B)"
    }
}
CURRENT_ENV = "common"
API_URL = ENV_CONFIG["common"]["url"]
API_MODEL = ENV_CONFIG["common"]["model"]

# ========================================
# System Prompt (íŒŒì¼ ê¸°ë°˜)
# ========================================
DEFAULT_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ 'MAGI (main1_First AI)'ì´ë¼ëŠ” PC AI ë¹„ì„œì…ë‹ˆë‹¤.

â˜…â˜…â˜… ì§ˆë¬¸ ìœ í˜• êµ¬ë¶„ (ì¤‘ìš”!) â˜…â˜…â˜…

[1] PC ì‘ì—… ìš”ì²­ â†’ ë°”ë¡œ PC ë„êµ¬ ì‚¬ìš©
ë‹¤ìŒ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì—†ì´ ë°”ë¡œ í•´ë‹¹ ë„êµ¬ ì‹¤í–‰:
- "í”„ë¡œê·¸ë¨", "ì‹¤í–‰", "ì¢…ë£Œ", "í”„ë¡œì„¸ìŠ¤", "ëª©ë¡" â†’ list_processes, run_program, kill_program
- "ìŠ¤í¬ë¦°ìƒ·", "ìº¡ì²˜", "í™”ë©´" â†’ screenshot
- "ì‹œìŠ¤í…œ", "CPU", "ë©”ëª¨ë¦¬", "ë””ìŠ¤í¬" â†’ get_system_info
- "ëª‡ì‹œ", "ì‹œê°„", "ë‚ ì§œ" â†’ get_time
- "íŒŒì¼ ì°¾ì•„", "íŒŒì¼ ê²€ìƒ‰" â†’ search_files
- "ê²€ìƒ‰í•´ì¤˜", "êµ¬ê¸€" â†’ google_search
- "ë‰´ìŠ¤" â†’ latest_news
- "í´ë”", "ë””ë ‰í† ë¦¬" â†’ list_directory

[2] ì§€ì‹/ì •ë³´ ì§ˆë¬¸ â†’ ì§€ì‹ë² ì´ìŠ¤ ë¨¼ì € ê²€ìƒ‰
í”„ë¡œì íŠ¸, ì½”ë“œ, ê¸°ìˆ ë¬¸ì„œ, ì—…ë¬´ ê´€ë ¨ ì§ˆë¬¸:
- ë¨¼ì € search_knowledgeë¡œ ê²€ìƒ‰
- ë¬¸ì„œ ìˆìœ¼ë©´ â†’ ë‚´ìš© ê¸°ë°˜ ë‹µë³€
- ë¬¸ì„œ ì—†ìœ¼ë©´ â†’ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€

[3] ì¼ë°˜ ëŒ€í™” â†’ ê·¸ëƒ¥ ëŒ€í™”
ì¸ì‚¬, ì¡ë‹´, ì¼ë°˜ ì§ˆë¬¸ì€ ë„êµ¬ ì—†ì´ ë°”ë¡œ ë‹µë³€

â˜…â˜…â˜… ë„êµ¬ í˜¸ì¶œ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”!) â˜…â˜…â˜…
ë„êµ¬ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
JSON ì•ë’¤ì— ì„¤ëª… í…ìŠ¤íŠ¸ë¥¼ ì ˆëŒ€ ë¶™ì´ì§€ ë§ˆì„¸ìš”.

[ë„êµ¬ í˜¸ì¶œ ì˜ˆì‹œ]
ì˜ˆì‹œ1) ì‚¬ìš©ì: "ë©”ëª¨ë¦¬ ì–¼ë§ˆë‚˜ ì“°ê³  ìˆì–´?"
ì˜¬ë°”ë¥¸ ì‘ë‹µ: {"tool": "get_system_info"}

ì˜ˆì‹œ2) ì‚¬ìš©ì: "M14 í”„ë¡œì íŠ¸ êµ¬ì¡° ì•Œë ¤ì¤˜"
ì˜¬ë°”ë¥¸ ì‘ë‹µ: {"tool": "search_knowledge", "keyword": "M14 í”„ë¡œì íŠ¸ êµ¬ì¡°"}

[PC ë„êµ¬]
- í”„ë¡œì„¸ìŠ¤ëª©ë¡: {"tool": "list_processes", "sort_by": "memory"}
- ì‹œìŠ¤í…œì •ë³´: {"tool": "get_system_info"}
- ìŠ¤í¬ë¦°ìƒ·: {"tool": "screenshot"}
- í˜„ì¬ì‹œê°„: {"tool": "get_time"}
- í”„ë¡œê·¸ë¨ì‹¤í–‰: {"tool": "run_program", "program": "notepad"}
- í”„ë¡œê·¸ë¨ì¢…ë£Œ: {"tool": "kill_program", "name": "notepad"}
- íŒŒì¼ê²€ìƒ‰: {"tool": "search_files", "keyword": "ë¬¸ì„œ", "path": "C:/"}
- í´ë”ë³´ê¸°: {"tool": "list_directory", "path": "C:/Users"}
- ì›¹ê²€ìƒ‰: {"tool": "google_search", "query": "ê²€ìƒ‰ì–´"}
- ìµœì‹ ë‰´ìŠ¤: {"tool": "latest_news"}

[ì§€ì‹ë² ì´ìŠ¤ ë„êµ¬]
- ì§€ì‹ê²€ìƒ‰: {"tool": "search_knowledge", "keyword": "í‚¤ì›Œë“œ"}
- ì§€ì‹ëª©ë¡: {"tool": "list_knowledge"}
- ì§€ì‹ì½ê¸°: {"tool": "read_knowledge", "filename": "íŒŒì¼ëª….md"}

â˜…â˜…â˜… ë‹µë³€ í’ˆì§ˆ ê·œì¹™ â˜…â˜…â˜…
1. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  "ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤" ë˜ëŠ” "í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤"ë¼ê³  í‘œì‹œí•˜ì„¸ìš”.
2. ë³µì¡í•œ ì§ˆë¬¸ì€ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•œ í›„ ë‹µë³€í•˜ì„¸ìš”.
3. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ì¼ê´€ëœ ë‹µë³€ì„ ìœ ì§€í•˜ì„¸ìš”.
4. ì¼ë°˜ ëŒ€í™”ëŠ” í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”."""

# â˜… ì§€ì‹ë² ì´ìŠ¤ ë‹µë³€ìš© ê³µí†µ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (CoT + ì •í™•ë„ ê°•í™”)
KNOWLEDGE_QA_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ì´ì ê¸°ìˆ  ë¬¸ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì‚¬ê³  ê³¼ì • - ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”]
1) ë¨¼ì € ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œì™€ ì˜ë„ë¥¼ íŒŒì•…í•˜ì„¸ìš”.
2) ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ìœ¼ì„¸ìš”.
3) ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.

[ë‹µë³€ í˜•ì‹]
**ğŸ“‹ í•µì‹¬ ìš”ì•½**
ì§ˆë¬¸ì— ëŒ€í•œ í•µì‹¬ ë‹µë³€ì„ 2~3ì¤„ë¡œ ìš”ì•½

**ğŸ“ ìƒì„¸ ë‚´ìš©**
ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ë‚´ìš©ì„ ì¶©ë¶„íˆ ìì„¸í•˜ê²Œ ì •ë¦¬:
- ì£¼ìš” ê¸°ëŠ¥/ëª©ì 
- êµ¬ì„± ìš”ì†Œ ë° ê´€ê³„
- ë™ì‘ ë°©ì‹/íë¦„
- ì¤‘ìš”í•œ ì„¤ì •ì´ë‚˜ íŒŒë¼ë¯¸í„°
- ì£¼ì˜ì‚¬í•­ì´ë‚˜ íŠ¹ì´ì‚¬í•­

[ë‹µë³€ ê·œì¹™]
1. ë¬¸ì„œ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  **ì¶©ë¶„íˆ ìƒì„¸í•˜ê²Œ** ë‹µë³€í•˜ì„¸ìš”.
2. ìƒì„¸ ë‚´ìš©ì€ ìµœì†Œ 10ì¤„ ì´ìƒ ì‘ì„±í•˜ì„¸ìš”. ë¬¸ì„œì— ìˆëŠ” ì¤‘ìš” ì •ë³´ëŠ” ë¹ ëœ¨ë¦¬ì§€ ë§ˆì„¸ìš”.
3. ì†ŒìŠ¤ì½”ë“œ ì›ë³¸ì€ ë³´ì—¬ì£¼ì§€ ë§ê³ , ì½”ë“œì˜ ê¸°ëŠ¥/ì—­í• /ë™ì‘ì„ ì„¤ëª…í•˜ì„¸ìš”.
4. ë§ˆí¬ë‹¤ìš´ í‘œ(| --- |) ì‚¬ìš© ê¸ˆì§€. "- í•­ëª©: ê°’" í˜•íƒœë¡œ ë‚˜ì—´í•˜ì„¸ìš”.
5. ## ### ëŒ€ì œëª© í—¤ë” ëŒ€ì‹  **ë³¼ë“œ**ì™€ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
6. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
7. ì ˆëŒ€ JSONì„ ì¶œë ¥í•˜ê±°ë‚˜ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”.
8. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  "ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤"ë¼ê³  í‘œì‹œí•˜ì„¸ìš”."""

DEFAULT_DOMAIN_KNOWLEDGE = """# ë„ë©”ì¸ ì§€ì‹
# ì´ íŒŒì¼ì— AIê°€ ì°¸ê³ í•  ë„ë©”ì¸ ì§€ì‹ì„ ì‘ì„±í•˜ì„¸ìš”.
# ì €ì¥í•˜ë©´ ì¦‰ì‹œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ë°˜ì˜ë©ë‹ˆë‹¤.
#
# ì˜ˆì‹œ:
# [í”„ë¡œì íŠ¸ ì •ë³´]
# - í”„ë¡œì íŠ¸ëª…: OOO
# - ì‚¬ìš© ê¸°ìˆ : FastAPI, Python, React
# - ì•„í‚¤í…ì²˜: ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤
#
# [ì½”ë”© ê·œì¹™]
# - Python 3.10+ ì‚¬ìš©
# - íƒ€ì… íŒíŠ¸ í•„ìˆ˜
# - docstring í•„ìˆ˜
#
# [ë‚´ë¶€ API]
# - ì—”ë“œí¬ì¸íŠ¸: http://xxx.xxx.com/v1/
# - ì¸ì¦: Bearer Token
"""

# â˜… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ & ë„ë©”ì¸ ì§€ì‹ ë¡œë“œ/ì €ì¥
def load_prompt_file(filepath: str, default_content: str) -> str:
    """TXT íŒŒì¼ì—ì„œ ë‚´ìš© ë¡œë“œ. íŒŒì¼ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒì„±"""
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            if content.strip():
                return content
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({filepath}): {e}")
    # íŒŒì¼ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒì„±
    save_prompt_file(filepath, default_content)
    return default_content


def save_prompt_file(filepath: str, content: str) -> bool:
    """TXT íŒŒì¼ì— ë‚´ìš© ì €ì¥"""
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        logger.info(f"âœ… íŒŒì¼ ì €ì¥: {filepath}")
        return True
    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ ({filepath}): {e}")
        return False


def get_effective_system_prompt() -> str:
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ë„ë©”ì¸ ì§€ì‹ì„ í•©ì³ì„œ ë°˜í™˜"""
    system_prompt = load_prompt_file(SYSTEM_PROMPT_FILE, DEFAULT_SYSTEM_PROMPT)
    domain_knowledge = load_prompt_file(DOMAIN_KNOWLEDGE_FILE, DEFAULT_DOMAIN_KNOWLEDGE)

    # ë„ë©”ì¸ ì§€ì‹ì—ì„œ ì£¼ì„(#ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¤„) ì œê±°í•œ ì‹¤ì œ ë‚´ìš© í™•ì¸
    dk_lines = [line for line in domain_knowledge.strip().split('\n')
                if line.strip() and not line.strip().startswith('#')]
    has_domain_knowledge = len(dk_lines) > 0

    if has_domain_knowledge:
        effective = f"""{system_prompt}

â˜…â˜…â˜… ë„ë©”ì¸ ì§€ì‹ (ë°˜ë“œì‹œ ì°¸ê³ !) â˜…â˜…â˜…
{domain_knowledge}

ìœ„ ë„ë©”ì¸ ì§€ì‹ì„ í•­ìƒ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."""
        logger.info(f"ğŸ“š ë„ë©”ì¸ ì§€ì‹ ì ìš©ë¨ ({len(dk_lines)}ì¤„)")
        return effective
    else:
        return system_prompt


# ì „ì—­ ë³€ìˆ˜ (í˜¸í™˜ì„± ìœ ì§€)
SYSTEM_PROMPT = get_effective_system_prompt()


# ========================================
# LLM Functions
# ========================================
def load_local_model(model_key: str = None):
    """ë¡œì»¬ GGUF ëª¨ë¸ ë¡œë“œ (model_keyë¡œ ëª¨ë¸ ì„ íƒ)"""
    global LOCAL_LLM, CURRENT_LOCAL_MODEL, GGUF_MODEL_PATH

    if model_key is None:
        model_key = CURRENT_LOCAL_MODEL

    if model_key not in AVAILABLE_MODELS:
        logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_key}")
        return None

    model_config = AVAILABLE_MODELS[model_key]
    model_path = model_config["path"]

    if not os.path.exists(model_path):
        logger.error(f"GGUF íŒŒì¼ ì—†ìŒ: {model_path}")
        return None

    try:
        from llama_cpp import Llama
        logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_config['name']}...")
        llm = Llama(
            model_path=model_path,
            n_ctx=model_config.get("ctx", 8192),
            n_threads=8,
            n_gpu_layers=model_config.get("gpu_layers", 50),
            n_batch=512,
            verbose=False
        )
        CURRENT_LOCAL_MODEL = model_key
        GGUF_MODEL_PATH = model_path
        logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_config['name']}")
        return llm
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def get_available_local_models() -> List[dict]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë¡œì»¬ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    models = []
    for key, config in AVAILABLE_MODELS.items():
        exists = os.path.exists(config["path"])
        models.append({
            "key": key,
            "name": config["name"],
            "desc": config.get("desc", ""),
            "available": exists,
            "current": key == CURRENT_LOCAL_MODEL,
            "korean_support": config.get("korean_support", True)
        })
    return models


def load_api_token():
    global API_TOKEN
    paths = [
        os.path.join(BASE_DIR, "token.txt"),
        os.path.join(BASE_DIR, "api_token.txt"),
        "token.txt",
        "../token.txt",
        os.path.expanduser("~/token.txt")
    ]
    for p in paths:
        if os.path.exists(p):
            try:
                with open(p, 'r', encoding='utf-8') as f:
                    API_TOKEN = f.read().strip()
                if API_TOKEN and "REPLACE" not in API_TOKEN:
                    logger.info(f"âœ… API í† í° ë¡œë“œ: {p}")
                    return True
            except Exception as e:
                logger.error(f"âŒ í† í° ë¡œë“œ ì‹¤íŒ¨: {e}")
    logger.warning("âš ï¸ API í† í° íŒŒì¼ ì—†ìŒ")
    return False


def call_local_llm(prompt: str, system_prompt: str = "", max_tokens: int = 4096, task_type: str = "") -> dict:
    global LOCAL_LLM, CURRENT_LOCAL_MODEL, LLM_PARAMS
    if LOCAL_LLM is None:
        logger.info("âš¡ LOCAL_LLMì´ None â†’ ìë™ ì¬ë¡œë“œ ì‹œë„")
        LOCAL_LLM = load_local_model()
        if LOCAL_LLM is None:
            return {"success": False, "error": "ë¡œì»¬ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GGUF íŒŒì¼ í™•ì¸ í•„ìš”."}

    # â˜… íƒœìŠ¤í¬ë³„ íŒŒë¼ë¯¸í„° ì ìš© (task_typeì´ ìˆìœ¼ë©´ í”„ë¡œí•„ ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹)
    if task_type:
        params = get_task_params(task_type)
        temperature = params["temperature"]
        repeat_penalty = params["repeat_penalty"]
        actual_max_tokens = params["max_tokens"]
    else:
        temperature = LLM_PARAMS.get("temperature", 0.7)
        repeat_penalty = LLM_PARAMS.get("repeat_penalty", 1.1)
        actual_max_tokens = LLM_PARAMS.get("max_tokens", max_tokens)

    # â˜… ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ í˜•ì‹
    model_config = AVAILABLE_MODELS.get(CURRENT_LOCAL_MODEL, {})
    chat_format = model_config.get("chat_format", "chatml")

    if chat_format == "gemma":
        # Gemma 3: Google í˜•ì‹
        full_prompt = f"""<start_of_turn>user
{system_prompt}

{prompt}<end_of_turn>
<start_of_turn>model
"""
        stop_tokens = ["<end_of_turn>", "<start_of_turn>"]
    elif chat_format == "llama-3":
        # Llama 3.1: Meta í˜•ì‹
        full_prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>

{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        stop_tokens = ["<|eot_id|>", "<|end_of_text|>"]
    else:
        # ChatML í˜•ì‹ (Qwen3, Qwen2.5, OH-DCFT ë“± ê¸°ë³¸)
        full_prompt = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{prompt}<|im_end|>
<|im_start|>assistant
"""
        stop_tokens = ["<|im_end|>", "<|im_start|>"]

    # â˜… ì¬ì‹œë„ ë¡œì§ (ì¼ì‹œì  ì˜¤ë¥˜ ì‹œ 1íšŒ ì¬ì‹œë„)
    for attempt in range(2):
        try:
            output = LOCAL_LLM(
                full_prompt,
                max_tokens=actual_max_tokens,
                temperature=temperature,
                repeat_penalty=repeat_penalty,  # â˜… ë°˜ë³µ ì–µì œ
                stop=stop_tokens,
                echo=False
            )
            content = output["choices"][0]["text"].strip()
            content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()

            # â˜… ë¹ˆ ì‘ë‹µ ê°ì§€ â†’ 1íšŒ ì¬ìƒì„±
            if len(content) < 10 and attempt == 0:
                logger.warning(f"âš ï¸ ë¡œì»¬ LLM ë¹ˆ/ì§§ì€ ì‘ë‹µ ({len(content)}ì) â†’ ì¬ì‹œë„")
                continue

            return {"success": True, "content": content}
        except RuntimeError as e:
            error_msg = str(e)
            if "out of memory" in error_msg.lower() or "ggml" in error_msg.lower():
                logger.error(f"GGUF ë©”ëª¨ë¦¬ ë¶€ì¡±: {e}")
                return {"success": False, "error": "GPU/CPU ë©”ëª¨ë¦¬ ë¶€ì¡± - ë” ì‘ì€ ëª¨ë¸ë¡œ ì „í™˜í•˜ê±°ë‚˜ max_tokensë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”."}
            if attempt == 0:
                logger.warning(f"âš ï¸ GGUF ëŸ°íƒ€ì„ ì˜¤ë¥˜ â†’ 1íšŒ ì¬ì‹œë„: {e}")
                time.sleep(1)
                continue
            logger.error(f"GGUF ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}", exc_info=True)
            return {"success": False, "error": f"ë¡œì»¬ ëª¨ë¸ ì¶”ë¡  ì˜¤ë¥˜: {error_msg}"}
        except Exception as e:
            logger.error(f"GGUF ì¶”ë¡  ì˜¤ë¥˜: {e}", exc_info=True)
            return {"success": False, "error": f"ë¡œì»¬ ëª¨ë¸ ì˜¤ë¥˜: {type(e).__name__} - {str(e)}"}

    return {"success": False, "error": "ë¡œì»¬ ëª¨ë¸ ì¬ì‹œë„ í•œë„ ì´ˆê³¼"}


def call_api_llm(prompt: str, system_prompt: str = "", max_tokens: int = 4096, task_type: str = "") -> dict:
    global API_TOKEN
    if not API_TOKEN:
        return {"success": False, "error": "API í† í° ì—†ìŒ"}

    # â˜… íƒœìŠ¤í¬ë³„ íŒŒë¼ë¯¸í„° ì ìš©
    if task_type:
        params = get_task_params(task_type)
        api_temperature = params["temperature"]
        api_max_tokens = params["max_tokens"]
    else:
        api_temperature = 0.3
        api_max_tokens = max_tokens

    headers = {
        "Authorization": f"Bearer {API_TOKEN}",
        "Content-Type": "application/json"
    }
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    data = {
        "model": API_MODEL,
        "messages": messages,
        "max_tokens": api_max_tokens,
        "temperature": api_temperature
    }
    # â˜… ì¬ì‹œë„ ë¡œì§ (429, 503, Timeout ì‹œ ìµœëŒ€ 2íšŒ ì¬ì‹œë„)
    max_retries = 2
    api_timeout = 120  # ê¸°ë³¸ 120ì´ˆ (ì½”ë“œìƒì„±ì‹œ 200ì´ˆ)
    if task_type == "coding":
        api_timeout = 200

    for attempt in range(max_retries + 1):
        try:
            response = requests.post(API_URL, headers=headers, json=data, timeout=api_timeout)
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()

                # â˜… ë¹ˆ ì‘ë‹µ ê°ì§€ â†’ 1íšŒ ì¬ìƒì„±
                if len(content) < 10 and attempt < max_retries:
                    logger.warning(f"âš ï¸ ë¹ˆ/ì§§ì€ ì‘ë‹µ ê°ì§€ ({len(content)}ì) â†’ ì¬ì‹œë„ {attempt+1}/{max_retries}")
                    time.sleep(1)
                    continue

                # â˜… í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì 
                usage = result.get("usage", {})
                if usage:
                    TOKEN_USAGE["prompt_tokens"] += usage.get("prompt_tokens", 0)
                    TOKEN_USAGE["completion_tokens"] += usage.get("completion_tokens", 0)
                    TOKEN_USAGE["total_tokens"] += usage.get("total_tokens", 0)
                    TOKEN_USAGE["call_count"] += 1
                    logger.info(f"ğŸ“Š í† í°: +{usage.get('total_tokens', 0)} (ëˆ„ì : {TOKEN_USAGE['total_tokens']})")

                return {"success": True, "content": content}
            elif response.status_code == 401:
                return {"success": False, "error": "API ì¸ì¦ ì‹¤íŒ¨ (401) - í† í°ì´ ë§Œë£Œë˜ì—ˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}
            elif response.status_code in (429, 503):
                if attempt < max_retries:
                    wait_time = 2 ** (attempt + 1)  # 2ì´ˆ, 4ì´ˆ
                    logger.warning(f"âš ï¸ API {response.status_code} â†’ {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ({attempt+1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                if response.status_code == 429:
                    return {"success": False, "error": "API ìš”ì²­ í•œë„ ì´ˆê³¼ (429) - ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}
                return {"success": False, "error": f"LLM ì„œë²„ ì‘ë‹µ ì—†ìŒ (503) - {CURRENT_ENV} ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."}
            else:
                return {"success": False, "error": f"API ì˜¤ë¥˜ ({response.status_code}) - {response.text[:200]}"}
        except requests.exceptions.Timeout:
            if attempt < max_retries:
                logger.warning(f"âš ï¸ API íƒ€ì„ì•„ì›ƒ â†’ ì¬ì‹œë„ ({attempt+1}/{max_retries})")
                continue
            return {"success": False, "error": f"API ìš”ì²­ ì‹œê°„ ì´ˆê³¼ ({api_timeout}ì´ˆ) - ë„¤íŠ¸ì›Œí¬ë¥¼ í™•ì¸í•˜ê±°ë‚˜ ë¡œì»¬ ëª¨ë“œë¡œ ì „í™˜í•´ì£¼ì„¸ìš”."}
        except requests.exceptions.ConnectionError:
            if attempt < max_retries:
                logger.warning(f"âš ï¸ ì—°ê²° ì˜¤ë¥˜ â†’ 2ì´ˆ í›„ ì¬ì‹œë„ ({attempt+1}/{max_retries})")
                time.sleep(2)
                continue
            return {"success": False, "error": f"LLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨ - {API_URL} ì— ì ‘ì†í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        except Exception as e:
            logger.error(f"API LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            return {"success": False, "error": f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {type(e).__name__} - {str(e)}"}

    return {"success": False, "error": "API í˜¸ì¶œ ì¬ì‹œë„ í•œë„ ì´ˆê³¼"}


def call_llm(prompt: str, system_prompt: str = "", max_tokens: int = 4096, task_type: str = "") -> dict:
    if LLM_MODE == "local":
        return call_local_llm(prompt, system_prompt, max_tokens, task_type=task_type)
    else:
        return call_api_llm(prompt, system_prompt, max_tokens, task_type=task_type)


# ========================================
# Tool Functions
# ========================================
def search_files(keyword: str, path: str = "C:/", limit: int = 50) -> List[dict]:
    results = []
    logger.info(f"íŒŒì¼ ê²€ìƒ‰: '{keyword}' in '{path}'")
    try:
        for root, dirs, files in os.walk(path):
            for name in files + dirs:
                if keyword.lower() in name.lower():
                    full_path = os.path.join(root, name)
                    is_dir = os.path.isdir(full_path)
                    try:
                        size = os.path.getsize(full_path) if not is_dir else 0
                        size_str = f"{size / (1024**3):.2f}GB" if size > 1024**3 else f"{size / (1024**2):.1f}MB" if size > 1024**2 else f"{size}B"
                    except (OSError, PermissionError):
                        size_str = "?"
                    results.append({
                        "name": name, "path": full_path,
                        "type": "í´ë”" if is_dir else "íŒŒì¼", "size": size_str
                    })
                    if len(results) >= limit:
                        return results
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    return results


def search_content(keyword: str, path: str = "C:/", limit: int = 30) -> List[dict]:
    results = []
    extensions = ['.txt', '.py', '.md', '.json', '.html', '.css', '.js', '.csv', '.log']
    try:
        for root, dirs, files in os.walk(path):
            for name in files:
                ext = os.path.splitext(name)[1].lower()
                if ext in extensions:
                    full_path = os.path.join(root, name)
                    try:
                        with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read(50000)
                            if keyword.lower() in content.lower():
                                idx = content.lower().find(keyword.lower())
                                snippet = content[max(0, idx-30):min(len(content), idx+70)].replace('\n', ' ')
                                results.append({"name": name, "path": full_path, "snippet": f"...{snippet}..."})
                                if len(results) >= limit:
                                    return results
                    except (OSError, PermissionError, UnicodeDecodeError):
                        continue
    except Exception as e:
        logger.error(f"ë‚´ìš© ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    return results


def get_system_info() -> dict:
    drives = []
    for p in psutil.disk_partitions():
        try:
            usage = psutil.disk_usage(p.mountpoint)
            drives.append({"drive": p.device, "total": f"{usage.total / (1024**3):.1f}GB", "used": f"{usage.percent}%"})
        except (OSError, PermissionError):
            pass
    return {
        "os": f"{platform.system()} {platform.release()}",
        "cpu": f"{psutil.cpu_count()}ì½”ì–´, {psutil.cpu_percent()}%",
        "memory": f"{psutil.virtual_memory().total // (1024**3)}GB, {psutil.virtual_memory().percent}%",
        "drives": drives
    }


def list_directory(path: str) -> List[dict]:
    items = []
    try:
        for name in os.listdir(path)[:50]:
            full_path = os.path.join(path, name)
            is_dir = os.path.isdir(full_path)
            try:
                size = os.path.getsize(full_path) if not is_dir else 0
                modified = datetime.datetime.fromtimestamp(os.path.getmtime(full_path)).strftime("%Y-%m-%d %H:%M")
            except (OSError, PermissionError):
                size = 0
                modified = "?"
            items.append({"name": name, "type": "í´ë”" if is_dir else "íŒŒì¼", "size": f"{size:,}" if not is_dir else "-", "modified": modified})
    except Exception as e:
        return [{"error": str(e)}]
    return items


def read_file(path: str, max_chars: int = 5000) -> str:
    try:
        with open(path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read(max_chars)
            if len(content) == max_chars:
                content += "\n... (íŒŒì¼ì´ ë„ˆë¬´ ì»¤ì„œ ì¼ë¶€ë§Œ í‘œì‹œ)"
            return content
    except Exception as e:
        return f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}"


def run_program(program: str) -> str:
    try:
        subprocess.Popen(program, shell=True)
        return f"'{program}' ì‹¤í–‰ë¨"
    except Exception as e:
        return f"ì‹¤í–‰ ì˜¤ë¥˜: {e}"


def kill_program(name: str) -> str:
    try:
        killed = 0
        for proc in psutil.process_iter(['name']):
            if name.lower() in proc.info['name'].lower():
                proc.kill()
                killed += 1
        return f"{killed}ê°œ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œë¨"
    except Exception as e:
        return f"ì¢…ë£Œ ì˜¤ë¥˜: {e}"


def open_web(url: str) -> str:
    if not url.startswith('http'):
        url = 'https://' + url
    webbrowser.open(url)
    return f"'{url}' ì—´ë¦¼"


def google_search(query: str) -> str:
    url = f"https://www.google.com/search?q={query}"
    webbrowser.open(url)
    return f"'{query}' ê²€ìƒ‰ ì¤‘..."


def get_time() -> str:
    now = datetime.datetime.now()
    return f"{now.strftime('%Yë…„ %mì›” %dì¼ %A %Hì‹œ %Më¶„ %Sì´ˆ')}"


# â˜… ìŠ¤í¬ë¦°ìƒ·: ì „ìš© í´ë” ì €ì¥ + URL ë°˜í™˜
def take_screenshot() -> dict:
    """ìŠ¤í¬ë¦°ìƒ· ì°ê³  ì „ìš© í´ë”ì— ì €ì¥, ì›¹ í‘œì‹œìš© URL ë°˜í™˜"""
    global screenshot_authenticated
    
    # â˜… ë¹„ë°€ë²ˆí˜¸ ì¸ì¦ ì²´í¬
    if not screenshot_authenticated:
        return {"success": False, "auth_required": True, "error": "ğŸ”’ ìŠ¤í¬ë¦°ìƒ·ì€ ë¹„ë°€ë²ˆí˜¸ ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤."}
    
    # ì¸ì¦ í›„ 1íšŒ ì‚¬ìš© â†’ ìë™ ì ê¸ˆ
    screenshot_authenticated = False
    
    try:
        from PIL import ImageGrab
        filename = f"screenshot_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        filepath = os.path.join(SCREENSHOT_DIR, filename)
        img = ImageGrab.grab()
        img.save(filepath)
        logger.info(f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {filepath}")
        return {
            "success": True,
            "filename": filename,
            "path": filepath,
            "url": f"/assistant/screenshots/{filename}"
        }
    except ImportError:
        return {"success": False, "error": "PIL(Pillow) ë¯¸ì„¤ì¹˜. pip install Pillow"}
    except Exception as e:
        return {"success": False, "error": str(e)}


# â˜… ìµœì‹ ë‰´ìŠ¤: ë…ë¦½ ë¸Œë¼ìš°ì € ì°½ ì—´ê¸° â†’ ìŠ¤í¬ë¦°ìƒ· â†’ ê·¸ ì°½ë§Œ ë‹«ê¸°
def latest_news() -> dict:
    """êµ¬ê¸€ë‰´ìŠ¤ë¥¼ ë…ë¦½ ë¸Œë¼ìš°ì €ë¡œ ì—´ê³ , ìŠ¤í¬ë¦°ìƒ· ì°ê³ , ê·¸ ì°½ë§Œ ë‹«ê¸°"""
    global screenshot_authenticated
    
    # â˜… ë¹„ë°€ë²ˆí˜¸ ì¸ì¦ ì²´í¬
    if not screenshot_authenticated:
        return {"success": False, "auth_required": True, "error": "ğŸ”’ ë‰´ìŠ¤ ìŠ¤í¬ë¦°ìƒ·ì€ ë¹„ë°€ë²ˆí˜¸ ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤."}
    
    screenshot_authenticated = False  # 1íšŒ ì‚¬ìš© í›„ ì ê¸ˆ

    import time

    news_proc = None
    temp_profile = None
    
    try:
        news_url = "https://news.google.com/home?hl=ko&gl=KR&ceid=KR:ko"
        
        # ì„ì‹œ í”„ë¡œí•„ í´ë” (ë…ë¦½ Chrome ì¸ìŠ¤í„´ìŠ¤ìš©)
        temp_profile = os.path.join(tempfile.gettempdir(), "chrome_news_temp")
        
        # 1. Chrome ì°¾ê¸°
        chrome_paths = [
            r"C:\Program Files\Google\Chrome\Application\chrome.exe",
            r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe",
            os.path.expanduser(r"~\AppData\Local\Google\Chrome\Application\chrome.exe"),
        ]
        
        chrome_exe = None
        for p in chrome_paths:
            if os.path.exists(p):
                chrome_exe = p
                break
        
        if chrome_exe:
            # ë…ë¦½ Chrome ì¸ìŠ¤í„´ìŠ¤ (ê¸°ì¡´ Chromeê³¼ ë³„ê°œ, ì „ì²´í™”ë©´)
            news_proc = subprocess.Popen([
                chrome_exe,
                f"--user-data-dir={temp_profile}",
                "--no-first-run",
                "--no-default-browser-check",
                "--start-maximized",
                "--disable-extensions",
                "--disable-sync",
                "--disable-translate",
                news_url
            ])
            logger.info(f"ğŸ“° êµ¬ê¸€ë‰´ìŠ¤ ë…ë¦½ ì°½ ì—´ê¸° (PID: {news_proc.pid})")
        else:
            webbrowser.open(news_url)
            logger.info("ğŸ“° êµ¬ê¸€ë‰´ìŠ¤ ì—´ê¸° (ê¸°ë³¸ ë¸Œë¼ìš°ì €)")
        
        # 2. ì´ˆê¸° ë¡œë”© ëŒ€ê¸° (ì„ì‹œ í”„ë¡œí•„ ì²« ì‹¤í–‰ì€ ëŠë¦¼)
        time.sleep(3)
        
        # 2.5. ê°•ì œ ì „ì²´í™”ë©´ (ì„ì‹œ í”„ë¡œí•„ì€ ìµœëŒ€í™” ë¬´ì‹œí•  ìˆ˜ ìˆìŒ)
        try:
            import ctypes
            import ctypes.wintypes
            
            # ê°€ì¥ ì•ì— ìˆëŠ” Chrome ì°½ ì°¾ì•„ì„œ ìµœëŒ€í™”
            user32 = ctypes.windll.user32
            hwnd = user32.GetForegroundWindow()
            if hwnd:
                SW_MAXIMIZE = 3
                user32.ShowWindow(hwnd, SW_MAXIMIZE)
                logger.info(f"ğŸ”² ë‰´ìŠ¤ ì°½ ìµœëŒ€í™” ì™„ë£Œ (hwnd: {hwnd})")
        except Exception as e:
            logger.warning(f"âš ï¸ ìµœëŒ€í™” ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
        
        # 3. ë‰´ìŠ¤ í˜ì´ì§€ ì™„ì „íˆ ë¡œë”©ë  ë•Œê¹Œì§€ ì¶©ë¶„íˆ ëŒ€ê¸°
        logger.info("â³ ë‰´ìŠ¤ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... (8ì´ˆ)")
        time.sleep(8)
        
        # 3. ìŠ¤í¬ë¦°ìƒ· ì°ê¸°
        from PIL import ImageGrab
        filename = f"news_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        filepath = os.path.join(SCREENSHOT_DIR, filename)
        img = ImageGrab.grab()
        img.save(filepath)
        logger.info(f"ğŸ“¸ ë‰´ìŠ¤ ìŠ¤í¬ë¦°ìƒ·: {filepath}")
        
        # 4. ë…ë¦½ Chromeë§Œ ì¢…ë£Œ
        time.sleep(0.5)
        if news_proc and news_proc.poll() is None:
            # ìì‹ í”„ë¡œì„¸ìŠ¤ í¬í•¨ ì „ì²´ ì¢…ë£Œ
            try:
                parent = psutil.Process(news_proc.pid)
                for child in parent.children(recursive=True):
                    child.terminate()
                parent.terminate()
                logger.info(f"ğŸ”’ ë‰´ìŠ¤ ì°½ ë‹«ê¸° ì™„ë£Œ (PID: {news_proc.pid})")
            except psutil.NoSuchProcess:
                pass
        
        # 5. ì„ì‹œ í”„ë¡œí•„ ì •ë¦¬ (ë°±ê·¸ë¼ìš´ë“œ)
        try:
            if temp_profile and os.path.exists(temp_profile):
                shutil.rmtree(temp_profile, ignore_errors=True)
        except OSError:
            pass

        return {
            "success": True,
            "filename": filename,
            "path": filepath,
            "url": f"/assistant/screenshots/{filename}"
        }
    except Exception as e:
        # ì—ëŸ¬ ì‹œì—ë„ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
        if news_proc and news_proc.poll() is None:
            try:
                news_proc.terminate()
            except (OSError, psutil.NoSuchProcess):
                pass
        return {"success": False, "error": str(e)}


# â˜… í”„ë¡œì„¸ìŠ¤ ëª©ë¡ ì¡°íšŒ
def list_processes(sort_by: str = "memory", limit: int = 30) -> List[dict]:
    """ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ëª©ë¡ ë°˜í™˜"""
    processes = []
    try:
        for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_info', 'status']):
            try:
                info = proc.info
                mem = info.get('memory_info')
                mem_mb = mem.rss / (1024 * 1024) if mem else 0
                processes.append({
                    "pid": info['pid'],
                    "name": info['name'],
                    "cpu": proc.cpu_percent(interval=0),
                    "memory_mb": round(mem_mb, 1),
                    "status": info.get('status', '?')
                })
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue

        # ì •ë ¬
        if sort_by == "cpu":
            processes.sort(key=lambda x: x['cpu'], reverse=True)
        else:
            processes.sort(key=lambda x: x['memory_mb'], reverse=True)

        return processes[:limit]
    except Exception as e:
        logger.error(f"í”„ë¡œì„¸ìŠ¤ ëª©ë¡ ì˜¤ë¥˜: {e}")
        return [{"error": str(e)}]


# â˜… ì§€ì‹ë² ì´ìŠ¤ í•¨ìˆ˜ë“¤
def list_knowledge() -> List[dict]:
    """ì§€ì‹ë² ì´ìŠ¤ íŒŒì¼ ëª©ë¡"""
    files = []
    try:
        for f in sorted(os.listdir(KNOWLEDGE_DIR)):
            if f.endswith(('.md', '.txt')):
                filepath = os.path.join(KNOWLEDGE_DIR, f)
                size = os.path.getsize(filepath)
                modified = datetime.datetime.fromtimestamp(os.path.getmtime(filepath)).strftime("%Y-%m-%d %H:%M")
                files.append({"filename": f, "size": f"{size:,}B", "modified": modified})
    except Exception as e:
        logger.error(f"ì§€ì‹ ëª©ë¡ ì˜¤ë¥˜: {e}")
    return files


def normalize_keyword(keyword: str) -> List[str]:
    """í‚¤ì›Œë“œë¥¼ ì—¬ëŸ¬ ë³€í˜•ìœ¼ë¡œ í™•ì¥ (ì–¸ë”ìŠ¤ì½”ì–´, ê³µë°±, í•˜ì´í”ˆ, í•œêµ­ì–´ ì¡°ì‚¬ ë¶„ë¦¬)"""
    keyword = keyword.strip().lower()
    variants = [keyword]

    # ì–¸ë”ìŠ¤ì½”ì–´ <-> ê³µë°± <-> í•˜ì´í”ˆ ë³€í™˜
    if '_' in keyword:
        variants.append(keyword.replace('_', ' '))
        variants.append(keyword.replace('_', '-'))
        variants.append(keyword.replace('_', ''))
    if ' ' in keyword:
        variants.append(keyword.replace(' ', '_'))
        variants.append(keyword.replace(' ', '-'))
        variants.append(keyword.replace(' ', ''))
    if '-' in keyword:
        variants.append(keyword.replace('-', '_'))
        variants.append(keyword.replace('-', ' '))
        variants.append(keyword.replace('-', ''))

    # ëŒ€ì†Œë¬¸ì ë³€í˜• ì¶”ê°€
    variants.append(keyword.upper())
    variants.append(keyword.title())

    # â˜… í•œêµ­ì–´ ì¡°ì‚¬ ë¶„ë¦¬ â†’ ì›í˜• ì¶”ê°€
    stripped = _KOREAN_PARTICLE_PATTERN.sub('', keyword)
    if stripped and stripped != keyword and len(stripped) >= 2:
        variants.append(stripped)

    # â˜… í•œêµ­ì–´ í† í°ë„ ê°œë³„ ë³€í˜•ìœ¼ë¡œ ì¶”ê°€
    for token in korean_tokenize(keyword):
        if token not in variants and len(token) >= 2:
            variants.append(token)

    return list(set(variants))


def calculate_relevance_score(filename: str, content: str, keyword: str, variants: List[str]) -> int:
    """ë¬¸ì„œì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ê´€ë ¨ì„± ë†’ìŒ)"""
    score = 0
    filename_lower = filename.lower()
    content_lower = content.lower()

    for variant in variants:
        v_lower = variant.lower()

        # íŒŒì¼ëª…ì— í‚¤ì›Œë“œ í¬í•¨ (+50ì )
        if v_lower in filename_lower:
            score += 50
            # íŒŒì¼ëª…ì´ í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ë©´ ì¶”ê°€ ì ìˆ˜
            if filename_lower.startswith(v_lower):
                score += 30

        # ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ë“±ì¥ íšŸìˆ˜ (ìµœëŒ€ 100ì )
        count = content_lower.count(v_lower)
        score += min(count * 5, 100)

        # ì œëª©/í—¤ë”ì— í‚¤ì›Œë“œ ìˆìœ¼ë©´ ì¶”ê°€ ì ìˆ˜
        lines = content.split('\n')[:20]  # ìƒìœ„ 20ì¤„ë§Œ í™•ì¸
        for line in lines:
            if line.startswith('#') and v_lower in line.lower():
                score += 40
                break

    return score


def search_knowledge(keyword: str) -> List[dict]:
    """ì§€ì‹ë² ì´ìŠ¤ì—ì„œ í‚¤ì›Œë“œë¡œ íŒŒì¼ ê²€ìƒ‰ (TF-IDF + í‚¤ì›Œë“œ ë§¤ì¹­ í•˜ì´ë¸Œë¦¬ë“œ)"""
    results = []
    seen_files = set()
    variants = normalize_keyword(keyword)

    # â˜… í‚¤ì›Œë“œë¥¼ í† í°ìœ¼ë¡œ ë¶„ë¦¬ (ê³µë°±, ì–¸ë”ìŠ¤ì½”ì–´ ë“±ìœ¼ë¡œ)
    keyword_tokens = re.split(r'[\s_\-\.]+', keyword.strip().lower())
    keyword_tokens = [t for t in keyword_tokens if len(t) > 1]

    logger.info(f"ğŸ” ì§€ì‹ê²€ìƒ‰: '{keyword}' â†’ ë³€í˜•: {variants[:5]}, í† í°: {keyword_tokens}")

    # ========================================
    # 1ì°¨: TF-IDF ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰ (ì˜ë¯¸ì  ìœ ì‚¬ì„±)
    # ========================================
    try:
        tfidf_results = tfidf_search(keyword, top_k=5)
        for r in tfidf_results:
            seen_files.add(r["filename"])
            results.append(r)
        if tfidf_results:
            logger.info(f"ğŸ“Š TF-IDF ê²°ê³¼: {len(tfidf_results)}ê°œ (ìƒìœ„: {[r['filename'] for r in tfidf_results[:3]]})")
    except Exception as e:
        logger.warning(f"TF-IDF ê²€ìƒ‰ ì‹¤íŒ¨ (í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ëŒ€ì²´): {e}")

    # ========================================
    # 2ì°¨: í‚¤ì›Œë“œ ë§¤ì¹­ (TF-IDFì—ì„œ ëª» ì°¾ì€ íŒŒì¼ ë³´ì™„)
    # ========================================
    try:
        for f in os.listdir(KNOWLEDGE_DIR):
            if not f.endswith(('.md', '.txt')) or f in seen_files:
                continue
            filepath = os.path.join(KNOWLEDGE_DIR, f)

            try:
                with open(filepath, 'r', encoding='utf-8', errors='ignore') as fh:
                    content = fh.read()
            except (OSError, PermissionError):
                continue

            matched = False
            snippet = ""
            for variant in variants:
                v_lower = variant.lower()
                if v_lower in f.lower() or v_lower in content.lower():
                    matched = True
                    idx = content.lower().find(v_lower)
                    if idx >= 0:
                        snippet = content[max(0, idx-50):min(len(content), idx+100)].replace('\n', ' ').strip()
                    break

            if not matched and keyword_tokens:
                f_lower = f.lower()
                content_lower = content.lower()
                token_matches = sum(1 for t in keyword_tokens if t in f_lower or t in content_lower)
                if token_matches >= max(1, len(keyword_tokens) * 0.5):
                    matched = True
                    for token in keyword_tokens:
                        idx = content_lower.find(token)
                        if idx >= 0:
                            snippet = content[max(0, idx-50):min(len(content), idx+100)].replace('\n', ' ').strip()
                            break

            if matched:
                score = calculate_relevance_score(f, content, keyword, variants)
                if keyword_tokens:
                    for token in keyword_tokens:
                        if token in f.lower():
                            score += 30
                results.append({
                    "filename": f,
                    "snippet": f"...{snippet}..." if snippet else "(í‚¤ì›Œë“œ ë§¤ì¹­)",
                    "score": score,
                    "method": "keyword"
                })

        results.sort(key=lambda x: x.get("score", 0), reverse=True)
        logger.info(f"ğŸ“Š ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ (ìƒìœ„: {[r['filename'] for r in results[:3]]})")

    except Exception as e:
        logger.error(f"ì§€ì‹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    return results


def generate_guided_questions(user_query: str) -> dict:
    """ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ LLMì´ íŒŒì¼ ëª©ë¡ì„ ë³´ê³  ì—­ì§ˆë¬¸ì„ ìƒì„±"""
    try:
        # 1. í˜„ì¬ ì§€ì‹ë² ì´ìŠ¤ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        kb_files = []
        for f in os.listdir(KNOWLEDGE_DIR):
            if f.endswith(('.md', '.txt')):
                filepath = os.path.join(KNOWLEDGE_DIR, f)
                try:
                    with open(filepath, 'r', encoding='utf-8', errors='ignore') as fh:
                        # ì²« 500ìë§Œ ì½ì–´ì„œ íŒíŠ¸ ì¶”ì¶œ
                        preview = fh.read(500)
                        # í—¤ë”/ì œëª© ì¶”ì¶œ
                        headers = [line.strip('# ').strip() for line in preview.split('\n')[:10] 
                                   if line.startswith('#')]
                    kb_files.append({
                        "filename": f,
                        "headers": headers[:3]
                    })
                except (OSError, UnicodeDecodeError):
                    kb_files.append({"filename": f, "headers": []})

        if not kb_files:
            return {
                "success": False,
                "message": "ì§€ì‹ë² ì´ìŠ¤ì— ë“±ë¡ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.",
                "suggestions": []
            }

        # 2. íŒŒì¼ ëª©ë¡ ë¬¸ìì—´ ìƒì„±
        file_list_str = "\n".join([
            f"- {f['filename']}" + (f" (ì£¼ìš” ë‚´ìš©: {', '.join(f['headers'])})" if f['headers'] else "")
            for f in kb_files
        ])

        # 3. LLMì—ê²Œ ì—­ì§ˆë¬¸ ìƒì„± ìš”ì²­
        guide_prompt = f"""ì‚¬ìš©ìê°€ "{user_query}"ë¼ê³  ì§ˆë¬¸í–ˆì§€ë§Œ, ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ì •í™•íˆ ë§¤ì¹­ë˜ëŠ” ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

í˜„ì¬ ì§€ì‹ë² ì´ìŠ¤ì— ë“±ë¡ëœ íŒŒì¼ ëª©ë¡:
{file_list_str}

ìœ„ íŒŒì¼ ëª©ë¡ì„ ë¶„ì„í•´ì„œ, ì‚¬ìš©ìì˜ ì˜ë„ì— ë§ëŠ” **êµ¬ì²´ì ì¸ ì¶”ì²œ ì§ˆë¬¸ 3~5ê°œ**ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

[ê·œì¹™]
1. íŒŒì¼ëª…ì—ì„œ í”„ë¡œì íŠ¸ëª…, ë²„ì „, í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì„œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë§Œë“œì„¸ìš”.
2. ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„± ë†’ì€ íŒŒì¼ì„ ìš°ì„  ì¶”ì²œí•˜ì„¸ìš”.
3. ê´€ë ¨ íŒŒì¼ì´ ì—†ìœ¼ë©´, ê°€ì¥ ìœ ì‚¬í•œ íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì„ ë§Œë“œì„¸ìš”.
4. ê° ì§ˆë¬¸ì€ ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ ê°€ëŠ¥í•œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

[ì¶œë ¥ í˜•ì‹ - ë°˜ë“œì‹œ ì´ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥]
{{"guide_message": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜¹ì‹œ ì´ëŸ° ë‚´ìš©ì„ ì°¾ìœ¼ì‹œë‚˜ìš”?", "suggestions": ["ì§ˆë¬¸1", "ì§ˆë¬¸2", "ì§ˆë¬¸3"]}}"""

        guide_system = """ë‹¹ì‹ ì€ ì§ˆë¬¸ ìœ ë„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ëª¨í˜¸í•œ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , ì§€ì‹ë² ì´ìŠ¤ íŒŒì¼ ëª©ë¡ì„ ì°¸ê³ í•˜ì—¬ ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ì¶”ì²œí•©ë‹ˆë‹¤.
ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”."""

        result = call_llm(guide_prompt, guide_system, max_tokens=1000)

        if result["success"]:
            content = result["content"].strip()
            # JSON ì¶”ì¶œ ì‹œë„
            try:
                # ```json ``` ë¸”ë¡ ì œê±°
                content = re.sub(r'```(?:json)?\s*', '', content)
                content = content.strip('`').strip()
                # JSON íŒŒì‹±
                guide_data = json.loads(content)
                return {
                    "success": True,
                    "message": guide_data.get("guide_message", "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."),
                    "suggestions": guide_data.get("suggestions", []),
                    "kb_files": [f["filename"] for f in kb_files]
                }
            except json.JSONDecodeError:
                logger.warning(f"âš ï¸ ì—­ì§ˆë¬¸ JSON íŒŒì‹± ì‹¤íŒ¨: {content[:200]}")
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ íŒŒì¼ ëª©ë¡ ê¸°ë°˜ ê¸°ë³¸ ì¶”ì²œ
                pass

        # 4. LLM ì‹¤íŒ¨ ì‹œ íŒŒì¼ëª… ê¸°ë°˜ ê¸°ë³¸ ì¶”ì²œ ìƒì„±
        suggestions = []
        for f in kb_files[:5]:
            fname = f["filename"].replace('.md', '').replace('.txt', '')
            # íŒŒì¼ëª…ì—ì„œ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ
            parts = re.split(r'[_\-\.]', fname)
            clean_name = ' '.join([p for p in parts if len(p) > 1])
            if clean_name:
                suggestions.append(f"{clean_name} ì•Œë ¤ì¤˜")

        return {
            "success": True,
            "message": f"'{user_query}'ì— ëŒ€í•œ ì •í™•í•œ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì¤‘ ì°¾ìœ¼ì‹œëŠ” ë‚´ìš©ì´ ìˆë‚˜ìš”?",
            "suggestions": suggestions,
            "kb_files": [f["filename"] for f in kb_files]
        }

    except Exception as e:
        logger.error(f"ì—­ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return {"success": False, "message": "ì—­ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨", "suggestions": []}


def read_knowledge(filename: str) -> str:
    """ì§€ì‹ë² ì´ìŠ¤ MD íŒŒì¼ ì½ê¸°"""
    filepath = os.path.join(KNOWLEDGE_DIR, filename)
    if not os.path.exists(filepath):
        for f in os.listdir(KNOWLEDGE_DIR):
            if filename.lower() in f.lower():
                filepath = os.path.join(KNOWLEDGE_DIR, f)
                break
        else:
            return f"âŒ '{filename}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read(30000)
            if len(content) == 30000:
                content += "\n\n... (ë¬¸ì„œê°€ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ í‘œì‹œ)"
            return content
    except Exception as e:
        return f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}"


def analyze_data(path: str) -> str:
    try:
        ext = os.path.splitext(path)[1].lower()
        if ext == '.csv':
            df = pd.read_csv(path, encoding='utf-8', errors='ignore')
        elif ext in ['.xlsx', '.xls']:
            df = pd.read_excel(path)
        else:
            return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {ext}"
        result = []
        result.append(f"íŒŒì¼: {os.path.basename(path)}")
        result.append(f"í¬ê¸°: {len(df):,}í–‰ x {len(df.columns)}ì—´")
        result.append(f"ì»¬ëŸ¼: {', '.join(df.columns.tolist()[:20])}")
        numeric_cols = df.select_dtypes(include=['number']).columns
        if len(numeric_cols) > 0:
            stats = df[numeric_cols].describe().to_string()
            result.append(f"í†µê³„:\n{stats}")
        result.append(f"ìƒ˜í”Œ:\n{df.head(5).to_string()}")
        return "\n".join(result)
    except Exception as e:
        return f"ë¶„ì„ ì˜¤ë¥˜: {e}"


# ========================================
# â˜… ìë™ ë¦¬ì†ŒìŠ¤ ì²¨ë¶€ (í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œ ê´€ë ¨ íŒŒì¼ ë§í¬ ìë™ ì¶”ê°€)
# ========================================
AUTO_RESOURCES = [
    {
        "keywords": ["amhs", "amos", "oht", "mcs", "stk", "cnv", "lft", "inv",
                     "foup", "pdt", "rtc", "fio", "ë°˜ì†¡", "ìŠ¤í† ì»¤", "ì»¨ë² ì´ì–´",
                     "ë¦¬í”„íŠ¸", "ì¸ë²„í„°", "ë¬¼ë¥˜", "ë°˜ì†¡ì°¨ëŸ‰", "êµ¬ì„±ë„", "ì‹œìŠ¤í…œ êµ¬ì„±"],
        "filename": "Amhs_ì‹œìŠ¤í…œêµ¬ì„±ë„.html",
        "label": "ğŸ“Š AMHS ì‹œìŠ¤í…œ êµ¬ì„±ë„",
        "desc": "ì¸í„°ë™í‹°ë¸Œ êµ¬ì„±ë„ (í´ë¦­í•˜ë©´ ìƒˆ íƒ­ì—ì„œ ì—´ë¦¼)"
    }
]


def auto_attach_resources(user_message: str, response: str) -> str:
    """ì‚¬ìš©ì ì§ˆë¬¸ì— ê´€ë ¨ ë¦¬ì†ŒìŠ¤ê°€ ìˆìœ¼ë©´ ì‘ë‹µ ëì— ë§í¬ ìë™ ì¶”ê°€"""
    msg_lower = user_message.lower()
    attached = []

    for res in AUTO_RESOURCES:
        # í‚¤ì›Œë“œ ë§¤ì¹­ (2ê°œ ì´ìƒ ë§¤ì¹­ë˜ê±°ë‚˜, í•µì‹¬ í‚¤ì›Œë“œ 1ê°œ ë§¤ì¹­)
        matched = [kw for kw in res["keywords"] if kw in msg_lower]
        core_keywords = ["amhs", "amos", "êµ¬ì„±ë„", "ì‹œìŠ¤í…œ êµ¬ì„±"]
        core_match = any(kw in msg_lower for kw in core_keywords)

        if len(matched) >= 1 or core_match:
            filepath = os.path.join(RESOURCES_DIR, res["filename"])
            if os.path.exists(filepath):
                attached.append(res)

    if attached:
        links = []
        for res in attached:
            url = f"/assistant/resources/{res['filename']}"
            links.append(f"\n\n---\nğŸ”— **[{res['label']}]({url})** - {res['desc']}")
        response += "".join(links)

    return response


# ========================================
# Tool ì‹¤í–‰ê¸°
# ========================================
def execute_tool(tool_data: dict) -> str:
    tool_name = tool_data.get("tool")

    if tool_name == "search_files":
        results = search_files(tool_data.get("keyword", ""), tool_data.get("path", "C:/"))
        return json.dumps(results[:20], ensure_ascii=False, indent=2)

    elif tool_name == "search_content":
        results = search_content(tool_data.get("keyword", ""), tool_data.get("path", "C:/"))
        return json.dumps(results[:10], ensure_ascii=False, indent=2)

    elif tool_name == "get_system_info":
        return json.dumps(get_system_info(), ensure_ascii=False, indent=2)

    elif tool_name == "list_directory":
        results = list_directory(tool_data.get("path", "C:/"))
        return json.dumps(results, ensure_ascii=False, indent=2)

    elif tool_name == "read_file":
        return read_file(tool_data.get("path", ""))

    elif tool_name == "run_program":
        return run_program(tool_data.get("program", ""))

    elif tool_name == "kill_program":
        return kill_program(tool_data.get("name", ""))

    elif tool_name == "open_web":
        return open_web(tool_data.get("url", ""))

    elif tool_name == "google_search":
        return google_search(tool_data.get("query", ""))

    elif tool_name == "get_time":
        return get_time()

    # â˜… ìŠ¤í¬ë¦°ìƒ· - JSON ë°˜í™˜
    elif tool_name == "screenshot":
        result = take_screenshot()
        return json.dumps(result, ensure_ascii=False)

    # â˜… ìµœì‹ ë‰´ìŠ¤ - êµ¬ê¸€ë‰´ìŠ¤ ì—´ê³  ìŠ¤í¬ë¦°ìƒ· ì°ê³  ë‹«ê¸°
    elif tool_name == "latest_news":
        result = latest_news()
        return json.dumps(result, ensure_ascii=False)

    elif tool_name == "analyze_data":
        return analyze_data(tool_data.get("path", ""))

    # â˜… ì§€ì‹ë² ì´ìŠ¤ ë„êµ¬ë“¤
    elif tool_name == "list_knowledge":
        results = list_knowledge()
        return json.dumps(results, ensure_ascii=False, indent=2)

    elif tool_name == "search_knowledge":
        results = search_knowledge(tool_data.get("keyword", ""))
        return json.dumps(results, ensure_ascii=False, indent=2)

    elif tool_name == "read_knowledge":
        return read_knowledge(tool_data.get("filename", ""))

    # â˜… í”„ë¡œì„¸ìŠ¤ ëª©ë¡
    elif tool_name == "list_processes":
        results = list_processes(tool_data.get("sort_by", "memory"), tool_data.get("limit", 30))
        return json.dumps(results, ensure_ascii=False, indent=2)

    return "ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬"


# ========================================
# JSON ê°ì§€
# ========================================
def extract_tool_json(text: str) -> Optional[dict]:
    # íŒ¨í„´ 1: ```json ì½”ë“œë¸”ë¡
    match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', text, re.DOTALL)
    if match:
        try:
            data = json.loads(match.group(1))
            if "tool" in data:
                return data
        except json.JSONDecodeError:
            pass

    # íŒ¨í„´ 2: ì¸ë¼ì¸ {"tool": "..."}
    match = re.search(r'(\{[^{}]*"tool"\s*:\s*"[^"]+?"[^{}]*\})', text, re.DOTALL)
    if match:
        try:
            data = json.loads(match.group(1))
            if "tool" in data:
                return data
        except json.JSONDecodeError:
            pass

    # íŒ¨í„´ 3: ì „ì²´ê°€ JSON
    stripped = text.strip()
    if stripped.startswith('{') and stripped.endswith('}'):
        try:
            data = json.loads(stripped)
            if "tool" in data:
                return data
        except json.JSONDecodeError:
            pass

    # íŒ¨í„´ 4: ë©€í‹°ë¼ì¸ JSON
    match = re.search(r'\{\s*"tool"\s*:.*?\}', text, re.DOTALL)
    if match:
        try:
            json_str = re.sub(r'[\n\r\t]', ' ', match.group(0))
            json_str = re.sub(r'\s+', ' ', json_str)
            data = json.loads(json_str)
            if "tool" in data:
                return data
        except json.JSONDecodeError:
            pass

    return None


# ========================================
# ========================================
# Chat Processing
# ========================================
# â˜… ëŒ€í™” ìš”ì•½ ìºì‹œ
_conversation_summary_cache = {
    "summary": "",
    "summarized_up_to": 0,  # ìš”ì•½ëœ ë§ˆì§€ë§‰ ì¸ë±ìŠ¤
}

def get_recent_context(max_turns: int = 4) -> str:
    """ìµœê·¼ ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜ (ë§¥ë½ ìœ ì§€ìš©) - ë™ì  ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬"""
    if not CHAT_HISTORY:
        return ""

    # â˜… ë™ì  ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
    if LLM_MODE == "local":
        model_ctx = AVAILABLE_MODELS.get(CURRENT_LOCAL_MODEL, {}).get("ctx", 4096)
        max_context_chars = int(model_ctx * 1.5)  # ì•½ 40% í• ë‹¹ (1í† í° â‰ˆ 3.5ì í•œê¸€)
    else:
        max_context_chars = 6000  # API ëª¨ë“œ: ë„‰ë„‰íˆ

    context_parts = []
    total_chars = 0

    # â˜… 8í„´ ì´ˆê³¼ì‹œ ì´ì „ ëŒ€í™” ìš”ì•½ í™œìš©
    history_len = len(CHAT_HISTORY)
    if history_len > 16:  # 8í„´ = 16ë©”ì‹œì§€ (user+assistant)
        summary = _get_or_create_summary()
        if summary:
            summary_text = f"[ì´ì „ ëŒ€í™” ìš”ì•½]\n{summary}\n"
            context_parts.append(summary_text)
            total_chars += len(summary_text)

    # â˜… ìµœê·¼ ëŒ€í™”: ìµœê·¼ 2í„´ì€ ì „ë¬¸, ê·¸ ì´ì „ì€ 1000ìê¹Œì§€
    recent = CHAT_HISTORY[-(max_turns * 2):]
    for i, msg in enumerate(recent):
        role = "ì‚¬ìš©ì" if msg["role"] == "user" else "ë¹„ì„œ"
        is_recent_2_turns = i >= len(recent) - 4  # ë§ˆì§€ë§‰ 4ê°œ (2í„´)
        char_limit = 2000 if is_recent_2_turns else 1000
        content = msg["content"][:char_limit]
        line = f"[{role}]: {content}"

        if total_chars + len(line) > max_context_chars:
            break
        context_parts.append(line)
        total_chars += len(line)

    return "\n".join(context_parts)


def _get_or_create_summary() -> str:
    """ëŒ€í™” ìš”ì•½ì„ ìƒì„±í•˜ê±°ë‚˜ ìºì‹œì—ì„œ ë°˜í™˜"""
    global _conversation_summary_cache
    history_len = len(CHAT_HISTORY)

    # ì´ë¯¸ ìµœì‹  ìš”ì•½ì´ ìˆìœ¼ë©´ ìºì‹œ ë°˜í™˜
    if (_conversation_summary_cache["summary"] and
        _conversation_summary_cache["summarized_up_to"] >= history_len - 20):
        return _conversation_summary_cache["summary"]

    # ìš”ì•½í•  ëŒ€í™” ë²”ìœ„: ì²˜ìŒë¶€í„° ìµœê·¼ 8í„´ ì´ì „ê¹Œì§€
    end_idx = max(0, history_len - 16)
    if end_idx < 4:  # ìš”ì•½í•  ê²Œ ë³„ë¡œ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        return ""

    messages_to_summarize = CHAT_HISTORY[:end_idx]
    # ìµœëŒ€ 20ê°œ ë©”ì‹œì§€ë§Œ ìš”ì•½ ëŒ€ìƒ
    if len(messages_to_summarize) > 20:
        messages_to_summarize = messages_to_summarize[-20:]

    summary_input = "\n".join([
        f"{'ì‚¬ìš©ì' if m['role'] == 'user' else 'ë¹„ì„œ'}: {m['content'][:300]}"
        for m in messages_to_summarize
    ])

    summary_prompt = f"""ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ 3ì¤„ ì´ë‚´ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•˜ì„¸ìš”.
ì£¼ìš” ì£¼ì œ, ê²°ì •ì‚¬í•­, ì¤‘ìš” ì •ë³´ë§Œ í¬í•¨í•˜ì„¸ìš”.

{summary_input}

ìš”ì•½:"""

    result = call_llm(summary_prompt, "ëŒ€í™” ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. 3ì¤„ ì´ë‚´ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•˜ì„¸ìš”.", max_tokens=300, task_type="general_chat")
    if result["success"] and len(result["content"]) > 10:
        _conversation_summary_cache["summary"] = result["content"].strip()
        _conversation_summary_cache["summarized_up_to"] = end_idx
        logger.info(f"ğŸ“ ëŒ€í™” ìš”ì•½ ìƒì„± ì™„ë£Œ ({end_idx}ê°œ ë©”ì‹œì§€ â†’ {len(result['content'])}ì)")
        return _conversation_summary_cache["summary"]

    return ""


def process_chat(user_message: str) -> str:
    global LOCAL_LLM, LLM_MODE

    if LLM_MODE == "local" and LOCAL_LLM is None:
        return "âŒ ë¡œì»¬ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    if LLM_MODE != "local" and not API_TOKEN:
        return "âŒ API í† í°ì´ ì—†ìŠµë‹ˆë‹¤."

    try:
        # â˜… íƒœìŠ¤í¬ ìœ í˜• ë¶„ë¥˜ (íŒŒë¼ë¯¸í„° ìµœì í™”ìš©)
        task_type = classify_task_type(user_message)
        logger.info(f"ğŸ“Š íƒœìŠ¤í¬ ìœ í˜•: {task_type} | ë©”ì‹œì§€: {user_message[:50]}")

        # â˜… AMHS ê´€ë ¨ í‚¤ì›Œë“œ â†’ LLM ìš°íšŒ, ì§ì ‘ ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰
        amhs_keywords = ["amhs", "amos", "êµ¬ì„±ë„", "ì‹œìŠ¤í…œ êµ¬ì„±", "oht", "mcs", "stk", "cnv", "lft", "inv",
                         "foup", "pdt", "rtc", "fio", "ë°˜ì†¡", "ìŠ¤í† ì»¤", "ì»¨ë² ì´ì–´", "ë¦¬í”„íŠ¸", "ì¸ë²„í„°"]
        msg_lower = user_message.lower()
        amhs_matched = [kw for kw in amhs_keywords if kw in msg_lower]
        if amhs_matched:
            logger.info(f"ğŸ”€ AMHS í‚¤ì›Œë“œ ê°ì§€ ({amhs_matched}) â†’ ì§€ì‹ë² ì´ìŠ¤ ê°•ì œ ê²€ìƒ‰")
            search_result = execute_tool({"tool": "search_knowledge", "keyword": amhs_matched[0]})
            if search_result and not search_result.startswith("âŒ"):
                try:
                    sr_data = json.loads(search_result)
                    if sr_data.get("results"):
                        best_file = sr_data["results"][0]["filename"]
                        doc_content = execute_tool({"tool": "read_knowledge", "filename": best_file})
                        if doc_content and not doc_content.startswith("âŒ"):
                            doc_limit = 12000 if LLM_MODE == "api" else 3000
                            doc_content = doc_content if len(doc_content) <= doc_limit else doc_content[:doc_limit] + "\n\n... (ì´í•˜ ìƒëµ)"
                            follow_up = f"[ì‚¬ìš©ì ì§ˆë¬¸]\n{user_message}\n\n[ì°¸ê³  ë¬¸ì„œ]\n{doc_content}\n\nìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”."
                            result2 = call_llm(follow_up, "ë‹¹ì‹ ì€ AMHS(ìë™ë¬¼ë¥˜ì‹œìŠ¤í…œ) ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. JSONì´ë‚˜ ë„êµ¬ í˜¸ì¶œì€ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”.", max_tokens=4096 if LLM_MODE == "api" else 1024, task_type="knowledge_qa")
                            if result2["success"]:
                                return result2["content"].strip()
                except (json.JSONDecodeError, KeyError, IndexError):
                    pass
            # ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ LLM íë¦„ìœ¼ë¡œ fallback

        # â˜… ëŒ€í™” ë§¥ë½ ì¶”ê°€
        recent_context = get_recent_context(max_turns=3)
        if recent_context:
            context_prompt = f"""[ì´ì „ ëŒ€í™”]
{recent_context}

[í˜„ì¬ ì§ˆë¬¸]
{user_message}"""
        else:
            context_prompt = user_message

        result = call_llm(context_prompt, get_effective_system_prompt(), task_type=task_type)
        if not result["success"]:
            return f"âŒ LLM ì˜¤ë¥˜: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"

        text = result["content"]
        logger.info(f"ğŸ“ LLM ì‘ë‹µ: {text[:200]}")

        tool_data = extract_tool_json(text)

        # â˜… JSON íŒŒì‹± ì‹¤íŒ¨í–ˆì§€ë§Œ tool í˜¸ì¶œ ì˜ë„ê°€ ë³´ì´ëŠ” ê²½ìš° â†’ 1íšŒ ì¬ìš”ì²­
        if tool_data is None and '"tool"' in text and task_type == "tool_call":
            logger.warning("âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ â†’ ì¬ìš”ì²­")
            retry_prompt = f"{context_prompt}\n\n[ì£¼ì˜] ì´ì „ ì‘ë‹µì—ì„œ JSON í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì˜ˆ: {{\"tool\": \"get_system_info\"}}"
            retry_result = call_llm(retry_prompt, get_effective_system_prompt(), task_type="tool_call")
            if retry_result["success"]:
                retry_data = extract_tool_json(retry_result["content"])
                if retry_data:
                    tool_data = retry_data
                    text = retry_result["content"]
                    logger.info(f"âœ… JSON ì¬íŒŒì‹± ì„±ê³µ: {tool_data}")

        if tool_data:
            try:
                if "keyword" in tool_data:
                    kw = tool_data["keyword"].replace("*", "").replace(".", "").strip()
                    if not kw:
                        return "âŒ ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
                    tool_data["keyword"] = kw

                logger.info(f"ğŸ”§ ë„êµ¬ ì‹¤í–‰: {tool_data}")
                tool_result = execute_tool(tool_data)
                logger.info(f"ğŸ“Š ë„êµ¬ ê²°ê³¼: {tool_result[:300]}")

                tool_name = tool_data.get("tool")

                # â˜… ìŠ¤í¬ë¦°ìƒ·: ì§ì ‘ í¬ë§·íŒ… (LLM 2ì°¨ í˜¸ì¶œ ë¶ˆí•„ìš”)
                if tool_name == "screenshot":
                    try:
                        sc_data = json.loads(tool_result)
                        if sc_data.get("auth_required"):
                            return "ğŸ”’ **ìŠ¤í¬ë¦°ìƒ· ì¸ì¦ í•„ìš”**\n\në¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì•¼ ìŠ¤í¬ë¦°ìƒ·ì„ ì°ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n<!--AUTH_REQUIRED:screenshot-->"
                        if sc_data.get("success"):
                            return f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ·ì„ ì°ì—ˆìŠµë‹ˆë‹¤!\n\n![ìŠ¤í¬ë¦°ìƒ·]({sc_data['url']})\n\nì €ì¥ ìœ„ì¹˜: `{sc_data['path']}`"
                        else:
                            return f"âŒ ìŠ¤í¬ë¦°ìƒ· ì‹¤íŒ¨: {sc_data.get('error', '?')}"
                    except (json.JSONDecodeError, KeyError) as e:
                        logger.error(f"ìŠ¤í¬ë¦°ìƒ· ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        return f"âŒ ìŠ¤í¬ë¦°ìƒ· ì²˜ë¦¬ ì˜¤ë¥˜: {e}"

                # â˜… ìµœì‹ ë‰´ìŠ¤: ì§ì ‘ í¬ë§·íŒ…
                if tool_name == "latest_news":
                    try:
                        news_data = json.loads(tool_result)
                        if news_data.get("auth_required"):
                            return "ğŸ”’ **ë‰´ìŠ¤ ìŠ¤í¬ë¦°ìƒ· ì¸ì¦ í•„ìš”**\n\në¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì•¼ ë‰´ìŠ¤ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n<!--AUTH_REQUIRED:news-->"
                        if news_data.get("success"):
                            return f"ğŸ“° **ìµœì‹  ë‰´ìŠ¤** (êµ¬ê¸€ë‰´ìŠ¤)\n\n![ë‰´ìŠ¤]({news_data['url']})\n\në¸Œë¼ìš°ì €ë¥¼ ë‹«ì•˜ìŠµë‹ˆë‹¤."
                        else:
                            return f"âŒ ë‰´ìŠ¤ í™•ì¸ ì‹¤íŒ¨: {news_data.get('error', '?')}"
                    except (json.JSONDecodeError, KeyError) as e:
                        logger.error(f"ë‰´ìŠ¤ ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        return f"âŒ ë‰´ìŠ¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}"

                # ========================================
                # â˜… ì§€ì‹ë² ì´ìŠ¤ í•¸ë“¤ëŸ¬ (3ê°€ì§€ êµ¬ì¡°)
                # ========================================

                # 1) read_knowledge â†’ "ì´ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í•´ë¼" + ê¸°ìˆ  ë¬¸ì„œ ì „ë¬¸ê°€
                if tool_name == "read_knowledge":
                    if tool_result.startswith("âŒ"):
                        return tool_result
                    
                    # ë¬¸ì„œ ê¸¸ì´ ì œí•œ (API vs GGUF)
                    doc_limit = 12000 if LLM_MODE == "api" else 3000
                    doc_content = tool_result if len(tool_result) <= doc_limit else tool_result[:doc_limit] + "\n\n... (ì´í•˜ ìƒëµ)"
                    
                    follow_up_prompt = f"""[ì‚¬ìš©ì ì§ˆë¬¸]
{user_message}

[ì°¸ê³  ë¬¸ì„œ]
{doc_content}

ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ë‹µë³€í•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”."""

                    result2_tokens = 4096 if LLM_MODE == "api" else 1024
                    result2 = call_llm(follow_up_prompt, KNOWLEDGE_QA_SYSTEM_PROMPT, max_tokens=result2_tokens, task_type="knowledge_qa")
                    if result2["success"]:
                        content = result2["content"].strip()
                        logger.info(f"ğŸ“ ì§€ì‹ì½ê¸° 2ì°¨ ì‘ë‹µ: {content[:200] if content else '(ë¹ˆ ì‘ë‹µ)'}")
                        if content and not extract_tool_json(content):
                            return content
                    # fallback: ë¬¸ì„œ ë‚´ìš© ì§ì ‘ ë°˜í™˜
                    logger.info("âš ï¸ 2ì°¨ LLM ì‘ë‹µ ì—†ìŒ â†’ ë¬¸ì„œ ì§ì ‘ ë°˜í™˜")
                    return f"ğŸ“„ **ë¬¸ì„œ ë‚´ìš©:**\n\n{doc_content[:5000]}"

                # 2) search_knowledge â†’ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë§Œ ì‚¬ìš©
                if tool_name == "search_knowledge":
                    try:
                        search_results = json.loads(tool_result)
                        if not search_results:
                            # â˜… ì—­ì§ˆë¬¸ ìœ ë„: LLMì´ íŒŒì¼ ëª©ë¡ ë³´ê³  ì¶”ì²œ ì§ˆë¬¸ ìƒì„±
                            guide = generate_guided_questions(user_message)
                            if guide["success"] and guide["suggestions"]:
                                lines = [f"ğŸ” **{guide['message']}**\n"]
                                for i, suggestion in enumerate(guide["suggestions"], 1):
                                    # <!--SUGGEST:ì§ˆë¬¸--> ë§ˆì»¤ë¡œ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í´ë¦­ ë²„íŠ¼ ìƒì„±
                                    lines.append(f"<!--SUGGEST:{suggestion}-->")
                                lines.append(f"\n\nğŸ’¡ ìœ„ ì¶”ì²œ ì§ˆë¬¸ì„ í´ë¦­í•˜ê±°ë‚˜, ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
                                if guide.get("kb_files"):
                                    lines.append(f"\nğŸ“š í˜„ì¬ ë“±ë¡ëœ ë¬¸ì„œ: {', '.join(guide['kb_files'][:5])}")
                                return "\n".join(lines)
                            else:
                                return "ğŸ” ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§€ì‹ë² ì´ìŠ¤ì— ë¬¸ì„œë¥¼ ë¨¼ì € ë“±ë¡í•´ì£¼ì„¸ìš”."

                        # â˜… ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ ì„ íƒ (ìƒìœ„ 3ê°œê¹Œì§€)
                        MAX_TOTAL_LENGTH = 15000 if LLM_MODE == "api" else 3000
                        MAX_DOCS = 3  # â˜… ìƒìœ„ 3ê°œ ë¬¸ì„œê¹Œì§€ë§Œ
                        merged_docs = []
                        total_length = 0
                        doc_names = []

                        top_score = search_results[0].get("score", 100)

                        for i, result in enumerate(search_results[:MAX_DOCS]):
                            filename = result["filename"]
                            score = result.get("score", 0)

                            # 1ìœ„ ë¬¸ì„œì™€ ì ìˆ˜ ì°¨ì´ê°€ 50% ì´ìƒì´ë©´ ì œì™¸
                            if i > 0 and score < top_score * 0.5:
                                logger.info(f"â­ï¸ ì ìˆ˜ ë‚®ì•„ ì œì™¸: {filename} (ì ìˆ˜: {score}, 1ìœ„: {top_score})")
                                break

                            doc_content = read_knowledge(filename)

                            if doc_content.startswith("âŒ"):
                                continue

                            # ë‚¨ì€ ê³µê°„ì— ë§ê²Œ ìë¥´ê¸°
                            remaining = MAX_TOTAL_LENGTH - total_length
                            if remaining <= 1000:
                                break

                            if len(doc_content) > remaining:
                                doc_content = doc_content[:remaining] + "\n\n... (ë¬¸ì„œ ì¼ë¶€ ìƒëµ)"

                            merged_docs.append(f"ğŸ“„ **[{filename}]**\n{doc_content}")
                            doc_names.append(filename)
                            total_length += len(doc_content)

                        if not merged_docs:
                            return "ğŸ” ë¬¸ì„œë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                        combined_content = "\n\n---\n\n".join(merged_docs)
                        doc_list = ", ".join(doc_names)
                        logger.info(f"ğŸ“š ì°¸ì¡° ë¬¸ì„œ: {doc_list} (ì´ {total_length}ì)")

                        follow_up_prompt = f"""[ì‚¬ìš©ì ì§ˆë¬¸]
{user_message}

[ì°¸ê³  ë¬¸ì„œ {len(doc_names)}ê°œ: {doc_list}]
{combined_content}

ìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”.
ì—¬ëŸ¬ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¢…í•©í•´ì„œ ë‹µë³€í•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”."""

                        result2_tokens = 4096 if LLM_MODE == "api" else 1024
                        result2 = call_llm(follow_up_prompt, KNOWLEDGE_QA_SYSTEM_PROMPT, max_tokens=result2_tokens, task_type="knowledge_qa")
                        if result2["success"]:
                            content = result2["content"].strip()
                            logger.info(f"ğŸ“ ì§€ì‹ê²€ìƒ‰ 2ì°¨ ì‘ë‹µ: {content[:200] if content else '(ë¹ˆ ì‘ë‹µ)'}")
                            if content and not extract_tool_json(content):
                                # ì°¸ì¡° ë¬¸ì„œ ëª©ë¡ ì¶”ê°€
                                source_info = f"\n\n---\nğŸ“š **ì°¸ì¡° ë¬¸ì„œ**: {doc_list}"
                                return content + source_info
                        # fallback: ë¬¸ì„œ ë‚´ìš© ì§ì ‘ ë°˜í™˜
                        logger.info("âš ï¸ 2ì°¨ LLM ì‘ë‹µ ì—†ìŒ â†’ ë¬¸ì„œ ì§ì ‘ ë°˜í™˜")
                        return f"ğŸ“„ **ì°¸ì¡° ë¬¸ì„œ:**\n\n{combined_content[:5000]}"
                    except Exception as e:
                        logger.error(f"ì§€ì‹ê²€ìƒ‰ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        pass

                # 3) list_knowledge â†’ LLM í˜¸ì¶œ ì—†ì´ ì§ì ‘ í¬ë§·íŒ… (API ë‚­ë¹„ ë°©ì§€)
                if tool_name == "list_knowledge":
                    try:
                        files = json.loads(tool_result)
                        if not files:
                            return "ğŸ“­ ì§€ì‹ë² ì´ìŠ¤ì— ë“±ë¡ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\n\nğŸ“š ì§€ì‹ë² ì´ìŠ¤ ë²„íŠ¼ìœ¼ë¡œ MD/TXT íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”."
                        lines = [f"## ğŸ“š ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ({len(files)}ê°œ)\n"]
                        for f in files:
                            lines.append(f"- ğŸ“„ **{f['filename']}** ({f['size']}, {f['modified']})")
                        lines.append(f"\nğŸ’¡ ë¬¸ì„œ ë‚´ìš©ì´ ê¶ê¸ˆí•˜ë©´ íŒŒì¼ëª…ìœ¼ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”. (ì˜ˆ: \"HID_INOUT ì•Œë ¤ì¤˜\")")
                        return "\n".join(lines)
                    except (json.JSONDecodeError, KeyError, TypeError) as e:
                        logger.error(f"ì§€ì‹ ëª©ë¡ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        pass

                # ê¸°íƒ€ ë„êµ¬: 2ì°¨ LLMìœ¼ë¡œ í•´ì„
                follow_up_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {user_message}

ë„êµ¬ ì‹¤í–‰ ê²°ê³¼:
{tool_result}

ìœ„ ê²°ê³¼ë¥¼ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ í•œêµ­ì–´ë¡œ ì •ë¦¬í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
- JSON ì›ë³¸ì„ ë³´ì—¬ì£¼ì§€ ë§ê³  í•µì‹¬ë§Œ ì •ë¦¬
- ë„êµ¬ë¥¼ ë‹¤ì‹œ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš” (JSON ì¶œë ¥ ê¸ˆì§€)
- ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ"""

                follow_up_system = """ë‹¹ì‹ ì€ MAGI (main1_First AI) ë¹„ì„œì…ë‹ˆë‹¤.
ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
ì ˆëŒ€ JSONì„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. ìì—°ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""

                result2 = call_llm(follow_up_prompt, follow_up_system, task_type="tool_call")
                if result2["success"]:
                    response = result2["content"]
                    if extract_tool_json(response):
                        return format_tool_result_fallback(tool_data, tool_result)
                    return response
                else:
                    return format_tool_result_fallback(tool_data, tool_result)

            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                return "âŒ ëª…ë ¹ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        # â˜…â˜…â˜… LLMì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šì€ ê²½ìš° â†’ ìë™ ì§€ì‹ë² ì´ìŠ¤ íƒìƒ‰ â˜…â˜…â˜…
        # PC ëª…ë ¹ì–´/ì¸ì‚¬ê°€ ì•„ë‹Œë° ë„êµ¬ë¥¼ ì•ˆ ë¶ˆë €ìœ¼ë©´ = ì§€ì‹ë² ì´ìŠ¤ë¥¼ ë†“ì¹œ ê²ƒ
        skip_keywords = ["í”„ë¡œê·¸ë¨", "ì‹¤í–‰", "ì¢…ë£Œ", "í”„ë¡œì„¸ìŠ¤", "ìŠ¤í¬ë¦°ìƒ·", "ìº¡ì²˜",
                         "ì‹œìŠ¤í…œ", "cpu", "ë©”ëª¨ë¦¬", "ë””ìŠ¤í¬", "ëª‡ì‹œ", "ì‹œê°„", "ë‚ ì§œ",
                         "íŒŒì¼ ì°¾", "íŒŒì¼ ê²€ìƒ‰", "ê²€ìƒ‰í•´", "êµ¬ê¸€", "ë‰´ìŠ¤", "í´ë”", "ë””ë ‰í† ë¦¬"]
        greeting_patterns = ["ì•ˆë…•", "í•˜ì´", "í—¬ë¡œ", "hi", "hello", "ã…ã…‡", "ë°˜ê°€",
                             "ê³ ë§ˆì›Œ", "ê°ì‚¬", "ã„±ã……", "ã…‹ã…‹", "ã…ã…", "ë„¤", "ì‘", "ã…‡ã…‡",
                             "ì•„ë‹ˆ", "ë­í•´", "ì‹¬ì‹¬", "ì˜ì", "ë°”ì´"]

        msg_lower = user_message.lower().strip()
        is_pc_cmd = any(kw in msg_lower for kw in skip_keywords)
        is_greeting = any(msg_lower.startswith(g) or msg_lower == g for g in greeting_patterns)
        is_short = len(msg_lower) <= 4

        # ì§€ì‹ë² ì´ìŠ¤ íŒŒì¼ì´ ìˆê³ , PCëª…ë ¹/ì¸ì‚¬/ì§§ì€ë§ì´ ì•„ë‹Œ ê²½ìš° â†’ ìë™ ê²€ìƒ‰
        kb_has_files = False
        try:
            kb_has_files = any(f.endswith(('.md', '.txt')) for f in os.listdir(KNOWLEDGE_DIR))
        except OSError:
            pass

        if kb_has_files and not is_pc_cmd and not is_greeting and not is_short:
            logger.info(f"ğŸ”„ ìë™ ì§€ì‹ë² ì´ìŠ¤ íƒìƒ‰: '{user_message}'")

            # í‚¤ì›Œë“œ ì¶”ì¶œ (ì¡°ì‚¬/ì–´ë¯¸ ì œê±°)
            clean_msg = re.sub(r'(ì•Œë ¤ì¤˜|ì„¤ëª…í•´ì¤˜|ë­ì•¼|ë­ì—ìš”|í•´ì¤˜|í• ë˜|ì— ëŒ€í•´|ì—ëŒ€í•´|ì¢€|ì¤˜|ìš”|ëŠ”|ì€|ì´|ê°€|ì„|ë¥¼|ì˜|ë¡œ|ìœ¼ë¡œ|ì—ì„œ|ë¶€í„°|ê¹Œì§€|ì´ë‘|ë‘|í•˜ê³ |ê·¸ë¦¬ê³ |ë˜ëŠ”|ì´ë‚˜|ë‚˜|ì´ë“ )', '', msg_lower).strip()
            if not clean_msg:
                clean_msg = msg_lower

            auto_results = search_knowledge(clean_msg)

            if auto_results:
                # ê²€ìƒ‰ ì„±ê³µ â†’ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ìƒì„± (ê¸°ì¡´ search_knowledge í•¸ë“¤ëŸ¬ ë™ì¼ ë¡œì§)
                logger.info(f"âœ… ìë™ ê²€ìƒ‰ ì„±ê³µ: {[r['filename'] for r in auto_results[:3]]}")
                MAX_TOTAL_LENGTH = 15000 if LLM_MODE == "api" else 3000
                MAX_DOCS = 3  # â˜… ìƒìœ„ 3ê°œ ë¬¸ì„œê¹Œì§€ë§Œ
                merged_docs = []
                total_length = 0
                doc_names = []
                top_score = auto_results[0].get("score", 100)

                for i, res_item in enumerate(auto_results[:MAX_DOCS]):
                    filename = res_item["filename"]
                    score = res_item.get("score", 0)
                    if i > 0 and score < top_score * 0.5:
                        break
                    doc_content = read_knowledge(filename)
                    if doc_content.startswith("âŒ"):
                        continue
                    remaining = MAX_TOTAL_LENGTH - total_length
                    if remaining <= 1000:
                        break
                    if len(doc_content) > remaining:
                        doc_content = doc_content[:remaining] + "\n\n... (ë¬¸ì„œ ì¼ë¶€ ìƒëµ)"
                    merged_docs.append(f"ğŸ“„ **[{filename}]**\n{doc_content}")
                    doc_names.append(filename)
                    total_length += len(doc_content)

                if merged_docs:
                    combined_content = "\n\n---\n\n".join(merged_docs)
                    doc_list = ", ".join(doc_names)

                    follow_up_prompt = f"""[ì‚¬ìš©ì ì§ˆë¬¸]
{user_message}

[ì°¸ê³  ë¬¸ì„œ {len(doc_names)}ê°œ: {doc_list}]
{combined_content}

ìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”.
ì—¬ëŸ¬ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¢…í•©í•´ì„œ ë‹µë³€í•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”."""

                    result2_tokens = 4096 if LLM_MODE == "api" else 1024
                    result2 = call_llm(follow_up_prompt, KNOWLEDGE_QA_SYSTEM_PROMPT, max_tokens=result2_tokens, task_type="knowledge_qa")
                    if result2["success"]:
                        content2 = result2["content"].strip()
                        if content2 and not extract_tool_json(content2):
                            source_info = f"\n\n---\nğŸ“š **ì°¸ì¡° ë¬¸ì„œ**: {doc_list}"
                            return content2 + source_info
            else:
                # â˜… ê²€ìƒ‰ ì‹¤íŒ¨ â†’ ì—­ì§ˆë¬¸ ìœ ë„
                logger.info(f"ğŸ”„ ìë™ ê²€ìƒ‰ ì‹¤íŒ¨ â†’ ì—­ì§ˆë¬¸ ìœ ë„")
                guide = generate_guided_questions(user_message)
                if guide["success"] and guide["suggestions"]:
                    lines = [f"ğŸ” **{guide['message']}**\n"]
                    for suggestion in guide["suggestions"]:
                        lines.append(f"<!--SUGGEST:{suggestion}-->")
                    lines.append(f"\n\nğŸ’¡ ìœ„ ì¶”ì²œ ì§ˆë¬¸ì„ í´ë¦­í•˜ê±°ë‚˜, ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
                    if guide.get("kb_files"):
                        lines.append(f"\nğŸ“š í˜„ì¬ ë“±ë¡ëœ ë¬¸ì„œ: {', '.join(guide['kb_files'][:5])}")
                    return "\n".join(lines)

        # ìœ„ ëª¨ë“  ê²½ìš°ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ LLM ì›ë˜ ì‘ë‹µ ë°˜í™˜
        return text

    except Exception as e:
        logger.error(f"âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return f"âŒ ì˜¤ë¥˜: {e}"


# Fallback í¬ë§·í„°
def format_tool_result_fallback(tool_data: dict, tool_result: str) -> str:
    tool_name = tool_data.get("tool", "")
    try:
        if tool_name == "get_system_info":
            info = json.loads(tool_result)
            lines = ["## ğŸ’» ì‹œìŠ¤í…œ ì •ë³´", f"- **OS**: {info.get('os', '?')}", f"- **CPU**: {info.get('cpu', '?')}", f"- **ë©”ëª¨ë¦¬**: {info.get('memory', '?')}"]
            for d in info.get('drives', []):
                lines.append(f"- **{d['drive']}**: {d['total']} (ì‚¬ìš©ë¥  {d['used']})")
            return "\n".join(lines)

        elif tool_name == "get_time":
            return f"ğŸ• í˜„ì¬ ì‹œê°„: {tool_result}"

        elif tool_name in ["search_files", "search_content"]:
            results = json.loads(tool_result)
            if not results:
                return f"ğŸ” '{tool_data.get('keyword', '')}' ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            lines = [f"ğŸ” ê²€ìƒ‰ ê²°ê³¼: **{len(results)}ê°œ** ë°œê²¬\n"]
            for r in results[:10]:
                if "snippet" in r:
                    lines.append(f"- ğŸ“„ `{r['name']}` â†’ {r['snippet']}")
                else:
                    lines.append(f"- {'ğŸ“' if r.get('type') == 'í´ë”' else 'ğŸ“„'} `{r['name']}` ({r.get('size', '?')})")
            return "\n".join(lines)

        elif tool_name == "list_directory":
            items = json.loads(tool_result)
            lines = [f"ğŸ“‚ `{tool_data.get('path', '')}` ë‚´ìš©:\n"]
            for item in items[:20]:
                icon = "ğŸ“" if item.get("type") == "í´ë”" else "ğŸ“„"
                lines.append(f"- {icon} `{item['name']}` ({item.get('size', '-')})")
            return "\n".join(lines)

        elif tool_name == "read_file":
            return f"ğŸ“„ **íŒŒì¼ ë‚´ìš©:**\n```\n{tool_result}\n```"

        elif tool_name in ["run_program", "kill_program", "open_web", "google_search"]:
            return f"âœ… {tool_result}"

        elif tool_name == "list_processes":
            procs = json.loads(tool_result)
            if not procs or "error" in procs[0]:
                return "âŒ í”„ë¡œì„¸ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            lines = [f"## ğŸ“‹ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ (ìƒìœ„ {len(procs)}ê°œ)\n"]
            lines.append("| ì´ë¦„ | PID | ë©”ëª¨ë¦¬(MB) | CPU% | ìƒíƒœ |")
            lines.append("|------|-----|-----------|------|------|")
            for p in procs:
                lines.append(f"| {p['name']} | {p['pid']} | {p['memory_mb']} | {p['cpu']}% | {p['status']} |")
            return "\n".join(lines)

        elif tool_name == "analyze_data":
            return f"ğŸ“Š **ë°ì´í„° ë¶„ì„:**\n```\n{tool_result}\n```"

    except Exception as e:
        logger.error(f"í¬ë§·íŒ… ì˜¤ë¥˜: {e}")

    return f"ğŸ“‹ **ê²°ê³¼:**\n```\n{tool_result}\n```"


# ========================================
# ëŒ€í™” ê¸°ë¡ (ì¸ì½”ë”© ì €ì¥ + ì„¸ì…˜ ê´€ë¦¬)
# ========================================
def _encode_history(data: list) -> str:
    """ëŒ€í™” ê¸°ë¡ì„ base64 ì¸ì½”ë”©í•˜ì—¬ ì €ì¥ (í‰ë¬¸ ë…¸ì¶œ ë°©ì§€)"""
    raw = json.dumps(data, ensure_ascii=False)
    return base64.b64encode(raw.encode('utf-8')).decode('ascii')


def _decode_history(encoded: str) -> list:
    """base64 ì¸ì½”ë”©ëœ ëŒ€í™” ê¸°ë¡ ë³µì›"""
    raw = base64.b64decode(encoded.encode('ascii')).decode('utf-8')
    return json.loads(raw)


def save_history():
    try:
        data_to_save = CHAT_HISTORY[-HISTORY_MAX:]
        encoded = _encode_history(data_to_save)
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump({"v": 2, "session": CURRENT_SESSION_ID, "data": encoded}, f)
    except (OSError, TypeError) as e:
        logger.warning(f"ëŒ€í™” ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")


def load_history():
    global CHAT_HISTORY
    try:
        if os.path.exists(HISTORY_FILE):
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                raw = json.load(f)
            # v2 ì¸ì½”ë”© í˜•ì‹
            if isinstance(raw, dict) and raw.get("v") == 2:
                CHAT_HISTORY = _decode_history(raw["data"])
            # v1 ë ˆê±°ì‹œ (í‰ë¬¸ ë¦¬ìŠ¤íŠ¸) â†’ ìë™ ë§ˆì´ê·¸ë ˆì´ì…˜
            elif isinstance(raw, list):
                CHAT_HISTORY = raw
                save_history()  # v2ë¡œ ì¬ì €ì¥
                logger.info("ëŒ€í™” ê¸°ë¡ v1â†’v2 ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
            else:
                CHAT_HISTORY = []
    except (OSError, json.JSONDecodeError, ValueError) as e:
        logger.warning(f"ëŒ€í™” ê¸°ë¡ ë¡œë“œ ì‹¤íŒ¨ (ì´ˆê¸°í™”): {e}")
        CHAT_HISTORY = []


def save_session(session_name: str = None) -> dict:
    """í˜„ì¬ ëŒ€í™”ë¥¼ ì„¸ì…˜ íŒŒì¼ë¡œ ì €ì¥"""
    global CURRENT_SESSION_ID
    if not CHAT_HISTORY:
        return {"success": False, "error": "ì €ì¥í•  ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤."}

    if not session_name:
        session_name = CURRENT_SESSION_ID

    # íŒŒì¼ëª… ì•ˆì „í•˜ê²Œ
    safe_name = re.sub(r'[^\wê°€-í£\-_]', '_', session_name)
    filepath = os.path.join(CHAT_SESSIONS_DIR, f"{safe_name}.json")

    try:
        encoded = _encode_history(CHAT_HISTORY)
        session_data = {
            "v": 2,
            "session_id": safe_name,
            "session_name": session_name,
            "created": datetime.datetime.now().isoformat(),
            "message_count": len(CHAT_HISTORY),
            "data": encoded
        }
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(session_data, f, ensure_ascii=False)
        return {"success": True, "session_id": safe_name, "message_count": len(CHAT_HISTORY)}
    except OSError as e:
        return {"success": False, "error": str(e)}


def load_session(session_id: str) -> dict:
    """ì €ì¥ëœ ì„¸ì…˜ ë¶ˆëŸ¬ì˜¤ê¸°"""
    global CHAT_HISTORY, CURRENT_SESSION_ID
    filepath = os.path.join(CHAT_SESSIONS_DIR, f"{session_id}.json")

    if not os.path.exists(filepath):
        return {"success": False, "error": f"ì„¸ì…˜ ì—†ìŒ: {session_id}"}

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            session_data = json.load(f)
        CHAT_HISTORY = _decode_history(session_data["data"])
        CURRENT_SESSION_ID = session_id
        save_history()
        return {"success": True, "session_id": session_id, "message_count": len(CHAT_HISTORY)}
    except (OSError, json.JSONDecodeError, ValueError) as e:
        return {"success": False, "error": str(e)}


def list_sessions() -> list:
    """ì €ì¥ëœ ì„¸ì…˜ ëª©ë¡"""
    sessions = []
    try:
        for f in sorted(os.listdir(CHAT_SESSIONS_DIR), reverse=True):
            if f.endswith('.json'):
                filepath = os.path.join(CHAT_SESSIONS_DIR, f)
                try:
                    with open(filepath, 'r', encoding='utf-8') as fh:
                        meta = json.load(fh)
                    sessions.append({
                        "session_id": f.replace('.json', ''),
                        "session_name": meta.get("session_name", f),
                        "created": meta.get("created", ""),
                        "message_count": meta.get("message_count", 0)
                    })
                except (json.JSONDecodeError, OSError):
                    continue
    except OSError:
        pass
    return sessions


def search_history(keyword: str, limit: int = 30) -> list:
    """ëŒ€í™” ê¸°ë¡ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰"""
    results = []
    kw_lower = keyword.lower()
    for i, msg in enumerate(CHAT_HISTORY):
        if kw_lower in msg.get("content", "").lower():
            results.append({
                "index": i,
                "role": msg["role"],
                "content": msg["content"][:200],
                "time": msg.get("time", "")
            })
            if len(results) >= limit:
                break
    return results


def export_history(format_type: str = "json") -> dict:
    """ëŒ€í™” ê¸°ë¡ ë‚´ë³´ë‚´ê¸° (json/txt/md)"""
    if not CHAT_HISTORY:
        return {"success": False, "error": "ë‚´ë³´ë‚¼ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤."}

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

    if format_type == "txt":
        lines = []
        for msg in CHAT_HISTORY:
            role = "ì‚¬ìš©ì" if msg["role"] == "user" else "MAGI"
            time_str = msg.get("time", "")
            lines.append(f"[{role}] ({time_str})")
            lines.append(msg["content"])
            lines.append("")
        content = "\n".join(lines)
        filename = f"chat_export_{timestamp}.txt"

    elif format_type == "md":
        lines = [f"# MAGI ëŒ€í™” ê¸°ë¡\n", f"ë‚´ë³´ë‚´ê¸°: {timestamp}\n", f"ì´ {len(CHAT_HISTORY)}ê°œ ë©”ì‹œì§€\n", "---\n"]
        for msg in CHAT_HISTORY:
            role = "**ì‚¬ìš©ì**" if msg["role"] == "user" else "**MAGI**"
            time_str = msg.get("time", "")
            lines.append(f"### {role} ({time_str})\n")
            lines.append(msg["content"])
            lines.append("\n---\n")
        content = "\n".join(lines)
        filename = f"chat_export_{timestamp}.md"

    else:  # json
        content = json.dumps(CHAT_HISTORY, ensure_ascii=False, indent=2)
        filename = f"chat_export_{timestamp}.json"

    # íŒŒì¼ë¡œ ì €ì¥
    export_dir = os.path.join(BASE_DIR, "chat_exports")
    os.makedirs(export_dir, exist_ok=True)
    filepath = os.path.join(export_dir, filename)

    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        return {"success": True, "filename": filename, "path": filepath, "format": format_type}
    except OSError as e:
        return {"success": False, "error": str(e)}


# ========================================
# API Models
# ========================================
class ChatRequest(BaseModel):
    message: str

class SearchRequest(BaseModel):
    keyword: str
    path: str = "C:/"
    file_content: bool = False

class EnvRequest(BaseModel):
    env: str

class ModelRequest(BaseModel):
    model_key: str


# ========================================
# Endpoints
# ========================================
def init_assistant():
    global LOCAL_LLM, LLM_MODE
    load_history()
    if load_api_token():
        LLM_MODE = "api"
        logger.info("âœ… ë¹„ì„œ: API ëª¨ë“œ")
    else:
        # ì„œë²„(Coding_llm_server_v2)ì—ì„œ ì´ë¯¸ ë¡œë“œí•œ GGUF ëª¨ë¸ ì¬ì‚¬ìš©
        try:
            import __main__ as server_main
            server_llm = getattr(server_main, 'LOCAL_LLM', None)
            if server_llm is not None:
                LOCAL_LLM = server_llm
                LLM_MODE = "local"
                logger.info("âœ… ë¹„ì„œ: LOCAL ëª¨ë“œ (ì„œë²„ ëª¨ë¸ ê³µìœ )")
                return
        except (ImportError, AttributeError):
            pass
        LOCAL_LLM = load_local_model()
        if LOCAL_LLM:
            LLM_MODE = "local"
            logger.info("âœ… ë¹„ì„œ: LOCAL ëª¨ë“œ")

    # â˜… TF-IDF ì¸ë±ìŠ¤ êµ¬ì¶•
    build_tfidf_index()


@router.get("/")
async def assistant_home():
    return FileResponse(os.path.join(BASE_DIR, "assistant_ui.html"))

@router.get("/magi.png")
async def magi_icon():
    return FileResponse(os.path.join(BASE_DIR, "magi.png"), media_type="image/png")

@router.get("/magi_f.png")
async def magi_f_icon():
    return FileResponse(os.path.join(BASE_DIR, "magi_f.png"), media_type="image/png")


# â˜… ìŠ¤í¬ë¦°ìƒ· ì´ë¯¸ì§€ ì„œë¹™
@router.get("/screenshots/{filename}")
async def serve_screenshot(filename: str):
    filepath = os.path.join(SCREENSHOT_DIR, filename)
    if os.path.exists(filepath):
        return FileResponse(filepath, media_type="image/png")
    return {"error": "íŒŒì¼ ì—†ìŒ"}


# â˜… ë¦¬ì†ŒìŠ¤ íŒŒì¼ ì„œë¹™ (HTML êµ¬ì„±ë„ ë“±)
@router.get("/resources/{filename}")
async def serve_resource(filename: str):
    filepath = os.path.join(RESOURCES_DIR, filename)
    if os.path.exists(filepath):
        # í™•ì¥ìë³„ MIME íƒ€ì…
        ext = os.path.splitext(filename)[1].lower()
        mime_types = {
            ".html": "text/html",
            ".htm": "text/html",
            ".png": "image/png",
            ".jpg": "image/jpeg",
            ".svg": "image/svg+xml",
            ".pdf": "application/pdf",
        }
        media_type = mime_types.get(ext, "application/octet-stream")
        return FileResponse(filepath, media_type=media_type)
    return JSONResponse(status_code=404, content={"error": "íŒŒì¼ ì—†ìŒ"})


# â˜… ìŠ¤í¬ë¦°ìƒ· ë¹„ë°€ë²ˆí˜¸ ì¸ì¦ API
@router.post("/api/screenshot/auth")
async def screenshot_auth(data: dict):
    global screenshot_authenticated
    password = data.get("password", "")
    if password == SCREENSHOT_PASSWORD:
        screenshot_authenticated = True
        logger.info("ğŸ”“ ìŠ¤í¬ë¦°ìƒ· ì¸ì¦ ì„±ê³µ")
        return {"success": True, "message": "ì¸ì¦ ì„±ê³µ"}
    else:
        logger.warning("ğŸ”’ ìŠ¤í¬ë¦°ìƒ· ì¸ì¦ ì‹¤íŒ¨")
        return {"success": False, "message": "ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤."}


# â˜… ìŠ¤í¬ë¦°ìƒ· ëª©ë¡
@router.get("/api/screenshots")
async def list_screenshots():
    files = []
    if os.path.exists(SCREENSHOT_DIR):
        for f in sorted(os.listdir(SCREENSHOT_DIR), reverse=True)[:20]:
            if f.endswith('.png'):
                filepath = os.path.join(SCREENSHOT_DIR, f)
                size = os.path.getsize(filepath)
                files.append({
                    "filename": f,
                    "url": f"/assistant/screenshots/{f}",
                    "size": f"{size / 1024:.0f}KB",
                    "time": datetime.datetime.fromtimestamp(os.path.getmtime(filepath)).strftime("%Y-%m-%d %H:%M:%S")
                })
    return {"screenshots": files}


@router.get("/api/status")
async def assistant_status():
    # í˜„ì¬ ë¡œì»¬ ëª¨ë¸ ì •ë³´
    current_model_name = "LOCAL"
    if LLM_MODE == "local" and CURRENT_LOCAL_MODEL in AVAILABLE_MODELS:
        current_model_name = AVAILABLE_MODELS[CURRENT_LOCAL_MODEL]["name"]
    elif LLM_MODE != "local":
        current_model_name = ENV_CONFIG.get(CURRENT_ENV, {}).get("name", "API")

    return {
        "mode": LLM_MODE,
        "env": CURRENT_ENV if LLM_MODE != "local" else "local",
        "model_loaded": LOCAL_LLM is not None if LLM_MODE == "local" else API_TOKEN is not None,
        "model_name": current_model_name,
        "current_local_model": CURRENT_LOCAL_MODEL,
        "system": get_system_info(),
        "history_count": len(CHAT_HISTORY),
        "token_usage": TOKEN_USAGE
    }


# â˜… ë¡œì»¬ ëª¨ë¸ ëª©ë¡ API
@router.get("/api/models")
async def list_local_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë¡œì»¬ GGUF ëª¨ë¸ ëª©ë¡"""
    return {
        "success": True,
        "models": get_available_local_models(),
        "current": CURRENT_LOCAL_MODEL
    }


# â˜… ë¡œì»¬ ëª¨ë¸ ë³€ê²½ API
@router.post("/api/models/switch")
async def switch_local_model(request: ModelRequest):
    """ë¡œì»¬ GGUF ëª¨ë¸ ë³€ê²½"""
    global LOCAL_LLM, LLM_MODE

    model_key = request.model_key

    if model_key not in AVAILABLE_MODELS:
        return {"success": False, "error": f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_key}"}

    if not os.path.exists(AVAILABLE_MODELS[model_key]["path"]):
        return {"success": False, "error": f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {AVAILABLE_MODELS[model_key]['name']}"}

    # ê¸°ì¡´ ëª¨ë¸ í•´ì œ
    if LOCAL_LLM is not None:
        del LOCAL_LLM
        LOCAL_LLM = None
        import gc
        gc.collect()

    # ìƒˆ ëª¨ë¸ ë¡œë“œ
    LOCAL_LLM = load_local_model(model_key)
    if LOCAL_LLM:
        LLM_MODE = "local"
        return {
            "success": True,
            "model_key": model_key,
            "model_name": AVAILABLE_MODELS[model_key]["name"]
        }
    else:
        return {"success": False, "error": "ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨"}


# â˜… í† í° ì‚¬ìš©ëŸ‰ API
@router.get("/api/tokens")
async def assistant_tokens():
    return {
        "success": True,
        "prompt_tokens": TOKEN_USAGE["prompt_tokens"],
        "completion_tokens": TOKEN_USAGE["completion_tokens"],
        "total_tokens": TOKEN_USAGE["total_tokens"],
        "call_count": TOKEN_USAGE["call_count"]
    }


@router.post("/api/tokens/reset")
async def assistant_reset_tokens():
    TOKEN_USAGE["prompt_tokens"] = 0
    TOKEN_USAGE["completion_tokens"] = 0
    TOKEN_USAGE["total_tokens"] = 0
    TOKEN_USAGE["call_count"] = 0
    return {"success": True, "message": "í† í° ì¹´ìš´í„° ì´ˆê¸°í™”ë¨"}


# â˜… íŒŒë¼ë¯¸í„° ì¡°íšŒ API
@router.get("/api/params")
async def get_params():
    return {"success": True, "params": LLM_PARAMS}


# â˜… íŒŒë¼ë¯¸í„° ë³€ê²½ API
@router.post("/api/params")
async def set_params(request: dict):
    global LLM_PARAMS
    if "temperature" in request:
        LLM_PARAMS["temperature"] = float(request["temperature"])
    if "repeat_penalty" in request:
        LLM_PARAMS["repeat_penalty"] = float(request["repeat_penalty"])
    if "max_tokens" in request:
        LLM_PARAMS["max_tokens"] = int(request["max_tokens"])
    logger.info(f"âš™ï¸ íŒŒë¼ë¯¸í„° ë³€ê²½: {LLM_PARAMS}")
    return {"success": True, "params": LLM_PARAMS}


# â˜… íƒœìŠ¤í¬ë³„ ìë™ ë¶„ë¥˜ í”„ë¡œí•„ API
@router.get("/api/task_profiles")
async def get_task_profiles():
    return {
        "success": True,
        "auto_mode": LLM_PARAMS.get("task_auto_mode", False),
        "profiles": TASK_PARAM_PROFILES
    }


@router.post("/api/task_profiles")
async def set_task_profiles(request: dict):
    global TASK_PARAM_PROFILES
    if "auto_mode" in request:
        LLM_PARAMS["task_auto_mode"] = bool(request["auto_mode"])
    if "profiles" in request:
        for key, val in request["profiles"].items():
            if key in TASK_PARAM_PROFILES and "temperature" in val:
                TASK_PARAM_PROFILES[key]["temperature"] = float(val["temperature"])
    logger.info(f"ğŸ¯ íƒœìŠ¤í¬ ìë™ ë¶„ë¥˜: {'ON' if LLM_PARAMS.get('task_auto_mode') else 'OFF'} | {TASK_PARAM_PROFILES}")
    return {
        "success": True,
        "auto_mode": LLM_PARAMS.get("task_auto_mode", False),
        "profiles": TASK_PARAM_PROFILES
    }


@router.post("/api/set_env")
async def assistant_set_env(request: EnvRequest):
    global LLM_MODE, LOCAL_LLM, CURRENT_ENV, API_URL, API_MODEL
    env = request.env.lower()

    if env == "local":
        if LOCAL_LLM is None:
            LOCAL_LLM = load_local_model()
        if LOCAL_LLM:
            LLM_MODE = "local"
            return {"success": True, "env": "local", "name": "LOCAL(14B-GGUF)"}
        return {"success": False, "error": "ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨"}

    elif env in ENV_CONFIG:
        if not API_TOKEN and not load_api_token():
            return {"success": False, "error": "API í† í° ì—†ìŒ"}
        LLM_MODE = "api"
        CURRENT_ENV = env
        API_URL = ENV_CONFIG[env]["url"]
        API_MODEL = ENV_CONFIG[env]["model"]
        return {"success": True, "env": env, "name": ENV_CONFIG[env]["name"]}

    return {"success": False, "error": f"ì•Œ ìˆ˜ ì—†ëŠ” í™˜ê²½: {env}"}


@router.post("/api/chat")
async def assistant_chat(request: ChatRequest):
    user_msg = request.message.strip()
    CHAT_HISTORY.append({"role": "user", "content": user_msg, "time": datetime.datetime.now().isoformat()})
    response = process_chat(user_msg)
    # â˜… ìë™ ë¦¬ì†ŒìŠ¤ ì²¨ë¶€ (AMHS ê´€ë ¨ ì§ˆë¬¸ ì‹œ êµ¬ì„±ë„ ë§í¬ ë“±)
    response = auto_attach_resources(user_msg, response)
    CHAT_HISTORY.append({"role": "assistant", "content": response, "time": datetime.datetime.now().isoformat()})
    save_history()
    return {"success": True, "response": response}


@router.post("/api/chat/stream")
async def assistant_chat_stream(request: ChatRequest):
    """SSE ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… - ì§„í–‰ ë‹¨ê³„ + íƒ€ì´í•‘ íš¨ê³¼"""
    user_msg = request.message.strip()
    CHAT_HISTORY.append({"role": "user", "content": user_msg, "time": datetime.datetime.now().isoformat()})

    async def event_generator():
        # 1ë‹¨ê³„: ìƒê° ì¤‘
        yield f"data: {json.dumps({'type': 'status', 'message': 'ìƒê° ì¤‘...'}, ensure_ascii=False)}\n\n"
        await asyncio.sleep(0.1)

        # 2ë‹¨ê³„: LLM í˜¸ì¶œ (ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰)
        loop = asyncio.get_event_loop()
        try:
            yield f"data: {json.dumps({'type': 'status', 'message': 'ì‘ë‹µ ìƒì„± ì¤‘...'}, ensure_ascii=False)}\n\n"
            response = await loop.run_in_executor(None, process_chat, user_msg)
            response = auto_attach_resources(user_msg, response)

            # 3ë‹¨ê³„: ì‘ë‹µì„ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ì „ì†¡ (íƒ€ì´í•‘ íš¨ê³¼)
            yield f"data: {json.dumps({'type': 'status', 'message': 'ë‹µë³€ ì‘ì„± ì¤‘...'}, ensure_ascii=False)}\n\n"

            # ì‘ë‹µì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë°
            chunks = []
            current = ""
            for char in response:
                current += char
                if char in '.!?\n' and len(current) >= 10:
                    chunks.append(current)
                    current = ""
            if current:
                chunks.append(current)

            if not chunks:
                chunks = [response]

            for chunk in chunks:
                yield f"data: {json.dumps({'type': 'chunk', 'content': chunk}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.03)

            # 4ë‹¨ê³„: ì™„ë£Œ
            CHAT_HISTORY.append({"role": "assistant", "content": response, "time": datetime.datetime.now().isoformat()})
            save_history()
            yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"

        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì˜¤ë¥˜: {e}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)}, ensure_ascii=False)}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")


@router.post("/api/search")
async def assistant_search(request: SearchRequest):
    if request.file_content:
        results = search_content(request.keyword, request.path)
    else:
        results = search_files(request.keyword, request.path)
    return {"success": True, "results": results, "count": len(results)}


@router.get("/api/drives")
async def assistant_drives():
    drives = []
    for p in psutil.disk_partitions():
        drives.append({"device": p.device, "mountpoint": p.mountpoint})
    return {"success": True, "drives": drives}


@router.get("/api/history")
async def assistant_get_history(limit: int = 50):
    return {"history": CHAT_HISTORY[-limit:], "total": len(CHAT_HISTORY), "session": CURRENT_SESSION_ID}


@router.delete("/api/history")
async def assistant_clear_history():
    global CHAT_HISTORY, CURRENT_SESSION_ID
    CHAT_HISTORY = []
    CURRENT_SESSION_ID = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    save_history()
    return {"success": True}


# â˜… ëŒ€í™” ê²€ìƒ‰
@router.get("/api/history/search")
async def assistant_search_history(keyword: str, limit: int = 30):
    """ëŒ€í™” ê¸°ë¡ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰"""
    results = search_history(keyword, limit)
    return {"success": True, "results": results, "count": len(results)}


# â˜… ëŒ€í™” ë‚´ë³´ë‚´ê¸°
@router.get("/api/history/export")
async def assistant_export_history(format: str = "json"):
    """ëŒ€í™” ê¸°ë¡ ë‚´ë³´ë‚´ê¸° (json/txt/md)"""
    if format not in ("json", "txt", "md"):
        return {"success": False, "error": "ì§€ì› í˜•ì‹: json, txt, md"}
    result = export_history(format)
    if result["success"]:
        return FileResponse(result["path"], filename=result["filename"], media_type="application/octet-stream")
    return result


# â˜… íˆìŠ¤í† ë¦¬ ìµœëŒ€ ê±´ìˆ˜ ì„¤ì •
@router.post("/api/history/config")
async def assistant_history_config(data: dict):
    """íˆìŠ¤í† ë¦¬ ìµœëŒ€ ê±´ìˆ˜ ë³€ê²½"""
    global HISTORY_MAX
    new_max = data.get("max", HISTORY_MAX)
    if isinstance(new_max, int) and 50 <= new_max <= 2000:
        HISTORY_MAX = new_max
        return {"success": True, "max": HISTORY_MAX}
    return {"success": False, "error": "50~2000 ë²”ìœ„ë§Œ ê°€ëŠ¥"}


# â˜… ì„¸ì…˜ ê´€ë¦¬ API
@router.get("/api/sessions")
async def assistant_list_sessions():
    """ì €ì¥ëœ ì„¸ì…˜ ëª©ë¡"""
    sessions = list_sessions()
    return {"success": True, "sessions": sessions, "current": CURRENT_SESSION_ID}


@router.post("/api/sessions/save")
async def assistant_save_session(data: dict = None):
    """í˜„ì¬ ëŒ€í™”ë¥¼ ì„¸ì…˜ìœ¼ë¡œ ì €ì¥"""
    name = data.get("name", "") if data else ""
    result = save_session(name if name else None)
    return result


@router.post("/api/sessions/load")
async def assistant_load_session(data: dict):
    """ì €ì¥ëœ ì„¸ì…˜ ë¶ˆëŸ¬ì˜¤ê¸°"""
    session_id = data.get("session_id", "")
    if not session_id:
        return {"success": False, "error": "session_id í•„ìš”"}
    result = load_session(session_id)
    return result


@router.delete("/api/sessions/{session_id}")
async def assistant_delete_session(session_id: str):
    """ì„¸ì…˜ ì‚­ì œ"""
    filepath = os.path.join(CHAT_SESSIONS_DIR, f"{session_id}.json")
    if os.path.exists(filepath):
        os.remove(filepath)
        return {"success": True, "message": f"ì„¸ì…˜ '{session_id}' ì‚­ì œë¨"}
    return {"success": False, "error": "ì„¸ì…˜ ì—†ìŒ"}


# â˜… ì§€ì‹ë² ì´ìŠ¤ ì—­ì§ˆë¬¸ ì¶”ì²œ API
@router.post("/api/knowledge/suggest")
async def api_suggest_questions(request: dict):
    """ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì—­ì§ˆë¬¸ ì¶”ì²œ"""
    query = request.get("query", "")
    if not query:
        return {"success": False, "error": "ê²€ìƒ‰ì–´ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
    guide = generate_guided_questions(query)
    return guide


# â˜… ì§€ì‹ë² ì´ìŠ¤ API
@router.get("/api/knowledge")
async def api_list_knowledge():
    """ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ëª©ë¡"""
    files = list_knowledge()
    return {"success": True, "files": files, "count": len(files)}


@router.post("/api/knowledge/upload")
async def api_upload_knowledge(file: UploadFile = File(...)):
    """ì§€ì‹ ë¬¸ì„œ ì—…ë¡œë“œ (MD/TXT/PDF/Excel ì§€ì›, ë²„ì „ ê´€ë¦¬)"""
    allowed_ext = ('.md', '.txt', '.pdf', '.xlsx', '.xls', '.csv')
    ext = os.path.splitext(file.filename)[1].lower()

    if ext not in allowed_ext:
        return {"success": False, "error": f"ì§€ì› í˜•ì‹: {', '.join(allowed_ext)}"}

    try:
        content = await file.read()
        target_filename = file.filename

        # PDF/Excel â†’ MD ë³€í™˜
        if ext == '.pdf':
            try:
                import tempfile as _tf
                with _tf.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
                    tmp.write(content)
                    tmp_path = tmp.name
                try:
                    from PyPDF2 import PdfReader
                    reader = PdfReader(tmp_path)
                    text_parts = []
                    for i, page in enumerate(reader.pages):
                        page_text = page.extract_text()
                        if page_text:
                            text_parts.append(f"## í˜ì´ì§€ {i+1}\n\n{page_text}")
                    md_content = f"# {file.filename}\n\n" + "\n\n".join(text_parts)
                except ImportError:
                    return {"success": False, "error": "PyPDF2 ë¯¸ì„¤ì¹˜. pip install PyPDF2"}
                finally:
                    os.unlink(tmp_path)
                target_filename = os.path.splitext(file.filename)[0] + '.md'
                content = md_content.encode('utf-8')
            except Exception as e:
                return {"success": False, "error": f"PDF ë³€í™˜ ì‹¤íŒ¨: {e}"}

        elif ext in ('.xlsx', '.xls', '.csv'):
            try:
                import tempfile as _tf
                import io
                with _tf.NamedTemporaryFile(delete=False, suffix=ext) as tmp:
                    tmp.write(content)
                    tmp_path = tmp.name
                try:
                    if ext == '.csv':
                        df = pd.read_csv(tmp_path, encoding='utf-8', errors='ignore')
                    else:
                        df = pd.read_excel(tmp_path)
                    md_lines = [f"# {file.filename}\n"]
                    md_lines.append(f"- í–‰: {len(df):,}ê°œ, ì—´: {len(df.columns)}ê°œ\n")
                    md_lines.append("## ì»¬ëŸ¼ ëª©ë¡\n")
                    for col in df.columns:
                        dtype = str(df[col].dtype)
                        md_lines.append(f"- **{col}** ({dtype})")
                    md_lines.append("\n## ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 20í–‰)\n")
                    md_lines.append(df.head(20).to_markdown(index=False))
                    numeric_cols = df.select_dtypes(include='number').columns
                    if len(numeric_cols) > 0:
                        md_lines.append("\n## ìˆ˜ì¹˜ í†µê³„\n")
                        md_lines.append(df[numeric_cols].describe().to_markdown())
                    md_content = "\n".join(md_lines)
                except ImportError:
                    return {"success": False, "error": "pandas ë¯¸ì„¤ì¹˜. pip install pandas openpyxl tabulate"}
                finally:
                    os.unlink(tmp_path)
                target_filename = os.path.splitext(file.filename)[0] + '.md'
                content = md_content.encode('utf-8')
            except Exception as e:
                return {"success": False, "error": f"Excel/CSV ë³€í™˜ ì‹¤íŒ¨: {e}"}

        # â˜… ë²„ì „ ê´€ë¦¬: ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë²„ì „ ë°±ì—…
        filepath = os.path.join(KNOWLEDGE_DIR, target_filename)
        version_info = None
        if os.path.exists(filepath):
            ver_ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            base, fext = os.path.splitext(target_filename)
            backup_name = f"{base}_v{ver_ts}{fext}"
            backup_path = os.path.join(KNOWLEDGE_ARCHIVE_DIR, backup_name)
            shutil.copy2(filepath, backup_path)
            version_info = {"backup": backup_name, "message": "ê¸°ì¡´ ë²„ì „ì´ ê³¼ê±°ì§€ì‹ì— ë°±ì—…ë¨"}
            logger.info(f"ì§€ì‹ë¬¸ì„œ ë²„ì „ ë°±ì—…: {target_filename} â†’ {backup_name}")

        with open(filepath, 'wb') as f:
            f.write(content)

        # â˜… TF-IDF ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
        build_tfidf_index()

        result = {"success": True, "filename": target_filename, "size": len(content),
                  "original": file.filename, "converted": ext != os.path.splitext(target_filename)[1]}
        if version_info:
            result["version"] = version_info
        return result
    except Exception as e:
        return {"success": False, "error": str(e)}


@router.delete("/api/knowledge/{filename}")
async def api_delete_knowledge(filename: str):
    """ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ì‚­ì œ (ìë™ìœ¼ë¡œ ê³¼ê±°ì§€ì‹ì— ë°±ì—…)"""
    filepath = os.path.join(KNOWLEDGE_DIR, filename)
    if os.path.exists(filepath):
        # ì‚­ì œ ì „ ìë™ ë°±ì—…
        ver_ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        base, ext = os.path.splitext(filename)
        backup_name = f"{base}_del{ver_ts}{ext}"
        backup_path = os.path.join(KNOWLEDGE_ARCHIVE_DIR, backup_name)
        try:
            shutil.copy2(filepath, backup_path)
        except OSError:
            pass
        os.remove(filepath)
        # â˜… TF-IDF ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
        build_tfidf_index()
        return {"success": True, "message": f"'{filename}' ì‚­ì œë¨ (ë°±ì—…: {backup_name})"}
    return {"success": False, "error": "íŒŒì¼ ì—†ìŒ"}


@router.get("/api/knowledge/download/{filename}")
async def api_download_knowledge(filename: str):
    """ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ"""
    filepath = os.path.join(KNOWLEDGE_DIR, filename)
    if os.path.exists(filepath):
        return FileResponse(filepath, filename=filename, media_type="application/octet-stream")
    return JSONResponse(status_code=404, content={"error": "íŒŒì¼ ì—†ìŒ"})


# ========================================
# ê³¼ê±°ì§€ì‹ ë³´ê´€ì†Œ API
# ========================================
@router.get("/api/knowledge/archive")
async def api_list_archive():
    """ê³¼ê±°ì§€ì‹ ë¬¸ì„œ ëª©ë¡"""
    files = []
    try:
        for f in sorted(os.listdir(KNOWLEDGE_ARCHIVE_DIR)):
            if f.lower().endswith(('.md', '.txt')):
                filepath = os.path.join(KNOWLEDGE_ARCHIVE_DIR, f)
                size = os.path.getsize(filepath)
                modified = datetime.datetime.fromtimestamp(os.path.getmtime(filepath)).strftime("%Y-%m-%d %H:%M")
                size_str = f"{size / 1024:.1f}KB" if size > 1024 else f"{size}B"
                files.append({"filename": f, "size": size_str, "modified": modified})
    except Exception as e:
        logger.error(f"ê³¼ê±°ì§€ì‹ ëª©ë¡ ì˜¤ë¥˜: {e}")
    return {"success": True, "files": files, "count": len(files)}


@router.post("/api/knowledge/archive/{filename}")
async def api_archive_knowledge(filename: str):
    """ì§€ì‹ë² ì´ìŠ¤ â†’ ê³¼ê±°ì§€ì‹ìœ¼ë¡œ ì´ë™"""
    src = os.path.join(KNOWLEDGE_DIR, filename)
    dst = os.path.join(KNOWLEDGE_ARCHIVE_DIR, filename)
    if not os.path.exists(src):
        return {"success": False, "error": f"'{filename}' íŒŒì¼ ì—†ìŒ"}
    try:
        shutil.move(src, dst)
        return {"success": True, "message": f"'{filename}' â†’ ê³¼ê±°ì§€ì‹ìœ¼ë¡œ ì´ë™ë¨"}
    except Exception as e:
        return {"success": False, "error": str(e)}


@router.post("/api/knowledge/restore/{filename}")
async def api_restore_knowledge(filename: str):
    """ê³¼ê±°ì§€ì‹ â†’ ì§€ì‹ë² ì´ìŠ¤ë¡œ ë³µì›"""
    src = os.path.join(KNOWLEDGE_ARCHIVE_DIR, filename)
    dst = os.path.join(KNOWLEDGE_DIR, filename)
    if not os.path.exists(src):
        return {"success": False, "error": f"'{filename}' íŒŒì¼ ì—†ìŒ"}
    try:
        shutil.move(src, dst)
        return {"success": True, "message": f"'{filename}' â†’ ì§€ì‹ë² ì´ìŠ¤ë¡œ ë³µì›ë¨"}
    except Exception as e:
        return {"success": False, "error": str(e)}


@router.delete("/api/knowledge/archive/{filename}")
async def api_delete_archive(filename: str):
    """ê³¼ê±°ì§€ì‹ ë¬¸ì„œ ì™„ì „ ì‚­ì œ"""
    filepath = os.path.join(KNOWLEDGE_ARCHIVE_DIR, filename)
    if os.path.exists(filepath):
        os.remove(filepath)
        return {"success": True, "message": f"'{filename}' ì™„ì „ ì‚­ì œë¨"}
    return {"success": False, "error": "íŒŒì¼ ì—†ìŒ"}


# â˜… ì§€ì‹ë¬¸ì„œ ë²„ì „ íˆìŠ¤í† ë¦¬ ì¡°íšŒ
@router.get("/api/knowledge/versions/{filename}")
async def api_knowledge_versions(filename: str):
    """íŠ¹ì • ë¬¸ì„œì˜ ë²„ì „ íˆìŠ¤í† ë¦¬ (ì•„ì¹´ì´ë¸Œì—ì„œ ê°™ì€ ì´ë¦„ì˜ ë°±ì—… ì°¾ê¸°)"""
    base = os.path.splitext(filename)[0]
    versions = []
    try:
        for f in sorted(os.listdir(KNOWLEDGE_ARCHIVE_DIR), reverse=True):
            if f.startswith(base) and f != filename and not f.endswith(".meta"):
                filepath = os.path.join(KNOWLEDGE_ARCHIVE_DIR, f)
                size = os.path.getsize(filepath)
                modified = datetime.datetime.fromtimestamp(os.path.getmtime(filepath)).strftime("%Y-%m-%d %H:%M")
                # ë©”ëª¨ ë©”íƒ€ë°ì´í„° ì½ê¸°
                memo = ""
                meta_path = filepath + ".meta"
                if os.path.exists(meta_path):
                    try:
                        with open(meta_path, 'r', encoding='utf-8') as mf:
                            meta = json.load(mf)
                            memo = meta.get("memo", "")
                    except Exception:
                        pass
                versions.append({"filename": f, "size": f"{size / 1024:.1f}KB", "modified": modified, "memo": memo})
    except OSError:
        pass
    return {"success": True, "document": filename, "versions": versions, "count": len(versions)}


@router.post("/api/knowledge/versions/{filename}")
async def api_create_version(filename: str, data: dict = None):
    """í˜„ì¬ ë¬¸ì„œì˜ ìŠ¤ëƒ…ìƒ·ì„ ìˆ˜ë™ìœ¼ë¡œ ë²„ì „ ì €ì¥"""
    filepath = os.path.join(KNOWLEDGE_DIR, filename)
    if not os.path.exists(filepath):
        return {"success": False, "error": f"íŒŒì¼ ì—†ìŒ: {filename}"}

    memo = data.get("memo", "") if data else ""
    ver_ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    base, ext = os.path.splitext(filename)
    backup_name = f"{base}_v{ver_ts}{ext}"
    backup_path = os.path.join(KNOWLEDGE_ARCHIVE_DIR, backup_name)

    try:
        shutil.copy2(filepath, backup_path)
        # ë©”ëª¨ê°€ ìˆìœ¼ë©´ ë©”íƒ€ íŒŒì¼ë¡œ ì €ì¥
        if memo:
            meta_path = backup_path + ".meta"
            with open(meta_path, 'w', encoding='utf-8') as mf:
                json.dump({"memo": memo, "created": ver_ts, "source": filename}, mf, ensure_ascii=False)
        logger.info(f"ğŸ“‹ ìˆ˜ë™ ë²„ì „ ì €ì¥: {filename} â†’ {backup_name} (ë©”ëª¨: {memo})")
        return {"success": True, "version": backup_name, "memo": memo}
    except Exception as e:
        return {"success": False, "error": str(e)}


# ========================================
# â˜… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ & ë„ë©”ì¸ ì§€ì‹ í¸ì§‘ API
# ========================================
@router.get("/api/prompt/system")
async def api_get_system_prompt():
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¡°íšŒ"""
    content = load_prompt_file(SYSTEM_PROMPT_FILE, DEFAULT_SYSTEM_PROMPT)
    return {
        "success": True,
        "content": content,
        "filepath": SYSTEM_PROMPT_FILE,
        "char_count": len(content)
    }


@router.post("/api/prompt/system")
async def api_save_system_prompt(request: dict):
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥"""
    content = request.get("content", "")
    if not content.strip():
        return {"success": False, "error": "ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}
    success = save_prompt_file(SYSTEM_PROMPT_FILE, content)
    if success:
        return {"success": True, "message": "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ", "char_count": len(content)}
    return {"success": False, "error": "ì €ì¥ ì‹¤íŒ¨"}


@router.post("/api/prompt/system/reset")
async def api_reset_system_prompt():
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸°ë³¸ê°’ ë³µì›"""
    success = save_prompt_file(SYSTEM_PROMPT_FILE, DEFAULT_SYSTEM_PROMPT)
    if success:
        return {"success": True, "message": "ê¸°ë³¸ê°’ìœ¼ë¡œ ë³µì›ë¨", "content": DEFAULT_SYSTEM_PROMPT}
    return {"success": False, "error": "ë³µì› ì‹¤íŒ¨"}


@router.get("/api/prompt/domain")
async def api_get_domain_knowledge():
    """ë„ë©”ì¸ ì§€ì‹ ì¡°íšŒ"""
    content = load_prompt_file(DOMAIN_KNOWLEDGE_FILE, DEFAULT_DOMAIN_KNOWLEDGE)
    active_lines = [l for l in content.strip().split('\n')
                    if l.strip() and not l.strip().startswith('#')]
    return {
        "success": True,
        "content": content,
        "filepath": DOMAIN_KNOWLEDGE_FILE,
        "char_count": len(content),
        "active_lines": len(active_lines)
    }


@router.post("/api/prompt/domain")
async def api_save_domain_knowledge(request: dict):
    """ë„ë©”ì¸ ì§€ì‹ ì €ì¥"""
    content = request.get("content", "")
    success = save_prompt_file(DOMAIN_KNOWLEDGE_FILE, content)
    if success:
        active_lines = [l for l in content.strip().split('\n')
                        if l.strip() and not l.strip().startswith('#')]
        return {
            "success": True,
            "message": "ë„ë©”ì¸ ì§€ì‹ ì €ì¥ ì™„ë£Œ",
            "char_count": len(content),
            "active_lines": len(active_lines)
        }
    return {"success": False, "error": "ì €ì¥ ì‹¤íŒ¨"}


@router.post("/api/prompt/domain/reset")
async def api_reset_domain_knowledge():
    """ë„ë©”ì¸ ì§€ì‹ ê¸°ë³¸ í…œí”Œë¦¿ ë³µì›"""
    success = save_prompt_file(DOMAIN_KNOWLEDGE_FILE, DEFAULT_DOMAIN_KNOWLEDGE)
    if success:
        return {"success": True, "message": "ê¸°ë³¸ í…œí”Œë¦¿ìœ¼ë¡œ ë³µì›ë¨", "content": DEFAULT_DOMAIN_KNOWLEDGE}
    return {"success": False, "error": "ë³µì› ì‹¤íŒ¨"}


@router.get("/api/prompt/preview")
async def api_preview_effective_prompt():
    """í˜„ì¬ í•©ì„±ëœ ìµœì¢… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°"""
    effective = get_effective_system_prompt()
    return {
        "success": True,
        "effective_prompt": effective,
        "total_chars": len(effective)
    }


# â˜… ë§ˆê¸°(main1_First) ì½”ë”© ì—ì´ì „íŠ¸ ë¼ìš°í„° ì—°ê²° (ëª¨ë“ˆ ë¡œë“œ ì‹œì ì— ë“±ë¡)
try:
    from coding_agent import agent_router
    app.include_router(agent_router)
    print("âš¡ ë§ˆê¸°(main1_First) ì½”ë”© ì—ì´ì „íŠ¸ ë¼ìš°í„° ì—°ê²° ì™„ë£Œ")
except ImportError as e:
    print(f"âš ï¸ coding_agent.py ì—†ìŒ â†’ ë§ˆê¸°(main1_First) ëª¨ë“œ ë¹„í™œì„±: {e}")
except Exception as e:
    print(f"âš ï¸ ë§ˆê¸°(main1_First) ë¡œë“œ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    import uvicorn
    app.include_router(router)

    @app.on_event("startup")
    async def standalone_startup():
        init_assistant()

    uvicorn.run(app, host="0.0.0.0", port=10002)