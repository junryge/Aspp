# -*- coding: utf-8 -*-
"""
================================================================================
V10_4 ML ì˜ˆì¸¡ ëª¨ë¸ - ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì½”ë“œ (30ë¶„ ì˜ˆì¸¡)
V10_3 ë°©ì‹ (XGBoost íšŒê·€ + LightGBM ë¶„ë¥˜ + íˆ¬í‘œ)
ì˜ˆì¸¡ê°’: XGB_íƒ€ê²Ÿ
================================================================================
"""

import os
import pickle
import warnings
import gc
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

warnings.filterwarnings('ignore')

# ============================================================================
# ì„¤ì •
# ============================================================================
CONFIG = {
    'model_file': 'models/v10_4_30min_m14_model.pkl',
    'sequence_length': 280,
    'prediction_offset': 30,
    'limit_value': 1700,
    'target_column': 'TOTALCNT',
}

# ============================================================================
# ëª¨ë¸ ë¡œë“œ
# ============================================================================
def load_model():
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
    print("=" * 70)
    print("ğŸš€ V10_4 ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - 30ë¶„ ì˜ˆì¸¡")
    print("   V10_3 ë°©ì‹ (XGB íšŒê·€ + LGBM ë¶„ë¥˜ + íˆ¬í‘œ)")
    print("   ì˜ˆì¸¡ê°’: XGB_íƒ€ê²Ÿ")
    print("=" * 70)
    
    if not os.path.exists(CONFIG['model_file']):
        raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {CONFIG['model_file']}")
    
    with open(CONFIG['model_file'], 'rb') as f:
        model_data = pickle.load(f)
    
    training_info = model_data.get('training_info', {})
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print(f"  - ëª¨ë¸ ë²„ì „: {training_info.get('version', 'V10_4')}")
    print(f"  - í•™ìŠµì¼: {training_info.get('train_date', 'N/A')}")
    print(f"  - ëª¨ë¸ ìˆ˜: {len(model_data['models'])}ê°œ")
    
    return model_data

# ============================================================================
# Feature ìƒì„± í•¨ìˆ˜ (í‰ê°€ ì½”ë“œì™€ ë™ì¼)
# ============================================================================
def create_sequence_features(df, feature_cols, seq_len, idx, limit_val=1700):
    features = []
    for col in feature_cols:
        seq = df[col].iloc[idx - seq_len:idx].values
        current_val = seq[-1]
        features.extend([
            np.mean(seq), np.std(seq), np.min(seq), np.max(seq), current_val,
            seq[-1] - seq[0],
            np.percentile(seq, 25), np.percentile(seq, 75),
            np.mean(seq[-10:]) - np.mean(seq[:10]),
            np.max(seq[-30:]) if len(seq) >= 30 else np.max(seq),
            seq[-1] - seq[-10] if len(seq) >= 10 else 0,
            seq[-1] - seq[-30] if len(seq) >= 30 else 0,
            seq[-1] - seq[-60] if len(seq) >= 60 else 0,
            (seq[-1] - seq[-10]) / 10 if len(seq) >= 10 else 0,
            (seq[-1] - seq[-30]) / 30 if len(seq) >= 30 else 0,
            np.max(seq[-10:]) - np.min(seq[-10:]) if len(seq) >= 10 else 0,
            np.max(seq[-30:]) - np.min(seq[-30:]) if len(seq) >= 30 else 0,
            limit_val - current_val,
            1 if current_val >= 1500 else 0,
            1 if current_val >= 1600 else 0,
        ])
    return features

# ============================================================================
# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================================
def preprocess_data(df, feature_groups):
    """ë°ì´í„° ì „ì²˜ë¦¬ (í‰ê°€ ì½”ë“œì™€ ë™ì¼)"""
    df = df.copy()
    
    df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), format='%Y%m%d%H%M', errors='coerce')
    df = df.dropna(subset=['CURRTIME']).sort_values('CURRTIME').reset_index(drop=True)
    
    if 'M14.QUE.ALL.CURRENTQCREATED' in df.columns and 'M14.QUE.ALL.CURRENTQCOMPLETED' in df.columns:
        df['QUEUE_GAP'] = df['M14.QUE.ALL.CURRENTQCREATED'] - df['M14.QUE.ALL.CURRENTQCOMPLETED']
    
    # ì—†ëŠ” ì»¬ëŸ¼ì€ 0ìœ¼ë¡œ ìƒì„± (feature ê°œìˆ˜ ë§ì¶”ê¸° ìœ„í•´ í•„ìˆ˜!)
    all_cols = []
    for group in feature_groups.values():
        all_cols.extend(group)
    for col in list(set(all_cols)):
        if col not in df.columns:
            df[col] = 0
            print(f"  âš  ì»¬ëŸ¼ ì—†ìŒ, 0ìœ¼ë¡œ ìƒì„±: {col}")
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
    
    return df

# ============================================================================
# ë‹¨ì¼ ì‹œì  ì˜ˆì¸¡ í•¨ìˆ˜ (í‰ê°€ ì½”ë“œ ë¡œì§ ê·¸ëŒ€ë¡œ)
# ============================================================================
def predict_single(df, idx, model_data):
    """
    ë‹¨ì¼ ì‹œì ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰
    Returns: dict (ì˜ˆì¸¡ê°’ = XGB_íƒ€ê²Ÿ)
    """
    models = model_data['models']
    scalers = model_data['scalers']
    FEATURE_GROUPS = model_data['feature_groups']
    
    seq_len = CONFIG['sequence_length']
    pred_offset = CONFIG['prediction_offset']
    limit_val = CONFIG['limit_value']
    target_col = CONFIG['target_column']
    
    if idx < seq_len:
        raise ValueError(f"ì¸ë±ìŠ¤ {idx}ëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´ {seq_len}ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤.")
    
    current_time = df['CURRTIME'].iloc[idx - 1]
    current_total = df[target_col].iloc[idx - 1]
    prediction_time = current_time + timedelta(minutes=pred_offset)
    
    feat_target = create_sequence_features(df, FEATURE_GROUPS['target'], seq_len, idx)
    feat_important = create_sequence_features(df, FEATURE_GROUPS['important'], seq_len, idx)
    feat_auxiliary = create_sequence_features(df, FEATURE_GROUPS['auxiliary'], seq_len, idx)
    
    X_target = scalers['target'].transform([feat_target])
    X_important = scalers['important'].transform([feat_important])
    X_auxiliary = scalers['auxiliary'].transform([feat_auxiliary])
    
    pred_xgb_target = models['xgb_target'].predict(X_target)[0]
    pred_xgb_important = models['xgb_important'].predict(X_important)[0]
    pred_xgb_auxiliary = models['xgb_auxiliary'].predict(X_auxiliary)[0]
    
    pred_lgb_target = models['lgb_target'].predict(X_target)[0]
    pred_lgb_important = models['lgb_important'].predict(X_important)[0]
    pred_lgb_auxiliary = models['lgb_auxiliary'].predict(X_auxiliary)[0]
    
    prob_lgb_target = models['lgb_target'].predict_proba(X_target)[0][1]
    prob_lgb_important = models['lgb_important'].predict_proba(X_important)[0][1]
    prob_lgb_auxiliary = models['lgb_auxiliary'].predict_proba(X_auxiliary)[0][1]
    
    pred_xgb_pdt, pred_lgb_pdt, prob_lgb_pdt = None, None, None
    if 'xgb_pdt_new' in models and 'pdt_new' in scalers:
        feat_pdt = create_sequence_features(df, FEATURE_GROUPS.get('pdt_new', []), seq_len, idx)
        if feat_pdt:
            X_pdt = scalers['pdt_new'].transform([feat_pdt])
            pred_xgb_pdt = models['xgb_pdt_new'].predict(X_pdt)[0]
            pred_lgb_pdt = models['lgb_pdt_new'].predict(X_pdt)[0]
            prob_lgb_pdt = models['lgb_pdt_new'].predict_proba(X_pdt)[0][1]
    
    # íˆ¬í‘œ (V10_3 ë°©ì‹)
    votes = [
        1 if pred_xgb_target >= limit_val else 0,
        1 if pred_xgb_important >= limit_val else 0,
        1 if pred_xgb_auxiliary >= limit_val else 0,
        pred_lgb_target,
        pred_lgb_important,
        pred_lgb_auxiliary,
    ]
    
    if pred_xgb_pdt is not None:
        votes.append(1 if pred_xgb_pdt >= limit_val else 0)
        votes.append(pred_lgb_pdt)
    
    vote_sum = sum(votes)
    total_votes = len(votes)
    
    # ìµœì¢… íŒì • ê·œì¹™ (V10_3 ë°©ì‹)
    rule1 = vote_sum >= 3
    rule2 = (prob_lgb_important >= 0.50) and (current_total >= 1450)
    rule3 = (pred_xgb_important >= 1680) and (current_total >= 1500)
    rule4 = (current_total >= 1600) and (vote_sum >= 2)
    rule5 = (pred_xgb_important >= 1700)
    
    final_pred_danger = 1 if (rule1 or rule2 or rule3 or rule4 or rule5) else 0
    
    if pred_xgb_pdt is not None:
        ensemble_pred = (pred_xgb_target + pred_xgb_important + pred_xgb_auxiliary + pred_xgb_pdt) / 4
    else:
        ensemble_pred = (pred_xgb_target + pred_xgb_important + pred_xgb_auxiliary) / 3
    
    return {
        'í˜„ì¬ì‹œê°„': current_time.strftime('%Y-%m-%d %H:%M'),
        'í˜„ì¬TOTALCNT': round(current_total, 2),
        'ì˜ˆì¸¡ì‹œì ': prediction_time.strftime('%Y-%m-%d %H:%M'),
        'ì˜ˆì¸¡ê°’': round(pred_xgb_target, 2),
        'XGB_íƒ€ê²Ÿ': round(pred_xgb_target, 2),
        'XGB_ì¤‘ìš”': round(pred_xgb_important, 2),
        'XGB_ë³´ì¡°': round(pred_xgb_auxiliary, 2),
        'XGB_PDT': round(pred_xgb_pdt, 2) if pred_xgb_pdt else None,
        'LGBM_ì¤‘ìš”_í™•ë¥ ': round(prob_lgb_important, 3),
        'ì•™ìƒë¸”ì˜ˆì¸¡': round(ensemble_pred, 2),
        f'íˆ¬í‘œ({total_votes}ê°œì¤‘)': vote_sum,
        'ìµœì¢…íŒì •': final_pred_danger,
        'ìœ„í—˜ì—¬ë¶€': 'ğŸ”´ ìœ„í—˜' if final_pred_danger == 1 else 'ğŸŸ¢ ì•ˆì „',
    }

# ============================================================================
# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
# ============================================================================
def realtime_monitoring(data_file, model_data):
    print(f"\nğŸ“¡ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘ - 30ë¶„ ì˜ˆì¸¡")
    print("-" * 70)
    
    try:
        df = pd.read_csv(data_file, encoding='utf-8', on_bad_lines='skip')
    except:
        try:
            df = pd.read_csv(data_file, encoding='cp949', on_bad_lines='skip')
        except:
            df = pd.read_csv(data_file, encoding='euc-kr', on_bad_lines='skip')
    
    FEATURE_GROUPS = model_data['feature_groups'].copy()
    df = preprocess_data(df, FEATURE_GROUPS)
    model_data['feature_groups'] = FEATURE_GROUPS
    
    seq_len = CONFIG['sequence_length']
    print(f"ë°ì´í„°: {len(df):,}í–‰, ì‹œì‘ ì¸ë±ìŠ¤: {seq_len}")
    print("-" * 70)
    
    for idx in range(seq_len, len(df)):
        try:
            result = predict_single(df, idx, model_data)
            print(f"\n[{result['í˜„ì¬ì‹œê°„']}] í˜„ì¬: {result['í˜„ì¬TOTALCNT']:.0f}")
            print(f"  ì˜ˆì¸¡ê°’(XGB_íƒ€ê²Ÿ): {result['ì˜ˆì¸¡ê°’']:.0f}, LGBMí™•ë¥ : {result['LGBM_ì¤‘ìš”_í™•ë¥ ']:.1%}")
            print(f"  â–¶ {result['ìœ„í—˜ì—¬ë¶€']}")
            if result['ìµœì¢…íŒì •'] == 1:
                print("  ğŸš¨ 30ë¶„ ë‚´ 1700 ì´ˆê³¼ ì˜ˆìƒ!")
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    model_data = load_model()
    data_file = input("\në°ì´í„° íŒŒì¼ ê²½ë¡œ: ").strip()
    if data_file and os.path.exists(data_file):
        realtime_monitoring(data_file, model_data)
    else:
        print("âŒ íŒŒì¼ ì—†ìŒ")