#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
M14 íˆìŠ¤í† ë¦¬ ë°ì´í„° LLM ë¶„ì„ ì„œë²„ (v1.0)
- M14 íˆìŠ¤í† ë¦¬ ë°ì´í„° ë¡œë“œ
- LLMì„ í†µí•œ ë°ì´í„° ë¶„ì„
- SK Hynix API + ë¡œì»¬ GGUF ì§€ì›
"""

import os
import re
import requests
import pandas as pd
from fastapi import FastAPI, UploadFile, File, Form, Request
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List
from datetime import datetime, timedelta
from io import StringIO
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="M14 LLM Analysis")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ========================================
# Global Variables
# ========================================
API_TOKEN = None
llm = None  # ë¡œì»¬ LLM (GGUF)

# LLM ëª¨ë“œ: "api" ë˜ëŠ” "local"
LLM_MODE = "local"

# í™˜ê²½ ì„¤ì •
ENV_MODE = "dev"

# ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ
LOCAL_MODEL_PATH = "Qwen3-14B-Q4_K_M.gguf"

# M14 ë°ì´í„° í´ë” ê²½ë¡œ (ë‚˜ì¤‘ì— ì„¤ì • ê°€ëŠ¥)
M14_DATA_DIR = "../NODE/UI/data"

ENV_CONFIG = {
    "dev": {
        "url": "http://dev.assistant.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Coder-30B-A3B-Instruct",
        "name": "ê°œë°œ(30B)"
    },
    "prod": {
        "url": "http://summary.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Next-80B-A3B-Instruct",
        "name": "ìš´ì˜(80B)"
    }
}

API_URL = ENV_CONFIG["dev"]["url"]
API_MODEL = ENV_CONFIG["dev"]["model"]

# ========================================
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í¸ì§‘ ê°€ëŠ¥)
# ========================================
SYSTEM_PROMPT_ANALYSIS = """ë‹¹ì‹ ì€ M14 ë°˜ì†¡ í ì˜ˆì¸¡ ëª¨ë¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ë¶„ì„ ì›ì¹™
1. **ê²°ë¡  ë¨¼ì €** - í•µì‹¬ ìˆ˜ì¹˜ì™€ íŒë‹¨ì„ ë¨¼ì € ì œì‹œ
2. **ê°„ê²°í•˜ê²Œ** - ë¶ˆí•„ìš”í•œ ì„¤ëª… ì—†ì´ í•µì‹¬ë§Œ
3. **ìˆ«ì ì¤‘ì‹¬** - ì •í™•í•œ ìˆ˜ì¹˜ì™€ ë¹„ìœ¨ ì œì‹œ
4. **ë¬¸ì œì  ëª…í™•íˆ** - ë¬´ì—‡ì´ ë¬¸ì œì¸ì§€ ì§ì ‘ì ìœ¼ë¡œ
5. **ê°œì„ ì•ˆ êµ¬ì²´ì ** - ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆ

## ì¶œë ¥ í˜•ì‹
- í…Œì´ë¸”ì€ ë§ˆí¬ë‹¤ìš´ í˜•ì‹, ìµœì†Œí•œì˜ ì»¬ëŸ¼ë§Œ
- ì´ëª¨ì§€ ì ì ˆíˆ ì‚¬ìš© (ğŸ“Š âœ… âš ï¸ ğŸ”´ ğŸ’¡)
- ì„¹ì…˜ì€ ## í—¤ë”ë¡œ êµ¬ë¶„
- ê¸´ ì„¤ëª… ëŒ€ì‹  í•µì‹¬ bullet point

## M14 ê¸°ì¤€ê°’
- ìœ„í—˜ ì„ê³„ê°’: TOTALCNT >= 1700
- ê²½ê³  ì„ê³„ê°’: TOTALCNT >= 1600
- ì˜ˆì¸¡ ëª¨ë¸: XGB_íƒ€ê²Ÿ, XGB_ì¤‘ìš”, XGB_ë³´ì¡°, XGB_PDT, XGB_Job
- ì„±ëŠ¥ ì§€í‘œ: TP(ì ì¤‘), TN(ì •ìƒì˜ˆì¸¡), FP(ì˜¤íƒ), FN(ë†“ì¹¨)"""

# ë¶„ì„ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
PROMPT_TEMPLATES = {
    "summary": """ìœ„ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥:
## ğŸ“Š í•µì‹¬ ìš”ì•½
- ì •í™•ë„, ì¬í˜„ìœ¨, ì •ë°€ë„ (í…Œì´ë¸”)
- TP/TN/FP/FN ìˆ˜ì¹˜
  - TP(ì ì¤‘): 1700+ ì˜ˆì¸¡ â†’ ì‹¤ì œ 1700+
  - TN(ì •ìƒ): ì •ìƒ ì˜ˆì¸¡ â†’ ì‹¤ì œ ì •ìƒ
  - FP(ì˜¤íƒ): 1700+ ì˜ˆì¸¡ â†’ ì‹¤ì œ ì •ìƒ
  - FN(ë†“ì¹¨): ì •ìƒ ì˜ˆì¸¡ â†’ ì‹¤ì œ 1700+

## âœ… ì¢‹ì€ ì 
- (1-2ê°œ)

## âš ï¸ ë¬¸ì œì 
- í•µì‹¬ ë¬¸ì œ (ìˆ˜ì¹˜ í¬í•¨)
- ì›ì¸ ë¶„ì„

## ğŸ’¡ ê°œì„  ì œì•ˆ
- êµ¬ì²´ì  ì¡°ì¹˜ (ìˆ˜ì¹˜ ê¸°ì¤€ í¬í•¨)""",

    "pattern": """ì‹œê°„ëŒ€ë³„/ì¼ë³„ íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥:
## ğŸ“ˆ íŒ¨í„´ ë¶„ì„
- í”¼í¬ ì‹œê°„ëŒ€ (ëª‡ì‹œ, í‰ê· ê°’)
- ì•ˆì • ì‹œê°„ëŒ€ (ëª‡ì‹œ, í‰ê· ê°’)

## âš ï¸ ì£¼ì˜ ì‹œê°„ëŒ€
- 1700+ ìì£¼ ë°œìƒí•˜ëŠ” ì‹œê°„
- íŒ¨í„´ì˜ ì›ì¸ ì¶”ì •

## ğŸ’¡ ìš´ì˜ ì œì•ˆ
- í”¼í¬ ì‹œê°„ ëŒ€ì‘ ë°©ì•ˆ
- ëª¨ë‹ˆí„°ë§ ê°•í™” ì‹œê°„ëŒ€""",

    "prediction": """ì˜ˆì¸¡ ëª¨ë¸ ì„±ëŠ¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥:
## ğŸ¯ ì˜ˆì¸¡ ì„±ëŠ¥
| ì§€í‘œ | ê°’ |
- ì •í™•ë„, ì¬í˜„ìœ¨, ì •ë°€ë„

## âš ï¸ ì˜ˆì¸¡ ë¬¸ì œì 
- FN (ë†“ì¹¨): ì›ì¸ ë¶„ì„
- FP (ì˜¤íƒ): ì›ì¸ ë¶„ì„

## ğŸ’¡ ëª¨ë¸ ê°œì„  ë°©í–¥
- threshold ì¡°ì • ì œì•ˆ
- ì¬í•™ìŠµ í•„ìš” ì—¬ë¶€"""
}


# ========================================
# API Token ê´€ë¦¬
# ========================================
def load_api_token():
    """API í† í° ë¡œë“œ"""
    global API_TOKEN
    
    token_paths = ["token.txt", "../token.txt", os.path.expanduser("~/token.txt")]
    
    for token_path in token_paths:
        if os.path.exists(token_path):
            try:
                with open(token_path, "r", encoding='utf-8') as f:
                    API_TOKEN = f.read().strip()
                if API_TOKEN and "REPLACE" not in API_TOKEN:
                    logger.info(f"âœ… API í† í° ë¡œë“œ ì™„ë£Œ: {token_path}")
                    return True
            except Exception as e:
                logger.error(f"âŒ í† í° ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    logger.warning("âš ï¸ í† í° íŒŒì¼ ì—†ìŒ")
    return False


# ========================================
# ë¡œì»¬ LLM ë¡œë“œ (GGUF)
# ========================================
def load_local_llm():
    """ë¡œì»¬ GGUF ëª¨ë¸ ë¡œë“œ"""
    global llm
    
    # ì—¬ëŸ¬ ê²½ë¡œì—ì„œ ëª¨ë¸ ì°¾ê¸°
    model_paths = [
        LOCAL_MODEL_PATH,
        f"../{LOCAL_MODEL_PATH}",
        f"../../{LOCAL_MODEL_PATH}",
        os.path.expanduser(f"~/{LOCAL_MODEL_PATH}")
    ]
    
    model_path = None
    for path in model_paths:
        if os.path.exists(path):
            model_path = path
            break
    
    if model_path is None:
        logger.warning(f"âš ï¸ ë¡œì»¬ ëª¨ë¸ ì—†ìŒ: {LOCAL_MODEL_PATH}")
        return False
    
    try:
        from llama_cpp import Llama
        logger.info(f"ğŸ“¦ ë¡œì»¬ LLM ë¡œë”© ì¤‘: {model_path}")
        llm = Llama(
            model_path=model_path,
            n_ctx=32768,
            n_gpu_layers=-1,
            verbose=False
        )
        logger.info("âœ… ë¡œì»¬ LLM ë¡œë“œ ì™„ë£Œ!")
        return True
    except ImportError:
        logger.warning("âš ï¸ llama-cpp-python ë¯¸ì„¤ì¹˜")
        return False
    except Exception as e:
        logger.error(f"âŒ ë¡œì»¬ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


# ========================================
# LLM í˜¸ì¶œ í•¨ìˆ˜ë“¤
# ========================================
def call_local_llm(prompt: str, system_prompt: str = "", max_tokens: int = 2000) -> dict:
    """ë¡œì»¬ GGUF ëª¨ë¸ í˜¸ì¶œ"""
    global llm
    
    if llm is None:
        return {"success": False, "error": "ë¡œì»¬ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    
    try:
        if not system_prompt:
            system_prompt = SYSTEM_PROMPT_ANALYSIS
        
        formatted_prompt = f"""<|im_start|>system
{system_prompt}
<|im_end|>
<|im_start|>user
{prompt}
<|im_end|>
<|im_start|>assistant
"""
        
        logger.info("ğŸ–¥ï¸ ë¡œì»¬ LLM í˜¸ì¶œ ì¤‘...")
        response = llm(
            formatted_prompt,
            max_tokens=max_tokens,
            temperature=0.3,
            stop=["<|im_end|>", "\n\n\n"]
        )
        
        content = response['choices'][0]['text'].strip()
        
        # <think> íƒœê·¸ ì œê±°
        content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
        content = re.sub(r'<think>.*', '', content, flags=re.DOTALL)
        content = content.strip()
        
        return {"success": True, "content": content}
        
    except Exception as e:
        logger.error(f"âŒ ë¡œì»¬ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": f"ë¡œì»¬ LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"}


def stream_local_llm(prompt: str, system_prompt: str = "", max_tokens: int = 2000):
    """ë¡œì»¬ LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ (Generator)"""
    global llm
    
    if llm is None:
        yield "data: [ERROR] ë¡œì»¬ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n"
        return
    
    try:
        formatted_prompt = f"""<|im_start|>system
{system_prompt}
<|im_end|>
<|im_start|>user
{prompt}
<|im_end|>
<|im_start|>assistant
"""
        
        logger.info("ğŸ–¥ï¸ ë¡œì»¬ LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘...")
        
        in_think = False
        buffer = ""
        
        for chunk in llm(
            formatted_prompt,
            max_tokens=max_tokens,
            temperature=0.3,
            stop=["<|im_end|>", "\n\n\n"],
            stream=True
        ):
            token = chunk['choices'][0]['text']
            buffer += token
            
            # <think> íƒœê·¸ ì²˜ë¦¬
            if '<think>' in buffer and not in_think:
                in_think = True
                # <think> ì´ì „ í…ìŠ¤íŠ¸ ì „ì†¡
                before_think = buffer.split('<think>')[0]
                if before_think:
                    safe_before = before_think.replace('\n', 'â')
                    yield f"data: {safe_before}\n\n"
                buffer = buffer.split('<think>', 1)[1] if '<think>' in buffer else ""
                continue
            
            if in_think:
                if '</think>' in buffer:
                    in_think = False
                    buffer = buffer.split('</think>', 1)[1] if '</think>' in buffer else ""
                continue
            
            # ì •ìƒ í† í° ì „ì†¡ (ì¤„ë°”ê¿ˆì„ íŠ¹ìˆ˜ ë¬¸ìë¡œ ì¹˜í™˜)
            if token and not in_think:
                safe_token = token.replace('\n', 'â')
                yield f"data: {safe_token}\n\n"
        
        yield "data: [DONE]\n\n"
        
    except Exception as e:
        logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
        yield f"data: [ERROR] {str(e)}\n\n"


def call_llm_api(prompt: str, system_prompt: str = "", max_tokens: int = 4000) -> dict:
    """LLM API í˜¸ì¶œ"""
    global API_TOKEN
    
    if not API_TOKEN:
        return {"success": False, "error": "API í† í°ì´ ì—†ìŠµë‹ˆë‹¤."}
    
    headers = {
        "Authorization": f"Bearer {API_TOKEN}",
        "Content-Type": "application/json"
    }
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    data = {
        "model": API_MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": 0.3
    }
    
    try:
        logger.info(f"ğŸš€ API í˜¸ì¶œ ì¤‘...")
        response = requests.post(API_URL, headers=headers, json=data, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
            return {"success": True, "content": content.strip()}
        else:
            return {"success": False, "error": f"API ì˜¤ë¥˜: {response.status_code}"}
            
    except Exception as e:
        return {"success": False, "error": f"API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"}


def call_llm(prompt: str, system_prompt: str = "", max_tokens: int = 4000) -> dict:
    """í†µí•© LLM í˜¸ì¶œ"""
    global LLM_MODE
    
    if LLM_MODE == "api":
        result = call_llm_api(prompt, system_prompt, max_tokens)
        if not result["success"] and llm is not None:
            logger.info("âš ï¸ API ì‹¤íŒ¨ â†’ ë¡œì»¬ í´ë°±")
            result = call_local_llm(prompt, system_prompt, min(max_tokens, 2000))
        return result
    else:
        if llm is None:
            return {"success": False, "error": "ë¡œì»¬ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        return call_local_llm(prompt, system_prompt, min(max_tokens, 2000))


# ========================================
# M14 ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
# ========================================
def load_m14_data(date_str: str) -> dict:
    """M14 ë°ì´í„° ë¡œë“œ"""
    data_file = os.path.join(M14_DATA_DIR, f'm14_data_{date_str}.csv')
    pred_file = os.path.join(M14_DATA_DIR, f'm14_pred_{date_str}.csv')
    alert_file = os.path.join(M14_DATA_DIR, f'm14_alert_{date_str}.csv')
    
    result = {
        "date": date_str,
        "data": None,
        "alerts_10": [],
        "alerts_30": [],
        "stats": {}
    }
    
    # ë°ì´í„° íŒŒì¼ ë¡œë“œ
    if os.path.exists(pred_file):
        try:
            df = pd.read_csv(pred_file)
            result["data"] = df.to_dict('records')
            
            # ê¸°ë³¸ í†µê³„
            if 'TOTALCNT' in df.columns:
                result["stats"] = {
                    "count": len(df),
                    "avg": round(df['TOTALCNT'].mean(), 1),
                    "max": int(df['TOTALCNT'].max()),
                    "min": int(df['TOTALCNT'].min()),
                    "over_1700": int((df['TOTALCNT'] >= 1700).sum()),
                    "over_1600": int((df['TOTALCNT'] >= 1600).sum())
                }
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    elif os.path.exists(data_file):
        try:
            df = pd.read_csv(data_file)
            result["data"] = df.to_dict('records')
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ì•ŒëŒ íŒŒì¼ ë¡œë“œ
    if os.path.exists(alert_file):
        try:
            df_alert = pd.read_csv(alert_file)
            df_alert['TYPE'] = df_alert['TYPE'].astype(str)
            result["alerts_10"] = df_alert[df_alert['TYPE'] == '10'].to_dict('records')
            result["alerts_30"] = df_alert[df_alert['TYPE'] == '30'].to_dict('records')
        except Exception as e:
            logger.error(f"ì•ŒëŒ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return result


def get_available_dates() -> List[str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ëª©ë¡"""
    dates = []
    
    if not os.path.exists(M14_DATA_DIR):
        return dates
    
    for f in os.listdir(M14_DATA_DIR):
        if f.startswith('m14_data_') and f.endswith('.csv'):
            date_str = f.replace('m14_data_', '').replace('.csv', '')
            if len(date_str) == 8 and date_str.isdigit():
                dates.append(date_str)
    
    return sorted(dates, reverse=True)


# ========================================
# Pydantic Models
# ========================================
class AnalysisRequest(BaseModel):
    date_start: str
    date_end: str
    time_start: str = "0000"
    time_end: str = "2359"
    question: str = ""
    analysis_type: str = "summary"  # summary, pattern, prediction, custom


# ========================================
# FastAPI Startup
# ========================================
@app.on_event("startup")
async def startup():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global LLM_MODE, M14_DATA_DIR
    
    # M14 ë°ì´í„° í´ë” í™•ì¸
    if not os.path.exists(M14_DATA_DIR):
        # ë‹¤ë¥¸ ê²½ë¡œ ì‹œë„
        alt_paths = ["data", "../data", "NODE/UI/data"]
        for path in alt_paths:
            if os.path.exists(path):
                M14_DATA_DIR = path
                break
    
    logger.info(f"ğŸ“ M14 ë°ì´í„° í´ë”: {M14_DATA_DIR}")
    
    # API í† í° ë¡œë“œ
    if load_api_token():
        LLM_MODE = "api"
    else:
        LLM_MODE = "local"
    
    # ë¡œì»¬ LLM ë¡œë“œ
    if load_local_llm():
        logger.info("âœ… ë¡œì»¬ LLM ì¤€ë¹„ ì™„ë£Œ")
        if not API_TOKEN:
            LLM_MODE = "local"
    
    logger.info(f"ğŸš€ M14 LLM ë¶„ì„ ì„œë²„ ì‹œì‘! ëª¨ë“œ: {LLM_MODE}")


# ========================================
# API Endpoints
# ========================================
@app.get("/")
async def home():
    """ë©”ì¸ í˜ì´ì§€"""
    return FileResponse("llm_analysis.html")


@app.get("/api/status")
async def api_status():
    """ì„œë²„ ìƒíƒœ"""
    return {
        "llm_mode": LLM_MODE,
        "api_available": API_TOKEN is not None,
        "local_available": llm is not None,
        "env": ENV_MODE,
        "model": API_MODEL if LLM_MODE == "api" else LOCAL_MODEL_PATH,
        "data_dir": M14_DATA_DIR,
        "data_exists": os.path.exists(M14_DATA_DIR)
    }


@app.get("/api/dates")
async def get_dates():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ëª©ë¡"""
    dates = get_available_dates()
    return {"dates": dates}


@app.get("/api/data/{date_str}")
async def get_data(date_str: str):
    """íŠ¹ì • ë‚ ì§œ ë°ì´í„° ì¡°íšŒ"""
    result = load_m14_data(date_str)
    if result["data"] is None:
        return JSONResponse(status_code=404, content={"error": f"{date_str} ë°ì´í„° ì—†ìŒ"})
    return result


@app.post("/api/set_mode")
async def set_mode(data: dict):
    """LLM ëª¨ë“œ ì „í™˜"""
    global LLM_MODE
    
    new_mode = data.get("mode", "local")
    
    if new_mode == "local" and llm is None:
        return {"success": False, "message": "ë¡œì»¬ LLMì´ ì—†ìŠµë‹ˆë‹¤."}
    if new_mode == "api" and API_TOKEN is None:
        return {"success": False, "message": "API í† í°ì´ ì—†ìŠµë‹ˆë‹¤."}
    
    LLM_MODE = new_mode
    return {"success": True, "mode": LLM_MODE}


@app.post("/api/set_data_dir")
async def set_data_dir(data: dict):
    """ë°ì´í„° í´ë” ê²½ë¡œ ì„¤ì •"""
    global M14_DATA_DIR
    
    new_dir = data.get("path", "")
    if os.path.exists(new_dir):
        M14_DATA_DIR = new_dir
        return {"success": True, "path": M14_DATA_DIR}
    else:
        return {"success": False, "message": "ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}


@app.get("/api/prompts")
async def get_prompts():
    """í˜„ì¬ í”„ë¡¬í”„íŠ¸ ì¡°íšŒ"""
    return {
        "system_prompt": SYSTEM_PROMPT_ANALYSIS,
        "templates": PROMPT_TEMPLATES
    }


@app.post("/api/prompts")
async def set_prompts(data: dict):
    """í”„ë¡¬í”„íŠ¸ ìˆ˜ì •"""
    global SYSTEM_PROMPT_ANALYSIS, PROMPT_TEMPLATES
    
    if "system_prompt" in data:
        SYSTEM_PROMPT_ANALYSIS = data["system_prompt"]
    
    if "templates" in data:
        for key, value in data["templates"].items():
            if key in PROMPT_TEMPLATES:
                PROMPT_TEMPLATES[key] = value
    
    return {"success": True, "message": "í”„ë¡¬í”„íŠ¸ ì €ì¥ë¨"}


@app.post("/api/analyze")
async def analyze(request: AnalysisRequest):
    """ë°ì´í„° ë¶„ì„ ìš”ì²­"""
    
    # ë°ì´í„° ë¡œë“œ
    all_data = []
    all_alerts_10 = []
    all_alerts_30 = []
    
    start = datetime.strptime(request.date_start, "%Y%m%d")
    end = datetime.strptime(request.date_end, "%Y%m%d")
    
    current = start
    while current <= end:
        date_str = current.strftime("%Y%m%d")
        result = load_m14_data(date_str)
        
        if result["data"]:
            all_data.extend(result["data"])
            all_alerts_10.extend(result["alerts_10"])
            all_alerts_30.extend(result["alerts_30"])
        
        current += timedelta(days=1)
    
    if not all_data:
        return {"success": False, "answer": "âŒ í•´ë‹¹ ê¸°ê°„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
    
    # ì‹œê°„ í•„í„°ë§
    start_dt = request.date_start + request.time_start
    end_dt = request.date_end + request.time_end
    
    filtered_data = [
        row for row in all_data
        if start_dt <= str(row.get('CURRTIME', ''))[:12] <= end_dt
    ]
    
    if not filtered_data:
        return {"success": False, "answer": "âŒ ì„ íƒí•œ ì‹œê°„ ë²”ìœ„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
    
    # ë°ì´í„° ìš”ì•½ ìƒì„±
    df = pd.DataFrame(filtered_data)
    
    summary_stats = {
        "period": f"{request.date_start} {request.time_start} ~ {request.date_end} {request.time_end}",
        "total_count": len(df),
        "avg_totalcnt": round(df['TOTALCNT'].mean(), 1) if 'TOTALCNT' in df.columns else 0,
        "max_totalcnt": int(df['TOTALCNT'].max()) if 'TOTALCNT' in df.columns else 0,
        "min_totalcnt": int(df['TOTALCNT'].min()) if 'TOTALCNT' in df.columns else 0,
        "over_1700_count": int((df['TOTALCNT'] >= 1700).sum()) if 'TOTALCNT' in df.columns else 0,
        "over_1600_count": int((df['TOTALCNT'] >= 1600).sum()) if 'TOTALCNT' in df.columns else 0,
        "alerts_10_count": len(all_alerts_10),
        "alerts_30_count": len(all_alerts_30)
    }
    
    # ë¶„ì„ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
    if request.analysis_type == "summary":
        prompt = f"""ë‹¤ìŒ M14 ë°˜ì†¡ í ëª¨ë‹ˆí„°ë§ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

## ë°ì´í„° ìš”ì•½
- ê¸°ê°„: {summary_stats['period']}
- ì´ ë°ì´í„° ìˆ˜: {summary_stats['total_count']}ê°œ
- í‰ê·  TOTALCNT: {summary_stats['avg_totalcnt']}
- ìµœëŒ€ TOTALCNT: {summary_stats['max_totalcnt']}
- ìµœì†Œ TOTALCNT: {summary_stats['min_totalcnt']}
- 1700+ ì´ˆê³¼ íšŸìˆ˜: {summary_stats['over_1700_count']}íšŒ
- 1600+ ê²½ê³  íšŸìˆ˜: {summary_stats['over_1600_count']}íšŒ
- 10ë¶„ ì˜ˆì¸¡ ì•ŒëŒ: {summary_stats['alerts_10_count']}íšŒ
- 30ë¶„ ì˜ˆì¸¡ ì•ŒëŒ: {summary_stats['alerts_30_count']}íšŒ

ì „ë°˜ì ì¸ ìƒí™© ë¶„ì„ê³¼ ê°œì„ ì ì„ ì œì•ˆí•´ì£¼ì„¸ìš”."""

    elif request.analysis_type == "pattern":
        # ì‹œê°„ëŒ€ë³„ í‰ê·  ê³„ì‚°
        if 'CURRTIME' in df.columns:
            df['hour'] = df['CURRTIME'].astype(str).str[8:10].astype(int)
            hourly_avg = df.groupby('hour')['TOTALCNT'].mean().round(1).to_dict()
        else:
            hourly_avg = {}
        
        prompt = f"""M14 ë°˜ì†¡ íì˜ ì‹œê°„ëŒ€ë³„ íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

## ë°ì´í„° ìš”ì•½
- ê¸°ê°„: {summary_stats['period']}
- ì´ ë°ì´í„°: {summary_stats['total_count']}ê°œ

## ì‹œê°„ëŒ€ë³„ í‰ê·  TOTALCNT
{hourly_avg}

ì‹œê°„ëŒ€ë³„ íŒ¨í„´, í”¼í¬ íƒ€ì„, ì£¼ì˜ê°€ í•„ìš”í•œ ì‹œê°„ëŒ€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."""

    elif request.analysis_type == "prediction":
        # ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚° ì‹œë„
        pred_accuracy = "ë°ì´í„° ë¶€ì¡±"
        if 'PREDICT_10' in df.columns and 'TOTALCNT' in df.columns:
            # ì˜ˆì¸¡ 1700+ vs ì‹¤ì œ 1700+
            pred_over = (df['PREDICT_10'] >= 1700).sum()
            actual_over = (df['TOTALCNT'] >= 1700).sum()
            pred_accuracy = f"ì˜ˆì¸¡ 1700+: {pred_over}íšŒ, ì‹¤ì œ 1700+: {actual_over}íšŒ"
        
        prompt = f"""M14 ë°˜ì†¡ í ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

## ë°ì´í„° ìš”ì•½
- ê¸°ê°„: {summary_stats['period']}
- ì´ ë°ì´í„°: {summary_stats['total_count']}ê°œ
- 10ë¶„ ì˜ˆì¸¡ ì•ŒëŒ: {summary_stats['alerts_10_count']}íšŒ
- 30ë¶„ ì˜ˆì¸¡ ì•ŒëŒ: {summary_stats['alerts_30_count']}íšŒ
- ì˜ˆì¸¡ ì •í™•ë„ ì°¸ê³ : {pred_accuracy}

ì˜ˆì¸¡ ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ê°œì„  ë°©í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."""

    else:  # custom
        if not request.question:
            return {"success": False, "answer": "âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}
        
        prompt = f"""M14 ë°˜ì†¡ í ëª¨ë‹ˆí„°ë§ ë°ì´í„°ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”.

## ë°ì´í„° ìš”ì•½
- ê¸°ê°„: {summary_stats['period']}
- ì´ ë°ì´í„°: {summary_stats['total_count']}ê°œ
- í‰ê·  TOTALCNT: {summary_stats['avg_totalcnt']}
- ìµœëŒ€ TOTALCNT: {summary_stats['max_totalcnt']}
- 1700+ ì´ˆê³¼: {summary_stats['over_1700_count']}íšŒ

## ì§ˆë¬¸
{request.question}"""

    # LLM í˜¸ì¶œ
    result = call_llm(prompt, SYSTEM_PROMPT_ANALYSIS)
    
    if result["success"]:
        return {
            "success": True,
            "answer": result["content"],
            "stats": summary_stats,
            "mode": LLM_MODE
        }
    else:
        return {"success": False, "answer": f"âŒ {result['error']}"}


@app.post("/api/quick_analyze")
async def quick_analyze(data: dict):
    """ë¹ ë¥¸ ë¶„ì„ (ê°„ë‹¨í•œ ì§ˆë¬¸)"""
    question = data.get("question", "")
    date_str = data.get("date", datetime.now().strftime("%Y%m%d"))
    
    if not question:
        return {"success": False, "answer": "âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}
    
    # ë°ì´í„° ë¡œë“œ
    result = load_m14_data(date_str)
    
    if result["data"] is None:
        return {"success": False, "answer": f"âŒ {date_str} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
    
    stats = result["stats"]
    
    prompt = f"""M14 ë°˜ì†¡ í ë°ì´í„° ({date_str})ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”.

## ì˜¤ëŠ˜ ë°ì´í„° ìš”ì•½
- ë°ì´í„° ìˆ˜: {stats.get('count', 0)}ê°œ
- í‰ê·  TOTALCNT: {stats.get('avg', 0)}
- ìµœëŒ€ TOTALCNT: {stats.get('max', 0)}
- 1700+ ì´ˆê³¼: {stats.get('over_1700', 0)}íšŒ

## ì§ˆë¬¸
{question}"""

    llm_result = call_llm(prompt, SYSTEM_PROMPT_ANALYSIS, max_tokens=1500)
    
    if llm_result["success"]:
        return {"success": True, "answer": llm_result["content"], "stats": stats}
    else:
        return {"success": False, "answer": f"âŒ {llm_result['error']}"}


# ========================================
# CSV ì—…ë¡œë“œ ë¶„ì„
# ========================================
@app.post("/api/upload_csv")
async def upload_csv(file: UploadFile = File(...)):
    """CSV íŒŒì¼ ì—…ë¡œë“œ ë° íŒŒì‹±"""
    try:
        content = await file.read()
        
        # ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
        encodings = ['utf-8', 'cp949', 'euc-kr', 'utf-8-sig', 'latin1']
        content_str = None
        
        for enc in encodings:
            try:
                content_str = content.decode(enc)
                logger.info(f"CSV ì¸ì½”ë”© ê°ì§€: {enc}")
                break
            except:
                continue
        
        if content_str is None:
            return {"success": False, "error": "íŒŒì¼ ì¸ì½”ë”©ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        df = pd.read_csv(StringIO(content_str))
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬ (ê³µë°± ì œê±°)
        df.columns = df.columns.str.strip()
        
        # ê¸°ë³¸ í†µê³„ ê³„ì‚°
        stats = {
            "filename": file.filename,
            "row_count": len(df),
            "columns": list(df.columns)
        }
        
        # ì£¼ìš” ì»¬ëŸ¼ë³„ í†µê³„
        if 'í˜„ì¬TOTALCNT' in df.columns:
            stats["avg_totalcnt"] = round(df['í˜„ì¬TOTALCNT'].mean(), 1)
            stats["max_totalcnt"] = int(df['í˜„ì¬TOTALCNT'].max())
            stats["min_totalcnt"] = int(df['í˜„ì¬TOTALCNT'].min())
        
        if 'ì‹¤ì œìœ„í—˜(1700+)' in df.columns:
            stats["danger_count"] = int(df['ì‹¤ì œìœ„í—˜(1700+)'].sum())
        
        if 'ìµœì¢…íŒì •' in df.columns:
            stats["pred_danger_count"] = int(df['ìµœì¢…íŒì •'].sum())
        
        if 'ì˜ˆì¸¡ìƒíƒœ' in df.columns:
            status_counts = df['ì˜ˆì¸¡ìƒíƒœ'].value_counts().to_dict()
            stats["status_counts"] = status_counts
            
            # TP, TN, FP, FN ê³„ì‚°
            stats["TP"] = sum(1 for s in df['ì˜ˆì¸¡ìƒíƒœ'] if 'TP' in str(s))
            stats["TN"] = sum(1 for s in df['ì˜ˆì¸¡ìƒíƒœ'] if 'TN' in str(s))
            stats["FP"] = sum(1 for s in df['ì˜ˆì¸¡ìƒíƒœ'] if 'FP' in str(s))
            stats["FN"] = sum(1 for s in df['ì˜ˆì¸¡ìƒíƒœ'] if 'FN' in str(s))
        
        # ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 10í–‰)
        sample_data = df.head(10).to_dict('records')
        
        return {
            "success": True,
            "stats": stats,
            "sample": sample_data,
            "data": df.to_dict('records')
        }
        
    except Exception as e:
        logger.error(f"CSV ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": f"CSV íŒŒì‹± ì‹¤íŒ¨: {str(e)}"}


@app.post("/api/analyze_csv")
async def analyze_csv(data: dict):
    """ì—…ë¡œë“œëœ CSV ë°ì´í„° LLM ë¶„ì„"""
    csv_data = data.get("data", [])
    stats = data.get("stats", {})
    question = data.get("question", "")
    analysis_type = data.get("analysis_type", "summary")
    
    if not csv_data:
        return {"success": False, "answer": "âŒ CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
    
    df = pd.DataFrame(csv_data)
    
    # í†µê³„ ìš”ì•½ ìƒì„±
    summary_text = f"""## CSV ë°ì´í„° ìš”ì•½
- íŒŒì¼ëª…: {stats.get('filename', 'unknown')}
- ë°ì´í„° ìˆ˜: {stats.get('row_count', len(df))}ê°œ
- ì»¬ëŸ¼: {', '.join(stats.get('columns', [])[:10])}{'...' if len(stats.get('columns', [])) > 10 else ''}
"""
    
    if 'avg_totalcnt' in stats:
        summary_text += f"""
## TOTALCNT í†µê³„
- í‰ê· : {stats.get('avg_totalcnt')}
- ìµœëŒ€: {stats.get('max_totalcnt')}
- ìµœì†Œ: {stats.get('min_totalcnt')}
- 1700+ ì‹¤ì œ ìœ„í—˜: {stats.get('danger_count', 0)}íšŒ
- 1700+ ì˜ˆì¸¡ ìœ„í—˜: {stats.get('pred_danger_count', 0)}íšŒ
"""
    
    if 'TP' in stats:
        total = stats.get('TP', 0) + stats.get('TN', 0) + stats.get('FP', 0) + stats.get('FN', 0)
        accuracy = round((stats.get('TP', 0) + stats.get('TN', 0)) / total * 100, 2) if total > 0 else 0
        recall = round(stats.get('TP', 0) / (stats.get('TP', 0) + stats.get('FN', 0)) * 100, 2) if (stats.get('TP', 0) + stats.get('FN', 0)) > 0 else 0
        precision = round(stats.get('TP', 0) / (stats.get('TP', 0) + stats.get('FP', 0)) * 100, 2) if (stats.get('TP', 0) + stats.get('FP', 0)) > 0 else 0
        
        summary_text += f"""
## ì˜ˆì¸¡ ì„±ëŠ¥
- TP (ì •í™•í•œ ìœ„í—˜ ì˜ˆì¸¡): {stats.get('TP', 0)}
- TN (ì •í™•í•œ ì •ìƒ ì˜ˆì¸¡): {stats.get('TN', 0)}
- FP (ì˜¤íƒ - ì˜ëª»ëœ ìœ„í—˜ ì˜ˆì¸¡): {stats.get('FP', 0)}
- FN (ë†“ì¹¨ - ìœ„í—˜ ë¯¸íƒì§€): {stats.get('FN', 0)}
- ì •í™•ë„: {accuracy}%
- ì¬í˜„ìœ¨: {recall}%
- ì •ë°€ë„: {precision}%
"""
    
    if 'status_counts' in stats:
        summary_text += f"""
## ì˜ˆì¸¡ìƒíƒœ ë¶„í¬
"""
        for status, count in stats.get('status_counts', {}).items():
            summary_text += f"- {status}: {count}ê°œ\n"
    
    # ë¶„ì„ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ (PROMPT_TEMPLATES ì‚¬ìš©)
    if analysis_type == "summary":
        template = PROMPT_TEMPLATES.get("summary", "ìœ„ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.")
        prompt = f"""{summary_text}

{template}"""

    elif analysis_type == "model":
        # ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’ í†µê³„
        model_cols = ['XGB_íƒ€ê²Ÿ', 'XGB_ì¤‘ìš”', 'XGB_ë³´ì¡°', 'XGB_PDT', 'XGB_Job']
        model_stats = ""
        for col in model_cols:
            if col in df.columns:
                over_1700 = (df[col] >= 1700).sum()
                model_stats += f"- {col}: í‰ê·  {df[col].mean():.1f}, ìµœëŒ€ {df[col].max():.1f}, 1700+ ì˜ˆì¸¡ {over_1700}ê°œ ({over_1700/len(df)*100:.2f}%)\n"
        
        template = PROMPT_TEMPLATES.get("model", "ëª¨ë¸ë³„ ì„±ëŠ¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.")
        prompt = f"""{summary_text}

## ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’ í†µê³„
{model_stats}

{template}"""

    elif analysis_type == "error":
        # FP, FN ì¼€ì´ìŠ¤ ë¶„ì„
        fp_cases = df[df['ì˜ˆì¸¡ìƒíƒœ'].str.contains('FP', na=False)] if 'ì˜ˆì¸¡ìƒíƒœ' in df.columns else pd.DataFrame()
        fn_cases = df[df['ì˜ˆì¸¡ìƒíƒœ'].str.contains('FN', na=False)] if 'ì˜ˆì¸¡ìƒíƒœ' in df.columns else pd.DataFrame()
        
        error_info = f"""
## ì˜¤ë¥˜ ì¼€ì´ìŠ¤ ë¶„ì„
- FP (ì˜¤íƒ) ì¼€ì´ìŠ¤: {len(fp_cases)}ê°œ
- FN (ë¯¸íƒ) ì¼€ì´ìŠ¤: {len(fn_cases)}ê°œ
"""
        if len(fp_cases) > 0 and 'í˜„ì¬TOTALCNT' in fp_cases.columns:
            error_info += f"- FP í‰ê·  TOTALCNT: {fp_cases['í˜„ì¬TOTALCNT'].mean():.1f}\n"
        if len(fn_cases) > 0 and 'í˜„ì¬TOTALCNT' in fn_cases.columns:
            error_info += f"- FN í‰ê·  TOTALCNT: {fn_cases['í˜„ì¬TOTALCNT'].mean():.1f}\n"
        
        template = PROMPT_TEMPLATES.get("error", "FP/FN ì˜¤ë¥˜ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.")
        prompt = f"""{summary_text}
{error_info}

{template}"""

    else:  # custom
        if not question:
            return {"success": False, "answer": "âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}
        
        prompt = f"""{summary_text}

## ì§ˆë¬¸
{question}"""

    # LLM í˜¸ì¶œ
    result = call_llm(prompt, SYSTEM_PROMPT_ANALYSIS)
    
    if result["success"]:
        return {
            "success": True,
            "answer": result["content"],
            "stats": stats,
            "mode": LLM_MODE
        }
    else:
        return {"success": False, "answer": f"âŒ {result['error']}"}


def build_csv_prompt(csv_data, stats, analysis_type, question=""):
    """CSV ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± - PROMPT_TEMPLATES ì‚¬ìš©"""
    df = pd.DataFrame(csv_data)
    
    # í†µê³„ ìš”ì•½ ìƒì„±
    summary_text = f"""## CSV ë°ì´í„° ìš”ì•½
- íŒŒì¼ëª…: {stats.get('filename', 'unknown')}
- ë°ì´í„° ìˆ˜: {stats.get('row_count', len(df))}ê°œ
"""
    
    if 'avg_totalcnt' in stats:
        summary_text += f"""
## TOTALCNT í†µê³„
| í•­ëª© | ê°’ |
|------|-----|
| í‰ê·  | {stats.get('avg_totalcnt')} |
| ìµœëŒ€ | {stats.get('max_totalcnt')} |
| ìµœì†Œ | {stats.get('min_totalcnt')} |
| 1700+ ì‹¤ì œ | {stats.get('danger_count', 0)}íšŒ |
| 1700+ ì˜ˆì¸¡ | {stats.get('pred_danger_count', 0)}íšŒ |
"""
    
    if 'TP' in stats:
        total = stats.get('TP', 0) + stats.get('TN', 0) + stats.get('FP', 0) + stats.get('FN', 0)
        accuracy = round((stats.get('TP', 0) + stats.get('TN', 0)) / total * 100, 2) if total > 0 else 0
        recall = round(stats.get('TP', 0) / (stats.get('TP', 0) + stats.get('FN', 0)) * 100, 2) if (stats.get('TP', 0) + stats.get('FN', 0)) > 0 else 0
        precision = round(stats.get('TP', 0) / (stats.get('TP', 0) + stats.get('FP', 0)) * 100, 2) if (stats.get('TP', 0) + stats.get('FP', 0)) > 0 else 0
        
        summary_text += f"""
## ì˜ˆì¸¡ ì„±ëŠ¥
| ì§€í‘œ | ê°’ |
|------|-----|
| TP | {stats.get('TP', 0)} |
| TN | {stats.get('TN', 0)} |
| FP (ì˜¤íƒ) | {stats.get('FP', 0)} |
| FN (ë†“ì¹¨) | {stats.get('FN', 0)} |
| ì •í™•ë„ | {accuracy}% |
| ì¬í˜„ìœ¨ | {recall}% |
| ì •ë°€ë„ | {precision}% |
"""
    
    if 'status_counts' in stats:
        summary_text += "\n## ì˜ˆì¸¡ìƒíƒœ ë¶„í¬\n| ìƒíƒœ | ê°œìˆ˜ |\n|------|------|\n"
        for status, count in stats.get('status_counts', {}).items():
            summary_text += f"| {status} | {count} |\n"
    
    # ë¶„ì„ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ (PROMPT_TEMPLATES ì‚¬ìš©)
    if analysis_type == "summary":
        template = PROMPT_TEMPLATES.get("summary", "ì¢…í•© ë¶„ì„í•´ì£¼ì„¸ìš”.")
        prompt = f"{summary_text}\n\n{template}"

    elif analysis_type == "pattern":
        # ì‹œê°„ëŒ€ë³„ í†µê³„ ì¶”ê°€
        hour_stats = "\n## ì‹œê°„ëŒ€ë³„ í†µê³„\n| ì‹œê°„ | í‰ê·  | ìµœëŒ€ | 1700+ |\n|------|------|------|-------|\n"
        if 'CURRTIME' in df.columns or 'í˜„ì¬ì‹œê°„' in df.columns:
            time_col = 'CURRTIME' if 'CURRTIME' in df.columns else 'í˜„ì¬ì‹œê°„'
            try:
                df['hour'] = df[time_col].astype(str).str[11:13].astype(int)
                for h in sorted(df['hour'].unique()):
                    h_data = df[df['hour'] == h]
                    tc_col = 'TOTALCNT' if 'TOTALCNT' in df.columns else 'í˜„ì¬TOTALCNT'
                    if tc_col in df.columns:
                        avg = h_data[tc_col].mean()
                        max_val = h_data[tc_col].max()
                        over = (h_data[tc_col] >= 1700).sum()
                        hour_stats += f"| {h:02d}ì‹œ | {avg:.0f} | {max_val:.0f} | {over}íšŒ |\n"
            except:
                hour_stats = "\n## ì‹œê°„ëŒ€ë³„ í†µê³„\n(ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨)\n"
        
        template = PROMPT_TEMPLATES.get("pattern", "íŒ¨í„´ ë¶„ì„í•´ì£¼ì„¸ìš”.")
        prompt = f"{summary_text}{hour_stats}\n{template}"

    elif analysis_type == "prediction":
        # ì˜ˆì¸¡ ì„±ëŠ¥ ì¶”ê°€ ì •ë³´
        pred_info = "\n## ì˜ˆì¸¡ ìƒì„¸\n"
        if 'ì˜ˆì¸¡ìƒíƒœ' in df.columns:
            fp_count = df['ì˜ˆì¸¡ìƒíƒœ'].str.contains('FP', na=False).sum()
            fn_count = df['ì˜ˆì¸¡ìƒíƒœ'].str.contains('FN', na=False).sum()
            pred_info += f"| FP (ì˜¤íƒ) | {fp_count}ê±´ |\n| FN (ë†“ì¹¨) | {fn_count}ê±´ |\n"
        
        template = PROMPT_TEMPLATES.get("prediction", "ì˜ˆì¸¡ ë¶„ì„í•´ì£¼ì„¸ìš”.")
        prompt = f"{summary_text}{pred_info}\n{template}"

    else:  # custom
        prompt = f"{summary_text}\n\n## ì§ˆë¬¸\n{question}"
    
    return prompt


@app.post("/api/analyze_csv_stream")
async def analyze_csv_stream(data: dict):
    """ìŠ¤íŠ¸ë¦¬ë° CSV ë¶„ì„"""
    csv_data = data.get("data", [])
    stats = data.get("stats", {})
    question = data.get("question", "")
    analysis_type = data.get("analysis_type", "summary")
    
    if not csv_data:
        async def error_gen():
            yield "data: [ERROR] CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
        return StreamingResponse(error_gen(), media_type="text/event-stream")
    
    if analysis_type == "custom" and not question:
        async def error_gen():
            yield "data: [ERROR] ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n\n"
        return StreamingResponse(error_gen(), media_type="text/event-stream")
    
    prompt = build_csv_prompt(csv_data, stats, analysis_type, question)
    
    # ë¡œì»¬ LLMë§Œ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
    if LLM_MODE == "local" and llm is not None:
        return StreamingResponse(
            stream_local_llm(prompt, SYSTEM_PROMPT_ANALYSIS),
            media_type="text/event-stream"
        )
    else:
        # API ëª¨ë“œëŠ” ì¼ë°˜ ì‘ë‹µ í›„ í•œë²ˆì— ì „ì†¡
        async def api_response_gen():
            result = call_llm(prompt, SYSTEM_PROMPT_ANALYSIS)
            if result["success"]:
                # í•œê¸€ìì”© ì „ì†¡ (ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼) - ì¤„ë°”ê¿ˆ ì´ìŠ¤ì¼€ì´í”„
                content = result["content"].replace('\n', 'â')
                for char in content:
                    yield f"data: {char}\n\n"
                yield "data: [DONE]\n\n"
            else:
                yield f"data: [ERROR] {result['error']}\n\n"
        
        return StreamingResponse(api_response_gen(), media_type="text/event-stream")


# ============================================================================
# ë§ˆí¬ë‹¤ìš´ -> HTML ë³€í™˜ API
# ============================================================================

@app.post("/api/markdown_to_html")
async def markdown_to_html(request: Request):
    """ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ HTMLë¡œ ë³€í™˜"""
    try:
        import markdown
        from markdown.extensions.tables import TableExtension
        from markdown.extensions.fenced_code import FencedCodeExtension
        from markdown.extensions.nl2br import Nl2BrExtension
        
        body = await request.json()
        md_text = body.get("text", "")
        
        if not md_text:
            return {"html": ""}
        
        # ë§ˆí¬ë‹¤ìš´ ì „ì²˜ë¦¬: ë¶™ì–´ìˆëŠ” í˜•ì‹ ë¶„ë¦¬
        md_text = md_text.replace('---####', '---\n\n####')
        md_text = md_text.replace('---###', '---\n\n###')
        md_text = md_text.replace('---##', '---\n\n##')
        md_text = md_text.replace('---#', '---\n\n#')
        
        # í…Œì´ë¸” ì•ë’¤ ì¤„ë°”ê¿ˆ ë³´ì¥
        import re
        md_text = re.sub(r'([^\n])(\n\|)', r'\1\n\2', md_text)
        md_text = re.sub(r'(\|[^\n]*\n)([^\|])', r'\1\n\2', md_text)
        
        # ë§ˆí¬ë‹¤ìš´ -> HTML ë³€í™˜
        md = markdown.Markdown(extensions=[
            TableExtension(),
            FencedCodeExtension(),
            Nl2BrExtension(),
            'md_in_html'
        ])
        
        html = md.convert(md_text)
        
        return {"html": html}
        
    except ImportError:
        # markdown ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ë³€í™˜
        logger.warning("markdown ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ, ê¸°ë³¸ ë³€í™˜ ì‚¬ìš©")
        return {"html": f"<pre>{md_text}</pre>"}
    except Exception as e:
        logger.error(f"ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì˜¤ë¥˜: {e}")
        return {"html": f"<pre>{body.get('text', '')}</pre>"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8002)
