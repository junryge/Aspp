#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
M14 íˆìŠ¤í† ë¦¬ ë°ì´í„° LLM ë¶„ì„ ì„œë²„ (v1.0)
- M14 íˆìŠ¤í† ë¦¬ ë°ì´í„° ë¡œë“œ
- LLMì„ í†µí•œ ë°ì´í„° ë¶„ì„
- SK Hynix API + ë¡œì»¬ GGUF ì§€ì›
"""

import os
import re
import requests
import pandas as pd
from fastapi import FastAPI, UploadFile, File, Form, Request
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List
from datetime import datetime, timedelta
from io import StringIO
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="M14 LLM Analysis")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ========================================
# Global Variables
# ========================================
API_TOKEN = None
llm = None  # ë¡œì»¬ LLM (GGUF)

# LLM ëª¨ë“œ: "api" ë˜ëŠ” "local"
LLM_MODE = "local"

# í™˜ê²½ ì„¤ì •
ENV_MODE = "dev"

# ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ
LOCAL_MODEL_PATH = "Qwen3-14B-Q4_K_M.gguf"

# M14 ë°ì´í„° í´ë” ê²½ë¡œ (ë‚˜ì¤‘ì— ì„¤ì • ê°€ëŠ¥)
M14_DATA_DIR = "../NODE/UI/data"

ENV_CONFIG = {
    "dev": {
        "url": "http://dev.assistant.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Coder-30B-A3B-Instruct",
        "name": "ê°œë°œ(30B)"
    },
    "prod": {
        "url": "http://summary.llm.skhynix.com/v1/chat/completions",
        "model": "Qwen3-Next-80B-A3B-Instruct",
        "name": "ìš´ì˜(80B)"
    }
}

API_URL = ENV_CONFIG["dev"]["url"]
API_MODEL = ENV_CONFIG["dev"]["model"]

# ========================================
# í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ
# ========================================
PROMPTS_DIR = os.path.join(os.path.dirname(__file__), "prompts")

def load_prompt(name):
    """txt íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""
    filepath = os.path.join(PROMPTS_DIR, f"{name}.txt")
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        logger.warning(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì—†ìŒ: {filepath}")
        return ""

def save_prompt(name, content):
    """txt íŒŒì¼ì— í”„ë¡¬í”„íŠ¸ ì €ì¥"""
    os.makedirs(PROMPTS_DIR, exist_ok=True)
    filepath = os.path.join(PROMPTS_DIR, f"{name}.txt")
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)
    logger.info(f"í”„ë¡¬í”„íŠ¸ ì €ì¥ë¨: {filepath}")

def get_system_prompt():
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ (ë§¤ë²ˆ íŒŒì¼ì—ì„œ ì½ìŒ)"""
    return load_prompt("system")

def get_template(name):
    """í…œí”Œë¦¿ í”„ë¡¬í”„íŠ¸ ë¡œë“œ (ë§¤ë²ˆ íŒŒì¼ì—ì„œ ì½ìŒ)"""
    return load_prompt(name)


# ========================================
# API Token ê´€ë¦¬
# ========================================
def load_api_token():
    """API í† í° ë¡œë“œ"""
    global API_TOKEN
    
    token_paths = ["token.txt", "../token.txt", os.path.expanduser("~/token.txt")]
    
    for token_path in token_paths:
        if os.path.exists(token_path):
            try:
                with open(token_path, "r", encoding='utf-8') as f:
                    API_TOKEN = f.read().strip()
                if API_TOKEN and "REPLACE" not in API_TOKEN:
                    logger.info(f"âœ… API í† í° ë¡œë“œ ì™„ë£Œ: {token_path}")
                    return True
            except Exception as e:
                logger.error(f"âŒ í† í° ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    logger.warning("âš ï¸ í† í° íŒŒì¼ ì—†ìŒ")
    return False


# ========================================
# ë¡œì»¬ LLM ë¡œë“œ (GGUF)
# ========================================
def load_local_llm():
    """ë¡œì»¬ GGUF ëª¨ë¸ ë¡œë“œ"""
    global llm
    
    # ì—¬ëŸ¬ ê²½ë¡œì—ì„œ ëª¨ë¸ ì°¾ê¸°
    model_paths = [
        LOCAL_MODEL_PATH,
        f"../{LOCAL_MODEL_PATH}",
        f"../../{LOCAL_MODEL_PATH}",
        os.path.expanduser(f"~/{LOCAL_MODEL_PATH}")
    ]
    
    model_path = None
    for path in model_paths:
        if os.path.exists(path):
            model_path = path
            break
    
    if model_path is None:
        logger.warning(f"âš ï¸ ë¡œì»¬ ëª¨ë¸ ì—†ìŒ: {LOCAL_MODEL_PATH}")
        return False
    
    try:
        from llama_cpp import Llama
        logger.info(f"ğŸ“¦ ë¡œì»¬ LLM ë¡œë”© ì¤‘: {model_path}")
        llm = Llama(
            model_path=model_path,
            n_ctx=32768,
            n_gpu_layers=-1,
            verbose=False
        )
        logger.info("âœ… ë¡œì»¬ LLM ë¡œë“œ ì™„ë£Œ!")
        return True
    except ImportError:
        logger.warning("âš ï¸ llama-cpp-python ë¯¸ì„¤ì¹˜")
        return False
    except Exception as e:
        logger.error(f"âŒ ë¡œì»¬ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


# ========================================
# LLM í˜¸ì¶œ í•¨ìˆ˜ë“¤
# ========================================
def call_local_llm(prompt: str, system_prompt: str = "", max_tokens: int = 2000) -> dict:
    """ë¡œì»¬ GGUF ëª¨ë¸ í˜¸ì¶œ"""
    global llm
    
    if llm is None:
        return {"success": False, "error": "ë¡œì»¬ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    
    try:
        if not system_prompt:
            system_prompt = get_system_prompt()
        
        formatted_prompt = f"""<|im_start|>system
{system_prompt}
<|im_end|>
<|im_start|>user
{prompt}
<|im_end|>
<|im_start|>assistant
"""
        
        logger.info("ğŸ–¥ï¸ ë¡œì»¬ LLM í˜¸ì¶œ ì¤‘...")
        response = llm(
            formatted_prompt,
            max_tokens=max_tokens,
            temperature=0.3,
            stop=["<|im_end|>", "\n\n\n"]
        )
        
        content = response['choices'][0]['text'].strip()
        
        # <think> íƒœê·¸ ì œê±°
        content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
        content = re.sub(r'<think>.*', '', content, flags=re.DOTALL)
        content = content.strip()
        
        return {"success": True, "content": content}
        
    except Exception as e:
        logger.error(f"âŒ ë¡œì»¬ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": f"ë¡œì»¬ LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"}


def stream_local_llm(prompt: str, system_prompt: str = "", max_tokens: int = 2000):
    """ë¡œì»¬ LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ (Generator)"""
    global llm
    
    if llm is None:
        yield "data: [ERROR] ë¡œì»¬ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n"
        return
    
    try:
        formatted_prompt = f"""<|im_start|>system
{system_prompt}
<|im_end|>
<|im_start|>user
{prompt}
<|im_end|>
<|im_start|>assistant
"""
        
        logger.info("ğŸ–¥ï¸ ë¡œì»¬ LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘...")
        
        in_think = False
        buffer = ""
        
        for chunk in llm(
            formatted_prompt,
            max_tokens=max_tokens,
            temperature=0.3,
            stop=["<|im_end|>", "\n\n\n"],
            stream=True
        ):
            token = chunk['choices'][0]['text']
            buffer += token
            
            # <think> íƒœê·¸ ì²˜ë¦¬
            if '<think>' in buffer and not in_think:
                in_think = True
                # <think> ì´ì „ í…ìŠ¤íŠ¸ ì „ì†¡
                before_think = buffer.split('<think>')[0]
                if before_think:
                    safe_before = before_think.replace('\n', 'â')
                    yield f"data: {safe_before}\n\n"
                buffer = buffer.split('<think>', 1)[1] if '<think>' in buffer else ""
                continue
            
            if in_think:
                if '</think>' in buffer:
                    in_think = False
                    buffer = buffer.split('</think>', 1)[1] if '</think>' in buffer else ""
                continue
            
            # ì •ìƒ í† í° ì „ì†¡ (ì¤„ë°”ê¿ˆì„ íŠ¹ìˆ˜ ë¬¸ìë¡œ ì¹˜í™˜)
            if token and not in_think:
                safe_token = token.replace('\n', 'â')
                yield f"data: {safe_token}\n\n"
        
        yield "data: [DONE]\n\n"
        
    except Exception as e:
        logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
        yield f"data: [ERROR] {str(e)}\n\n"


def call_llm_api(prompt: str, system_prompt: str = "", max_tokens: int = 4000) -> dict:
    """LLM API í˜¸ì¶œ"""
    global API_TOKEN
    
    if not API_TOKEN:
        return {"success": False, "error": "API í† í°ì´ ì—†ìŠµë‹ˆë‹¤."}
    
    headers = {
        "Authorization": f"Bearer {API_TOKEN}",
        "Content-Type": "application/json"
    }
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    data = {
        "model": API_MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": 0.3
    }
    
    try:
        logger.info(f"ğŸš€ API í˜¸ì¶œ ì¤‘...")
        response = requests.post(API_URL, headers=headers, json=data, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
            return {"success": True, "content": content.strip()}
        else:
            return {"success": False, "error": f"API ì˜¤ë¥˜: {response.status_code}"}
            
    except Exception as e:
        return {"success": False, "error": f"API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"}


def call_llm(prompt: str, system_prompt: str = "", max_tokens: int = 4000) -> dict:
    """í†µí•© LLM í˜¸ì¶œ"""
    global LLM_MODE
    
    if LLM_MODE == "api":
        result = call_llm_api(prompt, system_prompt, max_tokens)
        if not result["success"] and llm is not None:
            logger.info("âš ï¸ API ì‹¤íŒ¨ â†’ ë¡œì»¬ í´ë°±")
            result = call_local_llm(prompt, system_prompt, min(max_tokens, 2000))
        return result
    else:
        if llm is None:
            return {"success": False, "error": "ë¡œì»¬ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        return call_local_llm(prompt, system_prompt, min(max_tokens, 2000))


# ========================================
# M14 ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
# ========================================
def load_m14_data(date_str: str) -> dict:
    """M14 ë°ì´í„° ë¡œë“œ"""
    data_file = os.path.join(M14_DATA_DIR, f'm14_data_{date_str}.csv')
    pred_file = os.path.join(M14_DATA_DIR, f'm14_pred_{date_str}.csv')
    alert_file = os.path.join(M14_DATA_DIR, f'm14_alert_{date_str}.csv')
    
    result = {
        "date": date_str,
        "data": None,
        "alerts_10": [],
        "alerts_30": [],
        "stats": {}
    }
    
    # ë°ì´í„° íŒŒì¼ ë¡œë“œ
    if os.path.exists(pred_file):
        try:
            df = pd.read_csv(pred_file)
            result["data"] = df.to_dict('records')
            
            # ê¸°ë³¸ í†µê³„
            if 'TOTALCNT' in df.columns:
                result["stats"] = {
                    "count": len(df),
                    "avg": round(df['TOTALCNT'].mean(), 1),
                    "max": int(df['TOTALCNT'].max()),
                    "min": int(df['TOTALCNT'].min()),
                    "over_1700": int((df['TOTALCNT'] >= 1700).sum()),
                    "over_1600": int((df['TOTALCNT'] >= 1600).sum())
                }
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    elif os.path.exists(data_file):
        try:
            df = pd.read_csv(data_file)
            result["data"] = df.to_dict('records')
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ì•ŒëŒ íŒŒì¼ ë¡œë“œ
    if os.path.exists(alert_file):
        try:
            df_alert = pd.read_csv(alert_file)
            df_alert['TYPE'] = df_alert['TYPE'].astype(str)
            result["alerts_10"] = df_alert[df_alert['TYPE'] == '10'].to_dict('records')
            result["alerts_30"] = df_alert[df_alert['TYPE'] == '30'].to_dict('records')
        except Exception as e:
            logger.error(f"ì•ŒëŒ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return result


def get_available_dates() -> List[str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ëª©ë¡"""
    dates = []
    
    if not os.path.exists(M14_DATA_DIR):
        return dates
    
    for f in os.listdir(M14_DATA_DIR):
        if f.startswith('m14_data_') and f.endswith('.csv'):
            date_str = f.replace('m14_data_', '').replace('.csv', '')
            if len(date_str) == 8 and date_str.isdigit():
                dates.append(date_str)
    
    return sorted(dates, reverse=True)


# ========================================
# Pydantic Models
# ========================================
class AnalysisRequest(BaseModel):
    date_start: str
    date_end: str
    time_start: str = "0000"
    time_end: str = "2359"
    question: str = ""
    analysis_type: str = "summary"  # summary, pattern, prediction, custom


# ========================================
# FastAPI Startup
# ========================================
@app.on_event("startup")
async def startup():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global LLM_MODE, M14_DATA_DIR
    
    # M14 ë°ì´í„° í´ë” í™•ì¸
    if not os.path.exists(M14_DATA_DIR):
        # ë‹¤ë¥¸ ê²½ë¡œ ì‹œë„
        alt_paths = ["data", "../data", "NODE/UI/data"]
        for path in alt_paths:
            if os.path.exists(path):
                M14_DATA_DIR = path
                break
    
    logger.info(f"ğŸ“ M14 ë°ì´í„° í´ë”: {M14_DATA_DIR}")
    
    # API í† í° ë¡œë“œ
    if load_api_token():
        LLM_MODE = "api"
    else:
        LLM_MODE = "local"
    
    # ë¡œì»¬ LLM ë¡œë“œ
    if load_local_llm():
        logger.info("âœ… ë¡œì»¬ LLM ì¤€ë¹„ ì™„ë£Œ")
        if not API_TOKEN:
            LLM_MODE = "local"
    
    logger.info(f"ğŸš€ M14 LLM ë¶„ì„ ì„œë²„ ì‹œì‘! ëª¨ë“œ: {LLM_MODE}")


# ========================================
# API Endpoints
# ========================================
@app.get("/")
async def home():
    """ë©”ì¸ í˜ì´ì§€"""
    return FileResponse("llm_analysis.html")


@app.get("/api/status")
async def api_status():
    """ì„œë²„ ìƒíƒœ"""
    # ë¡œì»¬ GGUF íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
    local_model_name = os.path.basename(LOCAL_MODEL_PATH) if LOCAL_MODEL_PATH else "-"

    return {
        "llm_mode": LLM_MODE,
        "api_available": API_TOKEN is not None,
        "local_available": llm is not None,
        "env": ENV_MODE,
        "model": API_MODEL if LLM_MODE == "api" else local_model_name,
        "data_dir": M14_DATA_DIR,
        "data_exists": os.path.exists(M14_DATA_DIR),
        # ëª¨ë“  ëª¨ë¸ ì •ë³´
        "models": {
            "dev": ENV_CONFIG["dev"]["model"],
            "prod": ENV_CONFIG["prod"]["model"],
            "local": local_model_name
        }
    }


@app.get("/api/dates")
async def get_dates():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ëª©ë¡"""
    dates = get_available_dates()
    return {"dates": dates}


@app.get("/api/data/{date_str}")
async def get_data(date_str: str):
    """íŠ¹ì • ë‚ ì§œ ë°ì´í„° ì¡°íšŒ"""
    result = load_m14_data(date_str)
    if result["data"] is None:
        return JSONResponse(status_code=404, content={"error": f"{date_str} ë°ì´í„° ì—†ìŒ"})
    return result


@app.post("/api/set_mode")
async def set_mode(data: dict):
    """LLM ëª¨ë“œ ì „í™˜"""
    global LLM_MODE, ENV_MODE, API_URL, API_MODEL

    new_mode = data.get("mode", "local")
    new_env = data.get("env", "dev")

    # ì´ë¯¸ ê°™ì€ ëª¨ë“œë©´ ì„±ê³µ ì²˜ë¦¬ (ì¬í´ë¦­ í—ˆìš©)
    if new_mode == LLM_MODE and (new_mode == "local" or new_env == ENV_MODE):
        return {"success": True, "mode": LLM_MODE, "env": ENV_MODE, "model": API_MODEL if LLM_MODE == "api" else os.path.basename(LOCAL_MODEL_PATH)}

    if new_mode == "local" and llm is None:
        return {"success": False, "message": "ë¡œì»¬ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GGUF íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."}
    if new_mode == "api" and API_TOKEN is None:
        return {"success": False, "message": "API í† í°ì´ ì—†ìŠµë‹ˆë‹¤."}

    LLM_MODE = new_mode

    # API í™˜ê²½ ì„¤ì • (dev/prod)
    if new_mode == "api" and new_env in ENV_CONFIG:
        ENV_MODE = new_env
        API_URL = ENV_CONFIG[new_env]["url"]
        API_MODEL = ENV_CONFIG[new_env]["model"]

    return {"success": True, "mode": LLM_MODE, "env": ENV_MODE, "model": API_MODEL if LLM_MODE == "api" else LOCAL_MODEL_PATH}


@app.post("/api/llm_mode")
async def set_llm_mode(data: dict):
    """LLM ëª¨ë“œ ì „í™˜ (ë³„ì¹­)"""
    return await set_mode(data)


@app.post("/api/set_data_dir")
async def set_data_dir(data: dict):
    """ë°ì´í„° í´ë” ê²½ë¡œ ì„¤ì •"""
    global M14_DATA_DIR
    
    new_dir = data.get("path", "")
    if os.path.exists(new_dir):
        M14_DATA_DIR = new_dir
        return {"success": True, "path": M14_DATA_DIR}
    else:
        return {"success": False, "message": "ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}


@app.get("/api/prompts")
async def get_prompts():
    """í˜„ì¬ í”„ë¡¬í”„íŠ¸ ì¡°íšŒ (txt íŒŒì¼ì—ì„œ ë¡œë“œ)"""
    return {
        "system_prompt": load_prompt("system"),
        "templates": {
            "summary": load_prompt("summary"),
            "pattern": load_prompt("pattern"),
            "prediction": load_prompt("prediction")
        }
    }


@app.post("/api/prompts")
async def set_prompts(data: dict):
    """í”„ë¡¬í”„íŠ¸ ìˆ˜ì • (txt íŒŒì¼ì— ì €ì¥)"""
    if "system_prompt" in data:
        save_prompt("system", data["system_prompt"])

    if "templates" in data:
        for key, value in data["templates"].items():
            if key in ["summary", "pattern", "prediction"]:
                save_prompt(key, value)

    return {"success": True, "message": "í”„ë¡¬í”„íŠ¸ê°€ txt íŒŒì¼ì— ì €ì¥ë¨"}


@app.post("/api/analyze")
async def analyze(request: AnalysisRequest):
    """ë°ì´í„° ë¶„ì„ ìš”ì²­"""
    
    # ë°ì´í„° ë¡œë“œ
    all_data = []
    all_alerts_10 = []
    all_alerts_30 = []
    
    start = datetime.strptime(request.date_start, "%Y%m%d")
    end = datetime.strptime(request.date_end, "%Y%m%d")
    
    current = start
    while current <= end:
        date_str = current.strftime("%Y%m%d")
        result = load_m14_data(date_str)
        
        if result["data"]:
            all_data.extend(result["data"])
            all_alerts_10.extend(result["alerts_10"])
            all_alerts_30.extend(result["alerts_30"])
        
        current += timedelta(days=1)
    
    if not all_data:
        return {"success": False, "answer": "âŒ í•´ë‹¹ ê¸°ê°„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
    
    # ì‹œê°„ í•„í„°ë§
    start_dt = request.date_start + request.time_start
    end_dt = request.date_end + request.time_end
    
    filtered_data = [
        row for row in all_data
        if start_dt <= str(row.get('CURRTIME', ''))[:12] <= end_dt
    ]
    
    if not filtered_data:
        return {"success": False, "answer": "âŒ ì„ íƒí•œ ì‹œê°„ ë²”ìœ„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
    
    # ë°ì´í„° ìš”ì•½ ìƒì„±
    df = pd.DataFrame(filtered_data)
    
    summary_stats = {
        "period": f"{request.date_start} {request.time_start} ~ {request.date_end} {request.time_end}",
        "total_count": len(df),
        "avg_totalcnt": round(df['TOTALCNT'].mean(), 1) if 'TOTALCNT' in df.columns else 0,
        "max_totalcnt": int(df['TOTALCNT'].max()) if 'TOTALCNT' in df.columns else 0,
        "min_totalcnt": int(df['TOTALCNT'].min()) if 'TOTALCNT' in df.columns else 0,
        "over_1700_count": int((df['TOTALCNT'] >= 1700).sum()) if 'TOTALCNT' in df.columns else 0,
        "over_1600_count": int((df['TOTALCNT'] >= 1600).sum()) if 'TOTALCNT' in df.columns else 0,
        "alerts_10_count": len(all_alerts_10),
        "alerts_30_count": len(all_alerts_30)
    }
    
    # ë¶„ì„ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
    if request.analysis_type == "summary":
        prompt = f"""ë‹¤ìŒ M14 ë°˜ì†¡ í ëª¨ë‹ˆí„°ë§ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

## ë°ì´í„° ìš”ì•½
- ê¸°ê°„: {summary_stats['period']}
- ì´ ë°ì´í„° ìˆ˜: {summary_stats['total_count']}ê°œ
- í‰ê·  TOTALCNT: {summary_stats['avg_totalcnt']}
- ìµœëŒ€ TOTALCNT: {summary_stats['max_totalcnt']}
- ìµœì†Œ TOTALCNT: {summary_stats['min_totalcnt']}
- 1700+ ì´ˆê³¼ íšŸìˆ˜: {summary_stats['over_1700_count']}íšŒ
- 1600+ ê²½ê³  íšŸìˆ˜: {summary_stats['over_1600_count']}íšŒ
- 10ë¶„ ì˜ˆì¸¡ ì•ŒëŒ: {summary_stats['alerts_10_count']}íšŒ
- 30ë¶„ ì˜ˆì¸¡ ì•ŒëŒ: {summary_stats['alerts_30_count']}íšŒ

ì „ë°˜ì ì¸ ìƒí™© ë¶„ì„ê³¼ ê°œì„ ì ì„ ì œì•ˆí•´ì£¼ì„¸ìš”."""

    elif request.analysis_type == "pattern":
        # ì‹œê°„ëŒ€ë³„ í‰ê·  ê³„ì‚°
        if 'CURRTIME' in df.columns:
            df['hour'] = df['CURRTIME'].astype(str).str[8:10].astype(int)
            hourly_avg = df.groupby('hour')['TOTALCNT'].mean().round(1).to_dict()
        else:
            hourly_avg = {}
        
        prompt = f"""M14 ë°˜ì†¡ íì˜ ì‹œê°„ëŒ€ë³„ íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

## ë°ì´í„° ìš”ì•½
- ê¸°ê°„: {summary_stats['period']}
- ì´ ë°ì´í„°: {summary_stats['total_count']}ê°œ

## ì‹œê°„ëŒ€ë³„ í‰ê·  TOTALCNT
{hourly_avg}

ì‹œê°„ëŒ€ë³„ íŒ¨í„´, í”¼í¬ íƒ€ì„, ì£¼ì˜ê°€ í•„ìš”í•œ ì‹œê°„ëŒ€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."""

    elif request.analysis_type == "prediction":
        # ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚° ì‹œë„
        pred_accuracy = "ë°ì´í„° ë¶€ì¡±"
        if 'PREDICT_10' in df.columns and 'TOTALCNT' in df.columns:
            # ì˜ˆì¸¡ 1700+ vs ì‹¤ì œ 1700+
            pred_over = (df['PREDICT_10'] >= 1700).sum()
            actual_over = (df['TOTALCNT'] >= 1700).sum()
            pred_accuracy = f"ì˜ˆì¸¡ 1700+: {pred_over}íšŒ, ì‹¤ì œ 1700+: {actual_over}íšŒ"
        
        prompt = f"""M14 ë°˜ì†¡ í ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

## ë°ì´í„° ìš”ì•½
- ê¸°ê°„: {summary_stats['period']}
- ì´ ë°ì´í„°: {summary_stats['total_count']}ê°œ
- 10ë¶„ ì˜ˆì¸¡ ì•ŒëŒ: {summary_stats['alerts_10_count']}íšŒ
- 30ë¶„ ì˜ˆì¸¡ ì•ŒëŒ: {summary_stats['alerts_30_count']}íšŒ
- ì˜ˆì¸¡ ì •í™•ë„ ì°¸ê³ : {pred_accuracy}

ì˜ˆì¸¡ ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ê°œì„  ë°©í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."""

    else:  # custom
        if not request.question:
            return {"success": False, "answer": "âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}
        
        prompt = f"""M14 ë°˜ì†¡ í ëª¨ë‹ˆí„°ë§ ë°ì´í„°ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”.

## ë°ì´í„° ìš”ì•½
- ê¸°ê°„: {summary_stats['period']}
- ì´ ë°ì´í„°: {summary_stats['total_count']}ê°œ
- í‰ê·  TOTALCNT: {summary_stats['avg_totalcnt']}
- ìµœëŒ€ TOTALCNT: {summary_stats['max_totalcnt']}
- 1700+ ì´ˆê³¼: {summary_stats['over_1700_count']}íšŒ

## ì§ˆë¬¸
{request.question}"""

    # LLM í˜¸ì¶œ
    result = call_llm(prompt, get_system_prompt())
    
    if result["success"]:
        return {
            "success": True,
            "answer": result["content"],
            "stats": summary_stats,
            "mode": LLM_MODE
        }
    else:
        return {"success": False, "answer": f"âŒ {result['error']}"}


@app.post("/api/quick_analyze")
async def quick_analyze(data: dict):
    """ë¹ ë¥¸ ë¶„ì„ (ê°„ë‹¨í•œ ì§ˆë¬¸)"""
    question = data.get("question", "")
    date_str = data.get("date", datetime.now().strftime("%Y%m%d"))
    
    if not question:
        return {"success": False, "answer": "âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}
    
    # ë°ì´í„° ë¡œë“œ
    result = load_m14_data(date_str)
    
    if result["data"] is None:
        return {"success": False, "answer": f"âŒ {date_str} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
    
    stats = result["stats"]
    
    prompt = f"""M14 ë°˜ì†¡ í ë°ì´í„° ({date_str})ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”.

## ì˜¤ëŠ˜ ë°ì´í„° ìš”ì•½
- ë°ì´í„° ìˆ˜: {stats.get('count', 0)}ê°œ
- í‰ê·  TOTALCNT: {stats.get('avg', 0)}
- ìµœëŒ€ TOTALCNT: {stats.get('max', 0)}
- 1700+ ì´ˆê³¼: {stats.get('over_1700', 0)}íšŒ

## ì§ˆë¬¸
{question}"""

    llm_result = call_llm(prompt, get_system_prompt(), max_tokens=1500)
    
    if llm_result["success"]:
        return {"success": True, "answer": llm_result["content"], "stats": stats}
    else:
        return {"success": False, "answer": f"âŒ {llm_result['error']}"}


# ========================================
# CSV ì—…ë¡œë“œ ë¶„ì„
# ========================================
@app.post("/api/upload_csv")
async def upload_csv(file: UploadFile = File(...)):
    """CSV íŒŒì¼ ì—…ë¡œë“œ ë° íŒŒì‹±"""
    try:
        content = await file.read()
        
        # ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
        encodings = ['utf-8', 'cp949', 'euc-kr', 'utf-8-sig', 'latin1']
        content_str = None
        
        for enc in encodings:
            try:
                content_str = content.decode(enc)
                logger.info(f"CSV ì¸ì½”ë”© ê°ì§€: {enc}")
                break
            except:
                continue
        
        if content_str is None:
            return {"success": False, "error": "íŒŒì¼ ì¸ì½”ë”©ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        df = pd.read_csv(StringIO(content_str))
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬ (ê³µë°± ì œê±°)
        df.columns = df.columns.str.strip()
        
        # ê¸°ë³¸ í†µê³„ ê³„ì‚°
        stats = {
            "filename": file.filename,
            "row_count": len(df),
            "columns": list(df.columns)
        }
        
        # ì£¼ìš” ì»¬ëŸ¼ë³„ í†µê³„
        if 'í˜„ì¬TOTALCNT' in df.columns:
            stats["avg_totalcnt"] = round(df['í˜„ì¬TOTALCNT'].mean(), 1)
            stats["max_totalcnt"] = int(df['í˜„ì¬TOTALCNT'].max())
            stats["min_totalcnt"] = int(df['í˜„ì¬TOTALCNT'].min())
        
        if 'ì‹¤ì œìœ„í—˜(1700+)' in df.columns:
            stats["danger_count"] = int(df['ì‹¤ì œìœ„í—˜(1700+)'].sum())
        
        if 'ìµœì¢…íŒì •' in df.columns:
            stats["pred_danger_count"] = int(df['ìµœì¢…íŒì •'].sum())
        
        if 'ì˜ˆì¸¡ìƒíƒœ' in df.columns:
            status_counts = df['ì˜ˆì¸¡ìƒíƒœ'].value_counts().to_dict()
            stats["status_counts"] = status_counts
            
            # TP, TN, FP, FN ê³„ì‚°
            stats["TP"] = sum(1 for s in df['ì˜ˆì¸¡ìƒíƒœ'] if 'TP' in str(s))
            stats["TN"] = sum(1 for s in df['ì˜ˆì¸¡ìƒíƒœ'] if 'TN' in str(s))
            stats["FP"] = sum(1 for s in df['ì˜ˆì¸¡ìƒíƒœ'] if 'FP' in str(s))
            stats["FN"] = sum(1 for s in df['ì˜ˆì¸¡ìƒíƒœ'] if 'FN' in str(s))
        
        # ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 10í–‰)
        sample_data = df.head(10).to_dict('records')
        
        return {
            "success": True,
            "stats": stats,
            "sample": sample_data,
            "data": df.to_dict('records')
        }
        
    except Exception as e:
        logger.error(f"CSV ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": f"CSV íŒŒì‹± ì‹¤íŒ¨: {str(e)}"}


@app.post("/api/analyze_csv")
async def analyze_csv(data: dict):
    """ì—…ë¡œë“œëœ CSV ë°ì´í„° LLM ë¶„ì„"""
    csv_data = data.get("data", [])
    stats = data.get("stats", {})
    question = data.get("question", "")
    analysis_type = data.get("analysis_type", "summary")
    
    if not csv_data:
        return {"success": False, "answer": "âŒ CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
    
    df = pd.DataFrame(csv_data)
    
    # í†µê³„ ìš”ì•½ ìƒì„±
    summary_text = f"""## CSV ë°ì´í„° ìš”ì•½
- íŒŒì¼ëª…: {stats.get('filename', 'unknown')}
- ë°ì´í„° ìˆ˜: {stats.get('row_count', len(df))}ê°œ
- ì»¬ëŸ¼: {', '.join(stats.get('columns', [])[:10])}{'...' if len(stats.get('columns', [])) > 10 else ''}
"""
    
    if 'avg_totalcnt' in stats:
        summary_text += f"""
## TOTALCNT í†µê³„
- í‰ê· : {stats.get('avg_totalcnt')}
- ìµœëŒ€: {stats.get('max_totalcnt')}
- ìµœì†Œ: {stats.get('min_totalcnt')}
- 1700+ ì‹¤ì œ ìœ„í—˜: {stats.get('danger_count', 0)}íšŒ
- 1700+ ì˜ˆì¸¡ ìœ„í—˜: {stats.get('pred_danger_count', 0)}íšŒ
"""
    
    if 'TP' in stats:
        total = stats.get('TP', 0) + stats.get('TN', 0) + stats.get('FP', 0) + stats.get('FN', 0)
        accuracy = round((stats.get('TP', 0) + stats.get('TN', 0)) / total * 100, 2) if total > 0 else 0
        recall = round(stats.get('TP', 0) / (stats.get('TP', 0) + stats.get('FN', 0)) * 100, 2) if (stats.get('TP', 0) + stats.get('FN', 0)) > 0 else 0
        precision = round(stats.get('TP', 0) / (stats.get('TP', 0) + stats.get('FP', 0)) * 100, 2) if (stats.get('TP', 0) + stats.get('FP', 0)) > 0 else 0
        
        summary_text += f"""
## ì˜ˆì¸¡ ì„±ëŠ¥
- TP (ì •í™•í•œ ìœ„í—˜ ì˜ˆì¸¡): {stats.get('TP', 0)}
- TN (ì •í™•í•œ ì •ìƒ ì˜ˆì¸¡): {stats.get('TN', 0)}
- FP (ì˜¤íƒ - ì˜ëª»ëœ ìœ„í—˜ ì˜ˆì¸¡): {stats.get('FP', 0)}
- FN (ë†“ì¹¨ - ìœ„í—˜ ë¯¸íƒì§€): {stats.get('FN', 0)}
- ì •í™•ë„: {accuracy}%
- ì¬í˜„ìœ¨: {recall}%
- ì •ë°€ë„: {precision}%
"""
    
    if 'status_counts' in stats:
        summary_text += f"""
## ì˜ˆì¸¡ìƒíƒœ ë¶„í¬
"""
        for status, count in stats.get('status_counts', {}).items():
            summary_text += f"- {status}: {count}ê°œ\n"
    
    # ë¶„ì„ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ (PROMPT_TEMPLATES ì‚¬ìš©)
    if analysis_type == "summary":
        template = get_template("summary") or "ìœ„ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."
        prompt = f"""{summary_text}

{template}"""

    elif analysis_type == "model":
        # ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’ í†µê³„
        model_cols = ['XGB_íƒ€ê²Ÿ', 'XGB_ì¤‘ìš”', 'XGB_ë³´ì¡°', 'XGB_PDT', 'XGB_Job']
        model_stats = ""
        for col in model_cols:
            if col in df.columns:
                over_1700 = (df[col] >= 1700).sum()
                model_stats += f"- {col}: í‰ê·  {df[col].mean():.1f}, ìµœëŒ€ {df[col].max():.1f}, 1700+ ì˜ˆì¸¡ {over_1700}ê°œ ({over_1700/len(df)*100:.2f}%)\n"
        
        template = get_template("model") or "ëª¨ë¸ë³„ ì„±ëŠ¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
        prompt = f"""{summary_text}

## ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’ í†µê³„
{model_stats}

{template}"""

    elif analysis_type == "error":
        # FP, FN ì¼€ì´ìŠ¤ ë¶„ì„
        fp_cases = df[df['ì˜ˆì¸¡ìƒíƒœ'].str.contains('FP', na=False)] if 'ì˜ˆì¸¡ìƒíƒœ' in df.columns else pd.DataFrame()
        fn_cases = df[df['ì˜ˆì¸¡ìƒíƒœ'].str.contains('FN', na=False)] if 'ì˜ˆì¸¡ìƒíƒœ' in df.columns else pd.DataFrame()
        
        error_info = f"""
## ì˜¤ë¥˜ ì¼€ì´ìŠ¤ ë¶„ì„
- FP (ì˜¤íƒ) ì¼€ì´ìŠ¤: {len(fp_cases)}ê°œ
- FN (ë¯¸íƒ) ì¼€ì´ìŠ¤: {len(fn_cases)}ê°œ
"""
        if len(fp_cases) > 0 and 'í˜„ì¬TOTALCNT' in fp_cases.columns:
            error_info += f"- FP í‰ê·  TOTALCNT: {fp_cases['í˜„ì¬TOTALCNT'].mean():.1f}\n"
        if len(fn_cases) > 0 and 'í˜„ì¬TOTALCNT' in fn_cases.columns:
            error_info += f"- FN í‰ê·  TOTALCNT: {fn_cases['í˜„ì¬TOTALCNT'].mean():.1f}\n"
        
        template = get_template("error") or "FP/FN ì˜¤ë¥˜ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."
        prompt = f"""{summary_text}
{error_info}

{template}"""

    else:  # custom
        if not question:
            return {"success": False, "answer": "âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}
        
        prompt = f"""{summary_text}

## ì§ˆë¬¸
{question}"""

    # LLM í˜¸ì¶œ
    result = call_llm(prompt, get_system_prompt())
    
    if result["success"]:
        return {
            "success": True,
            "answer": result["content"],
            "stats": stats,
            "mode": LLM_MODE
        }
    else:
        return {"success": False, "answer": f"âŒ {result['error']}"}


def build_csv_prompt(csv_data, stats, analysis_type, question=""):
    """CSV ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± - PROMPT_TEMPLATES ì‚¬ìš©"""
    df = pd.DataFrame(csv_data)
    
    # í†µê³„ ìš”ì•½ ìƒì„±
    summary_text = f"""## CSV ë°ì´í„° ìš”ì•½
- íŒŒì¼ëª…: {stats.get('filename', 'unknown')}
- ë°ì´í„° ìˆ˜: {stats.get('row_count', len(df))}ê°œ
"""
    
    if 'avg_totalcnt' in stats:
        summary_text += f"""
## TOTALCNT í†µê³„
| í•­ëª© | ê°’ |
|------|-----|
| í‰ê·  | {stats.get('avg_totalcnt')} |
| ìµœëŒ€ | {stats.get('max_totalcnt')} |
| ìµœì†Œ | {stats.get('min_totalcnt')} |
| 1700+ ì‹¤ì œ | {stats.get('danger_count', 0)}íšŒ |
| 1700+ ì˜ˆì¸¡ | {stats.get('pred_danger_count', 0)}íšŒ |
"""
    
    if 'TP' in stats:
        total = stats.get('TP', 0) + stats.get('TN', 0) + stats.get('FP', 0) + stats.get('FN', 0)
        accuracy = round((stats.get('TP', 0) + stats.get('TN', 0)) / total * 100, 2) if total > 0 else 0
        recall = round(stats.get('TP', 0) / (stats.get('TP', 0) + stats.get('FN', 0)) * 100, 2) if (stats.get('TP', 0) + stats.get('FN', 0)) > 0 else 0
        precision = round(stats.get('TP', 0) / (stats.get('TP', 0) + stats.get('FP', 0)) * 100, 2) if (stats.get('TP', 0) + stats.get('FP', 0)) > 0 else 0
        
        summary_text += f"""
## ì˜ˆì¸¡ ì„±ëŠ¥
| ì§€í‘œ | ê°’ |
|------|-----|
| TP | {stats.get('TP', 0)} |
| TN | {stats.get('TN', 0)} |
| FP (ì˜¤íƒ) | {stats.get('FP', 0)} |
| FN (ë†“ì¹¨) | {stats.get('FN', 0)} |
| ì •í™•ë„ | {accuracy}% |
| ì¬í˜„ìœ¨ | {recall}% |
| ì •ë°€ë„ | {precision}% |
"""
    
    if 'status_counts' in stats:
        summary_text += "\n## ì˜ˆì¸¡ìƒíƒœ ë¶„í¬\n| ìƒíƒœ | ê°œìˆ˜ |\n|------|------|\n"
        for status, count in stats.get('status_counts', {}).items():
            summary_text += f"| {status} | {count} |\n"
    
    # ë¶„ì„ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ (PROMPT_TEMPLATES ì‚¬ìš©)
    if analysis_type == "summary":
        template = get_template("summary") or "ì¢…í•© ë¶„ì„í•´ì£¼ì„¸ìš”."
        prompt = f"{summary_text}\n\n{template}"

    elif analysis_type == "pattern":
        # ì‹œê°„ëŒ€ë³„ í†µê³„ ì¶”ê°€
        hour_stats = "\n## ì‹œê°„ëŒ€ë³„ í†µê³„\n| ì‹œê°„ | í‰ê·  | ìµœëŒ€ | 1700+ |\n|------|------|------|-------|\n"
        if 'CURRTIME' in df.columns or 'í˜„ì¬ì‹œê°„' in df.columns:
            time_col = 'CURRTIME' if 'CURRTIME' in df.columns else 'í˜„ì¬ì‹œê°„'
            try:
                df['hour'] = df[time_col].astype(str).str[11:13].astype(int)
                for h in sorted(df['hour'].unique()):
                    h_data = df[df['hour'] == h]
                    tc_col = 'TOTALCNT' if 'TOTALCNT' in df.columns else 'í˜„ì¬TOTALCNT'
                    if tc_col in df.columns:
                        avg = h_data[tc_col].mean()
                        max_val = h_data[tc_col].max()
                        over = (h_data[tc_col] >= 1700).sum()
                        hour_stats += f"| {h:02d}ì‹œ | {avg:.0f} | {max_val:.0f} | {over}íšŒ |\n"
            except:
                hour_stats = "\n## ì‹œê°„ëŒ€ë³„ í†µê³„\n(ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨)\n"
        
        template = get_template("pattern") or "íŒ¨í„´ ë¶„ì„í•´ì£¼ì„¸ìš”."
        prompt = f"{summary_text}{hour_stats}\n{template}"

    elif analysis_type == "prediction":
        # ì˜ˆì¸¡ ì„±ëŠ¥ ì¶”ê°€ ì •ë³´
        pred_info = "\n## ì˜ˆì¸¡ ìƒì„¸\n"
        if 'ì˜ˆì¸¡ìƒíƒœ' in df.columns:
            fp_count = df['ì˜ˆì¸¡ìƒíƒœ'].str.contains('FP', na=False).sum()
            fn_count = df['ì˜ˆì¸¡ìƒíƒœ'].str.contains('FN', na=False).sum()
            pred_info += f"| FP (ì˜¤íƒ) | {fp_count}ê±´ |\n| FN (ë†“ì¹¨) | {fn_count}ê±´ |\n"
        
        template = get_template("prediction") or "ì˜ˆì¸¡ ë¶„ì„í•´ì£¼ì„¸ìš”."
        prompt = f"{summary_text}{pred_info}\n{template}"

    else:  # custom
        prompt = f"{summary_text}\n\n## ì§ˆë¬¸\n{question}"
    
    return prompt


@app.post("/api/analyze_csv_stream")
async def analyze_csv_stream(data: dict):
    """ìŠ¤íŠ¸ë¦¬ë° CSV ë¶„ì„"""
    csv_data = data.get("data", [])
    stats = data.get("stats", {})
    question = data.get("question", "")
    analysis_type = data.get("analysis_type", "summary")
    
    if not csv_data:
        async def error_gen():
            yield "data: [ERROR] CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
        return StreamingResponse(error_gen(), media_type="text/event-stream")
    
    if analysis_type == "custom" and not question:
        async def error_gen():
            yield "data: [ERROR] ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n\n"
        return StreamingResponse(error_gen(), media_type="text/event-stream")
    
    prompt = build_csv_prompt(csv_data, stats, analysis_type, question)
    
    # ë¡œì»¬ LLMë§Œ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
    if LLM_MODE == "local" and llm is not None:
        return StreamingResponse(
            stream_local_llm(prompt, get_system_prompt()),
            media_type="text/event-stream"
        )
    else:
        # API ëª¨ë“œëŠ” ì¼ë°˜ ì‘ë‹µ í›„ í•œë²ˆì— ì „ì†¡
        async def api_response_gen():
            result = call_llm(prompt, get_system_prompt())
            if result["success"]:
                # í•œê¸€ìì”© ì „ì†¡ (ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼) - ì¤„ë°”ê¿ˆ ì´ìŠ¤ì¼€ì´í”„
                content = result["content"].replace('\n', 'â')
                for char in content:
                    yield f"data: {char}\n\n"
                yield "data: [DONE]\n\n"
            else:
                yield f"data: [ERROR] {result['error']}\n\n"
        
        return StreamingResponse(api_response_gen(), media_type="text/event-stream")


# ============================================================================
# ë§ˆí¬ë‹¤ìš´ -> HTML ë³€í™˜ API
# ============================================================================

@app.post("/api/markdown_to_html")
async def markdown_to_html(request: Request):
    """ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ HTMLë¡œ ë³€í™˜"""
    try:
        import markdown
        from markdown.extensions.tables import TableExtension
        from markdown.extensions.fenced_code import FencedCodeExtension
        from markdown.extensions.nl2br import Nl2BrExtension
        
        body = await request.json()
        md_text = body.get("text", "")
        
        if not md_text:
            return {"html": ""}
        
        # ë§ˆí¬ë‹¤ìš´ ì „ì²˜ë¦¬: ë¶™ì–´ìˆëŠ” í˜•ì‹ ë¶„ë¦¬
        md_text = md_text.replace('---####', '---\n\n####')
        md_text = md_text.replace('---###', '---\n\n###')
        md_text = md_text.replace('---##', '---\n\n##')
        md_text = md_text.replace('---#', '---\n\n#')
        
        # í…Œì´ë¸” ì•ë’¤ ì¤„ë°”ê¿ˆ ë³´ì¥
        import re
        md_text = re.sub(r'([^\n])(\n\|)', r'\1\n\2', md_text)
        md_text = re.sub(r'(\|[^\n]*\n)([^\|])', r'\1\n\2', md_text)
        
        # ë§ˆí¬ë‹¤ìš´ -> HTML ë³€í™˜
        md = markdown.Markdown(extensions=[
            TableExtension(),
            FencedCodeExtension(),
            Nl2BrExtension(),
            'md_in_html'
        ])
        
        html = md.convert(md_text)
        
        return {"html": html}
        
    except ImportError:
        # markdown ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ë³€í™˜
        logger.warning("markdown ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ, ê¸°ë³¸ ë³€í™˜ ì‚¬ìš©")
        return {"html": f"<pre>{md_text}</pre>"}
    except Exception as e:
        logger.error(f"ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì˜¤ë¥˜: {e}")
        return {"html": f"<pre>{body.get('text', '')}</pre>"}


# ============================================================================
# HISTORY.HTMLìš© LLM ë¶„ì„ API (ìŠ¤íŠ¸ë¦¬ë°)
# ============================================================================

@app.post("/api/llm_history_analyze")
async def llm_history_analyze(data: dict):
    """HISTORY í˜ì´ì§€ì—ì„œ í˜„ì¬ ì¡°íšŒëœ ë°ì´í„°ë¥¼ LLMìœ¼ë¡œ ë¶„ì„ (ìŠ¤íŠ¸ë¦¬ë°)"""
    history_data = data.get("data", [])
    stats = data.get("stats", {})
    question = data.get("question", "")
    analysis_type = data.get("analysis_type", "summary")

    if not stats:
        async def error_gen():
            yield "data: [ERROR] ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
        return StreamingResponse(error_gen(), media_type="text/event-stream")

    if analysis_type == "custom" and not question:
        async def error_gen():
            yield "data: [ERROR] ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n\n"
        return StreamingResponse(error_gen(), media_type="text/event-stream")

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = build_history_prompt(stats, analysis_type, question)

    # ë¡œì»¬ LLMë§Œ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
    if LLM_MODE == "local" and llm is not None:
        return StreamingResponse(
            stream_local_llm(prompt, get_system_prompt()),
            media_type="text/event-stream"
        )
    else:
        # API ëª¨ë“œëŠ” ì¼ë°˜ ì‘ë‹µ í›„ í•œë²ˆì— ì „ì†¡
        async def api_response_gen():
            result = call_llm(prompt, get_system_prompt())
            if result["success"]:
                # í•œê¸€ìì”© ì „ì†¡ (ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼) - ì¤„ë°”ê¿ˆ ì´ìŠ¤ì¼€ì´í”„
                content = result["content"].replace('\n', 'â')
                for char in content:
                    yield f"data: {char}\n\n"
                yield "data: [DONE]\n\n"
            else:
                yield f"data: [ERROR] {result['error']}\n\n"

        return StreamingResponse(api_response_gen(), media_type="text/event-stream")


def build_history_prompt(stats: dict, analysis_type: str, question: str = "") -> str:
    """HISTORY ë°ì´í„° ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""

    # TP/TN/FP/FN ì •ë³´
    tp = stats.get('tp', 0)
    tn = stats.get('tn', 0)
    fp = stats.get('fp', 0)
    fn = stats.get('fn', 0)
    accuracy = stats.get('accuracy', 0)

    # ê¸°ë³¸ í†µê³„ ìš”ì•½
    summary_text = f"""## M14 íˆìŠ¤í† ë¦¬ ë°ì´í„° ìš”ì•½
- ì¡°íšŒ ê¸°ê°„: {stats.get('period', 'N/A')}
- ì´ ë°ì´í„° ìˆ˜: {stats.get('total_count', 0)}ê°œ

## TOTALCNT í†µê³„
| í•­ëª© | ê°’ |
|------|-----|
| í‰ê·  | {stats.get('avg_totalcnt', 0)} |
| ìµœëŒ€ | {stats.get('max_totalcnt', 0)} |
| ìµœì†Œ | {stats.get('min_totalcnt', 0)} |
| 1700+ ì´ˆê³¼ (ì‹¤ì œ ìœ„í—˜) | {stats.get('over_1700_count', 0)}íšŒ |
| 1600+ ê²½ê³  | {stats.get('over_1600_count', 0)}íšŒ |

## ì˜ˆì¸¡ ì„±ëŠ¥ (10ë¶„ ì˜ˆì¸¡ ê¸°ì¤€)
| í•­ëª© | ê°’ | ì„¤ëª… |
|------|-----|------|
| ì •í™•ë„ | {accuracy}% | (TP+TN) / ì „ì²´ |
| TP (ì ì¤‘) | {tp}íšŒ | 1700+ ì˜ˆì¸¡ â†’ ì‹¤ì œ 1700+ |
| TN (ì •ìƒì˜ˆì¸¡) | {tn}íšŒ | ì •ìƒ ì˜ˆì¸¡ â†’ ì‹¤ì œ ì •ìƒ |
| FP (ì˜¤íƒ) | {fp}íšŒ | 1700+ ì˜ˆì¸¡ â†’ ì‹¤ì œ ì •ìƒ |
| FN (ë†“ì¹¨) | {fn}íšŒ | ì •ìƒ ì˜ˆì¸¡ â†’ ì‹¤ì œ 1700+ |

## ì•ŒëŒ ë°œìƒ í˜„í™©
| ì¢…ë¥˜ | ë°œìƒ íšŸìˆ˜ |
|------|-----------|
| 10ë¶„ ì˜ˆì¸¡ ì•ŒëŒ | {stats.get('alerts_10_count', 0)}íšŒ |
| 30ë¶„ ì˜ˆì¸¡ ì•ŒëŒ | {stats.get('alerts_30_count', 0)}íšŒ |
| ë¡œê·¸í”„ë ˆì†Œ ì•ŒëŒ | {stats.get('logpresso_alarms', 0)}íšŒ |
"""

    # ì‹œê°„ëŒ€ë³„ í†µê³„ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    hourly_stats = stats.get('hourly_stats', {})
    if hourly_stats:
        summary_text += "\n## ì‹œê°„ëŒ€ë³„ í†µê³„\n| ì‹œê°„ | í‰ê·  | 1700+ íšŸìˆ˜ |\n|------|------|------------|\n"
        for hour, data in sorted(hourly_stats.items()):
            if isinstance(data, dict):
                summary_text += f"| {hour} | {data.get('avg', 0)} | {data.get('over1700', 0)}íšŒ |\n"
            else:
                summary_text += f"| {hour} | {data} | - |\n"

    # ê¸‰ì¦ ì»¬ëŸ¼(SPIKE_INFO) í†µê³„ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    spike_stats = stats.get('spike_stats', {})
    if spike_stats:
        summary_text += f"""
## 10ë¶„ ê¸‰ì¦ ì»¬ëŸ¼ ë¶„ì„ (SPIKE_INFO)
- ìœ„í—˜ ê¸‰ì¦(D, +60 ì´ìƒ): {spike_stats.get('total_danger_spikes', 0)}íšŒ
- ê²½ê³  ê¸‰ì¦(W, +50~59): {spike_stats.get('total_warning_spikes', 0)}íšŒ
- ì´ ê¸‰ì¦ ë°œìƒ: {spike_stats.get('total_spikes', 0)}íšŒ
"""
        # ê¸‰ì¦ ì»¬ëŸ¼ ìˆœìœ„
        column_ranking = spike_stats.get('column_ranking', [])
        if column_ranking:
            summary_text += "\n### ê¸‰ì¦ ì»¬ëŸ¼ ìˆœìœ„ (TOP 10)\n| ìˆœìœ„ | ì»¬ëŸ¼ëª… | ìœ„í—˜(D) | ê²½ê³ (W) | ì´í•© | ìµœëŒ€ë³€í™”ëŸ‰ |\n|------|--------|---------|---------|------|------------|\n"
            for i, col in enumerate(column_ranking[:10], 1):
                summary_text += f"| {i} | {col.get('column', '')} | {col.get('danger_count', 0)} | {col.get('warning_count', 0)} | {col.get('total_count', 0)} | +{col.get('max_change', 0)} |\n"
        
        # ê¸‰ì¦ ë°œìƒ ì‹œì  ìƒ˜í”Œ
        spike_times = spike_stats.get('spike_times', [])
        if spike_times:
            summary_text += "\n### ê¸‰ì¦ ë°œìƒ ì‹œì  (ìµœê·¼ 10ê±´)\n| ì‹œê°„ | TOTALCNT | ê¸‰ì¦ ì •ë³´ |\n|------|----------|----------|\n"
            for spike in spike_times[:10]:
                summary_text += f"| {spike.get('time', '')} | {spike.get('totalcnt', 0)} | {spike.get('info', '')} |\n"

    # ìƒíƒœ ì˜ˆìƒ(STATUS_PREDICTION) ë¶„í¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    status_stats = stats.get('status_stats', {})
    if status_stats:
        distribution = status_stats.get('distribution', {})
        summary_text += f"""
## ìƒíƒœ ì˜ˆìƒ ë¶„í¬ (STATUS_PREDICTION)
| ìƒíƒœ | ê±´ìˆ˜ | ë¹„ìœ¨ |
|------|------|------|
| ë³‘ëª©ì˜ˆìƒ | {distribution.get('ë³‘ëª©ì˜ˆìƒ', 0)} | {status_stats.get('bottleneck_ratio', 0)}% |
| ìœ„í—˜ì˜ˆìƒ | {distribution.get('ìœ„í—˜ì˜ˆìƒ', 0)} | {status_stats.get('danger_ratio', 0)}% |
| ê´€ì°° | {distribution.get('ê´€ì°°', 0)} | - |
| ì–‘í˜¸ì˜ˆìƒ | {distribution.get('ì–‘í˜¸ì˜ˆìƒ', 0)} | - |
| ë³‘ëª© ì¿¨íƒ€ì„ | {distribution.get('ë³‘ëª©_ì¿¨íƒ€ì„', 0)} | - |
| ìœ„í—˜ ì¿¨íƒ€ì„ | {distribution.get('ìœ„í—˜_ì¿¨íƒ€ì„', 0)} | - |
"""
        # ìƒíƒœ ë³€í™” ì´ë ¥
        status_changes = status_stats.get('status_changes', [])
        if status_changes:
            summary_text += "\n### ì£¼ìš” ìƒíƒœ ë³€í™” ì´ë ¥ (ìµœê·¼ 10ê±´)\n| ì‹œê°„ | ë³€í™” | TOTALCNT |\n|------|------|----------|\n"
            for change in status_changes[:10]:
                summary_text += f"| {change.get('time', '')} | {change.get('from', '')} â†’ {change.get('to', '')} | {change.get('totalcnt', 0)} |\n"

    # ë¶„ì„ ìœ í˜•ë³„ í…œí”Œë¦¿ ì ìš©
    if analysis_type == "summary":
        template = get_template("summary") or """ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ì¢…í•© ë¶„ì„í•´ì£¼ì„¸ìš”:

1. **TOTALCNT í˜„í™©**: ìœ„í—˜ êµ¬ê°„ ë°œìƒ íŒ¨í„´ ë° ì‹œê°„ëŒ€ë³„ íŠ¹ì´ì‚¬í•­
2. **ê¸‰ì¦ ì»¬ëŸ¼(SPIKE_INFO) ë¶„ì„**: ë¹ˆë²ˆí•˜ê²Œ ê¸‰ì¦í•œ ì»¬ëŸ¼ TOP 3, ìœ„í—˜ê¸‰ì¦(D)/ê²½ê³ ê¸‰ì¦(W) íŒ¨í„´, TOTALCNTì™€ì˜ ìƒê´€ê´€ê³„
3. **ìƒíƒœ ì˜ˆìƒ(STATUS_PREDICTION) ë¶„ì„**: ë³‘ëª©ì˜ˆìƒ/ìœ„í—˜ì˜ˆìƒ ë°œìƒ ë¹ˆë„, ìƒíƒœ ë³€í™” íë¦„
4. **ì˜ˆì¸¡ ì„±ëŠ¥**: TP/FN/FP ì›ì¸ ë¶„ì„
5. **ì¢…í•© ì†Œê²¬**: ì£¼ìš” ë°œê²¬ì‚¬í•­ 3ê°€ì§€ì™€ ìš´ì˜ì ê¶Œê³ ì‚¬í•­"""
        prompt = f"{summary_text}\n\n{template}"

    elif analysis_type == "pattern":
        template = get_template("pattern") or """ìœ„ ë°ì´í„°ì—ì„œ ë‹¤ìŒ íŒ¨í„´ì„ ì‹¬ì¸µ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. **ê¸‰ì¦ ì»¬ëŸ¼ íŒ¨í„´**: TOTALCNT ê¸‰ë“± ì „ ì„ í–‰ ì§€í‘œ, ì»¬ëŸ¼ ê°„ ì—°ì‡„ ë°˜ì‘, ìœ„í—˜ ì¡°í•©
2. **ì‹œê°„ëŒ€ë³„ íŒ¨í„´**: ê¸‰ì¦ ì§‘ì¤‘ ì‹œê°„ëŒ€, ë³‘ëª© ë°œìƒ ì‹œê°„ëŒ€
3. **ìƒíƒœ ì „ì´ íŒ¨í„´**: ì–‘í˜¸â†’ìœ„í—˜â†’ë³‘ëª© ì „í™˜ ì†Œìš”ì‹œê°„, ë³‘ëª© ì „ ì „ì¡° íŒ¨í„´
4. **ì˜ˆì¸¡ ì •í™•ë„ íŒ¨í„´**: FN/FP ë°œìƒ ì‹œ ê³µí†µ íŒ¨í„´
5. **í•µì‹¬ ì¸ì‚¬ì´íŠ¸**: ë°œê²¬ëœ ì£¼ìš” íŒ¨í„´ 3ê°€ì§€ì™€ ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸"""
        prompt = f"{summary_text}\n\n{template}"

    elif analysis_type == "prediction":
        # ì˜ˆì¸¡ ê´€ë ¨ ì¶”ê°€ ì •ë³´
        pred_info = "\n## ì˜ˆì¸¡ ì•ŒëŒ ë¶„ì„\n"

        alerts_10 = stats.get('alerts_10_count', 0)
        alerts_30 = stats.get('alerts_30_count', 0)
        over_1700 = stats.get('over_1700_count', 0)

        if over_1700 > 0:
            # ê°„ë‹¨í•œ ì ì¤‘ë¥  ì¶”ì • (ì‹¤ì œë¡œëŠ” ë°ì´í„°ì—ì„œ ê³„ì‚°í•´ì•¼ í•¨)
            pred_info += f"- ì‹¤ì œ 1700+ ë°œìƒ: {over_1700}íšŒ\n"
            pred_info += f"- 10ë¶„ ì˜ˆì¸¡ ì•ŒëŒ: {alerts_10}íšŒ\n"
            pred_info += f"- 30ë¶„ ì˜ˆì¸¡ ì•ŒëŒ: {alerts_30}íšŒ\n"

        template = get_template("prediction") or """ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ ì‹¬ì¸µ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. **ì˜ˆì¸¡ ì„±ëŠ¥ ì¢…í•©**: ì •í™•ë„/ì¬í˜„ìœ¨/ì •ë°€ë„ í•´ì„, TP/TN/FP/FN ë¹„ì¤‘
2. **ê¸‰ì¦ ì»¬ëŸ¼ ê¸°ë°˜ ë¶„ì„**: SPIKE_INFOì™€ ì˜ˆì¸¡ ì ì¤‘ë¥  ê´€ê³„, ê¸‰ì¦ íŒ¨í„´ë³„ ì˜ˆì¸¡ ì •í™•ë„
3. **ìƒíƒœ ì˜ˆìƒ ì •í™•ë„**: STATUS_PREDICTION ì ì¤‘ë¥ , 10ë¶„/30ë¶„ ì•ŒëŒê³¼ì˜ ì¼ì¹˜ë„
4. **FN(ë†“ì¹¨) ë¶„ì„**: ë°œìƒ ì‹œì  ë° ê¸‰ì¦ ì»¬ëŸ¼ íŠ¹ì§•, ì‚¬ì „ ê°ì§€ ê°€ëŠ¥ ì—¬ë¶€
5. **FP(ì˜¤íƒ) ë¶„ì„**: ë°œìƒ ì›ì¸ ì¶”ì •, ê°ì†Œ ë°©ì•ˆ
6. **ê°œì„  ê¶Œê³ **: ê¸‰ì¦ ì»¬ëŸ¼ í™œìš© ë°©ì•ˆ, ìƒíƒœ ì˜ˆìƒ ë¡œì§ ê°œì„ , ì„ê³„ê°’ ì¡°ì • í•„ìš”ì„±"""
        prompt = f"{summary_text}{pred_info}\n{template}"

    else:  # custom
        prompt = f"{summary_text}\n\n## ì§ˆë¬¸\n{question}"

    return prompt


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8002)