# Logpresso 쿼리 명령어 종합 가이드

## 목차
1. [쿼리 문법 기초](#쿼리-문법-기초)
2. [데이터 소스 명령어](#데이터-소스-명령어)
3. [데이터 변환 명령어](#데이터-변환-명령어)
4. [통계 및 집계 명령어](#통계-및-집계-명령어)
5. [필터링 및 검색 명령어](#필터링-및-검색-명령어)
6. [파싱 명령어](#파싱-명령어)
7. [외부 데이터 연동 명령어](#외부-데이터-연동-명령어)
8. [시스템 명령어](#시스템-명령어)
9. [쿼리 유형](#쿼리-유형)
10. [함수](#함수)

---

## 쿼리 문법 기초

### 기본 구조
로그프레소 쿼리는 1개 이상의 명령문으로 구성됩니다. 명령문의 기본 형식:
```
command-name [opt_1=VALUE] [opt_2=VALUE] ... OBJECT[, ...]
```

### 파이프라인
명령문은 파이프(|)를 이용해 출력을 다른 명령문에 입력으로 전달합니다.

**예시:**
```
table araqne_query_logs | search login_name == "root" | timechart span=10m count
```
- 첫 번째: araqne_query_logs 테이블에서 데이터 조회
- 두 번째: login_name이 "root"인 데이터만 필터링
- 세 번째: 10분 단위로 통계 집계

### 서브쿼리
서브쿼리는 대괄호 쌍([ ])으로 감싸서 표현합니다. 서브쿼리는 상위 쿼리보다 먼저 실행됩니다.

**구조:**
```
command [ SUBCOMMAND_STATEMENT ]
```

### 주석 처리
`#` 기호를 사용하여 주석을 추가할 수 있습니다.

**규칙:**
- `#` 뒤에 공백문자 필수
- 서브쿼리 전체를 주석 처리: `# [ ... ]`

**예시:**
```
# 최근 1시간 CPU 로그 조회
table duration=1h sys_cpu_logs | 
# eval total = kernel + user
sort _time
```

### 쿼리 매개변수
`set` 또는 `setq` 명령으로 매개변수를 정의하고 `$()` 함수로 참조합니다.

**예시:**
```
set threshold = 80
table duration=1d sys_cpu_logs | search kernel + user >= $(threshold)
```

---

## 데이터 소스 명령어

### table
로그프레소 테이블에서 데이터를 조회합니다.

**문법:**
```
table [duration=TIME_SPEC] [from=DATE] [to=DATE] [reverse=BOOL] TABLE_NAME[, ...]
```

**주요 옵션:**
- `duration`: 조회 기간 (예: `1h`, `1d`, `7d`)
- `from`, `to`: 조회 시작/종료 일시
- `reverse=t`: 최신 데이터부터 조회 (기본값: 과거부터)
- `meta()`: 메타데이터 조건으로 테이블 선택

**특징:**
- 테이블 이름에 와일드카드(`*`) 사용 가능
- 물음표(`?`)로 테이블이 없어도 오류 없이 처리
- 여러 테이블 동시 조회 가능 (`,`로 구분)

**예시:**
```
table araqne_query_logs
table duration=1h sys_cpu_logs
table from=20200601 to=20200701 ssh_log
table sys_*  # sys_로 시작하는 모든 테이블
table web_log?  # web_log가 없어도 오류 없음
```

### csvfile
CSV 파일에서 데이터를 읽어옵니다.

**문법:**
```
csvfile [encoding=CHARSET] [tab=t] [offset=INT] [limit=INT] PATH
```

**주요 옵션:**
- `encoding`: 문자 인코딩 (예: `utf-8`, `euc-kr`)
- `tab=t`: 탭으로 구분된 파일
- `offset`: 건너뛸 행 수
- `limit`: 읽을 최대 행 수

### jsonfile
JSON 파일에서 데이터를 읽어옵니다.

**문법:**
```
jsonfile [overlay=t] [encoding=CHARSET] PATH
```

**주요 옵션:**
- `overlay=t`: 원본 line 필드 포함
- `encoding`: 문자 인코딩

**예시:**
```
jsonfile /opt/logpresso/samples/wp-nginx.json
jsonfile overlay=t /opt/logpresso/samples/wp-nginx.json
```

### textfile
텍스트 파일에서 데이터를 읽어옵니다.

**문법:**
```
textfile [encoding=CHARSET] [offset=INT] [limit=INT] PATH
```

### wget
웹에서 URL로 지정된 리소스를 가져옵니다.

**문법:**
```
wget url=URL [method={GET|POST}] [header=HEADER] [body=BODY]
```

**예시:**
```
wget url="https://api.github.com/repos/logpresso/CVE-2021-44228-Scanner/releases?per_page=100"
```

---

## 데이터 변환 명령어

### eval
새로운 필드를 생성하거나 기존 필드를 수정합니다.

**문법:**
```
eval FIELD_NAME = EXPRESSION [, ...]
```

**예시:**
```
table sys_cpu_logs | eval total = kernel + user
table sys_cpu_logs | eval cpu_pct = (kernel + user) * 100 / total
json "{}" | eval new=replace("hello world", "world", "logpresso")
```

### fields
출력할 필드를 선택합니다.

**문법:**
```
fields [+|-] FIELD_1 [, FIELD_2, ...]
```

**옵션:**
- `+`: 지정한 필드만 포함
- `-`: 지정한 필드 제외

**예시:**
```
table web_log | fields date, remote_host, request
table web_log | fields -sensitive_data
```

### rename
필드 이름을 변경합니다.

**문법:**
```
rename OLD_NAME as NEW_NAME [, ...]
```

**예시:**
```
table web_log | rename remote_host as client_ip
```

### replace()
문자열을 치환합니다.

**문법:**
```
replace(STRING, OLD, NEW [, "re"])
```

**매개변수:**
- `STRING`: 대상 문자열
- `OLD`: 찾을 문자열 (또는 정규표현식)
- `NEW`: 치환할 문자열
- `"re"`: 정규표현식 모드

**예시:**
```
json "{}" | eval new=replace("hello world", "world", "logpresso")
# 결과: "hello logpresso"

json "{}" | eval new=replace("123412345", "12", "!")
# 결과: "!34!345"

json "{}" | eval new=replace("google", "^g", "b", "re")
# 결과: "boogle"

json "{}" | eval new=replace("A:2 B:3 C:5", "A:(\\d+) B:\\d+ C:(\\d+)", "$1 $2", "re")
# 결과: "2 5"
```

---

## 통계 및 집계 명령어

### stats
데이터를 집계합니다.

**문법:**
```
stats AGG_FUNC(FIELD) [as ALIAS] [by GROUP_FIELD [, ...]]
```

**집계 함수:**
- `count()`: 레코드 개수
- `sum(FIELD)`: 합계
- `avg(FIELD)`: 평균
- `min(FIELD)`: 최소값
- `max(FIELD)`: 최대값
- `dc(FIELD)`: 고유값 개수 (distinct count)

**예시:**
```
table web_log | stats count() by status
table web_log | stats sum(bytes) as total_bytes, avg(response_time) by host
table web_log | stats dc(remote_host) as unique_visitors
```

### timechart
시간 기반으로 통계를 집계합니다.

**문법:**
```
timechart [span=TIME_SPEC] AGG_FUNC(FIELD) [by GROUP_FIELD]
```

**주요 옵션:**
- `span`: 집계 시간 간격 (예: `1m`, `10m`, `1h`, `1d`)

**예시:**
```
table araqne_query_logs | timechart span=10m count()
table web_log | timechart span=1h count() by status
```

### sort
데이터를 정렬합니다.

**문법:**
```
sort [+|-] FIELD_1 [, [+|-] FIELD_2, ...]
```

**옵션:**
- `+`: 오름차순 (기본값)
- `-`: 내림차순

**예시:**
```
table sys_cpu_logs | sort _time
table web_log | sort -bytes, +_time
```

### limit
출력 레코드 개수를 제한합니다.

**문법:**
```
limit [offset=INT] COUNT
```

**예시:**
```
table web_log | limit 100
table web_log | limit offset=10 50  # 10번째부터 50개
```

---

## 필터링 및 검색 명령어

### search
조건에 맞는 데이터만 필터링합니다.

**문법:**
```
search [limit=INT] EXPRESSION
```

**연산자:**
- `==`: 같음
- `!=`: 다름
- `<`, `<=`, `>`, `>=`: 비교
- `in()`: 포함 여부
- `and`, `or`, `not`: 논리 연산

**예시:**
```
table araqne_query_logs | search login_name == "root"
table web_log | search status >= 400 and status < 500
table web_log | search request == "*POST /wp-admin*"
table sys_cpu_logs | search kernel + user >= 80
```

---

## 파싱 명령어

### parse
데이터를 파싱합니다.

**문법:**
```
parse [parser=PARSER] [field=TARGET_FIELD] [overlay=t] [PARSING_RULE, ...]
```

**파싱 규칙 형식:**
```
"START_ANCHOR*STOP_ANCHOR" as FIELD_NAME
```

**주요 옵션:**
- `parser`: 사용할 파서 (예: `openssh`, `nginx`)
- `field`: 파싱할 필드 (기본값: `line`)
- `overlay=t`: 원본 데이터 유지

**예시:**
```
table ssh_log | parse openssh

textfile sample.txt | parse "session: *," as session, "Proto:*," as proto
```

### rex
정규표현식으로 필드를 추출합니다.

**문법:**
```
rex [field=FIELD] [overlay=t] REGEX_PATTERN
```

**정규표현식 그룹 형식:**
```
(?<FIELD_NAME>PATTERN)
```

**예시:**
```
rex field=line "(GET|POST) /game/flash/(?<filename>([^ ]*))"
rex field=line "(?<timestamp>\d+-\d+-\d+ \d+:\d+:\d+)"
rex field=line "(GET|POST) (?<url>[^ ]*) (?<querystring>[^ ]*)"
```

### parsejson
JSON 문자열을 파싱합니다.

**문법:**
```
parsejson [field=FIELD] [overlay=t]
```

**예시:**
```
wget url="https://api.github.com/repos/..." | parsejson
```

### parsemap
맵 형태의 데이터를 필드로 분해합니다.

**문법:**
```
parsemap [field=FIELD] [overlay=t]
```

### explode
배열 또는 리스트를 개별 레코드로 분해합니다.

**문법:**
```
explode FIELD
```

**예시:**
```
wget url="..." | parsejson | explode line | parsemap field=line
```

---

## 외부 데이터 연동 명령어

### dbquery
데이터베이스에 SQL 쿼리를 실행합니다.

**문법:**
```
dbquery PROFILE SQL_QUERY
```

**매개변수 사용:**
- 입력 매개변수: `:name` 형식
- `set` 명령으로 정의한 쿼리 매개변수 삽입

**예시:**
```
set id = 1000
dbquery oracle "SELECT * FROM users WHERE id = :id"
```

### dbcall
데이터베이스 저장 프로시저를 호출합니다.

**문법:**
```
dbcall PROFILE {call PROCEDURE_NAME(PARAMS)}
```

**출력 매개변수 형식:**
```
:name(type)  # type: varchar, int, datetime
```

**예시:**
```
dbcall mssql {call msdb.dbo.sp_columns("log_shipping_primaries")}
```

### sftp
SFTP 서버에서 파일을 읽거나 쓰기합니다.

**문법:**
```
sftp PROFILE {cat|ls|put} [OPTIONS] PATH
```

**작업 유형:**
- `cat`: 파일 내용 읽기
- `ls`: 파일 목록 조회
- `put`: 파일 쓰기

**put 옵션:**
- `format={csv|json|text|tsv}`: 출력 형식
- `fields=FIELD_1[,FIELD_2,...]`: 출력할 필드
- `append=t`: 파일에 추가
- `overwrite=t`: 파일 덮어쓰기
- `partition=t`: 시간 기반 파티셔닝

**예시:**
```
sftp srv cat /tmp/data.txt
sftp srv ls /home/user/
table classloading | sftp srv put format=csv fields=LoadedClassCount,UnloadedClassCount /tmp/class.csv
table classloading | sftp srv put format=json partition=t {logtime:/yyyy/MM/dd/}{now:HHmm}.txt
```

### ftp
FTP 서버에서 파일을 읽거나 쓰기합니다.

**문법:**
```
ftp PROFILE {cat|ls|put} [OPTIONS] PATH
```

(옵션은 sftp와 유사)

---

## 시스템 명령어

### system queries
현재 실행 중인 쿼리 상태를 조회합니다.

**문법:**
```
system queries
```

**출력 필드:**
- `id`: 쿼리 ID
- `query_string`: 쿼리 문자열
- `is_end` / `is_eof`: 쿼리 종료 여부
- `commands`: 명령어별 실행 상태
- `login_name`: 실행 계정
- `rows`: 결과 레코드 개수

**주의사항:**
- 관리자는 모든 쿼리 조회 가능
- 일반 사용자는 자신의 쿼리만 조회 가능
- 쿼리문에서는 `is_eof` 사용 권장 (is_end는 레거시)

### system count
테이블의 레코드 개수를 확인합니다.

**문법:**
```
system count [from=DATE] [to=DATE] [TABLE_1, TABLE_2, ...]
```

**옵션:**
- `from`, `to`: 조회 범위
- 테이블 미지정 시: 읽기 권한이 있는 모든 테이블
- 와일드카드(`*`) 사용 가능

**예시:**
```
system count
system count sys_*
system count from=20200601 to=20200630 web_log
```

### proc
프로시저를 실행합니다.

**문법:**
```
proc PROCEDURE_NAME [PARAM_1=VALUE_1, ...]
```

**프로시저란:**
- 미리 정의된 쿼리 명령을 함수처럼 호출
- 반복적인 쿼리 모듈화
- 프로파일 권한 관리로 보안 강화

**예시:**
```
proc daily_report date="2020-06-01"
```

---

## 쿼리 유형

### 애드혹 쿼리 (Ad-hoc Query)
사용자가 임의로 실행하는 쿼리입니다.

**실행 방법:**
- 웹 콘솔: 쿼리 > 쿼리
- SSH 터미널
- REST API

**특징:**
- 단축키: `Ctrl + Enter` 또는 `Shift + Enter`
- 백그라운드 전환 가능

### 실시간 쿼리 (Real-time Query)
실시간으로 수신되는 데이터를 처리합니다.

**대상:**
- 로그 수집기
- 스트림 쿼리 출력
- 테이블 실시간 입력

### 스트림 쿼리 (Stream Query)
무한히 실행되는 쿼리입니다.

**모드:**
- 스트리밍 모드: 스트리밍 가능한 명령어만 사용
- 리프레시 모드: 일정 주기로 통계/정렬 수행

**활용:**
- 실시간 통계 산출
- 중간 통계 테이블 생성

### 예약된 쿼리 (Scheduled Query)
지정한 일정에 따라 자동 실행됩니다.

**설정 옵션:**
- 실행 주기: cron 형식 지원
- 쿼리 결과 저장
- 부팅 시 자동 실행
- 경보 메일 발송

**경보 설정:**
- 경보 쿼리문: 조건 판단 쿼리
- 경보 무시 주기: 재발송 방지 기간
- 메일 수신자 설정

**예시:**
```
0 8-20 * */2 1,3,5  # 2개월 주기, 월/수/금, 8시-20시 정각
```

---

## 조인 및 집합 연산

### join
두 데이터 소스를 조인합니다.

**문법:**
```
join [type={inner|left|right}] KEY_FIELD [SUBQUERY]
```

### union
여러 데이터 소스를 합칩니다.

**문법:**
```
union [SUBQUERY_1], [SUBQUERY_2], ...
```

**예시:**
```
table sys_cpu_logs | union [table sys_mem_logs]
```

---

## 조회 (Lookup)

### memlookup
인메모리 조회 테이블을 생성/관리합니다.

**문법:**
```
memlookup [op={build|drop|list}] [name=TABLE_NAME] [key=KEY_FIELD] [FIELD_1, FIELD_2, ...]
```

**작업 유형:**
- `build`: 조회 테이블 생성
- `drop`: 조회 테이블 삭제
- `list`: 조회 테이블 목록/레코드 조회

**예시:**
```
# 조회 테이블 생성
csvfile http_status.csv | memlookup op=build name=http_status key=status desc1, desc2

# 조회 테이블 목록
memlookup op=list

# 특정 조회 테이블 레코드
memlookup op=list name=http_status
```

### lookup
조회 테이블에서 값을 찾아 필드를 추가합니다.

**문법:**
```
lookup name=TABLE_NAME KEY_FIELD [OUTPUT_FIELD_1, OUTPUT_FIELD_2, ...]
```

**예시:**
```
table web_log | lookup name=http_status status desc1, desc2
```

---

## 리눅스 시스템 명령어

### linux-users
리눅스 사용자 목록을 조회합니다.

**문법:**
```
linux-users
```

**데이터 소스:**
`/etc/passwd` 파일 파싱

### linux-shell-sessions
현재 로그인 상태의 셸 세션 목록을 조회합니다.

**문법:**
```
linux-shell-sessions
```

**데이터 소스:**
`/var/run/utmp` 로그

### linux-system-info
리눅스 시스템 정보를 조회합니다.

**문법:**
```
linux-system-info
```

**출력 정보:**
- 호스트명
- 커널 버전
- 가동 시간
- 평균 부하
- UMASK

### linux-cron-jobs
예약된 작업(cron jobs) 정보를 조회합니다.

**문법:**
```
linux-cron-jobs
```

**데이터 소스:**
- `/var/cron/tabs/`
- `/var/spool/cron/`
- `/var/spool/cron/crontabs/`

### linux-network-interfaces
네트워크 인터페이스 설정 및 통계를 조회합니다.

**문법:**
```
linux-network-interfaces
```

**데이터 소스:**
- `/sys/class/net/` 통계
- `ip address show` 명령 결과

### linux-tmp-files
임시 디렉터리의 숨겨진 파일 목록을 조회합니다.

**문법:**
```
linux-tmp-files
```

**조회 디렉터리:**
- `/tmp`
- `/dev`
- `/home`

---

## 네트워크 명령어

### nslookup
DNS 조회를 수행합니다.

**문법:**
```
nslookup [server=DNS_SERVER] HOSTNAME
```

**예시:**
```
nslookup www.google.com
nslookup server=8.8.8.8 www.google.com
```

---

## 함수

### 문자열 함수

#### urldecode()
URL 인코딩된 문자열을 디코딩합니다.

**문법:**
```
urldecode(STRING [, CHARSET])
```

**예시:**
```
json "{ 'url': 'ko.logpresso.com/documents/%EB%A1%9C%EA%B7%B8%EB%B6%84%EC%84%9D' }" 
| eval decode=urldecode(field("url"))
# 결과: ko.logpresso.com/documents/로그분석

json "{ 'url': 'ko.logpresso.com/documents/�α׺м�' }" 
| eval decode=urldecode(field("url"), "EUC-KR")
# 결과: ko.logpresso.com/documents/로그분석
```

### 날짜/시간 함수
- `now()`: 현재 시간
- `date()`: 날짜 변환
- `dateadd()`: 날짜 연산
- `datediff()`: 날짜 차이

### 집계 함수
- `count()`: 개수
- `sum()`: 합계
- `avg()`: 평균
- `min()`: 최소값
- `max()`: 최대값
- `dc()`: 고유값 개수

### 기타 함수
- `field()`: 필드 값 참조
- `if()`: 조건부 표현식
- `isnull()`: NULL 체크
- `nvl()`: NULL 대체

---

## 쿼리 작성 팁

### 1. 파이프라인 활용
명령어를 순차적으로 연결하여 복잡한 작업을 단순하게 표현합니다.

```
table web_log 
| search status >= 400 
| stats count() by status 
| sort -count
```

### 2. 서브쿼리 활용
복잡한 조인이나 집합 연산에 서브쿼리를 사용합니다.

```
table web_log 
| join remote_host [table geoip | fields ip, country]
```

### 3. 정규표현식 활용
비정형 로그에서 필요한 정보를 추출합니다.

```
textfile /var/log/syslog 
| rex "(?<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) (?<level>\w+) (?<message>.*)"
```

### 4. 통계 집계
시계열 데이터 분석에 `timechart` 활용합니다.

```
table web_log 
| timechart span=1h count() by status
```

### 5. 조회 테이블
반복적인 조인을 인메모리 조회로 최적화합니다.

```
csvfile status_codes.csv | memlookup op=build name=status_lookup key=code desc
table web_log | lookup name=status_lookup status desc
```

---

## 실전 예제

### 예제 1: 웹 로그 분석
```
table web_log 
| search status >= 400 
| rex field=request "(?<method>GET|POST) (?<url>[^ ]*)" 
| stats count() as error_count by status, url 
| sort -error_count 
| limit 10
```

### 예제 2: CPU 사용률 모니터링
```
table duration=1h sys_cpu_logs 
| eval total_cpu = kernel + user 
| search total_cpu > 80 
| timechart span=5m avg(total_cpu) as avg_cpu
```

### 예제 3: 데이터베이스 연동
```
set start_date = "2020-06-01"
set end_date = "2020-06-30"
dbquery oracle "SELECT * FROM sales WHERE sale_date BETWEEN :start_date AND :end_date" 
| stats sum(amount) as total_sales by product_category
```

### 예제 4: API 데이터 수집 및 분석
```
wget url="https://api.github.com/repos/logpresso/CVE-2021-44228-Scanner/releases?per_page=100" 
| parsejson 
| explode line 
| parsemap field=line 
| explode assets 
| parsemap field=assets 
| stats sum(download_count) as download_count by name 
| stats sum(download_count)
```

### 예제 5: 로그 파싱 및 SFTP 전송
```
table web_log 
| search status >= 500 
| fields _time, remote_host, status, request 
| sftp backup put format=csv fields=_time,remote_host,status,request /logs/{logtime:/yyyy/MM/dd/}errors.csv
```

---

## 성능 최적화 팁

### 1. 필터링 먼저
가능한 한 빨리 데이터를 필터링하여 처리량을 줄입니다.

```
# 좋음
table web_log | search status >= 500 | stats count() by url

# 나쁨  
table web_log | stats count() by url | search count > 100
```

### 2. 필요한 필드만 선택
`fields` 명령으로 불필요한 필드를 제거합니다.

```
table web_log | fields _time, status, remote_host | ...
```

### 3. limit 활용
테스트 시 `limit`으로 처리 데이터 양을 제한합니다.

```
table web_log | limit 1000 | ...
```

### 4. 조회 테이블 활용
반복적인 조인은 `memlookup`으로 최적화합니다.

### 5. 적절한 시간 범위
`duration`, `from`, `to`로 조회 범위를 제한합니다.

---

## 문법 표기 규칙

| 표기 | 의미 | 예시 |
|------|------|------|
| `UPPERCASE` | 필수 매개변수 | `TABLE_NAME` |
| `[optional]` | 선택 매개변수 | `[limit=INT]` |
| `{a\|b}` | 선택지 | `{GET\|POST}` |
| `...` | 반복 가능 | `FIELD_1[, ...]` |
| `t` | true (불리언) | `overlay=t` |

---

## 추가 리소스

### 공식 문서
- 로그프레소 쿼리 설명서: https://docs.logpresso.com/ko/query
- GitHub 예제 데이터셋: https://github.com/logpresso/dataset

### 도움말 기능
- 쿼리 입력창에서 `Ctrl + Space`: 자동 완성 및 도움말
- 신택스 하이라이트: 명령어, 주석, 타입 자동 강조

### 정규표현식 테스트
- 웹 콘솔에서 정규표현식 검증 기능 제공
- Java 정규표현식 명세: https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/regex/Pattern.html

---

## 주요 주의사항

1. **프로파일 권한**: `dbquery`, `sftp`, `ftp` 등은 접속 프로파일 권한 필요
2. **관리자 권한**: 시스템 테이블은 관리자만 접근 가능
3. **쿼리 취소**: 장시간 실행 쿼리는 적절히 취소 구현 필요
4. **인코딩**: 한글 데이터는 적절한 charset 지정 (`utf-8`, `euc-kr` 등)
5. **정규표현식**: 특수문자는 이스케이프 필요 (`\\`, `\d+` 등)

---

## 버전 정보

이 문서는 Logpresso 쿼리 엔진의 일반적인 명령어를 다룹니다. 일부 명령어는 특정 버전부터 지원될 수 있습니다.

**참고:**
- ENT: Enterprise 버전
- SNR: Sonar 버전
- STD: Standard 버전

---

## 연락처 및 지원

추가 정보가 필요하거나 질문이 있는 경우:
- 공식 문서: https://docs.logpresso.com
- GitHub: https://github.com/logpresso

---

*문서 생성 일시: 2025년 11월 11일*
*출처: Logpresso 공식 문서 (https://docs.logpresso.com/ko/query)*