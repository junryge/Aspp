"""
CNN-LSTM Multi-Task ëª¨ë¸ ì¦‰ì‹œ í•™ìŠµ ì‹¤í–‰
=====================================
ì´ íŒŒì¼ì„ ì‹¤í–‰í•˜ë©´ TO.CSV ë°ì´í„°ë¡œ ë°”ë¡œ ëª¨ë¸ì´ í•™ìŠµë©ë‹ˆë‹¤!
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import (Input, Conv1D, LSTM, Dense, Dropout, 
                                    BatchNormalization, Bidirectional, 
                                    MaxPooling1D, Activation)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import os
from datetime import datetime, timedelta
import joblib
import warnings

warnings.filterwarnings('ignore')

# CPU ëª¨ë“œ ì„¤ì •
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
tf.config.set_visible_devices([], 'GPU')

# ëœë¤ ì‹œë“œ ê³ ì •
RANDOM_SEED = 2079936
tf.random.set_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

print("="*60)
print("ğŸš€ CNN-LSTM Multi-Task ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
print("="*60)

# ===================================
# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ===================================

def load_and_preprocess_data(data_path):
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    print(f"\nğŸ“ ë°ì´í„° ë¡œë”©: {data_path}")
    
    # ë°ì´í„° ë¡œë“œ
    data = pd.read_csv(data_path)
    print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ - shape: {data.shape}")
    
    # ì‹œê°„ ì»¬ëŸ¼ ë³€í™˜
    data['CURRTIME'] = pd.to_datetime(data['CURRTIME'], format='%Y%m%d%H%M')
    data['TIME'] = pd.to_datetime(data['TIME'], format='%Y%m%d%H%M')
    
    # SUM ì»¬ëŸ¼ ì œê±°
    columns_to_drop = [col for col in data.columns if 'SUM' in col]
    data = data.drop(columns=columns_to_drop)
    
    # ì¸ë±ìŠ¤ë¥¼ ì‹œê°„ìœ¼ë¡œ ì„¤ì •
    data.set_index('CURRTIME', inplace=True)
    
    return data

def create_features(data):
    """íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§"""
    print("\nğŸ”§ íŠ¹ì§• ìƒì„± ì¤‘...")
    
    features_data = data.copy()
    
    # ì‹œê°„ íŠ¹ì§•
    features_data['hour'] = features_data.index.hour
    features_data['dayofweek'] = features_data.index.dayofweek
    features_data['is_weekend'] = (features_data.index.dayofweek >= 5).astype(int)
    
    # íŒ¹ ê°„ ë¶ˆê· í˜• ì§€í‘œ
    features_data['imbalance_M14A_M10A'] = features_data['M14AM10A'] - features_data['M10AM14A']
    features_data['imbalance_M14A_M14B'] = features_data['M14AM14B'] - features_data['M14BM14A']
    features_data['imbalance_M14A_M16'] = features_data['M14AM16'] - features_data['M16M14A']
    
    # ì´ë™ í‰ê· 
    features_data['MA_5'] = features_data['TOTALCNT'].rolling(window=5, min_periods=1).mean()
    features_data['MA_10'] = features_data['TOTALCNT'].rolling(window=10, min_periods=1).mean()
    features_data['MA_30'] = features_data['TOTALCNT'].rolling(window=30, min_periods=1).mean()
    
    # í‘œì¤€í¸ì°¨
    features_data['STD_5'] = features_data['TOTALCNT'].rolling(window=5, min_periods=1).std()
    features_data['STD_10'] = features_data['TOTALCNT'].rolling(window=10, min_periods=1).std()
    
    # íŒ¹ë³„ ë¶€í•˜ìœ¨
    features_data['load_M14A_out'] = (features_data['M14AM10A'] + features_data['M14AM14B'] + 
                                      features_data['M14AM16']) / features_data['TOTALCNT']
    features_data['load_M14A_in'] = (features_data['M10AM14A'] + features_data['M14BM14A'] + 
                                     features_data['M16M14A']) / features_data['TOTALCNT']
    
    # ë³€í™”ìœ¨
    features_data['change_rate'] = features_data['TOTALCNT'].pct_change()
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬
    features_data = features_data.fillna(method='ffill').fillna(0)
    
    print(f"âœ“ íŠ¹ì§• ìƒì„± ì™„ë£Œ - shape: {features_data.shape}")
    
    return features_data

def create_targets(data, future_minutes=10):
    """íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±"""
    print(f"\nğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘ ({future_minutes}ë¶„ í›„ ì˜ˆì¸¡)...")
    
    # 1. ë¬¼ë¥˜ëŸ‰ íƒ€ê²Ÿ (íšŒê·€)
    data['FUTURE_TOTALCNT'] = pd.NA
    
    for i in data.index:
        future_time = i + pd.Timedelta(minutes=future_minutes)
        if (future_time <= data.index.max()) & (future_time in data.index):
            data.loc[i, 'FUTURE_TOTALCNT'] = data.loc[future_time, 'TOTALCNT']
    
    # 2. ë³‘ëª© ìœ„ì¹˜ íƒ€ê²Ÿ (ë¶„ë¥˜)
    thresholds = {
        'total': data['TOTALCNT'].quantile(0.85),  # ìƒìœ„ 15%
        'm14a_m10a': 300,
        'm14a_m14b': 350,
        'm14a_m16': 300
    }
    
    data['BOTTLENECK_LOCATION'] = 0  # 0: ë³‘ëª© ì—†ìŒ
    
    for i in data.index:
        future_time = i + pd.Timedelta(minutes=future_minutes)
        if (future_time <= data.index.max()) & (future_time in data.index):
            future_total = data.loc[future_time, 'TOTALCNT']
            
            if future_total > thresholds['total']:
                route_loads = {
                    1: data.loc[future_time, 'M14AM10A'] + data.loc[future_time, 'M10AM14A'],
                    2: data.loc[future_time, 'M14AM14B'] + data.loc[future_time, 'M14BM14A'],
                    3: data.loc[future_time, 'M14AM16'] + data.loc[future_time, 'M16M14A']
                }
                
                max_route = max(route_loads.items(), key=lambda x: x[1])
                if max_route[1] > thresholds[f'm14a_{["m10a", "m14b", "m16"][max_route[0]-1]}']:
                    data.loc[i, 'BOTTLENECK_LOCATION'] = max_route[0]
    
    # NA ì œê±°
    data = data.dropna(subset=['FUTURE_TOTALCNT'])
    
    print(f"âœ“ íƒ€ê²Ÿ ìƒì„± ì™„ë£Œ")
    print(f"  - ë³‘ëª© ë¶„í¬: {data['BOTTLENECK_LOCATION'].value_counts().to_dict()}")
    
    return data

def scale_features(data, feature_columns):
    """íŠ¹ì§• ìŠ¤ì¼€ì¼ë§"""
    scaler = StandardScaler()
    
    scale_columns = [col for col in feature_columns if col in data.columns]
    scaled_data = scaler.fit_transform(data[scale_columns])
    
    scaled_df = pd.DataFrame(
        scaled_data, 
        columns=[f'scaled_{col}' for col in scale_columns],
        index=data.index
    )
    
    result = pd.concat([data, scaled_df], axis=1)
    
    return result, scaler

def create_sequences(data, feature_cols, target_cols, seq_length=30):
    """ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
    X, y_regression, y_classification = [], [], []
    
    time_diff = data.index.to_series().diff()
    split_points = time_diff > pd.Timedelta(minutes=1)
    segment_ids = split_points.cumsum()
    
    for segment_id in segment_ids.unique():
        segment = data[segment_ids == segment_id]
        
        if len(segment) > seq_length:
            feature_data = segment[feature_cols].values
            regression_data = segment[target_cols[0]].values
            classification_data = segment[target_cols[1]].values
            
            for i in range(len(segment) - seq_length):
                X.append(feature_data[i:i+seq_length])
                y_regression.append(regression_data[i+seq_length])
                y_classification.append(classification_data[i+seq_length])
    
    return np.array(X), np.array(y_regression), np.array(y_classification)

# ===================================
# CNN-LSTM Multi-Task ëª¨ë¸
# ===================================

def build_cnn_lstm_multitask_model(input_shape, num_classes=4):
    """CNN-LSTM Multi-Task ëª¨ë¸ êµ¬ì¶•"""
    
    inputs = Input(shape=input_shape, name='input')
    
    # CNN íŒŒíŠ¸
    x = Conv1D(filters=64, kernel_size=3, padding='same')(inputs)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.2)(x)
    
    x = Conv1D(filters=128, kernel_size=3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.2)(x)
    
    x = Conv1D(filters=128, kernel_size=3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    
    x = MaxPooling1D(pool_size=2)(x)
    
    # LSTM íŒŒíŠ¸
    x = Bidirectional(LSTM(128, return_sequences=True))(x)
    x = Dropout(0.3)(x)
    
    x = Bidirectional(LSTM(64, return_sequences=False))(x)
    x = Dropout(0.3)(x)
    
    # ê³µìœ  Dense ë ˆì´ì–´
    shared = Dense(128, activation='relu', name='shared_layer')(x)
    shared = BatchNormalization()(shared)
    shared = Dropout(0.3)(shared)
    
    # Multi-Task ì¶œë ¥
    # Task 1: ë¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡ (íšŒê·€)
    logistics_output = Dense(64, activation='relu')(shared)
    logistics_output = Dense(1, name='logistics_output')(logistics_output)
    
    # Task 2: ë³‘ëª© ìœ„ì¹˜ ì˜ˆì¸¡ (ë¶„ë¥˜)
    bottleneck_output = Dense(64, activation='relu')(shared)
    bottleneck_output = Dense(num_classes, activation='softmax', name='bottleneck_output')(bottleneck_output)
    
    model = Model(inputs=inputs, outputs=[logistics_output, bottleneck_output])
    
    return model

# ===================================
# í•™ìŠµ í•¨ìˆ˜
# ===================================

def train_model(model, X_train, y_train_reg, y_train_cls, X_val, y_val_reg, y_val_cls, 
                epochs=50, batch_size=32):  # ì—í¬í¬ë¥¼ 50ìœ¼ë¡œ ì¤„ì—¬ì„œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
    """ëª¨ë¸ í•™ìŠµ"""
    
    losses = {
        'logistics_output': 'mse',
        'bottleneck_output': 'sparse_categorical_crossentropy'
    }
    
    loss_weights = {
        'logistics_output': 0.7,
        'bottleneck_output': 0.3
    }
    
    metrics = {
        'logistics_output': ['mae'],
        'bottleneck_output': ['accuracy']
    }
    
    model.compile(
        optimizer=Adam(learning_rate=0.0005),
        loss=losses,
        loss_weights=loss_weights,
        metrics=metrics
    )
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('model', exist_ok=True)
    
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-6,
            verbose=1
        ),
        ModelCheckpoint(
            'model/cnn_lstm_multitask_best.keras',
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        )
    ]
    
    print("\nğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    print(f"  - Epochs: {epochs}")
    print(f"  - Batch size: {batch_size}")
    print(f"  - Train samples: {len(X_train)}")
    print(f"  - Validation samples: {len(X_val)}")
    
    history = model.fit(
        X_train,
        {'logistics_output': y_train_reg, 'bottleneck_output': y_train_cls},
        validation_data=(
            X_val,
            {'logistics_output': y_val_reg, 'bottleneck_output': y_val_cls}
        ),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=1
    )
    
    return history

# ===================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ===================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # 1. ë°ì´í„° ë¡œë“œ - TO.CSV íŒŒì¼ ìë™ íƒìƒ‰
    data_paths = ['TO.CSV', 'data/TO.CSV', './TO.CSV', '../TO.CSV']
    data_loaded = False
    
    for path in data_paths:
        if os.path.exists(path):
            data = load_and_preprocess_data(path)
            data_loaded = True
            break
    
    if not data_loaded:
        print("âŒ TO.CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("   í˜„ì¬ ë””ë ‰í† ë¦¬ì— TO.CSV íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        return None, None, None
    
    # 2. íŠ¹ì§• ìƒì„±
    data = create_features(data)
    
    # 3. íƒ€ê²Ÿ ìƒì„±
    data = create_targets(data)
    
    # 4. íŠ¹ì§• ìŠ¤ì¼€ì¼ë§
    scale_features_list = [
        'TOTALCNT', 'M14AM10A', 'M10AM14A', 'M14AM14B', 'M14BM14A', 'M14AM16', 'M16M14A',
        'imbalance_M14A_M10A', 'imbalance_M14A_M14B', 'imbalance_M14A_M16',
        'MA_5', 'MA_10', 'MA_30', 'STD_5', 'STD_10',
        'load_M14A_out', 'load_M14A_in'
    ]
    
    data, scaler = scale_features(data, scale_features_list)
    
    # 5. ì‹œí€€ìŠ¤ ìƒì„±
    sequence_features = [col for col in data.columns if col.startswith('scaled_')]
    target_features = ['FUTURE_TOTALCNT', 'BOTTLENECK_LOCATION']
    
    X, y_regression, y_classification = create_sequences(
        data, sequence_features, target_features, seq_length=30
    )
    
    print(f"\nğŸ“Š ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì™„ë£Œ:")
    print(f"  - X shape: {X.shape}")
    print(f"  - y_regression shape: {y_regression.shape}")
    print(f"  - y_classification shape: {y_classification.shape}")
    
    # 6. ë°ì´í„° ë¶„í• 
    train_size = int(0.7 * len(X))
    val_size = int(0.15 * len(X))
    
    X_train = X[:train_size]
    X_val = X[train_size:train_size+val_size]
    X_test = X[train_size+val_size:]
    
    y_train_reg = y_regression[:train_size]
    y_val_reg = y_regression[train_size:train_size+val_size]
    y_test_reg = y_regression[train_size+val_size:]
    
    y_train_cls = y_classification[:train_size]
    y_val_cls = y_classification[train_size:train_size+val_size]
    y_test_cls = y_classification[train_size+val_size:]
    
    # 7. ëª¨ë¸ ìƒì„±
    input_shape = (X_train.shape[1], X_train.shape[2])
    num_classes = len(np.unique(y_classification))
    
    print(f"\nğŸ—ï¸ ëª¨ë¸ êµ¬ì¶• ì¤‘...")
    print(f"  - Input shape: {input_shape}")
    print(f"  - Number of classes: {num_classes}")
    
    model = build_cnn_lstm_multitask_model(input_shape, num_classes)
    model.summary()
    
    # 8. ëª¨ë¸ í•™ìŠµ
    history = train_model(
        model, 
        X_train, y_train_reg, y_train_cls,
        X_val, y_val_reg, y_val_cls,
        epochs=50,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 50 ì—í¬í¬
        batch_size=32
    )
    
    # 9. ëª¨ë¸ í‰ê°€
    print("\nğŸ“ˆ ëª¨ë¸ í‰ê°€ ì¤‘...")
    
    predictions = model.predict(X_test)
    pred_logistics = predictions[0].flatten()
    pred_bottleneck = predictions[1]
    
    # ë¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡ í‰ê°€
    mae = mean_absolute_error(y_test_reg, pred_logistics)
    mse = mean_squared_error(y_test_reg, pred_logistics)
    
    print(f"\nğŸ“Š ë¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡ ì„±ëŠ¥:")
    print(f"  - MAE: {mae:.2f}")
    print(f"  - MSE: {mse:.2f}")
    print(f"  - RMSE: {np.sqrt(mse):.2f}")
    
    # ë³‘ëª© ì˜ˆì¸¡ í‰ê°€
    pred_bottleneck_classes = np.argmax(pred_bottleneck, axis=1)
    accuracy = accuracy_score(y_test_cls, pred_bottleneck_classes)
    
    print(f"\nğŸš¨ ë³‘ëª© ìœ„ì¹˜ ì˜ˆì¸¡ ì„±ëŠ¥:")
    print(f"  - Accuracy: {accuracy:.2%}")
    print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
    print(classification_report(y_test_cls, pred_bottleneck_classes, 
                              target_names=['ì •ìƒ', 'M14A-M10A', 'M14A-M14B', 'M14A-M16']))
    
    # 10. ëª¨ë¸ ì €ì¥
    print("\nğŸ’¾ ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì¤‘...")
    
    os.makedirs('model', exist_ok=True)
    os.makedirs('scaler', exist_ok=True)
    
    model.save('model/cnn_lstm_multitask_final.keras')
    joblib.dump(scaler, 'scaler/multitask_scaler.pkl')
    
    print("âœ“ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: model/cnn_lstm_multitask_final.keras")
    print("âœ“ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ: scaler/multitask_scaler.pkl")
    
    # 11. í•™ìŠµ ê³¡ì„  ì‹œê°í™”
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 3, 1)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title('Total Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.subplot(1, 3, 2)
    plt.plot(history.history['logistics_output_mae'], label='Train MAE')
    plt.plot(history.history['val_logistics_output_mae'], label='Val MAE')
    plt.title('Logistics MAE')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.legend()
    
    plt.subplot(1, 3, 3)
    plt.plot(history.history['bottleneck_output_accuracy'], label='Train Acc')
    plt.plot(history.history['val_bottleneck_output_accuracy'], label='Val Acc')
    plt.title('Bottleneck Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('training_history.png')
    print("\nâœ“ í•™ìŠµ ê³¡ì„  ì €ì¥: training_history.png")
    
    print("\n" + "="*60)
    print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    print("="*60)
    
    return model, scaler, history

# ===================================
# ì‹¤í–‰!!
# ===================================

if __name__ == "__main__":
    print("\nğŸ”¥ CNN-LSTM Multi-Task ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("   TO.CSV ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n")
    
    # í•™ìŠµ ì‹¤í–‰
    model, scaler, history = main()
    
    if model is not None:
        print("\nâœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ!")
        print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
        print("   - model/cnn_lstm_multitask_final.keras (ëª¨ë¸)")
        print("   - scaler/multitask_scaler.pkl (ìŠ¤ì¼€ì¼ëŸ¬)")
        print("   - training_history.png (í•™ìŠµ ê³¡ì„ )")
        print("\nì´ì œ ì´ ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")