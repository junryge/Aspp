"""
###ìˆ˜ì •
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    logger.info("="*60)
    logger.info("CNN-LSTM Multi-Task ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ì‹¤ì œ ë°ì´í„°)")
    logger.info("="*60)
    
    # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    data_path = 'data/20240201_TO_202507281705.csv'  # ì‹¤ì œ ì „ì²´ ë°ì´í„°
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(data_path):
        logger.error(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        return None, None, None
    
    data = load_and_preprocess_data(data_path)
    
    # 2. íŠ¹ì§• ìƒì„±
    data = create_features(data)
    
    # 3. íƒ€ê²Ÿ ìƒì„±
    data = create_targets(data)
    
    # 4. íŠ¹ì§• ì„ íƒ
    # ìŠ¤ì¼€ì¼ë§í•  íŠ¹ì§• (ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¹„ìœ¨ íŠ¹ì§• ì œì™¸ ê°€ëŠ¥)
    scale_features_list = [
        'TOTALCNT', 'M14AM10A', 'M10AM14A', 'M14AM14B', 'M14BM14A', 'M14AM16', 'M16M14A',
        'imbalance_M14A_M10A', 'imbalance_M14A_M14B', 'imbalance_M14A_M16',
        'MA_5', 'MA_10', 'MA_30', 'MA_60',
        'STD_5', 'STD_10', 'STD_30',
        'MAX_10', 'MIN_10',
        'load_M14A_out', 'load_M14A_in',
        'ratio_M14A_M10A', 'ratio_M14A_M14B', 'ratio_M14A_M16',
        'change_rate', 'change_rate_5', 'change_rate_10',
        'acceleration'
    ]
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
    scale_features_list = [col for col in scale_features_list if col in data.columns]
    logger.info(f"ìŠ¤ì¼€ì¼ë§í•  íŠ¹ì§• ìˆ˜: {len(scale_features_list)}")
    
    # 5. ìŠ¤ì¼€ì¼ë§
    data, scaler = scale_features(data, scale_features_list)
    
    # 6. ì‹œí€€ìŠ¤ìš© íŠ¹ì§• ì„ íƒ
    sequence_features = [col for col in data.columns if col.startswith('scaled_')]
    target_features = ['FUTURE_TOTALCNT', 'BOTTLENECK_LOCATION']
    
    # 7. ì‹œí€€ìŠ¤ ìƒì„± (60ë¶„ ì‹œí€€ìŠ¤)
    X, y_regression, y_classification = create_sequences(
        data, 
        sequence_features, 
        target_features,
        seq_length=60  # 1ì‹œê°„ ì‹œí€€ìŠ¤
    )
    
    # ==================== í´ë˜ìŠ¤ ë ˆì´ë¸” ì¬ë§¤í•‘ ì¶”ê°€! ====================
    # í´ë˜ìŠ¤ [0, 2, 3]ì„ [0, 1, 2]ë¡œ ë³€ê²½
    unique_classes = np.unique(y_classification)
    class_mapping = {old_class: new_class for new_class, old_class in enumerate(unique_classes)}
    y_classification_mapped = np.array([class_mapping[cls] for cls in y_classification])
    
    logger.info(f"ì›ë³¸ í´ë˜ìŠ¤: {unique_classes}")
    logger.info(f"í´ë˜ìŠ¤ ë§¤í•‘: {class_mapping}")
    logger.info(f"ì‹œí€€ìŠ¤ shape - X: {X.shape}, y_reg: {y_regression.shape}, y_cls: {y_classification_mapped.shape}")
    # ====================================================================
    
    # 8. ë°ì´í„° ë¶„í• 
    # ì‹œê°„ ìˆœì„œ ìœ ì§€ë¥¼ ìœ„í•´ ìˆœì°¨ì ìœ¼ë¡œ ë¶„í• 
    train_size = int(0.7 * len(X))
    val_size = int(0.15 * len(X))
    
    X_train = X[:train_size]
    X_val = X[train_size:train_size+val_size]
    X_test = X[train_size+val_size:]
    
    y_train_reg = y_regression[:train_size]
    y_val_reg = y_regression[train_size:train_size+val_size]
    y_test_reg = y_regression[train_size+val_size:]
    
    # ë§¤í•‘ëœ í´ë˜ìŠ¤ ì‚¬ìš©
    y_train_cls = y_classification_mapped[:train_size]
    y_val_cls = y_classification_mapped[train_size:train_size+val_size]
    y_test_cls = y_classification_mapped[train_size+val_size:]
    
    logger.info(f"\në°ì´í„° ë¶„í• :")
    logger.info(f"  - Train: {len(X_train)} samples")
    logger.info(f"  - Validation: {len(X_val)} samples")
    logger.info(f"  - Test: {len(X_test)} samples")
    
    # 9. ëª¨ë¸ ìƒì„±
    input_shape = (X_train.shape[1], X_train.shape[2])
    
    # ì‹¤ì œ í´ë˜ìŠ¤ ê°œìˆ˜ í™•ì¸ (ë§¤í•‘ëœ í´ë˜ìŠ¤ ê¸°ì¤€)
    unique_classes_mapped = np.unique(y_classification_mapped)
    num_classes = len(unique_classes_mapped)
    logger.info(f"ë§¤í•‘ëœ ë³‘ëª© í´ë˜ìŠ¤: {unique_classes_mapped}, ì´ {num_classes}ê°œ")
    
    model = build_cnn_lstm_multitask_model(input_shape, num_classes)
    model.summary()
    
    # 10. ëª¨ë¸ í•™ìŠµ
    logger.info("\nëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    history = train_model(
        model, 
        X_train, y_train_reg, y_train_cls,
        X_val, y_val_reg, y_val_cls,
        epochs=200,  # ì‹¤ì œ ë°ì´í„°ìš©
        batch_size=64
    )
    
    # 11. ëª¨ë¸ í‰ê°€
    logger.info("\nëª¨ë¸ í‰ê°€ ì¤‘...")
    
    # ì˜ˆì¸¡
    predictions = model.predict(X_test)
    pred_logistics = predictions[0].flatten()
    pred_bottleneck = predictions[1]
    
    # ë¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡ í‰ê°€
    mae = mean_absolute_error(y_test_reg, pred_logistics)
    mse = mean_squared_error(y_test_reg, pred_logistics)
    
    logger.info(f"\në¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡ ì„±ëŠ¥:")
    logger.info(f"  MAE: {mae:.2f}")
    logger.info(f"  MSE: {mse:.2f}")
    logger.info(f"  RMSE: {np.sqrt(mse):.2f}")
    
    # ë³‘ëª© ì˜ˆì¸¡ í‰ê°€
    pred_bottleneck_classes = np.argmax(pred_bottleneck, axis=1)
    accuracy = accuracy_score(y_test_cls, pred_bottleneck_classes)
    
    logger.info(f"\në³‘ëª© ìœ„ì¹˜ ì˜ˆì¸¡ ì„±ëŠ¥:")
    logger.info(f"  Accuracy: {accuracy:.2%}")
    logger.info("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
    
    # target_names ì„¤ì • (ì›ë³¸ í´ë˜ìŠ¤ ê¸°ì¤€)
    if set(unique_classes) == {0, 2, 3}:
        target_names = ['ì •ìƒ', 'M14A-M14B', 'M14A-M16']
    elif num_classes == 4:
        target_names = ['ì •ìƒ', 'M14A-M10A', 'M14A-M14B', 'M14A-M16']
    else:
        target_names = [f'Class_{i}' for i in range(num_classes)]
    
    print(classification_report(y_test_cls, pred_bottleneck_classes, 
                              target_names=target_names))
    
    # 12. ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    logger.info("\nëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì¤‘...")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('model', exist_ok=True)
    os.makedirs('scaler', exist_ok=True)
    os.makedirs('config', exist_ok=True)
    
    # ëª¨ë¸ ì €ì¥
    model.save('model/cnn_lstm_multitask_final.keras')
    logger.info("ëª¨ë¸ ì €ì¥ ì™„ë£Œ: model/cnn_lstm_multitask_final.keras")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    joblib.dump(scaler, 'scaler/multitask_scaler.pkl')
    logger.info("ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ: scaler/multitask_scaler.pkl")
    
    # í´ë˜ìŠ¤ ë§¤í•‘ ì •ë³´ ì €ì¥
    import json
    with open('config/class_mapping.json', 'w') as f:
        json.dump(class_mapping, f, indent=4)
    logger.info("í´ë˜ìŠ¤ ë§¤í•‘ ì €ì¥ ì™„ë£Œ: config/class_mapping.json")
    
    # 13. í•™ìŠµ ê³¡ì„  ì‹œê°í™”
    plot_training_history(history)
    
    # 14. ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
    plot_predictions(y_test_reg, pred_logistics, y_test_cls, pred_bottleneck_classes, 
                    num_classes, class_mapping)
    
    logger.info("\n" + "="*60)
    logger.info("í•™ìŠµ ì™„ë£Œ!")
    logger.info("="*60)
    
    return model, scaler, history
###ë§¤ì¸ìˆ˜ì •ë

###
def plot_predictions(y_true_reg, y_pred_reg, y_true_cls, y_pred_cls, num_classes, class_mapping=None):
    """ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # ë¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡ ë¹„êµ
    sample_size = min(200, len(y_true_reg))
    axes[0, 0].plot(y_true_reg[:sample_size], label='Actual', color='blue')
    axes[0, 0].plot(y_pred_reg[:sample_size], label='Predicted', color='red', alpha=0.7)
    axes[0, 0].set_title('Logistics Prediction (First 200 samples)')
    axes[0, 0].set_xlabel('Sample')
    axes[0, 0].set_ylabel('TOTALCNT')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # ë¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡ ì‚°ì ë„
    axes[0, 1].scatter(y_true_reg, y_pred_reg, alpha=0.5)
    axes[0, 1].plot([y_true_reg.min(), y_true_reg.max()], 
                    [y_true_reg.min(), y_true_reg.max()], 
                    'r--', lw=2)
    axes[0, 1].set_title('Logistics Prediction Scatter')
    axes[0, 1].set_xlabel('Actual')
    axes[0, 1].set_ylabel('Predicted')
    axes[0, 1].grid(True)
    
    # ë³‘ëª© ì˜ˆì¸¡ í˜¼ë™ í–‰ë ¬
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_true_cls, y_pred_cls)
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])
    axes[1, 0].set_title('Bottleneck Prediction Confusion Matrix')
    axes[1, 0].set_xlabel('Predicted')
    axes[1, 0].set_ylabel('Actual')
    
    # ë™ì  ë¼ë²¨ ì„¤ì • (í´ë˜ìŠ¤ ë§¤í•‘ ê³ ë ¤)
    if class_mapping:
        # ì›ë³¸ í´ë˜ìŠ¤ë¡œ ì—­ë§¤í•‘
        reverse_mapping = {v: k for k, v in class_mapping.items()}
        original_classes = sorted([reverse_mapping.get(i, i) for i in range(num_classes)])
        
        if set(original_classes) == {0, 2, 3}:
            labels = ['Normal', 'M14A-M14B', 'M14A-M16']
        elif set(original_classes) == {0, 1, 2, 3}:
            labels = ['Normal', 'M14A-M10A', 'M14A-M14B', 'M14A-M16']
        else:
            labels = [f'Class_{i}' for i in range(num_classes)]
    else:
        if num_classes == 3:
            labels = ['Normal', 'Route_1', 'Route_2']
        elif num_classes == 4:
            labels = ['Normal', 'M14A-M10A', 'M14A-M14B', 'M14A-M16']
        else:
            labels = [f'Class_{i}' for i in range(num_classes)]
    
    axes[1, 0].set_xticklabels(labels)
    axes[1, 0].set_yticklabels(labels)
    
    # ë³‘ëª© ë°œìƒ ì‹œì  í‘œì‹œ
    bottleneck_points = np.where(y_true_cls > 0)[0]
    if len(bottleneck_points) > 0:
        axes[1, 1].scatter(bottleneck_points[:100], 
                          y_true_reg[bottleneck_points[:100]], 
                          color='red', s=50, label='Actual Bottleneck')
    
    predicted_bottleneck = np.where(y_pred_cls > 0)[0]
    if len(predicted_bottleneck) > 0:
        axes[1, 1].scatter(predicted_bottleneck[:100], 
                          y_pred_reg[predicted_bottleneck[:100]], 
                          color='orange', s=30, alpha=0.5, label='Predicted Bottleneck')
    
    axes[1, 1].set_title('Bottleneck Detection')
    axes[1, 1].set_xlabel('Sample')
    axes[1, 1].set_ylabel('TOTALCNT')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.savefig('prediction_results_multitask.png', dpi=300, bbox_inches='tight')
    plt.close()
###ìˆ˜ì •ì™„ë£Œ


CNN-LSTM Multi-Task ê¸°ë°˜ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ëª¨ë¸ - ì˜¤ë¥˜ ìˆ˜ì • ì™„ë£Œ ë²„ì „
==================================================================
ì‹¤ì œ ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
ëª¨ë“  ì˜¤ë¥˜ê°€ ìˆ˜ì •ëœ ë²„ì „ì…ë‹ˆë‹¤.

ì‚¬ìš© ë°ì´í„°: data/20240201_TO_202507281705.csv
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import (Input, Conv1D, LSTM, Dense, Dropout, 
                                    BatchNormalization, Bidirectional, 
                                    MaxPooling1D, Activation)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import os
from datetime import datetime, timedelta
import joblib
import logging
import warnings

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')

# ===================================
# 1. í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
# ===================================

# CPU ëª¨ë“œ ì„¤ì •
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
tf.config.set_visible_devices([], 'GPU')

# ëœë¤ ì‹œë“œ ê³ ì •
RANDOM_SEED = 2079936
tf.random.set_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ===================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
# ===================================

def load_and_preprocess_data(data_path):
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    logger.info("ë°ì´í„° ë¡œë”© ì¤‘...")
    
    # ë°ì´í„° ë¡œë“œ
    Full_Data = pd.read_csv(data_path)
    logger.info(f"ì›ë³¸ ë°ì´í„° shape: {Full_Data.shape}")
    
    # ì‹œê°„ ì»¬ëŸ¼ ë³€í™˜
    Full_Data['CURRTIME'] = pd.to_datetime(Full_Data['CURRTIME'], format='%Y%m%d%H%M')
    Full_Data['TIME'] = pd.to_datetime(Full_Data['TIME'], format='%Y%m%d%H%M')
    
    # SUM ì»¬ëŸ¼ ì œê±°
    columns_to_drop = [col for col in Full_Data.columns if 'SUM' in col]
    Full_Data = Full_Data.drop(columns=columns_to_drop)
    
    # íŠ¹ì • ë‚ ì§œ ë²”ìœ„ë§Œ ì‚¬ìš© (ì˜µì…˜)
    start_date = pd.to_datetime('2024-02-01 00:00:00')
    end_date = pd.to_datetime('2025-07-27 23:59:59')
    Full_Data = Full_Data[(Full_Data['TIME'] >= start_date) & (Full_Data['TIME'] <= end_date)]
    
    # ì¸ë±ìŠ¤ë¥¼ ì‹œê°„ìœ¼ë¡œ ì„¤ì •
    Full_Data.set_index('CURRTIME', inplace=True)
    
    # ì´ìƒì¹˜ ì²˜ë¦¬ (PM ê¸°ê°„ ê³ ë ¤)
    PM_start_date = pd.to_datetime('2024-10-23 00:00:00')
    PM_end_date = pd.to_datetime('2024-10-23 23:59:59')
    
    within_PM = Full_Data[(Full_Data['TIME'] >= PM_start_date) & (Full_Data['TIME'] <= PM_end_date)]
    outside_PM = Full_Data[(Full_Data['TIME'] < PM_start_date) | (Full_Data['TIME'] > PM_end_date)]
    
    # PM ê¸°ê°„ ì™¸ ë°ì´í„°ëŠ” ì •ìƒ ë²”ìœ„ë§Œ ì‚¬ìš©
    outside_PM_filtered = outside_PM[(outside_PM['TOTALCNT'] >= 800) & (outside_PM['TOTALCNT'] <= 2500)]
    
    # ë°ì´í„° í•©ì¹˜ê¸°
    Full_Data = pd.concat([within_PM, outside_PM_filtered])
    Full_Data = Full_Data.sort_index()
    
    logger.info(f"ì „ì²˜ë¦¬ í›„ ë°ì´í„° shape: {Full_Data.shape}")
    
    return Full_Data

def create_features(data):
    """íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§"""
    logger.info("íŠ¹ì§• ìƒì„± ì¤‘...")
    
    # ê¸°ë³¸ íŠ¹ì§•
    features_data = data.copy()
    
    # ì‹œê°„ íŠ¹ì§•
    features_data['hour'] = features_data.index.hour
    features_data['dayofweek'] = features_data.index.dayofweek
    features_data['is_weekend'] = (features_data.index.dayofweek >= 5).astype(int)
    features_data['month'] = features_data.index.month
    features_data['day'] = features_data.index.day
    
    # í”¼í¬ ì‹œê°„ëŒ€
    features_data['is_peak_hour'] = features_data.index.hour.isin([8, 9, 14, 15, 16, 17]).astype(int)
    
    # íŒ¹ ê°„ ë¶ˆê· í˜• ì§€í‘œ
    features_data['imbalance_M14A_M10A'] = features_data['M14AM10A'] - features_data['M10AM14A']
    features_data['imbalance_M14A_M14B'] = features_data['M14AM14B'] - features_data['M14BM14A']
    features_data['imbalance_M14A_M16'] = features_data['M14AM16'] - features_data['M16M14A']
    
    # ì´ë™ í‰ê·  (ë‹¤ì–‘í•œ ìœˆë„ìš°)
    for window in [5, 10, 30, 60]:
        features_data[f'MA_{window}'] = features_data['TOTALCNT'].rolling(window=window, min_periods=1).mean()
    
    # í‘œì¤€í¸ì°¨ (ë³€ë™ì„±)
    for window in [5, 10, 30]:
        features_data[f'STD_{window}'] = features_data['TOTALCNT'].rolling(window=window, min_periods=1).std()
    
    # ìµœëŒ€/ìµœì†Œê°’
    features_data['MAX_10'] = features_data['TOTALCNT'].rolling(window=10, min_periods=1).max()
    features_data['MIN_10'] = features_data['TOTALCNT'].rolling(window=10, min_periods=1).min()
    
    # íŒ¹ë³„ ë¶€í•˜ìœ¨ (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
    total_safe = features_data['TOTALCNT'].replace(0, 1)  # 0ì„ 1ë¡œ ì¹˜í™˜
    features_data['load_M14A_out'] = (features_data['M14AM10A'] + features_data['M14AM14B'] + 
                                      features_data['M14AM16']) / total_safe
    features_data['load_M14A_in'] = (features_data['M10AM14A'] + features_data['M14BM14A'] + 
                                     features_data['M16M14A']) / total_safe
    
    # ê²½ë¡œë³„ ë¹„ìœ¨ (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
    features_data['ratio_M14A_M10A'] = (features_data['M14AM10A'] + features_data['M10AM14A']) / total_safe
    features_data['ratio_M14A_M14B'] = (features_data['M14AM14B'] + features_data['M14BM14A']) / total_safe
    features_data['ratio_M14A_M16'] = (features_data['M14AM16'] + features_data['M16M14A']) / total_safe
    
    # ë³€í™”ìœ¨
    features_data['change_rate'] = features_data['TOTALCNT'].pct_change()
    features_data['change_rate_5'] = features_data['TOTALCNT'].pct_change(5)
    features_data['change_rate_10'] = features_data['TOTALCNT'].pct_change(10)
    
    # ê°€ì†ë„ (ë³€í™”ìœ¨ì˜ ë³€í™”)
    features_data['acceleration'] = features_data['change_rate'].diff()
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬
    features_data = features_data.fillna(method='ffill').fillna(0)
    
    # ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
    features_data = features_data.replace([np.inf, -np.inf], 0)
    
    # ì´ìƒì¹˜ í´ë¦¬í•‘ (ê·¹ë‹¨ì ì¸ ê°’ ì œí•œ)
    numeric_columns = features_data.select_dtypes(include=[np.number]).columns
    for col in numeric_columns:
        if col not in ['TIME', 'CURRTIME']:  # ì‹œê°„ ì»¬ëŸ¼ ì œì™¸
            # 99.9 í¼ì„¼íƒ€ì¼ë¡œ í´ë¦¬í•‘
            upper_limit = features_data[col].quantile(0.999)
            lower_limit = features_data[col].quantile(0.001)
            features_data[col] = features_data[col].clip(lower=lower_limit, upper=upper_limit)
    
    logger.info(f"íŠ¹ì§• ìƒì„± ì™„ë£Œ - shape: {features_data.shape}")
    logger.info(f"ë¬´í•œëŒ€ ê°’ ì²´í¬: {np.isinf(features_data.select_dtypes(include=[np.number])).any().any()}")
    logger.info(f"NaN ê°’ ì²´í¬: {features_data.isnull().any().any()}")
    
    return features_data

def create_targets(data, future_minutes=10):
    """íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ë¬¼ë¥˜ëŸ‰ + ë³‘ëª© ìœ„ì¹˜)"""
    logger.info("íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")
    
    # 1. ë¬¼ë¥˜ëŸ‰ íƒ€ê²Ÿ (íšŒê·€)
    data['FUTURE_TOTALCNT'] = pd.NA
    
    for i in data.index:
        future_time = i + pd.Timedelta(minutes=future_minutes)
        if (future_time <= data.index.max()) & (future_time in data.index):
            data.loc[i, 'FUTURE_TOTALCNT'] = data.loc[future_time, 'TOTALCNT']
    
    # 2. ë³‘ëª© ìœ„ì¹˜ íƒ€ê²Ÿ (ë¶„ë¥˜)
    # ë™ì  ì„ê³„ê°’ ì„¤ì •
    thresholds = {
        'total': data['TOTALCNT'].quantile(0.90),  # ìƒìœ„ 10%
        'm14a_m10a': np.percentile(data['M14AM10A'] + data['M10AM14A'], 90),
        'm14a_m14b': np.percentile(data['M14AM14B'] + data['M14BM14A'], 90),
        'm14a_m16': np.percentile(data['M14AM16'] + data['M16M14A'], 90)
    }
    
    logger.info(f"ë³‘ëª© ì„ê³„ê°’ - ì „ì²´: {thresholds['total']:.0f}")
    
    data['BOTTLENECK_LOCATION'] = 0  # 0: ë³‘ëª© ì—†ìŒ
    
    for i in data.index:
        future_time = i + pd.Timedelta(minutes=future_minutes)
        if (future_time <= data.index.max()) & (future_time in data.index):
            future_total = data.loc[future_time, 'TOTALCNT']
            
            if future_total > thresholds['total']:
                # ì–´ëŠ ê²½ë¡œê°€ ê°€ì¥ í˜¼ì¡í•œì§€ í™•ì¸
                route_loads = {
                    1: data.loc[future_time, 'M14AM10A'] + data.loc[future_time, 'M10AM14A'],
                    2: data.loc[future_time, 'M14AM14B'] + data.loc[future_time, 'M14BM14A'],
                    3: data.loc[future_time, 'M14AM16'] + data.loc[future_time, 'M16M14A']
                }
                
                # ê°€ì¥ í˜¼ì¡í•œ ê²½ë¡œë¥¼ ë³‘ëª©ìœ¼ë¡œ ì§€ì •
                max_route = max(route_loads.items(), key=lambda x: x[1])
                if max_route[0] == 1 and max_route[1] > thresholds['m14a_m10a']:
                    data.loc[i, 'BOTTLENECK_LOCATION'] = 1
                elif max_route[0] == 2 and max_route[1] > thresholds['m14a_m14b']:
                    data.loc[i, 'BOTTLENECK_LOCATION'] = 2
                elif max_route[0] == 3 and max_route[1] > thresholds['m14a_m16']:
                    data.loc[i, 'BOTTLENECK_LOCATION'] = 3
    
    # NA ì œê±°
    data = data.dropna(subset=['FUTURE_TOTALCNT'])
    
    logger.info(f"íƒ€ê²Ÿ ìƒì„± ì™„ë£Œ - ë³‘ëª© ë¶„í¬: {data['BOTTLENECK_LOCATION'].value_counts()}")
    
    return data

def scale_features(data, feature_columns):
    """íŠ¹ì§• ìŠ¤ì¼€ì¼ë§"""
    scaler = StandardScaler()
    
    # ìŠ¤ì¼€ì¼ë§í•  ì»¬ëŸ¼ ì„ íƒ
    scale_columns = [col for col in feature_columns if col in data.columns]
    
    # ìŠ¤ì¼€ì¼ë§ ì „ ë°ì´í„° ê²€ì¦
    scale_data = data[scale_columns].copy()
    
    # ë¬´í•œëŒ€ ê°’ ì²´í¬ ë° ì²˜ë¦¬
    if np.isinf(scale_data.values).any():
        logger.warning("ë¬´í•œëŒ€ ê°’ ë°œê²¬! ì²˜ë¦¬ ì¤‘...")
        scale_data = scale_data.replace([np.inf, -np.inf], np.nan)
        scale_data = scale_data.fillna(scale_data.mean())
    
    # NaN ê°’ ì²´í¬ ë° ì²˜ë¦¬
    if scale_data.isnull().any().any():
        logger.warning("NaN ê°’ ë°œê²¬! ì²˜ë¦¬ ì¤‘...")
        scale_data = scale_data.fillna(scale_data.mean())
    
    # ìŠ¤ì¼€ì¼ë§
    scaled_data = scaler.fit_transform(scale_data)
    
    # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    scaled_df = pd.DataFrame(
        scaled_data, 
        columns=[f'scaled_{col}' for col in scale_columns],
        index=data.index
    )
    
    # ì›ë³¸ ë°ì´í„°ì™€ ë³‘í•©
    result = pd.concat([data, scaled_df], axis=1)
    
    return result, scaler

def create_sequences(data, feature_cols, target_cols, seq_length=60):
    """ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
    X, y_regression, y_classification = [], [], []
    
    # ì—°ì†ì„± í™•ì¸
    time_diff = data.index.to_series().diff()
    split_points = time_diff > pd.Timedelta(minutes=1)
    segment_ids = split_points.cumsum()
    
    for segment_id in segment_ids.unique():
        segment = data[segment_ids == segment_id]
        
        if len(segment) > seq_length:
            feature_data = segment[feature_cols].values
            regression_data = segment[target_cols[0]].values  # FUTURE_TOTALCNT
            classification_data = segment[target_cols[1]].values  # BOTTLENECK_LOCATION
            
            for i in range(len(segment) - seq_length):
                X.append(feature_data[i:i+seq_length])
                y_regression.append(regression_data[i+seq_length])
                y_classification.append(classification_data[i+seq_length])
    
    return np.array(X), np.array(y_regression), np.array(y_classification)

# ===================================
# 3. CNN-LSTM Multi-Task ëª¨ë¸ (ê°•í™”ëœ ë²„ì „)
# ===================================

def build_cnn_lstm_multitask_model(input_shape, num_classes=4):
    """CNN-LSTM Multi-Task ëª¨ë¸ êµ¬ì¶• (ì‹¤ì œ ë°ì´í„°ìš© ê°•í™” ë²„ì „)"""
    
    # ì…ë ¥ ë ˆì´ì–´
    inputs = Input(shape=input_shape, name='input')
    
    # === ê°•í™”ëœ CNN íŒŒíŠ¸ ===
    # ì²« ë²ˆì§¸ Conv1D ë¸”ë¡
    x = Conv1D(filters=128, kernel_size=5, padding='same')(inputs)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.3)(x)
    
    # ë‘ ë²ˆì§¸ Conv1D ë¸”ë¡
    x = Conv1D(filters=256, kernel_size=3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.3)(x)
    
    # ì„¸ ë²ˆì§¸ Conv1D ë¸”ë¡
    x = Conv1D(filters=256, kernel_size=3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.3)(x)
    
    # ë„¤ ë²ˆì§¸ Conv1D ë¸”ë¡
    x = Conv1D(filters=128, kernel_size=3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    
    # Max Pooling
    x = MaxPooling1D(pool_size=2)(x)
    
    # === ê°•í™”ëœ LSTM íŒŒíŠ¸ ===
    # ì²« ë²ˆì§¸ Bidirectional LSTM
    x = Bidirectional(LSTM(256, return_sequences=True))(x)
    x = Dropout(0.4)(x)
    
    # ë‘ ë²ˆì§¸ Bidirectional LSTM
    x = Bidirectional(LSTM(128, return_sequences=True))(x)
    x = Dropout(0.4)(x)
    
    # ì„¸ ë²ˆì§¸ Bidirectional LSTM
    x = Bidirectional(LSTM(64, return_sequences=False))(x)
    x = Dropout(0.4)(x)
    
    # === ê³µìœ  Dense ë ˆì´ì–´ ===
    shared = Dense(256, activation='relu', name='shared_layer')(x)
    shared = BatchNormalization()(shared)
    shared = Dropout(0.4)(shared)
    
    shared = Dense(128, activation='relu')(shared)
    shared = BatchNormalization()(shared)
    shared = Dropout(0.3)(shared)
    
    # === Multi-Task ì¶œë ¥ ===
    # Task 1: ë¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡ (íšŒê·€)
    logistics_branch = Dense(128, activation='relu')(shared)
    logistics_branch = Dropout(0.3)(logistics_branch)
    logistics_branch = Dense(64, activation='relu')(logistics_branch)
    logistics_output = Dense(1, name='logistics_output')(logistics_branch)
    
    # Task 2: ë³‘ëª© ìœ„ì¹˜ ì˜ˆì¸¡ (ë¶„ë¥˜)
    bottleneck_branch = Dense(128, activation='relu')(shared)
    bottleneck_branch = Dropout(0.3)(bottleneck_branch)
    bottleneck_branch = Dense(64, activation='relu')(bottleneck_branch)
    bottleneck_output = Dense(num_classes, activation='softmax', name='bottleneck_output')(bottleneck_branch)
    
    # ëª¨ë¸ ìƒì„±
    model = Model(inputs=inputs, outputs=[logistics_output, bottleneck_output])
    
    return model

# ===================================
# 4. í•™ìŠµ í”„ë¡œì„¸ìŠ¤
# ===================================

def train_model(model, X_train, y_train_reg, y_train_cls, X_val, y_val_reg, y_val_cls, 
                epochs=200, batch_size=64):
    """ëª¨ë¸ í•™ìŠµ (ì‹¤ì œ ë°ì´í„°ìš© ì„¤ì •)"""
    
    # ì†ì‹¤ í•¨ìˆ˜ì™€ ê°€ì¤‘ì¹˜ ì„¤ì •
    losses = {
        'logistics_output': 'mse',
        'bottleneck_output': 'sparse_categorical_crossentropy'
    }
    
    loss_weights = {
        'logistics_output': 0.7,  # ë¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        'bottleneck_output': 0.3
    }
    
    metrics = {
        'logistics_output': ['mae'],
        'bottleneck_output': ['accuracy']
    }
    
    # ì»´íŒŒì¼
    model.compile(
        optimizer=Adam(learning_rate=0.0001),  # ë‚®ì€ í•™ìŠµë¥ 
        loss=losses,
        loss_weights=loss_weights,
        metrics=metrics
    )
    
    # ì½œë°± ì„¤ì •
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=30,  # ë” ê¸´ patience
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=15,
            min_lr=1e-7,
            verbose=1
        ),
        ModelCheckpoint(
            'model/cnn_lstm_multitask_best.keras',
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        )
    ]
    
    # í•™ìŠµ
    history = model.fit(
        X_train,
        {'logistics_output': y_train_reg, 'bottleneck_output': y_train_cls},
        validation_data=(
            X_val,
            {'logistics_output': y_val_reg, 'bottleneck_output': y_val_cls}
        ),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=1
    )
    
    return history

# ===================================
# 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ===================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    logger.info("="*60)
    logger.info("CNN-LSTM Multi-Task ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ì‹¤ì œ ë°ì´í„°)")
    logger.info("="*60)
    
    # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    data_path = 'data/20240201_TO_202507281705.csv'  # ì‹¤ì œ ì „ì²´ ë°ì´í„°
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(data_path):
        logger.error(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        return None, None, None
    
    data = load_and_preprocess_data(data_path)
    
    # 2. íŠ¹ì§• ìƒì„±
    data = create_features(data)
    
    # 3. íƒ€ê²Ÿ ìƒì„±
    data = create_targets(data)
    
    # 4. íŠ¹ì§• ì„ íƒ
    # ìŠ¤ì¼€ì¼ë§í•  íŠ¹ì§• (ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¹„ìœ¨ íŠ¹ì§• ì œì™¸ ê°€ëŠ¥)
    scale_features_list = [
        'TOTALCNT', 'M14AM10A', 'M10AM14A', 'M14AM14B', 'M14BM14A', 'M14AM16', 'M16M14A',
        'imbalance_M14A_M10A', 'imbalance_M14A_M14B', 'imbalance_M14A_M16',
        'MA_5', 'MA_10', 'MA_30', 'MA_60',
        'STD_5', 'STD_10', 'STD_30',
        'MAX_10', 'MIN_10',
        'load_M14A_out', 'load_M14A_in',
        'ratio_M14A_M10A', 'ratio_M14A_M14B', 'ratio_M14A_M16',
        'change_rate', 'change_rate_5', 'change_rate_10',
        'acceleration'
    ]
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
    scale_features_list = [col for col in scale_features_list if col in data.columns]
    logger.info(f"ìŠ¤ì¼€ì¼ë§í•  íŠ¹ì§• ìˆ˜: {len(scale_features_list)}")
    
    # 5. ìŠ¤ì¼€ì¼ë§
    data, scaler = scale_features(data, scale_features_list)
    
    # 6. ì‹œí€€ìŠ¤ìš© íŠ¹ì§• ì„ íƒ
    sequence_features = [col for col in data.columns if col.startswith('scaled_')]
    target_features = ['FUTURE_TOTALCNT', 'BOTTLENECK_LOCATION']
    
    # 7. ì‹œí€€ìŠ¤ ìƒì„± (60ë¶„ ì‹œí€€ìŠ¤)
    X, y_regression, y_classification = create_sequences(
        data, 
        sequence_features, 
        target_features,
        seq_length=60  # 1ì‹œê°„ ì‹œí€€ìŠ¤
    )
    
    logger.info(f"ì‹œí€€ìŠ¤ shape - X: {X.shape}, y_reg: {y_regression.shape}, y_cls: {y_classification.shape}")
    
    # 8. ë°ì´í„° ë¶„í• 
    # ì‹œê°„ ìˆœì„œ ìœ ì§€ë¥¼ ìœ„í•´ ìˆœì°¨ì ìœ¼ë¡œ ë¶„í• 
    train_size = int(0.7 * len(X))
    val_size = int(0.15 * len(X))
    
    X_train = X[:train_size]
    X_val = X[train_size:train_size+val_size]
    X_test = X[train_size+val_size:]
    
    y_train_reg = y_regression[:train_size]
    y_val_reg = y_regression[train_size:train_size+val_size]
    y_test_reg = y_regression[train_size+val_size:]
    
    y_train_cls = y_classification[:train_size]
    y_val_cls = y_classification[train_size:train_size+val_size]
    y_test_cls = y_classification[train_size+val_size:]
    
    logger.info(f"\në°ì´í„° ë¶„í• :")
    logger.info(f"  - Train: {len(X_train)} samples")
    logger.info(f"  - Validation: {len(X_val)} samples")
    logger.info(f"  - Test: {len(X_test)} samples")
    
    # 9. ëª¨ë¸ ìƒì„±
    input_shape = (X_train.shape[1], X_train.shape[2])
    
    # ì‹¤ì œ í´ë˜ìŠ¤ ê°œìˆ˜ í™•ì¸
    unique_classes = np.unique(y_classification)
    num_classes = len(unique_classes)
    logger.info(f"ë³‘ëª© í´ë˜ìŠ¤: {unique_classes}, ì´ {num_classes}ê°œ")
    
    model = build_cnn_lstm_multitask_model(input_shape, num_classes)
    model.summary()
    
    # 10. ëª¨ë¸ í•™ìŠµ
    logger.info("\nëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    history = train_model(
        model, 
        X_train, y_train_reg, y_train_cls,
        X_val, y_val_reg, y_val_cls,
        epochs=200,  # ì‹¤ì œ ë°ì´í„°ìš©
        batch_size=64
    )
    
    # 11. ëª¨ë¸ í‰ê°€
    logger.info("\nëª¨ë¸ í‰ê°€ ì¤‘...")
    
    # ì˜ˆì¸¡
    predictions = model.predict(X_test)
    pred_logistics = predictions[0].flatten()
    pred_bottleneck = predictions[1]
    
    # ë¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡ í‰ê°€
    mae = mean_absolute_error(y_test_reg, pred_logistics)
    mse = mean_squared_error(y_test_reg, pred_logistics)
    
    logger.info(f"\në¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡ ì„±ëŠ¥:")
    logger.info(f"  MAE: {mae:.2f}")
    logger.info(f"  MSE: {mse:.2f}")
    logger.info(f"  RMSE: {np.sqrt(mse):.2f}")
    
    # ë³‘ëª© ì˜ˆì¸¡ í‰ê°€
    pred_bottleneck_classes = np.argmax(pred_bottleneck, axis=1)
    accuracy = accuracy_score(y_test_cls, pred_bottleneck_classes)
    
    logger.info(f"\në³‘ëª© ìœ„ì¹˜ ì˜ˆì¸¡ ì„±ëŠ¥:")
    logger.info(f"  Accuracy: {accuracy:.2%}")
    logger.info("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
    
    # target_names ë™ì  ìƒì„±
    if num_classes == 3:
        target_names = ['ì •ìƒ', 'M14A-M10A', 'M14A-M14B']
    elif num_classes == 4:
        target_names = ['ì •ìƒ', 'M14A-M10A', 'M14A-M14B', 'M14A-M16']
    else:
        target_names = [f'Class_{i}' for i in range(num_classes)]
    
    print(classification_report(y_test_cls, pred_bottleneck_classes, 
                              target_names=target_names))
    
    # 12. ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    logger.info("\nëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì¤‘...")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('model', exist_ok=True)
    os.makedirs('scaler', exist_ok=True)
    
    # ëª¨ë¸ ì €ì¥
    model.save('model/cnn_lstm_multitask_final.keras')
    logger.info("ëª¨ë¸ ì €ì¥ ì™„ë£Œ: model/cnn_lstm_multitask_final.keras")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    joblib.dump(scaler, 'scaler/multitask_scaler.pkl')
    logger.info("ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ: scaler/multitask_scaler.pkl")
    
    # 13. í•™ìŠµ ê³¡ì„  ì‹œê°í™”
    plot_training_history(history)
    
    # 14. ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
    plot_predictions(y_test_reg, pred_logistics, y_test_cls, pred_bottleneck_classes, num_classes)
    
    logger.info("\n" + "="*60)
    logger.info("í•™ìŠµ ì™„ë£Œ!")
    logger.info("="*60)
    
    return model, scaler, history

# ===================================
# 6. ì‹œê°í™” í•¨ìˆ˜
# ===================================

def plot_training_history(history):
    """í•™ìŠµ ì´ë ¥ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # ì „ì²´ ì†ì‹¤
    axes[0, 0].plot(history.history['loss'], label='Train Loss')
    axes[0, 0].plot(history.history['val_loss'], label='Val Loss')
    axes[0, 0].set_title('Total Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # ë¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡ ì†ì‹¤
    axes[0, 1].plot(history.history['logistics_output_loss'], label='Train')
    axes[0, 1].plot(history.history['val_logistics_output_loss'], label='Val')
    axes[0, 1].set_title('Logistics Prediction Loss')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('MSE')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # ë³‘ëª© ì˜ˆì¸¡ ì†ì‹¤
    axes[1, 0].plot(history.history['bottleneck_output_loss'], label='Train')
    axes[1, 0].plot(history.history['val_bottleneck_output_loss'], label='Val')
    axes[1, 0].set_title('Bottleneck Prediction Loss')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Cross Entropy')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # ë³‘ëª© ì˜ˆì¸¡ ì •í™•ë„
    axes[1, 1].plot(history.history['bottleneck_output_accuracy'], label='Train')
    axes[1, 1].plot(history.history['val_bottleneck_output_accuracy'], label='Val')
    axes[1, 1].set_title('Bottleneck Prediction Accuracy')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Accuracy')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.savefig('training_history_multitask.png', dpi=300, bbox_inches='tight')
    plt.close()

def plot_predictions(y_true_reg, y_pred_reg, y_true_cls, y_pred_cls, num_classes):
    """ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # ë¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡ ë¹„êµ
    sample_size = min(200, len(y_true_reg))
    axes[0, 0].plot(y_true_reg[:sample_size], label='Actual', color='blue')
    axes[0, 0].plot(y_pred_reg[:sample_size], label='Predicted', color='red', alpha=0.7)
    axes[0, 0].set_title('Logistics Prediction (First 200 samples)')
    axes[0, 0].set_xlabel('Sample')
    axes[0, 0].set_ylabel('TOTALCNT')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # ë¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡ ì‚°ì ë„
    axes[0, 1].scatter(y_true_reg, y_pred_reg, alpha=0.5)
    axes[0, 1].plot([y_true_reg.min(), y_true_reg.max()], 
                    [y_true_reg.min(), y_true_reg.max()], 
                    'r--', lw=2)
    axes[0, 1].set_title('Logistics Prediction Scatter')
    axes[0, 1].set_xlabel('Actual')
    axes[0, 1].set_ylabel('Predicted')
    axes[0, 1].grid(True)
    
    # ë³‘ëª© ì˜ˆì¸¡ í˜¼ë™ í–‰ë ¬
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_true_cls, y_pred_cls)
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])
    axes[1, 0].set_title('Bottleneck Prediction Confusion Matrix')
    axes[1, 0].set_xlabel('Predicted')
    axes[1, 0].set_ylabel('Actual')
    
    # ë™ì  ë¼ë²¨ ì„¤ì •
    if num_classes == 3:
        labels = ['Normal', 'M14A-M10A', 'M14A-M14B']
    elif num_classes == 4:
        labels = ['Normal', 'M14A-M10A', 'M14A-M14B', 'M14A-M16']
    else:
        labels = [f'Class_{i}' for i in range(num_classes)]
    
    axes[1, 0].set_xticklabels(labels)
    axes[1, 0].set_yticklabels(labels)
    
    # ë³‘ëª© ë°œìƒ ì‹œì  í‘œì‹œ
    bottleneck_points = np.where(y_true_cls > 0)[0]
    if len(bottleneck_points) > 0:
        axes[1, 1].scatter(bottleneck_points[:100], 
                          y_true_reg[bottleneck_points[:100]], 
                          color='red', s=50, label='Actual Bottleneck')
    
    predicted_bottleneck = np.where(y_pred_cls > 0)[0]
    if len(predicted_bottleneck) > 0:
        axes[1, 1].scatter(predicted_bottleneck[:100], 
                          y_pred_reg[predicted_bottleneck[:100]], 
                          color='orange', s=30, alpha=0.5, label='Predicted Bottleneck')
    
    axes[1, 1].set_title('Bottleneck Detection')
    axes[1, 1].set_xlabel('Sample')
    axes[1, 1].set_ylabel('TOTALCNT')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.savefig('prediction_results_multitask.png', dpi=300, bbox_inches='tight')
    plt.close()

# ===================================
# 7. ì‹¤í–‰
# ===================================

if __name__ == "__main__":
    # ì‹¤ì œ ë°ì´í„°ë¡œ í•™ìŠµ ì‹¤í–‰!
    model, scaler, history = main()
    
    print("\n" + "="*60)
    print("ğŸ‰ ì‹¤ì œ ë°ì´í„° í•™ìŠµ ì™„ë£Œ!")
    print("="*60)
    print("\nìƒì„±ëœ íŒŒì¼:")
    print("  - model/cnn_lstm_multitask_final.keras")
    print("  - scaler/multitask_scaler.pkl")
    print("  - training_history_multitask.png")
    print("  - prediction_results_multitask.png")