"""
CNN-LSTM Multi-Task ê¸°ë°˜ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ëª¨ë¸ - ì¬ì‹œì‘ ê°€ëŠ¥ ë²„ì „
==================================================================
ì¤‘ê°„ ì €ì¥ê³¼ ì¬ì‹œì‘ì´ ê°€ëŠ¥í•œ ë²„ì „ì…ë‹ˆë‹¤.
ì¤‘ë‹¨ë˜ì–´ë„ ì´ì–´ì„œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ìŠ¤ì¼€ì¼ëŸ¬ë„ ì¤‘ê°„ ì €ì¥ë©ë‹ˆë‹¤!

ì‚¬ìš© ë°ì´í„°: data/20240201_TO_202507281705.csv
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import (Input, Conv1D, LSTM, Dense, Dropout,
                                    BatchNormalization, Bidirectional,
                                    MaxPooling1D, Activation)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import os
import json
import pickle
from datetime import datetime, timedelta
import joblib
import logging
import warnings
import traceback

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')

# ===================================
# 1. í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
# ===================================

# CPU ëª¨ë“œ ì„¤ì •
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
tf.config.set_visible_devices([], 'GPU')

# ëœë¤ ì‹œë“œ ê³ ì •
RANDOM_SEED = 2079936
tf.random.set_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training_multitask.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ===================================
# 2. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ í´ë˜ìŠ¤
# ===================================

class CheckpointManager:
    """í•™ìŠµ ìƒíƒœë¥¼ ì €ì¥í•˜ê³  ë³µì›í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, checkpoint_dir='checkpoints_multitask'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ì €ì¥ ê²½ë¡œë“¤
        self.state_file = os.path.join(checkpoint_dir, 'training_state.json')
        self.data_file = os.path.join(checkpoint_dir, 'preprocessed_data.pkl')
        self.scaler_file = os.path.join(checkpoint_dir, 'scaler_checkpoint.pkl')
        self.history_file = os.path.join(checkpoint_dir, 'training_history.pkl')
        self.class_mapping_file = os.path.join(checkpoint_dir, 'class_mapping.json')
        
    def save_state(self, state_dict):
        """í˜„ì¬ í•™ìŠµ ìƒíƒœ ì €ì¥"""
        # JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ numpy íƒ€ì… ë³€í™˜
        def convert_numpy(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            return obj
        
        # ì¬ê·€ì ìœ¼ë¡œ ë³€í™˜
        state_dict_converted = json.loads(
            json.dumps(state_dict, default=convert_numpy)
        )
        
        with open(self.state_file, 'w') as f:
            json.dump(state_dict_converted, f, indent=4)
        logger.info(f"âœ“ í•™ìŠµ ìƒíƒœ ì €ì¥ë¨: {self.state_file}")
        
    def load_state(self):
        """ì €ì¥ëœ í•™ìŠµ ìƒíƒœ ë¡œë“œ"""
        if os.path.exists(self.state_file):
            with open(self.state_file, 'r') as f:
                state = json.load(f)
            logger.info(f"âœ“ í•™ìŠµ ìƒíƒœ ë¡œë“œë¨: {self.state_file}")
            return state
        return None
        
    def save_data(self, data_dict):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        with open(self.data_file, 'wb') as f:
            pickle.dump(data_dict, f)
        logger.info(f"âœ“ ë°ì´í„° ì €ì¥ë¨: {self.data_file}")
        
    def load_data(self):
        """ì €ì¥ëœ ë°ì´í„° ë¡œë“œ"""
        if os.path.exists(self.data_file):
            with open(self.data_file, 'rb') as f:
                data = pickle.load(f)
            logger.info(f"âœ“ ë°ì´í„° ë¡œë“œë¨: {self.data_file}")
            return data
        return None
        
    def save_scaler(self, scaler):
        """ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ - ì¤‘ìš”!"""
        joblib.dump(scaler, self.scaler_file)
        logger.info(f"âœ“ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ë¨: {self.scaler_file}")
        
    def load_scaler(self):
        """ì €ì¥ëœ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        if os.path.exists(self.scaler_file):
            scaler = joblib.load(self.scaler_file)
            logger.info(f"âœ“ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œë¨: {self.scaler_file}")
            return scaler
        return None
        
    def save_class_mapping(self, class_mapping):
        """í´ë˜ìŠ¤ ë§¤í•‘ ì •ë³´ ì €ì¥"""
        with open(self.class_mapping_file, 'w') as f:
            json.dump(class_mapping, f, indent=4)
        logger.info(f"âœ“ í´ë˜ìŠ¤ ë§¤í•‘ ì €ì¥ë¨: {self.class_mapping_file}")
        
    def load_class_mapping(self):
        """í´ë˜ìŠ¤ ë§¤í•‘ ì •ë³´ ë¡œë“œ"""
        if os.path.exists(self.class_mapping_file):
            with open(self.class_mapping_file, 'r') as f:
                mapping = json.load(f)
            logger.info(f"âœ“ í´ë˜ìŠ¤ ë§¤í•‘ ë¡œë“œë¨: {self.class_mapping_file}")
            return mapping
        return None
        
    def save_history(self, history):
        """í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥"""
        with open(self.history_file, 'wb') as f:
            pickle.dump(history, f)
        logger.info(f"âœ“ í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥ë¨: {self.history_file}")
        
    def load_history(self):
        """í•™ìŠµ íˆìŠ¤í† ë¦¬ ë¡œë“œ"""
        if os.path.exists(self.history_file):
            with open(self.history_file, 'rb') as f:
                history = pickle.load(f)
            logger.info(f"âœ“ í•™ìŠµ íˆìŠ¤í† ë¦¬ ë¡œë“œë¨: {self.history_file}")
            return history
        return None
        
    def save_model_checkpoint(self, model, epoch):
        """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        model_path = os.path.join(self.checkpoint_dir, f'model_epoch_{epoch}.keras')
        model.save(model_path)
        return model_path

# ===================================
# 3. ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì½œë°±
# ===================================

class PeriodicCheckpoint(Callback):
    """ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ê³¼ ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” ì½œë°±"""
    
    def __init__(self, checkpoint_manager, save_freq=5, scaler=None):
        super().__init__()
        self.checkpoint_manager = checkpoint_manager
        self.save_freq = save_freq
        self.scaler = scaler
        self.history = {'loss': [], 'val_loss': [], 
                       'logistics_output_loss': [], 'val_logistics_output_loss': [],
                       'bottleneck_output_loss': [], 'val_bottleneck_output_loss': [],
                       'logistics_output_mae': [], 'val_logistics_output_mae': [],
                       'bottleneck_output_accuracy': [], 'val_bottleneck_output_accuracy': []}
        
    def on_epoch_end(self, epoch, logs=None):
        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        for key in self.history.keys():
            if logs and key in logs:
                self.history[key].append(logs[key])
        
        # ì£¼ê¸°ì  ì €ì¥
        if (epoch + 1) % self.save_freq == 0:
            logger.info(f"\n{'='*60}")
            logger.info(f"ì—í­ {epoch + 1}: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì¤‘...")
            
            # 1. ëª¨ë¸ ì €ì¥
            model_path = self.checkpoint_manager.save_model_checkpoint(self.model, epoch + 1)
            
            # 2. ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (ì¤‘ìš”!)
            if self.scaler is not None:
                self.checkpoint_manager.save_scaler(self.scaler)
            
            # 3. ìƒíƒœ ì €ì¥
            state = {
                'current_epoch': epoch + 1,
                'model_path': model_path,
                'best_val_loss': min(self.history['val_loss']) if self.history['val_loss'] else float('inf'),
                'training_completed': False,
                'last_save_time': datetime.now().isoformat()
            }
            self.checkpoint_manager.save_state(state)
            
            # 4. íˆìŠ¤í† ë¦¬ ì €ì¥
            self.checkpoint_manager.save_history(self.history)
            
            logger.info(f"âœ“ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ (ì—í­ {epoch + 1})")
            logger.info(f"{'='*60}\n")

# ===================================
# 4. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼)
# ===================================

def load_and_preprocess_data(data_path):
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    logger.info("ë°ì´í„° ë¡œë”© ì¤‘...")
    
    # ë°ì´í„° ë¡œë“œ
    Full_Data = pd.read_csv(data_path)
    logger.info(f"ì›ë³¸ ë°ì´í„° shape: {Full_Data.shape}")
    
    # ì‹œê°„ ì»¬ëŸ¼ ë³€í™˜
    Full_Data['CURRTIME'] = pd.to_datetime(Full_Data['CURRTIME'], format='%Y%m%d%H%M')
    Full_Data['TIME'] = pd.to_datetime(Full_Data['TIME'], format='%Y%m%d%H%M')
    
    # SUM ì»¬ëŸ¼ ì œê±°
    columns_to_drop = [col for col in Full_Data.columns if 'SUM' in col]
    Full_Data = Full_Data.drop(columns=columns_to_drop)
    
    # íŠ¹ì • ë‚ ì§œ ë²”ìœ„ë§Œ ì‚¬ìš©
    start_date = pd.to_datetime('2024-02-01 00:00:00')
    end_date = pd.to_datetime('2025-07-27 23:59:59')
    Full_Data = Full_Data[(Full_Data['TIME'] >= start_date) & (Full_Data['TIME'] <= end_date)]
    
    # ì¸ë±ìŠ¤ë¥¼ ì‹œê°„ìœ¼ë¡œ ì„¤ì •
    Full_Data.set_index('CURRTIME', inplace=True)
    
    # ì´ìƒì¹˜ ì²˜ë¦¬
    PM_start_date = pd.to_datetime('2024-10-23 00:00:00')
    PM_end_date = pd.to_datetime('2024-10-23 23:59:59')
    
    within_PM = Full_Data[(Full_Data['TIME'] >= PM_start_date) & (Full_Data['TIME'] <= PM_end_date)]
    outside_PM = Full_Data[(Full_Data['TIME'] < PM_start_date) | (Full_Data['TIME'] > PM_end_date)]
    outside_PM_filtered = outside_PM[(outside_PM['TOTALCNT'] >= 800) & (outside_PM['TOTALCNT'] <= 2500)]
    
    Full_Data = pd.concat([within_PM, outside_PM_filtered])
    Full_Data = Full_Data.sort_index()
    
    logger.info(f"ì „ì²˜ë¦¬ í›„ ë°ì´í„° shape: {Full_Data.shape}")
    return Full_Data

def create_features(data):
    """íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§"""
    logger.info("íŠ¹ì§• ìƒì„± ì¤‘...")
    
    features_data = data.copy()
    
    # ì‹œê°„ íŠ¹ì§•
    features_data['hour'] = features_data.index.hour
    features_data['dayofweek'] = features_data.index.dayofweek
    features_data['is_weekend'] = (features_data.index.dayofweek >= 5).astype(int)
    features_data['month'] = features_data.index.month
    features_data['day'] = features_data.index.day
    features_data['is_peak_hour'] = features_data.index.hour.isin([8, 9, 14, 15, 16, 17]).astype(int)
    
    # íŒ¹ ê°„ ë¶ˆê· í˜• ì§€í‘œ
    features_data['imbalance_M14A_M10A'] = features_data['M14AM10A'] - features_data['M10AM14A']
    features_data['imbalance_M14A_M14B'] = features_data['M14AM14B'] - features_data['M14BM14A']
    features_data['imbalance_M14A_M16'] = features_data['M14AM16'] - features_data['M16M14A']
    
    # ì´ë™ í‰ê· 
    for window in [5, 10, 30, 60]:
        features_data[f'MA_{window}'] = features_data['TOTALCNT'].rolling(window=window, min_periods=1).mean()
    
    # í‘œì¤€í¸ì°¨
    for window in [5, 10, 30]:
        features_data[f'STD_{window}'] = features_data['TOTALCNT'].rolling(window=window, min_periods=1).std()
    
    # ìµœëŒ€/ìµœì†Œê°’
    features_data['MAX_10'] = features_data['TOTALCNT'].rolling(window=10, min_periods=1).max()
    features_data['MIN_10'] = features_data['TOTALCNT'].rolling(window=10, min_periods=1).min()
    
    # íŒ¹ë³„ ë¶€í•˜ìœ¨
    total_safe = features_data['TOTALCNT'].replace(0, 1)
    features_data['load_M14A_out'] = (features_data['M14AM10A'] + features_data['M14AM14B'] +
                                      features_data['M14AM16']) / total_safe
    features_data['load_M14A_in'] = (features_data['M10AM14A'] + features_data['M14BM14A'] +
                                     features_data['M16M14A']) / total_safe
    
    # ê²½ë¡œë³„ ë¹„ìœ¨
    features_data['ratio_M14A_M10A'] = (features_data['M14AM10A'] + features_data['M10AM14A']) / total_safe
    features_data['ratio_M14A_M14B'] = (features_data['M14AM14B'] + features_data['M14BM14A']) / total_safe
    features_data['ratio_M14A_M16'] = (features_data['M14AM16'] + features_data['M16M14A']) / total_safe
    
    # ë³€í™”ìœ¨
    features_data['change_rate'] = features_data['TOTALCNT'].pct_change()
    features_data['change_rate_5'] = features_data['TOTALCNT'].pct_change(5)
    features_data['change_rate_10'] = features_data['TOTALCNT'].pct_change(10)
    features_data['acceleration'] = features_data['change_rate'].diff()
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬
    features_data = features_data.fillna(method='ffill').fillna(0)
    features_data = features_data.replace([np.inf, -np.inf], 0)
    
    # ì´ìƒì¹˜ í´ë¦¬í•‘
    numeric_columns = features_data.select_dtypes(include=[np.number]).columns
    for col in numeric_columns:
        if col not in ['TIME', 'CURRTIME']:
            upper_limit = features_data[col].quantile(0.999)
            lower_limit = features_data[col].quantile(0.001)
            features_data[col] = features_data[col].clip(lower=lower_limit, upper=upper_limit)
    
    logger.info(f"íŠ¹ì§• ìƒì„± ì™„ë£Œ - shape: {features_data.shape}")
    return features_data

def create_targets(data, future_minutes=10):
    """íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±"""
    logger.info("íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")
    
    # ë¬¼ë¥˜ëŸ‰ íƒ€ê²Ÿ
    data['FUTURE_TOTALCNT'] = pd.NA
    for i in data.index:
        future_time = i + pd.Timedelta(minutes=future_minutes)
        if (future_time <= data.index.max()) & (future_time in data.index):
            data.loc[i, 'FUTURE_TOTALCNT'] = data.loc[future_time, 'TOTALCNT']
    
    # ë³‘ëª© ìœ„ì¹˜ íƒ€ê²Ÿ
    thresholds = {
        'total': data['TOTALCNT'].quantile(0.90),
        'm14a_m10a': np.percentile(data['M14AM10A'] + data['M10AM14A'], 90),
        'm14a_m14b': np.percentile(data['M14AM14B'] + data['M14BM14A'], 90),
        'm14a_m16': np.percentile(data['M14AM16'] + data['M16M14A'], 90)
    }
    
    data['BOTTLENECK_LOCATION'] = 0
    for i in data.index:
        future_time = i + pd.Timedelta(minutes=future_minutes)
        if (future_time <= data.index.max()) & (future_time in data.index):
            future_total = data.loc[future_time, 'TOTALCNT']
            if future_total > thresholds['total']:
                route_loads = {
                    1: data.loc[future_time, 'M14AM10A'] + data.loc[future_time, 'M10AM14A'],
                    2: data.loc[future_time, 'M14AM14B'] + data.loc[future_time, 'M14BM14A'],
                    3: data.loc[future_time, 'M14AM16'] + data.loc[future_time, 'M16M14A']
                }
                max_route = max(route_loads.items(), key=lambda x: x[1])
                if max_route[0] == 1 and max_route[1] > thresholds['m14a_m10a']:
                    data.loc[i, 'BOTTLENECK_LOCATION'] = 1
                elif max_route[0] == 2 and max_route[1] > thresholds['m14a_m14b']:
                    data.loc[i, 'BOTTLENECK_LOCATION'] = 2
                elif max_route[0] == 3 and max_route[1] > thresholds['m14a_m16']:
                    data.loc[i, 'BOTTLENECK_LOCATION'] = 3
    
    data = data.dropna(subset=['FUTURE_TOTALCNT'])
    logger.info(f"íƒ€ê²Ÿ ìƒì„± ì™„ë£Œ - ë³‘ëª© ë¶„í¬: {data['BOTTLENECK_LOCATION'].value_counts()}")
    return data

def scale_features(data, feature_columns, scaler=None):
    """íŠ¹ì§• ìŠ¤ì¼€ì¼ë§"""
    if scaler is None:
        scaler = StandardScaler()
        fit_scaler = True
    else:
        fit_scaler = False
    
    scale_columns = [col for col in feature_columns if col in data.columns]
    scale_data = data[scale_columns].copy()
    
    # ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
    if np.isinf(scale_data.values).any():
        scale_data = scale_data.replace([np.inf, -np.inf], np.nan)
        scale_data = scale_data.fillna(scale_data.mean())
    
    # NaN ê°’ ì²˜ë¦¬
    if scale_data.isnull().any().any():
        scale_data = scale_data.fillna(scale_data.mean())
    
    # ìŠ¤ì¼€ì¼ë§
    if fit_scaler:
        scaled_data = scaler.fit_transform(scale_data)
    else:
        scaled_data = scaler.transform(scale_data)
    
    # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    scaled_df = pd.DataFrame(
        scaled_data,
        columns=[f'scaled_{col}' for col in scale_columns],
        index=data.index
    )
    
    result = pd.concat([data, scaled_df], axis=1)
    return result, scaler

def create_sequences(data, feature_cols, target_cols, seq_length=60):
    """ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
    X, y_regression, y_classification = [], [], []
    
    time_diff = data.index.to_series().diff()
    split_points = time_diff > pd.Timedelta(minutes=1)
    segment_ids = split_points.cumsum()
    
    for segment_id in segment_ids.unique():
        segment = data[segment_ids == segment_id]
        if len(segment) > seq_length:
            feature_data = segment[feature_cols].values
            regression_data = segment[target_cols[0]].values
            classification_data = segment[target_cols[1]].values
            
            for i in range(len(segment) - seq_length):
                X.append(feature_data[i:i+seq_length])
                y_regression.append(regression_data[i+seq_length])
                y_classification.append(classification_data[i+seq_length])
    
    return np.array(X), np.array(y_regression), np.array(y_classification)

# ===================================
# 5. CNN-LSTM Multi-Task ëª¨ë¸ (ê¸°ì¡´ê³¼ ë™ì¼)
# ===================================

def build_cnn_lstm_multitask_model(input_shape, num_classes=4):
    """CNN-LSTM Multi-Task ëª¨ë¸ êµ¬ì¶•"""
    
    inputs = Input(shape=input_shape, name='input')
    
    # CNN íŒŒíŠ¸
    x = Conv1D(filters=128, kernel_size=5, padding='same')(inputs)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.3)(x)
    
    x = Conv1D(filters=256, kernel_size=3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.3)(x)
    
    x = Conv1D(filters=256, kernel_size=3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.3)(x)
    
    x = Conv1D(filters=128, kernel_size=3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    
    x = MaxPooling1D(pool_size=2)(x)
    
    # LSTM íŒŒíŠ¸
    x = Bidirectional(LSTM(256, return_sequences=True))(x)
    x = Dropout(0.4)(x)
    
    x = Bidirectional(LSTM(128, return_sequences=True))(x)
    x = Dropout(0.4)(x)
    
    x = Bidirectional(LSTM(64, return_sequences=False))(x)
    x = Dropout(0.4)(x)
    
    # ê³µìœ  Dense ë ˆì´ì–´
    shared = Dense(256, activation='relu', name='shared_layer')(x)
    shared = BatchNormalization()(shared)
    shared = Dropout(0.4)(shared)
    
    shared = Dense(128, activation='relu')(shared)
    shared = BatchNormalization()(shared)
    shared = Dropout(0.3)(shared)
    
    # Multi-Task ì¶œë ¥
    logistics_branch = Dense(128, activation='relu')(shared)
    logistics_branch = Dropout(0.3)(logistics_branch)
    logistics_branch = Dense(64, activation='relu')(logistics_branch)
    logistics_output = Dense(1, name='logistics_output')(logistics_branch)
    
    bottleneck_branch = Dense(128, activation='relu')(shared)
    bottleneck_branch = Dropout(0.3)(bottleneck_branch)
    bottleneck_branch = Dense(64, activation='relu')(bottleneck_branch)
    bottleneck_output = Dense(num_classes, activation='softmax', name='bottleneck_output')(bottleneck_branch)
    
    model = Model(inputs=inputs, outputs=[logistics_output, bottleneck_output])
    
    return model

# ===================================
# 6. í•™ìŠµ í”„ë¡œì„¸ìŠ¤ (ìˆ˜ì •ë¨)
# ===================================

def train_model_with_checkpoint(model, X_train, y_train_reg, y_train_cls, 
                                X_val, y_val_reg, y_val_cls,
                                checkpoint_manager, scaler, start_epoch=0,
                                epochs=200, batch_size=64):
    """ì²´í¬í¬ì¸íŠ¸ë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸ í•™ìŠµ"""
    
    # ì†ì‹¤ í•¨ìˆ˜ì™€ ê°€ì¤‘ì¹˜ ì„¤ì •
    losses = {
        'logistics_output': 'mse',
        'bottleneck_output': 'sparse_categorical_crossentropy'
    }
    
    loss_weights = {
        'logistics_output': 0.7,
        'bottleneck_output': 0.3
    }
    
    metrics = {
        'logistics_output': ['mae'],
        'bottleneck_output': ['accuracy']
    }
    
    # ì»´íŒŒì¼
    model.compile(
        optimizer=Adam(learning_rate=0.0001),
        loss=losses,
        loss_weights=loss_weights,
        metrics=metrics
    )
    
    # ê¸°ì¡´ íˆìŠ¤í† ë¦¬ ë¡œë“œ
    previous_history = checkpoint_manager.load_history()
    
    # ì½œë°± ì„¤ì •
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=30,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=15,
            min_lr=1e-7,
            verbose=1
        ),
        ModelCheckpoint(
            'model/cnn_lstm_multitask_best.keras',
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        ),
        # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì½œë°± ì¶”ê°€!
        PeriodicCheckpoint(
            checkpoint_manager,
            save_freq=5,  # 5ì—í­ë§ˆë‹¤ ì €ì¥
            scaler=scaler  # ìŠ¤ì¼€ì¼ëŸ¬ë„ í•¨ê»˜ ì €ì¥!
        )
    ]
    
    # ì´ì „ íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì½œë°±ì— ì „ë‹¬
    if previous_history:
        callbacks[-1].history = previous_history
    
    try:
        # í•™ìŠµ
        history = model.fit(
            X_train,
            {'logistics_output': y_train_reg, 'bottleneck_output': y_train_cls},
            validation_data=(
                X_val,
                {'logistics_output': y_val_reg, 'bottleneck_output': y_val_cls}
            ),
            initial_epoch=start_epoch,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        # í•™ìŠµ ì™„ë£Œ ìƒíƒœ ì €ì¥
        state = {
            'current_epoch': epochs,
            'training_completed': True,
            'completion_time': datetime.now().isoformat()
        }
        checkpoint_manager.save_state(state)
        
        return history
        
    except KeyboardInterrupt:
        logger.warning("\ní•™ìŠµì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # í˜„ì¬ ìƒíƒœ ì €ì¥
        current_epoch = len(callbacks[-1].history['loss'])
        state = {
            'current_epoch': start_epoch + current_epoch,
            'training_completed': False,
            'interrupted': True,
            'interrupt_time': datetime.now().isoformat()
        }
        checkpoint_manager.save_state(state)
        checkpoint_manager.save_history(callbacks[-1].history)
        checkpoint_manager.save_scaler(scaler)  # ìŠ¤ì¼€ì¼ëŸ¬ë„ ì €ì¥!
        raise
        
    except Exception as e:
        logger.error(f"\ní•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(traceback.format_exc())
        # ì˜¤ë¥˜ ì‹œì  ìƒíƒœ ì €ì¥
        current_epoch = len(callbacks[-1].history['loss']) if callbacks else 0
        state = {
            'current_epoch': start_epoch + current_epoch,
            'training_completed': False,
            'error': str(e),
            'error_time': datetime.now().isoformat()
        }
        checkpoint_manager.save_state(state)
        checkpoint_manager.save_history(callbacks[-1].history)
        checkpoint_manager.save_scaler(scaler)  # ìŠ¤ì¼€ì¼ëŸ¬ë„ ì €ì¥!
        raise

# ===================================
# 7. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ìˆ˜ì •ë¨)
# ===================================

def main(resume=False):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    checkpoint_manager = CheckpointManager()
    
    # ì¬ì‹œì‘ ëª¨ë“œ í™•ì¸
    if resume:
        state = checkpoint_manager.load_state()
        if state:
            logger.info("="*60)
            logger.info("ì´ì „ í•™ìŠµ ìƒíƒœì—ì„œ ì¬ì‹œì‘í•©ë‹ˆë‹¤.")
            logger.info(f"ë§ˆì§€ë§‰ ì—í­: {state.get('current_epoch', 0)}")
            logger.info(f"í•™ìŠµ ì™„ë£Œ ì—¬ë¶€: {state.get('training_completed', False)}")
            logger.info("="*60)
            
            # ì €ì¥ëœ ë°ì´í„° ë¡œë“œ
            saved_data = checkpoint_manager.load_data()
            saved_scaler = checkpoint_manager.load_scaler()
            saved_class_mapping = checkpoint_manager.load_class_mapping()
            
            if saved_data and saved_scaler:
                logger.info("ì €ì¥ëœ ë°ì´í„°ì™€ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                
                # ë°ì´í„° ë³µì›
                X_train = saved_data['X_train']
                X_val = saved_data['X_val']
                X_test = saved_data['X_test']
                y_train_reg = saved_data['y_train_reg']
                y_val_reg = saved_data['y_val_reg']
                y_test_reg = saved_data['y_test_reg']
                y_train_cls = saved_data['y_train_cls']
                y_val_cls = saved_data['y_val_cls']
                y_test_cls = saved_data['y_test_cls']
                input_shape = saved_data['input_shape']
                num_classes = saved_data['num_classes']
                
                # ìŠ¤ì¼€ì¼ëŸ¬ ë³µì›
                scaler = saved_scaler
                
                # í´ë˜ìŠ¤ ë§¤í•‘ ë³µì›
                class_mapping = saved_class_mapping if saved_class_mapping else {}
                
                # ëª¨ë¸ ì¬ìƒì„± ë˜ëŠ” ë¡œë“œ
                start_epoch = state.get('current_epoch', 0)
                
                if 'model_path' in state and os.path.exists(state['model_path']):
                    logger.info(f"ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ: {state['model_path']}")
                    model = load_model(state['model_path'])
                else:
                    logger.info("ëª¨ë¸ì„ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                    model = build_cnn_lstm_multitask_model(input_shape, num_classes)
            else:
                logger.warning("ì €ì¥ëœ ë°ì´í„°ë‚˜ ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
                resume = False
        else:
            logger.info("ì €ì¥ëœ í•™ìŠµ ìƒíƒœê°€ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
            resume = False
    
    # ì²˜ìŒë¶€í„° ì‹œì‘í•˜ëŠ” ê²½ìš°
    if not resume:
        logger.info("="*60)
        logger.info("CNN-LSTM Multi-Task ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ì‹¤ì œ ë°ì´í„°)")
        logger.info("="*60)
        
        # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        data_path = 'data/20240201_TO_202507281705.csv'
        
        if not os.path.exists(data_path):
            logger.error(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
            return None, None, None
        
        data = load_and_preprocess_data(data_path)
        data = create_features(data)
        data = create_targets(data)
        
        # íŠ¹ì§• ì„ íƒ
        scale_features_list = [
            'TOTALCNT', 'M14AM10A', 'M10AM14A', 'M14AM14B', 'M14BM14A', 'M14AM16', 'M16M14A',
            'imbalance_M14A_M10A', 'imbalance_M14A_M14B', 'imbalance_M14A_M16',
            'MA_5', 'MA_10', 'MA_30', 'MA_60',
            'STD_5', 'STD_10', 'STD_30',
            'MAX_10', 'MIN_10',
            'load_M14A_out', 'load_M14A_in',
            'ratio_M14A_M10A', 'ratio_M14A_M14B', 'ratio_M14A_M16',
            'change_rate', 'change_rate_5', 'change_rate_10',
            'acceleration'
        ]
        
        scale_features_list = [col for col in scale_features_list if col in data.columns]
        
        # ìŠ¤ì¼€ì¼ë§
        data, scaler = scale_features(data, scale_features_list)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì¦‰ì‹œ ì €ì¥!
        checkpoint_manager.save_scaler(scaler)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        sequence_features = [col for col in data.columns if col.startswith('scaled_')]
        target_features = ['FUTURE_TOTALCNT', 'BOTTLENECK_LOCATION']
        
        X, y_regression, y_classification = create_sequences(
            data, sequence_features, target_features, seq_length=60
        )
        
        # í´ë˜ìŠ¤ ë ˆì´ë¸” ì¬ë§¤í•‘
        unique_classes = np.unique(y_classification)
        class_mapping = {old_class: new_class for new_class, old_class in enumerate(unique_classes)}
        y_classification_mapped = np.array([class_mapping[cls] for cls in y_classification])
        
        # í´ë˜ìŠ¤ ë§¤í•‘ ì €ì¥
        checkpoint_manager.save_class_mapping(class_mapping)
        
        # ë°ì´í„° ë¶„í• 
        train_size = int(0.7 * len(X))
        val_size = int(0.15 * len(X))
        
        X_train = X[:train_size]
        X_val = X[train_size:train_size+val_size]
        X_test = X[train_size+val_size:]
        
        y_train_reg = y_regression[:train_size]
        y_val_reg = y_regression[train_size:train_size+val_size]
        y_test_reg = y_regression[train_size+val_size:]
        
        y_train_cls = y_classification_mapped[:train_size]
        y_val_cls = y_classification_mapped[train_size:train_size+val_size]
        y_test_cls = y_classification_mapped[train_size+val_size:]
        
        # ë°ì´í„° ì €ì¥
        checkpoint_manager.save_data({
            'X_train': X_train, 'X_val': X_val, 'X_test': X_test,
            'y_train_reg': y_train_reg, 'y_val_reg': y_val_reg, 'y_test_reg': y_test_reg,
            'y_train_cls': y_train_cls, 'y_val_cls': y_val_cls, 'y_test_cls': y_test_cls,
            'input_shape': (X_train.shape[1], X_train.shape[2]),
            'num_classes': len(np.unique(y_classification_mapped))
        })
        
        # ëª¨ë¸ ìƒì„±
        input_shape = (X_train.shape[1], X_train.shape[2])
        num_classes = len(np.unique(y_classification_mapped))
        model = build_cnn_lstm_multitask_model(input_shape, num_classes)
        model.summary()
        
        start_epoch = 0
    
    # ëª¨ë¸ í•™ìŠµ
    logger.info(f"\nëª¨ë¸ í•™ìŠµ ì‹œì‘... (ì—í­ {start_epoch}ë¶€í„°)")
    
    try:
        history = train_model_with_checkpoint(
            model,
            X_train, y_train_reg, y_train_cls,
            X_val, y_val_reg, y_val_cls,
            checkpoint_manager, scaler,
            start_epoch=start_epoch,
            epochs=200,
            batch_size=64
        )
    except KeyboardInterrupt:
        logger.warning("\ní•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ ìƒíƒœê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        logger.info("ë‹¤ì‹œ ì‹œì‘í•˜ë ¤ë©´: python script.py --resume")
        return None, None, None
    except Exception as e:
        logger.error(f"\ní•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.info("ë‹¤ì‹œ ì‹œì‘í•˜ë ¤ë©´: python script.py --resume")
        return None, None, None
    
    # ëª¨ë¸ í‰ê°€
    logger.info("\nëª¨ë¸ í‰ê°€ ì¤‘...")
    predictions = model.predict(X_test)
    pred_logistics = predictions[0].flatten()
    pred_bottleneck = predictions[1]
    
    mae = mean_absolute_error(y_test_reg, pred_logistics)
    mse = mean_squared_error(y_test_reg, pred_logistics)
    
    logger.info(f"\në¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡ ì„±ëŠ¥:")
    logger.info(f"  MAE: {mae:.2f}")
    logger.info(f"  MSE: {mse:.2f}")
    logger.info(f"  RMSE: {np.sqrt(mse):.2f}")
    
    pred_bottleneck_classes = np.argmax(pred_bottleneck, axis=1)
    accuracy = accuracy_score(y_test_cls, pred_bottleneck_classes)
    
    logger.info(f"\në³‘ëª© ìœ„ì¹˜ ì˜ˆì¸¡ ì„±ëŠ¥:")
    logger.info(f"  Accuracy: {accuracy:.2%}")
    
    # ìµœì¢… ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    logger.info("\nìµœì¢… ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì¤‘...")
    
    os.makedirs('model', exist_ok=True)
    os.makedirs('scaler', exist_ok=True)
    os.makedirs('config', exist_ok=True)
    
    model.save('model/cnn_lstm_multitask_final.keras')
    joblib.dump(scaler, 'scaler/multitask_scaler.pkl')
    
    with open('config/class_mapping.json', 'w') as f:
        json.dump(class_mapping, f, indent=4)
    
    logger.info("\n" + "="*60)
    logger.info("í•™ìŠµ ì™„ë£Œ!")
    logger.info("="*60)
    
    return model, scaler, history

# ===================================
# 8. ì‹¤í–‰
# ===================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='CNN-LSTM Multi-Task ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--resume', action='store_true', 
                       help='ì´ì „ í•™ìŠµì„ ì´ì–´ì„œ ì§„í–‰')
    parser.add_argument('--reset', action='store_true',
                       help='ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚­ì œí•˜ê³  ì²˜ìŒë¶€í„° ì‹œì‘')
    
    args = parser.parse_args()
    
    if args.reset:
        import shutil
        if os.path.exists('checkpoints_multitask'):
            shutil.rmtree('checkpoints_multitask')
            logger.info("ì²´í¬í¬ì¸íŠ¸ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # ì‹¤í–‰
    model, scaler, history = main(resume=args.resume)
    
    if model is not None:
        print("\n" + "="*60)
        print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        print("="*60)
        print("\nìƒì„±ëœ íŒŒì¼:")
        print("  - model/cnn_lstm_multitask_final.keras")
        print("  - scaler/multitask_scaler.pkl")
        print("  - checkpoints_multitask/ (ì¤‘ê°„ ì €ì¥ íŒŒì¼ë“¤)")