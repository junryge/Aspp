import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random
import json
import os

def generate_integrated_mcs_data(num_records=10000, days=30):
    """
    í†µí•© MCS ë°ì´í„° ìƒì„± - ë”¥ëŸ¬ë‹ ì˜ˆì¸¡ìš©
    ì‹œê³„ì—´ íŒ¨í„´ì´ ê°•í™”ëœ ë°ì´í„° ìƒì„±
    """
    
    # ê¸°ì¤€ ì‹œì‘ ì‹œê°„
    base_time = datetime(2025, 7, 1, 0, 0, 0)
    
    # FAB ë¼ì¸ ë° ë² ì´ ì •ë³´
    fab_lines = ['FAB1', 'FAB2', 'FAB3']
    bays = {
        'PHOTO': 'í¬í† ê³µì •',
        'ETCH': 'ì‹ê°ê³µì •',
        'DIFF': 'í™•ì‚°ê³µì •',
        'IMPLANT': 'ì´ì˜¨ì£¼ì…',
        'CVD': 'CVDê³µì •',
        'PVD': 'PVDê³µì •',
        'CMP': 'CMPê³µì •',
        'CLEAN': 'ì„¸ì •ê³µì •',
        'METROLOGY': 'ê³„ì¸¡ê³µì •',
        'TEST': 'í…ŒìŠ¤íŠ¸ê³µì •'
    }
    
    # ì¥ë¹„ ì •ë³´
    equipment_info = {
        'PHOTO': ['ASML_NXT_1980', 'ASML_NXT_2050', 'ASML_XT_1460'],
        'ETCH': ['LAM_KIYO_01', 'LAM_VERSYS_01', 'TEL_ETCH_01'],
        'CVD': ['AMAT_CENTURA_01', 'AMAT_PRODUCER_01'],
        'PVD': ['AMAT_ENDURA_01', 'AMAT_ENDURA_02'],
        'CMP': ['AMAT_REFLEXION_01', 'EBARA_F_REX_01'],
        'CLEAN': ['TEL_CLEAN_01', 'DNS_CLEAN_01'],
        'DIFF': ['TEL_UNITY_01', 'HITACHI_DD_01'],
        'IMPLANT': ['AMAT_VARIAN_01', 'AXCELIS_PURION_01'],
        'METROLOGY': ['KLA_2935', 'KLA_SP5', 'ASML_YS350'],
        'TEST': ['TEL_PRECIO_01', 'ACCRETECH_UF3000']
    }
    
    # ìºë¦¬ì–´ íƒ€ì…
    carrier_types = ['FOUP', 'FOSB', 'OPEN_CASSETTE']
    
    # ì›¨ì´í¼ ì‚¬ì´ì¦ˆ
    wafer_sizes = ['200mm', '300mm']
    
    # ì œí’ˆ ì½”ë“œ
    product_codes = ['DRAM_16G', 'DRAM_32G', 'NAND_512G', 'NAND_1T', 
                     'LOGIC_7NM', 'LOGIC_5NM', 'ANALOG_28NM']
    
    # ìš°ì„ ìˆœìœ„
    priorities = ['NORMAL', 'HOT_LOT', 'SUPER_HOT', 'ENGINEERING', 'PILOT']
    
    # MCS ì´ë²¤íŠ¸ íƒ€ì…
    event_types = [
        'LOAD_REQUEST',      # ì ì¬ ìš”ì²­
        'UNLOAD_REQUEST',    # í•˜ì—­ ìš”ì²­
        'TRANSFER_START',    # ì´ì†¡ ì‹œì‘
        'TRANSFER_COMPLETE', # ì´ì†¡ ì™„ë£Œ
        'PROCESS_START',     # ê³µì • ì‹œì‘
        'PROCESS_END',       # ê³µì • ì¢…ë£Œ
        'STOCKER_IN',       # ìŠ¤í† ì»¤ ì…ê³ 
        'STOCKER_OUT',      # ìŠ¤í† ì»¤ ì¶œê³ 
        'SENSOR_UPDATE',    # ì„¼ì„œ ë°ì´í„° ì—…ë°ì´íŠ¸
        'ALARM_OCCURRED',   # ì•ŒëŒ ë°œìƒ
        'ALARM_CLEARED'     # ì•ŒëŒ í•´ì œ
    ]
    
    # ìš´ë°˜ ì¥ë¹„
    transport_vehicles = ['OHT', 'OHS', 'AGV', 'RGV', 'MANUAL']
    
    # ì•ŒëŒ ì½”ë“œ
    alarm_codes = ['E84_TIMEOUT', 'CARRIER_ID_MISMATCH', 'VEHICLE_COLLISION_RISK', 
                   'LOADPORT_ERROR', 'STOCKER_FULL', 'PATH_BLOCKED', 'SENSOR_OUT_OF_RANGE',
                   'VIBRATION_HIGH', 'TEMPERATURE_HIGH', 'PRESSURE_LOW', 'PARTICLE_HIGH']
    
    # ì„¼ì„œ íƒ€ì…
    sensor_types = ['VIBRATION', 'TEMPERATURE', 'PRESSURE', 'PARTICLE', 'HUMIDITY', 'FLOW_RATE']
    
    records = []
    current_time = base_time
    
    # ì¥ë¹„ë³„ ì„¼ì„œ ê¸°ì¤€ê°’ ì„¤ì •
    equipment_sensor_baseline = {}
    for bay, equipments in equipment_info.items():
        for eq in equipments:
            equipment_sensor_baseline[eq] = {
                'vibration': random.uniform(0.5, 1.5),  # mm/s
                'temperature': random.uniform(20, 25),   # Â°C
                'pressure': random.uniform(750, 760),    # Torr
                'particle': random.randint(10, 50),      # count
                'humidity': random.uniform(40, 50),      # %
                'flow_rate': random.uniform(90, 100)     # %
            }
    
    # ì‹œê³„ì—´ íŒ¨í„´ ìƒì„±ì„ ìœ„í•œ íŒŒë¼ë¯¸í„°
    daily_pattern = lambda h: 1.0 + 0.3 * np.sin(2 * np.pi * h / 24 - np.pi/2)  # ì¼ì¼ íŒ¨í„´
    weekly_pattern = lambda d: 1.0 if d < 5 else 0.7  # ì£¼ë§ íš¨ê³¼
    
    # ë³‘ëª© í˜„ìƒ ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ìƒíƒœ
    bottleneck_state = {bay: 0 for bay in bays.keys()}
    
    # ì¥ë¹„ë³„ ëˆ„ì  ì‚¬ìš© ì‹œê°„ (ì„¼ì„œ ì´ìƒ ì‹œë®¬ë ˆì´ì…˜ìš©)
    equipment_usage_hours = {eq: 0 for eqs in equipment_info.values() for eq in eqs}
    
    # ì‹œê°„ë³„ ë ˆì½”ë“œ ìƒì„±
    for day in range(days):
        for hour in range(24):
            # ì‹œê°„ëŒ€ë³„ ì´ë²¤íŠ¸ ìˆ˜ ì¡°ì •
            hour_factor = daily_pattern(hour)
            day_factor = weekly_pattern(day % 7)
            events_this_hour = int(num_records / (days * 24) * hour_factor * day_factor)
            
            for _ in range(events_this_hour):
                # ì‹œê°„ ì¦ê°€
                current_time += timedelta(seconds=random.randint(10, 300))
                
                # ì´ë²¤íŠ¸ íƒ€ì… ì„ íƒ (ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë°˜ì˜)
                if 6 <= hour <= 22:  # ì£¼ê°„
                    event_weights = [10, 10, 20, 20, 15, 15, 5, 5, 30, 5, 5]
                else:  # ì•¼ê°„
                    event_weights = [5, 5, 15, 15, 10, 10, 10, 10, 40, 10, 10]
                
                event_type = random.choices(event_types, weights=event_weights)[0]
                
                # ê¸°ë³¸ MCS ë°ì´í„°
                base_data = {
                    'timestamp': current_time.strftime('%Y-%m-%d %H:%M:%S'),
                    'event_type': event_type,
                    'lot_id': f"LOT{2025070000 + (day * 100 + hour):010d}",
                    'carrier_id': f"CAR{random.randint(10000, 99999)}",
                    'carrier_type': random.choice(carrier_types),
                    'product_code': random.choice(product_codes),
                    'wafer_size': random.choice(wafer_sizes),
                    'wafer_count': 25 if random.choice(wafer_sizes) == '300mm' else 50,
                    'priority': random.choice(priorities),
                    'fab_line': random.choice(fab_lines),
                    'status': 'NORMAL',
                    'alarm_code': None,
                }
                
                # ì´ë²¤íŠ¸ë³„ ì¶”ê°€ ë°ì´í„°
                if event_type in ['LOAD_REQUEST', 'UNLOAD_REQUEST', 'PROCESS_START', 'PROCESS_END']:
                    # ë³‘ëª© í˜„ìƒì„ ê³ ë ¤í•œ ë² ì´ ì„ íƒ
                    bay_weights = [1.0 / (1.0 + bottleneck_state[bay]) for bay in bays.keys()]
                    bay = random.choices(list(bays.keys()), weights=bay_weights)[0]
                    equipment = random.choice(equipment_info[bay])
                    
                    # ì¥ë¹„ ì‚¬ìš© ì‹œê°„ ëˆ„ì 
                    equipment_usage_hours[equipment] += 0.5
                    
                    base_data.update({
                        'location': bay,
                        'equipment_id': equipment,
                        'recipe': f"{bay}_RECIPE_{random.randint(1, 5)}",
                        'step_number': f"{random.randint(1, 10) * 100}",
                    })
                    
                    # ë³‘ëª© ìƒíƒœ ì—…ë°ì´íŠ¸
                    if event_type == 'PROCESS_START':
                        bottleneck_state[bay] += 1
                    elif event_type == 'PROCESS_END':
                        bottleneck_state[bay] = max(0, bottleneck_state[bay] - 1)
                    
                    # ê³µì • ì‹œì‘/ì¢…ë£Œ ì‹œ ì„¼ì„œ ë°ì´í„° í¬í•¨
                    if event_type in ['PROCESS_START', 'PROCESS_END']:
                        baseline = equipment_sensor_baseline[equipment]
                        usage_factor = 1.0 + equipment_usage_hours[equipment] / 1000  # ì‚¬ìš© ì‹œê°„ì— ë”°ë¥¸ ì—´í™”
                        
                        base_data.update({
                            'sensor_vibration_mm_s': round(baseline['vibration'] * usage_factor + random.gauss(0, 0.1), 3),
                            'sensor_temperature_c': round(baseline['temperature'] + usage_factor * 2 + random.gauss(0, 1), 1),
                            'sensor_pressure_torr': round(baseline['pressure'] + random.gauss(0, 2), 1),
                            'sensor_particle_count': int(baseline['particle'] * usage_factor + random.randint(-5, 10)),
                            'sensor_humidity_pct': round(baseline['humidity'] + random.gauss(0, 2), 1),
                            'sensor_flow_rate_pct': round(baseline['flow_rate'] - usage_factor + random.gauss(0, 3), 1),
                        })
                        
                elif event_type in ['TRANSFER_START', 'TRANSFER_COMPLETE']:
                    # ì´ì†¡ ê´€ë ¨ ì´ë²¤íŠ¸
                    from_bay = random.choice(list(bays.keys()) + ['STOCKER_01', 'STOCKER_02'])
                    to_bay = random.choice(list(bays.keys()) + ['STOCKER_01', 'STOCKER_02'])
                    
                    # ë³‘ëª© êµ¬ê°„ìœ¼ë¡œì˜ ì´ì†¡ì€ ì‹œê°„ì´ ë” ê±¸ë¦¼
                    transfer_time = 120  # ê¸°ë³¸ ì´ì†¡ ì‹œê°„
                    if to_bay in bays and bottleneck_state.get(to_bay, 0) > 5:
                        transfer_time += bottleneck_state[to_bay] * 10
                    
                    base_data.update({
                        'from_location': from_bay,
                        'to_location': to_bay,
                        'vehicle_id': f"{random.choice(transport_vehicles)}_{random.randint(100, 999)}",
                        'transfer_time_sec': transfer_time + random.randint(-20, 50) if event_type == 'TRANSFER_COMPLETE' else None,
                    })
                    
                    # OHTì˜ ê²½ìš° ì§„ë™ ë°ì´í„° ì¶”ê°€
                    if 'OHT' in base_data.get('vehicle_id', ''):
                        congestion_factor = sum(bottleneck_state.values()) / len(bottleneck_state)
                        base_data['oht_vibration_mm_s'] = round(0.3 + congestion_factor * 0.1 + random.uniform(0, 0.2), 3)
                        
                elif event_type == 'SENSOR_UPDATE':
                    # ì„¼ì„œ ë°ì´í„° ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸
                    bay = random.choice(list(bays.keys()))
                    equipment = random.choice(equipment_info[bay])
                    baseline = equipment_sensor_baseline[equipment]
                    usage_factor = 1.0 + equipment_usage_hours[equipment] / 1000
                    
                    # ì‹œê°„ëŒ€ë³„ ì´ìƒ ë°œìƒ í™•ë¥ 
                    anomaly_prob = 0.05 if 6 <= hour <= 22 else 0.15  # ì•¼ê°„ì— ì´ìƒ ë°œìƒ í™•ë¥  ë†’ìŒ
                    anomaly = random.random() < anomaly_prob
                    anomaly_factor = random.uniform(1.5, 3.0) if anomaly else 1.0
                    
                    base_data.update({
                        'location': bay,
                        'equipment_id': equipment,
                        'sensor_type': random.choice(sensor_types),
                        'sensor_vibration_mm_s': round(baseline['vibration'] * usage_factor * anomaly_factor + random.gauss(0, 0.1), 3),
                        'sensor_temperature_c': round(baseline['temperature'] + usage_factor * 2 + 
                                                    (random.uniform(5, 15) if anomaly else random.gauss(0, 1)), 1),
                        'sensor_pressure_torr': round(baseline['pressure'] + random.gauss(0, 2), 1),
                        'sensor_particle_count': int(baseline['particle'] * usage_factor + 
                                                   (random.randint(50, 200) if anomaly else random.randint(-5, 10))),
                        'sensor_humidity_pct': round(baseline['humidity'] + random.gauss(0, 2), 1),
                        'sensor_flow_rate_pct': round(baseline['flow_rate'] - usage_factor + random.gauss(0, 3), 1),
                        'sensor_status': 'ANOMALY' if anomaly else 'NORMAL'
                    })
                    
                    # ì´ìƒì¹˜ ë°œìƒ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
                    if anomaly:
                        base_data['status'] = 'WARNING'
                        
                elif event_type == 'ALARM_OCCURRED':
                    # ì•ŒëŒ ë°œìƒ
                    bay = random.choice(list(bays.keys()))
                    equipment = random.choice(equipment_info[bay])
                    
                    base_data.update({
                        'location': bay,
                        'equipment_id': equipment,
                        'alarm_code': random.choice(alarm_codes),
                        'status': 'ALARM',
                        'alarm_severity': random.choice(['WARNING', 'CRITICAL', 'INFO']),
                    })
                    
                    # ì•ŒëŒ ë°œìƒ ì‹œ ì„¼ì„œ ë°ì´í„°ë„ í¬í•¨
                    baseline = equipment_sensor_baseline[equipment]
                    usage_factor = 1.0 + equipment_usage_hours[equipment] / 1000
                    
                    if 'VIBRATION' in base_data['alarm_code']:
                        base_data['sensor_vibration_mm_s'] = round(baseline['vibration'] * usage_factor * random.uniform(2, 4), 3)
                    elif 'TEMPERATURE' in base_data['alarm_code']:
                        base_data['sensor_temperature_c'] = round(baseline['temperature'] + usage_factor * 2 + random.uniform(10, 20), 1)
                    elif 'PARTICLE' in base_data['alarm_code']:
                        base_data['sensor_particle_count'] = int(baseline['particle'] * usage_factor + random.randint(100, 500))
                        
                elif event_type in ['STOCKER_IN', 'STOCKER_OUT']:
                    # ìŠ¤í† ì»¤ ì…ì¶œê³ 
                    base_data.update({
                        'location': f"STOCKER_{random.randint(1, 5):02d}",
                        'stocker_capacity_pct': random.randint(40, 95),
                    })
                    
                records.append(base_data)
    
    # DataFrame ìƒì„±
    df = pd.DataFrame(records)
    
    # ì‹œê°„ìˆœ ì •ë ¬
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df = df.sort_values('timestamp').reset_index(drop=True)
    
    # ì¶”ê°€ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (ë”¥ëŸ¬ë‹ ëª¨ë¸ìš©)
    df['hour'] = df['timestamp'].dt.hour
    df['day_of_week'] = df['timestamp'].dt.dayofweek
    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
    df['shift'] = pd.cut(df['hour'], bins=[0, 8, 16, 24], labels=['Night', 'Morning', 'Evening'])
    
    return df


def create_time_series_features(df):
    """
    ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ìœ„í•œ ì‹œê³„ì—´ íŠ¹ì„± ìƒì„±
    """
    # ì´ì†¡ ì‹œê°„ í†µê³„ (LSTMìš©)
    transfer_times = df[df['transfer_time_sec'].notna()].groupby(
        [pd.Grouper(key='timestamp', freq='H'), 'from_location', 'to_location']
    )['transfer_time_sec'].agg(['mean', 'std', 'count']).reset_index()
    
    # ë³‘ëª© ì§€í‘œ (RNNìš©)
    bottleneck_metrics = df[df['event_type'] == 'PROCESS_START'].groupby(
        [pd.Grouper(key='timestamp', freq='H'), 'location']
    ).size().reset_index(name='queue_size')
    
    # ì²˜ë¦¬ëŸ‰ ë°ì´í„° (ARIMAìš©)
    throughput = df[df['event_type'] == 'PROCESS_END'].groupby(
        pd.Grouper(key='timestamp', freq='H')
    ).size().reset_index(name='hourly_throughput')
    
    # ì„¼ì„œ ì´ìƒ íŒ¨í„´ (ì´ìƒíƒì§€ìš©)
    sensor_anomalies = df[df['sensor_status'] == 'ANOMALY'].groupby(
        [pd.Grouper(key='timestamp', freq='H'), 'equipment_id']
    ).size().reset_index(name='anomaly_count')
    
    return {
        'transfer_times': transfer_times,
        'bottleneck_metrics': bottleneck_metrics,
        'throughput': throughput,
        'sensor_anomalies': sensor_anomalies
    }


def analyze_mcs_data(df):
    """
    MCS ë°ì´í„° ë¶„ì„ ë° ìš”ì•½ - ë”¥ëŸ¬ë‹ ê´€ì 
    """
    print("\nğŸ“Š MCS ë°ì´í„° ë¶„ì„ ê²°ê³¼ (ë”¥ëŸ¬ë‹ ì˜ˆì¸¡ìš©):")
    print(f"- ì´ ë ˆì½”ë“œ ìˆ˜: {len(df):,}")
    print(f"- ê¸°ê°„: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
    print(f"- ê³ ìœ  LOT ìˆ˜: {df['lot_id'].nunique()}")
    print(f"- ê³ ìœ  ìºë¦¬ì–´ ìˆ˜: {df['carrier_id'].nunique()}")
    
    print("\nğŸ“ˆ ì´ë²¤íŠ¸ íƒ€ì…ë³„ ë¶„í¬:")
    event_dist = df['event_type'].value_counts()
    for event, count in event_dist.items():
        print(f"  - {event}: {count:,} ({count/len(df)*100:.1f}%)")
    
    # ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„
    print("\nâ° ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„:")
    hourly_events = df.groupby(df['timestamp'].dt.hour).size()
    peak_hour = hourly_events.idxmax()
    print(f"  - í”¼í¬ ì‹œê°„ëŒ€: {peak_hour}ì‹œ ({hourly_events[peak_hour]:,}ê±´)")
    print(f"  - í‰ê·  ì‹œê°„ë‹¹ ì´ë²¤íŠ¸: {hourly_events.mean():.1f}ê±´")
    
    # ì´ì†¡ ì‹œê°„ ë¶„ì„ (LSTMìš©)
    transfer_data = df[df['transfer_time_sec'].notna()]
    if len(transfer_data) > 0:
        print(f"\nğŸšš ì´ì†¡ ì‹œê°„ ë¶„ì„ (LSTM ì˜ˆì¸¡ ëŒ€ìƒ):")
        print(f"  - í‰ê·  ì´ì†¡ ì‹œê°„: {transfer_data['transfer_time_sec'].mean():.1f}ì´ˆ")
        print(f"  - í‘œì¤€í¸ì°¨: {transfer_data['transfer_time_sec'].std():.1f}ì´ˆ")
        print(f"  - ìµœëŒ€ ì´ì†¡ ì‹œê°„: {transfer_data['transfer_time_sec'].max()}ì´ˆ")
        
    # ë³‘ëª© ë¶„ì„ (RNNìš©)
    process_starts = df[df['event_type'] == 'PROCESS_START']
    if len(process_starts) > 0:
        bay_loads = process_starts['location'].value_counts()
        print(f"\nğŸ”„ ë² ì´ë³„ ë¶€í•˜ ë¶„ì„ (RNN ë³‘ëª© ì˜ˆì¸¡ ëŒ€ìƒ):")
        for bay, count in bay_loads.head(5).items():
            print(f"  - {bay}: {count:,}ê±´ (ë¶€í•˜ìœ¨: {count/len(process_starts)*100:.1f}%)")
    
    # ì²˜ë¦¬ëŸ‰ ë¶„ì„ (ARIMAìš©)
    process_ends = df[df['event_type'] == 'PROCESS_END']
    if len(process_ends) > 0:
        hourly_throughput = process_ends.groupby(df['timestamp'].dt.floor('H')).size()
        print(f"\nğŸ“Š ì²˜ë¦¬ëŸ‰ ë¶„ì„ (ARIMA ì˜ˆì¸¡ ëŒ€ìƒ):")
        print(f"  - í‰ê·  ì‹œê°„ë‹¹ ì²˜ë¦¬ëŸ‰: {hourly_throughput.mean():.1f} ì›¨ì´í¼")
        print(f"  - ìµœëŒ€ ì‹œê°„ë‹¹ ì²˜ë¦¬ëŸ‰: {hourly_throughput.max()} ì›¨ì´í¼")
        print(f"  - ì²˜ë¦¬ëŸ‰ ë³€ë™ê³„ìˆ˜: {hourly_throughput.std()/hourly_throughput.mean():.3f}")
    
    # ì„¼ì„œ ì´ìƒ ë¶„ì„
    sensor_cols = [col for col in df.columns if col.startswith('sensor_') and col.endswith(('_mm_s', '_c', '_torr', '_count', '_pct'))]
    if sensor_cols:
        print("\nğŸ” ì„¼ì„œ ì´ìƒ íƒì§€ ë¶„ì„:")
        
        # ê° ì„¼ì„œë³„ ì´ìƒì¹˜ ë¹„ìœ¨
        for col in sensor_cols[:5]:  # ì£¼ìš” 5ê°œë§Œ
            if col in df.columns:
                data = df[col].dropna()
                if len(data) > 0:
                    # IQR ë°©ì‹ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€
                    Q1 = data.quantile(0.25)
                    Q3 = data.quantile(0.75)
                    IQR = Q3 - Q1
                    outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).sum()
                    print(f"  - {col}: {outliers}ê°œ ì´ìƒì¹˜ ({outliers/len(data)*100:.2f}%)")
    
    # ì•ŒëŒ íŒ¨í„´ ë¶„ì„
    alarm_data = df[df['event_type'] == 'ALARM_OCCURRED']
    if len(alarm_data) > 0:
        print(f"\nâš ï¸ ì•ŒëŒ íŒ¨í„´ ë¶„ì„:")
        print(f"  - ì´ ì•ŒëŒ ìˆ˜: {len(alarm_data):,}")
        print(f"  - ì•ŒëŒ ë°œìƒë¥ : {len(alarm_data)/len(df)*100:.2f}%")
        print(f"  - ì£¼ìš” ì•ŒëŒ íƒ€ì…:")
        for alarm, count in alarm_data['alarm_code'].value_counts().head(3).items():
            print(f"    - {alarm}: {count}ê±´")
    
    return df


def save_integrated_mcs_data(df, time_series_features):
    """
    í†µí•© MCS ë°ì´í„° ì €ì¥ - ë”¥ëŸ¬ë‹ í•™ìŠµìš©
    """
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = "integrated_mcs_data_dl"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # ë©”ì¸ ë°ì´í„° ì €ì¥
    csv_path = os.path.join(output_dir, 'mcs_events.csv')
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"\nâœ… ë©”ì¸ ë°ì´í„° ì €ì¥: {csv_path}")
    
    # ì‹œê³„ì—´ íŠ¹ì„± ë°ì´í„° ì €ì¥
    for name, data in time_series_features.items():
        feature_path = os.path.join(output_dir, f'{name}.csv')
        data.to_csv(feature_path, index=False, encoding='utf-8-sig')
        print(f"âœ… {name} ì €ì¥: {feature_path}")
    
    # JSON í˜•ì‹ìœ¼ë¡œë„ ì €ì¥
    df_json = df.copy()
    df_json['timestamp'] = df_json['timestamp'].astype(str)
    json_path = os.path.join(output_dir, 'mcs_events.json')
    df_json.to_json(json_path, orient='records', force_ascii=False, indent=2)
    print(f"âœ… JSON íŒŒì¼ ì €ì¥: {json_path}")
    
    # Excel ì €ì¥ (ìƒì„¸ ë¶„ì„ í¬í•¨)
    excel_path = os.path.join(output_dir, 'mcs_analysis.xlsx')
    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
        # ì „ì²´ ë°ì´í„° (ìƒ˜í”Œ)
        df.head(1000).to_excel(writer, sheet_name='Sample_Data', index=False)
        
        # ì´ë²¤íŠ¸ ìš”ì•½
        event_summary = df['event_type'].value_counts().reset_index()
        event_summary.columns = ['ì´ë²¤íŠ¸_íƒ€ì…', 'ê±´ìˆ˜']
        event_summary.to_excel(writer, sheet_name='Event_Summary', index=False)
        
        # ì‹œê°„ëŒ€ë³„ íŒ¨í„´
        hourly_pattern = df.groupby([df['timestamp'].dt.hour, 'event_type']).size().unstack(fill_value=0)
        hourly_pattern.to_excel(writer, sheet_name='Hourly_Pattern')
        
        # ì¥ë¹„ë³„ ì„¼ì„œ í†µê³„
        sensor_cols = [col for col in df.columns if col.startswith('sensor_') and col.endswith(('_mm_s', '_c', '_torr', '_count', '_pct'))]
        if sensor_cols:
            sensor_stats = df.groupby('equipment_id')[sensor_cols].agg(['mean', 'std', 'min', 'max'])
            sensor_stats.to_excel(writer, sheet_name='Sensor_Stats')
        
        # ì•ŒëŒ ë¶„ì„
        alarm_data = df[df['event_type'] == 'ALARM_OCCURRED']
        if len(alarm_data) > 0:
            alarm_summary = alarm_data.groupby(['equipment_id', 'alarm_code']).size().reset_index(name='count')
            alarm_summary.to_excel(writer, sheet_name='Alarm_Analysis', index=False)
    
    print(f"âœ… Excel ë¶„ì„ íŒŒì¼ ì €ì¥: {excel_path}")
    
    return output_dir


def create_dl_training_guides():
    """
    ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ê°€ì´ë“œ ìƒì„±
    """
    guides = {
        'lstm_guide.md': """# LSTM ì´ì†¡ì‹œê°„ ì˜ˆì¸¡ ëª¨ë¸ ê°€ì´ë“œ

## ë°ì´í„° ì¤€ë¹„
- ì…ë ¥ íŒŒì¼: `transfer_times.csv`
- ì£¼ìš” íŠ¹ì„±: timestamp, from_location, to_location, mean, std, count

## ëª¨ë¸ êµ¬ì¡°
```python
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(sequence_length, n_features)),
    Dropout(0.2),
    LSTM(64, return_sequences=True),
    Dropout(0.2),
    LSTM(32),
    Dense(1)
])
```

## í•™ìŠµ íŒŒë¼ë¯¸í„°
- Sequence Length: 24 (24ì‹œê°„ ì´ë ¥)
- Batch Size: 32
- Epochs: 100
- Learning Rate: 0.001

## ì˜ˆì¸¡ ëŒ€ìƒ
- ë‹¤ìŒ 1ì‹œê°„ ì´ì†¡ ì‹œê°„
- ì‹ ë¢° êµ¬ê°„ í¬í•¨
""",

        'rnn_guide.md': """# RNN ë³‘ëª© ì˜ˆì¸¡ ëª¨ë¸ ê°€ì´ë“œ

## ë°ì´í„° ì¤€ë¹„
- ì…ë ¥ íŒŒì¼: `bottleneck_metrics.csv`
- ì£¼ìš” íŠ¹ì„±: timestamp, location, queue_size

## ëª¨ë¸ êµ¬ì¡°
```python
model = Sequential([
    SimpleRNN(128, return_sequences=True, input_shape=(sequence_length, n_features)),
    Dropout(0.3),
    SimpleRNN(64),
    Dense(n_locations, activation='softmax')
])
```

## í•™ìŠµ íŒŒë¼ë¯¸í„°
- Sequence Length: 48 (48ì‹œê°„ ì´ë ¥)
- Batch Size: 64
- Epochs: 150
- Learning Rate: 0.0005

## ì˜ˆì¸¡ ëŒ€ìƒ
- ê° ë² ì´ë³„ ë³‘ëª© ë°œìƒ í™•ë¥ 
- ì˜ˆìƒ ëŒ€ê¸° ì‹œê°„
""",

        'arima_guide.md': """# ARIMA ì²˜ë¦¬ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ ê°€ì´ë“œ

## ë°ì´í„° ì¤€ë¹„
- ì…ë ¥ íŒŒì¼: `throughput.csv`
- ì£¼ìš” íŠ¹ì„±: timestamp, hourly_throughput

## ëª¨ë¸ ì„ íƒ
```python
from statsmodels.tsa.arima.model import ARIMA
from pmdarima import auto_arima

# ìë™ íŒŒë¼ë¯¸í„° íƒìƒ‰
model = auto_arima(data, 
                   start_p=1, start_q=1,
                   max_p=5, max_q=5, 
                   seasonal=True, m=24,
                   stepwise=True)
```

## ìµœì  íŒŒë¼ë¯¸í„° (ì˜ˆì‹œ)
- ARIMA(2,1,2)(1,1,1)[24]
- AIC: ìµœì†Œê°’ ê¸°ì¤€

## ì˜ˆì¸¡ ëŒ€ìƒ
- í–¥í›„ 24ì‹œê°„ ì²˜ë¦¬ëŸ‰
- 95% ì‹ ë¢° êµ¬ê°„
""",

        'anomaly_guide.md': """# ì„¼ì„œ ì´ìƒíƒì§€ ëª¨ë¸ ê°€ì´ë“œ

## ë°ì´í„° ì¤€ë¹„
- ì…ë ¥ íŒŒì¼: `mcs_events.csv` (ì„¼ì„œ ë°ì´í„° í•„í„°ë§)
- ì£¼ìš” íŠ¹ì„±: ëª¨ë“  sensor_* ì»¬ëŸ¼

## ëª¨ë¸ ì˜µì…˜

### 1. Isolation Forest
```python
from sklearn.ensemble import IsolationForest
model = IsolationForest(contamination=0.1, random_state=42)
```

### 2. LSTM Autoencoder
```python
encoder = Sequential([
    LSTM(64, activation='relu', input_shape=(timesteps, n_features)),
    Dense(32, activation='relu'),
    Dense(16, activation='relu')
])

decoder = Sequential([
    Dense(32, activation='relu'),
    Dense(64, activation='relu'),
    Dense(n_features)
])
```

## í‰ê°€ ì§€í‘œ
- Precision, Recall, F1-Score
- ì¡°ê¸° ê²½ë³´ ì„±ê³µë¥ 
"""
    }
    
    # ê°€ì´ë“œ íŒŒì¼ ì €ì¥
    output_dir = "integrated_mcs_data_dl"
    for filename, content in guides.items():
        filepath = os.path.join(output_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"ğŸ“„ ê°€ì´ë“œ ìƒì„±: {filepath}")


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("="*60)
    print("ğŸ­ ë°˜ë„ì²´ FAB í†µí•© MCS ë°ì´í„° ìƒì„± (ë”¥ëŸ¬ë‹ ì˜ˆì¸¡ìš©)")
    print("="*60)
    print(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. ë°ì´í„° ìƒì„±
    print("\n1ï¸âƒ£ í†µí•© MCS ë°ì´í„° ìƒì„± ì¤‘...")
    df = generate_integrated_mcs_data(num_records=10000, days=30)
    print(f"   - ìƒì„±ëœ ë ˆì½”ë“œ: {len(df):,}ê°œ")
    
    # 2. ì‹œê³„ì—´ íŠ¹ì„± ìƒì„±
    print("\n2ï¸âƒ£ ë”¥ëŸ¬ë‹ìš© ì‹œê³„ì—´ íŠ¹ì„± ìƒì„± ì¤‘...")
    time_series_features = create_time_series_features(df)
    for name, data in time_series_features.items():
        print(f"   - {name}: {len(data):,}ê°œ ë ˆì½”ë“œ")
    
    # 3. ë°ì´í„° ë¶„ì„
    print("\n3ï¸âƒ£ ë°ì´í„° ë¶„ì„ ì¤‘...")
    analyze_mcs_data(df)
    
    # 4. íŒŒì¼ ì €ì¥
    print("\n4ï¸âƒ£ ë°ì´í„° íŒŒì¼ ì €ì¥ ì¤‘...")
    output_dir = save_integrated_mcs_data(df, time_series_features)
    
    # 5. í•™ìŠµ ê°€ì´ë“œ ìƒì„±
    print("\n5ï¸âƒ£ ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ê°€ì´ë“œ ìƒì„± ì¤‘...")
    create_dl_training_guides()
    
    # 6. ê²°ê³¼ ìš”ì•½
    print("\n" + "="*60)
    print("âœ¨ ë”¥ëŸ¬ë‹ìš© MCS ë°ì´í„° ìƒì„± ì™„ë£Œ!")
    print("="*60)
    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ ìœ„ì¹˜: {os.path.abspath(output_dir)}")
    print("\nğŸ“„ ìƒì„±ëœ íŒŒì¼ ëª©ë¡:")
    print("  - mcs_events.csv : ì „ì²´ MCS ì´ë²¤íŠ¸ ë°ì´í„°")
    print("  - transfer_times.csv : LSTMìš© ì´ì†¡ì‹œê°„ ë°ì´í„°")
    print("  - bottleneck_metrics.csv : RNNìš© ë³‘ëª© ì§€í‘œ")
    print("  - throughput.csv : ARIMAìš© ì²˜ë¦¬ëŸ‰ ë°ì´í„°")
    print("  - sensor_anomalies.csv : ì„¼ì„œ ì´ìƒ íŒ¨í„´")
    print("  - mcs_analysis.xlsx : ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸")
    print("  - *_guide.md : ê° ëª¨ë¸ë³„ í•™ìŠµ ê°€ì´ë“œ")
    
    print("\nğŸ’¡ ë°ì´í„° íŠ¹ì§•:")
    print("  - ì‹œê³„ì—´ íŒ¨í„´ì´ ê°•í™”ëœ MCS ì´ë²¤íŠ¸ ë¡œê·¸")
    print("  - ì¼ì¼/ì£¼ê°„ ì£¼ê¸°ì„± ë°˜ì˜")
    print("  - ë³‘ëª© í˜„ìƒ ì‹œë®¬ë ˆì´ì…˜")
    print("  - ì¥ë¹„ ì‚¬ìš©ì— ë”°ë¥¸ ì„¼ì„œ ê°’ ë³€í™”")
    print("  - ë”¥ëŸ¬ë‹ ëª¨ë¸ë³„ ìµœì í™”ëœ íŠ¹ì„± ë°ì´í„°")
    
    # ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    print("\nğŸ“Š ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 5ê°œ):")
    pd.set_option('display.max_columns', 10)
    pd.set_option('display.width', 120)
    sample_cols = ['timestamp', 'event_type', 'equipment_id', 'sensor_vibration_mm_s', 'transfer_time_sec']
    display_cols = [col for col in sample_cols if col in df.columns]
    print(df[display_cols].head())
    
    return df


if __name__ == "__main__":
    # pandas ì˜µì…˜ ì„¤ì •
    pd.set_option('display.max_colwidth', 50)
    
    # ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
    mcs_df = main()