"""
ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ULTIMATE v5.0 - 1400+ ì˜ˆì¸¡ ê°•í™” ì™„ì „íŒ
========================================================
ëª¨ë“  ê¸°ëŠ¥ í¬í•¨ + ì¬ì‹œì‘ ê°€ëŠ¥ + 1400+ ì˜ˆì¸¡ ê°œì„ 

í•µì‹¬ ê°œì„ :
1. ê°€ì¤‘ ì†ì‹¤ í•¨ìˆ˜ (1400+ì— 10ë°° ê°€ì¤‘ì¹˜)
2. ë°ì´í„° ì¦ê°• (1400+ ìƒ˜í”Œ 3ë°° ì¦ê°€)
3. ê°•í™”ëœ ê¸‰ë³€ ê°ì§€ê¸°
4. ë™ì  ì•™ìƒë¸” (ìµœëŒ€ 20% ë¶€ìŠ¤íŒ…)
5. íŠ¹ì„±ì€ v4ì™€ 100% ë™ì¼ (ìºì‹œ í˜¸í™˜)

ì‚¬ìš©ë²•:
python model_v5_ultimate.py          # ì²˜ìŒ ì‹œì‘
python model_v5_ultimate.py --resume # ì´ì–´ì„œ ì‹œì‘
python model_v5_ultimate.py --reset  # ì´ˆê¸°í™” í›„ ì‹œì‘
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, RobustScaler
from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.keras.layers import (Input, LSTM, Dense, Dropout, BatchNormalization,
                                     GRU, Conv1D, MaxPooling1D, GlobalAveragePooling1D,
                                     Bidirectional)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l1_l2
from tensorflow.keras import backend as K
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score
import matplotlib.pyplot as plt
import os
import sys
from datetime import datetime
import joblib
import logging
import warnings
import json
import pickle
import traceback
import argparse
import shutil
import time

# ê²½ê³  ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')

# ===================================
# 1. í™˜ê²½ ì„¤ì •
# ===================================

# CPU ëª¨ë“œ ì„¤ì • - GPUë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  CPUë¡œë§Œ í•™ìŠµí•˜ë„ë¡ ì„¤ì •
# ì´ëŠ” GPU ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ í”¼í•˜ê³  ì¬í˜„ì„±ì„ ë†’ì´ê¸° ìœ„í•¨
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
tf.config.set_visible_devices([], 'GPU')

# ëœë¤ ì‹œë“œ ì„¤ì • - ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´ ëª¨ë“  ëœë¤ ìš”ì†Œì— ë™ì¼í•œ ì‹œë“œ ì ìš©
RANDOM_SEED = 2079936
tf.random.set_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# ë¡œê¹… ì„¤ì • - í•™ìŠµ ê³¼ì •ì„ íŒŒì¼ê³¼ ì½˜ì†”ì— ë™ì‹œì— ê¸°ë¡
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training_v5.log'),  # íŒŒì¼ì— ë¡œê·¸ ì €ì¥
        logging.StreamHandler()                   # ì½˜ì†”ì—ë„ ì¶œë ¥
    ]
)
logger = logging.getLogger(__name__)

# ===================================
# 2. ê°€ì¤‘ ì†ì‹¤ í•¨ìˆ˜ (1400+ ê°•í™”)
# ===================================

def create_weighted_mse(spike_threshold=0.5):
    """
    1400+ ê°’ì— 10ë°° ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” ì†ì‹¤ í•¨ìˆ˜
    
    ì´ í•¨ìˆ˜ì˜ í•µì‹¬ì€ ë†’ì€ ë¬¼ë¥˜ëŸ‰(1400+)ì„ ì˜ˆì¸¡í•  ë•Œ ë°œìƒí•˜ëŠ” ì˜¤ì°¨ì— 
    ë” í° í˜ë„í‹°ë¥¼ ë¶€ì—¬í•˜ì—¬ ëª¨ë¸ì´ ì´ëŸ¬í•œ ê¸‰ì¦ ìƒí™©ì„ ë” ì˜ í•™ìŠµí•˜ë„ë¡ í•¨
    
    Args:
        spike_threshold: ìŠ¤ì¼€ì¼ëœ ë°ì´í„°ì—ì„œ 1400+ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì„ê³„ê°’
    
    Returns:
        weighted_mse: ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ MSE ì†ì‹¤ í•¨ìˆ˜
    """
    def weighted_mse(y_true, y_pred):
        # ë†’ì€ ê°’(1400+ ë¬¼ë¥˜ëŸ‰)ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ê³„ì‚°
        # y_true > spike_thresholdì¸ ê²½ìš° 10ë°°ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©
        weights = tf.where(
            y_true > spike_threshold,  # ì¡°ê±´: ì‹¤ì œê°’ì´ ì„ê³„ê°’ë³´ë‹¤ í°ê°€?
            10.0,  # Trueì¼ ë•Œ: 1400+ ê°’ì— 10ë°° ê°€ì¤‘ì¹˜
            1.0    # Falseì¼ ë•Œ: ì¼ë°˜ ê°’ì— ê¸°ë³¸ ê°€ì¤‘ì¹˜
        )
        
        # ê°€ì¤‘ MSE ê³„ì‚°
        squared_diff = tf.square(y_true - y_pred)  # ì œê³± ì˜¤ì°¨
        weighted_loss = squared_diff * weights      # ê°€ì¤‘ì¹˜ ì ìš©
        
        return tf.reduce_mean(weighted_loss)  # í‰ê·  ê³„ì‚°
    
    return weighted_mse

# ===================================
# 3. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
# ===================================

class UltimateCheckpointManager:
    """
    ì™„ë²½í•œ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” í•™ìŠµ ì¤‘ë‹¨ ì‹œ ì¬ì‹œì‘ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ìœ„í•´
    ëª¨ë“  í•™ìŠµ ìƒíƒœ, ë°ì´í„°, ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì €ì¥/ë¡œë“œí•¨
    """
    
    def __init__(self, checkpoint_dir='checkpoints_v5'):
        """
        ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ì™€ íŒŒì¼ ê²½ë¡œ ì´ˆê¸°í™”
        """
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # íŒŒì¼ ê²½ë¡œë“¤
        self.state_file = os.path.join(checkpoint_dir, 'training_state.json')  # í•™ìŠµ ìƒíƒœ
        self.data_file = os.path.join(checkpoint_dir, 'preprocessed_data.pkl')  # ì „ì²˜ë¦¬ëœ ë°ì´í„°
        self.models_dir = os.path.join(checkpoint_dir, 'models')  # ëª¨ë¸ ê°€ì¤‘ì¹˜
        os.makedirs(self.models_dir, exist_ok=True)
        
    def save_state(self, state_dict):
        """
        í•™ìŠµ ìƒíƒœë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        ê¸°ì¡´ ìƒíƒœê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        
        Args:
            state_dict: ì €ì¥í•  ìƒíƒœ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        if os.path.exists(self.state_file):
            with open(self.state_file, 'r') as f:
                existing = json.load(f)
        else:
            existing = {}
        
        existing.update(state_dict)  # ê¸°ì¡´ ìƒíƒœì— ìƒˆë¡œìš´ ì •ë³´ ì—…ë°ì´íŠ¸
        
        with open(self.state_file, 'w') as f:
            json.dump(existing, f, indent=4, default=str)  # default=strë¡œ datetime ê°ì²´ ì²˜ë¦¬
            
    def load_state(self):
        """
        ì €ì¥ëœ í•™ìŠµ ìƒíƒœë¥¼ ë¡œë“œ
        
        Returns:
            ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        if os.path.exists(self.state_file):
            with open(self.state_file, 'r') as f:
                return json.load(f)
        return None
    
    def save_data(self, data_dict):
        """
        ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ pickle íŒŒì¼ë¡œ ì €ì¥
        ì´ë¥¼ í†µí•´ ì¬ì‹œì‘ ì‹œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ë‹¤ì‹œ í•˜ì§€ ì•Šì•„ë„ ë¨
        
        Args:
            data_dict: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        with open(self.data_file, 'wb') as f:
            pickle.dump(data_dict, f)
        logger.info(f"ğŸ’¾ ë°ì´í„° ì €ì¥: {self.data_file}")
        
    def load_data(self):
        """
        ì €ì¥ëœ ì „ì²˜ë¦¬ ë°ì´í„°ë¥¼ ë¡œë“œ
        
        Returns:
            ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        if os.path.exists(self.data_file):
            with open(self.data_file, 'rb') as f:
                data = pickle.load(f)
            logger.info(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ: {self.data_file}")
            return data
        return None
    
    def save_model_weights(self, model, model_name, epoch):
        """
        ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥
        5 ì—í­ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ ì¤‘ê°„ ì§€ì ìœ¼ë¡œ ëŒì•„ê°ˆ ìˆ˜ ìˆê²Œ í•¨
        
        Args:
            model: ì €ì¥í•  ëª¨ë¸
            model_name: ëª¨ë¸ ì´ë¦„
            epoch: í˜„ì¬ ì—í­
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        path = os.path.join(self.models_dir, f'{model_name}_epoch_{epoch}.h5')
        model.save_weights(path)
        return path
    
    def load_model_weights(self, model, model_name, epoch):
        """
        ì €ì¥ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œ
        
        Args:
            model: ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•  ëª¨ë¸
            model_name: ëª¨ë¸ ì´ë¦„
            epoch: ë¡œë“œí•  ì—í­
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        path = os.path.join(self.models_dir, f'{model_name}_epoch_{epoch}.h5')
        if os.path.exists(path):
            model.load_weights(path)
            logger.info(f"âœ… ê°€ì¤‘ì¹˜ ë¡œë“œ: {path}")
            return True
        return False
    
    def get_latest_epoch(self, model_name):
        """
        íŠ¹ì • ëª¨ë¸ì˜ ë§ˆì§€ë§‰ í•™ìŠµ ì—í­ ë²ˆí˜¸ë¥¼ ë°˜í™˜
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            
        Returns:
            ë§ˆì§€ë§‰ ì—í­ ë²ˆí˜¸
        """
        state = self.load_state()
        if state and 'model_progress' in state:
            return state['model_progress'].get(model_name, {}).get('last_epoch', 0)
        return 0

# ===================================
# 4. ì»¤ìŠ¤í…€ ì½œë°± (ì§„í–‰ìƒí™© ì €ì¥)
# ===================================

class CheckpointCallback(Callback):
    """
    ë§¤ ì—í­ë§ˆë‹¤ í•™ìŠµ ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°±
    
    ì´ ì½œë°±ì€ í•™ìŠµ ì¤‘ ì–¸ì œë“ ì§€ ì¤‘ë‹¨í•˜ê³  ì¬ì‹œì‘í•  ìˆ˜ ìˆë„ë¡
    ì—í­ë§ˆë‹¤ ì§„í–‰ ìƒí™©ì„ ì €ì¥í•¨
    """
    
    def __init__(self, checkpoint_manager, model_name):
        super().__init__()
        self.checkpoint_manager = checkpoint_manager
        self.model_name = model_name
        # í•™ìŠµ íˆìŠ¤í† ë¦¬ ì¶”ì 
        self.history = {'loss': [], 'val_loss': [], 'mae': [], 'val_mae': []}
        
    def on_epoch_end(self, epoch, logs=None):
        """
        ì—í­ ì¢…ë£Œ ì‹œ í˜¸ì¶œë˜ëŠ” ë©”ì„œë“œ
        
        Args:
            epoch: í˜„ì¬ ì—í­ ë²ˆí˜¸
            logs: ì—í­ì˜ ë©”íŠ¸ë¦­ ë¡œê·¸
        """
        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        for key in self.history.keys():
            if key in logs:
                self.history[key].append(logs[key])
        
        # 5 ì—í­ë§ˆë‹¤ ê°€ì¤‘ì¹˜ ì €ì¥ (ë©”ëª¨ë¦¬ì™€ ì†ë„ì˜ ê· í˜•)
        if (epoch + 1) % 5 == 0:
            self.checkpoint_manager.save_model_weights(self.model, self.model_name, epoch + 1)
            
        # í˜„ì¬ ìƒíƒœ ì €ì¥
        state = {
            'current_model': self.model_name,
            'model_progress': {
                self.model_name: {
                    'last_epoch': epoch + 1,
                    'history': self.history,
                    'best_val_loss': min(self.history['val_loss']) if self.history['val_loss'] else 999
                }
            },
            'last_update': datetime.now().isoformat()
        }
        self.checkpoint_manager.save_state(state)
        
        # ì§„í–‰ ìƒí™© ë¡œê·¸ ì¶œë ¥
        logger.info(f"[{self.model_name}] Epoch {epoch+1} - "
                   f"Loss: {logs.get('loss', 0):.4f}, "
                   f"Val Loss: {logs.get('val_loss', 0):.4f}, "
                   f"Val MAE: {logs.get('val_mae', 0):.4f}")

# ===================================
# 5. ë°ì´í„° ì¦ê°• í•¨ìˆ˜ (1400+ ê°•í™”)
# ===================================

def augment_high_value_data(X_train, y_train, spike_train):
    """
    1400+ ìƒ˜í”Œì„ ì¦ê°•í•˜ì—¬ ë¶ˆê· í˜• í•´ê²°
    
    ì´ í•¨ìˆ˜ëŠ” 1400 ì´ìƒì˜ ë¬¼ë¥˜ëŸ‰ ë°ì´í„°ê°€ ì „ì²´ ë°ì´í„°ì—ì„œ ì°¨ì§€í•˜ëŠ” 
    ë¹„ìœ¨ì´ ë‚®ì€ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì´ëŸ¬í•œ ìƒ˜í”Œë“¤ì„ ì¸ìœ„ì ìœ¼ë¡œ 
    ì¦ê°•í•˜ì—¬ ëª¨ë¸ì´ ë” ì˜ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•¨
    
    Args:
        X_train: í•™ìŠµ ì…ë ¥ ë°ì´í„°
        y_train: í•™ìŠµ íƒ€ê²Ÿ ë°ì´í„°
        spike_train: 1400+ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¼ë²¨
        
    Returns:
        ì¦ê°•ëœ ë°ì´í„° (X, y, spike_labels)
    """
    
    logger.info("ğŸ”§ 1400+ ë°ì´í„° ì¦ê°• ì‹œì‘...")
    
    # 1400+ ì¸ë±ìŠ¤ì™€ ì¼ë°˜ ì¸ë±ìŠ¤ êµ¬ë¶„
    high_indices = np.where(spike_train == 1)[0]  # 1400+ ìƒ˜í”Œì˜ ì¸ë±ìŠ¤
    normal_indices = np.where(spike_train == 0)[0]  # ì¼ë°˜ ìƒ˜í”Œì˜ ì¸ë±ìŠ¤
    
    logger.info(f"   ì›ë³¸ 1400+ ìƒ˜í”Œ: {len(high_indices)}ê°œ")
    logger.info(f"   ì›ë³¸ ì¼ë°˜ ìƒ˜í”Œ: {len(normal_indices)}ê°œ")
    
    if len(high_indices) == 0:
        return X_train, y_train, spike_train
    
    # ì¦ê°• ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    augmented_X = []
    augmented_y = []
    augmented_spike = []
    
    # 1400+ ìƒ˜í”Œ 3ë°° ì¦ê°•
    for idx in high_indices:
        # ì›ë³¸ ë°ì´í„° ì¶”ê°€
        augmented_X.append(X_train[idx])
        augmented_y.append(y_train[idx])
        augmented_spike.append(1)
        
        # ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•œ ë²„ì „ 3ê°œ ìƒì„±
        for i in range(3):
            # ì ì§„ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ë…¸ì´ì¦ˆ ë ˆë²¨ (1%, 2%, 3%)
            noise_level = 0.01 * (i + 1)
            # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ìƒì„±
            noise = np.random.normal(0, noise_level, X_train[idx].shape)
            # ì›ë³¸ì— ë…¸ì´ì¦ˆ ì¶”ê°€
            augmented_sample = X_train[idx] + noise
            
            augmented_X.append(augmented_sample)
            # íƒ€ê²Ÿë„ ì•½ê°„ì˜ ë³€ë™ ì¶”ê°€ (Â±2% ë²”ìœ„)
            augmented_y.append(y_train[idx] * (1 + np.random.uniform(-0.02, 0.02)))
            augmented_spike.append(1)
    
    # ì›ë³¸ ë°ì´í„°ì™€ ì¦ê°• ë°ì´í„° ê²°í•©
    X_combined = np.concatenate([X_train, np.array(augmented_X)])
    y_combined = np.concatenate([y_train, np.array(augmented_y)])
    spike_combined = np.concatenate([spike_train, np.array(augmented_spike)])
    
    # ë°ì´í„° ì…”í”Œ (ì›ë³¸ ìˆœì„œì˜ ì˜í–¥ì„ ì œê±°)
    indices = np.random.permutation(len(X_combined))
    X_augmented = X_combined[indices]
    y_augmented = y_combined[indices]
    spike_augmented = spike_combined[indices]
    
    logger.info(f"   âœ… ì¦ê°• ì™„ë£Œ! ì´ {len(X_augmented)}ê°œ ìƒ˜í”Œ")
    logger.info(f"   âœ… 1400+ ë¹„ìœ¨: {spike_augmented.mean():.2%}")
    
    return X_augmented, y_augmented, spike_augmented

# ===================================
# 6. ë°ì´í„° ì „ì²˜ë¦¬ (v4ì™€ ë™ì¼)
# ===================================

def load_and_preprocess_data(data_path, checkpoint_manager, force_reload=False):
    """
    ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (v4ì™€ 100% ë™ì¼í•˜ì—¬ ìºì‹œ í˜¸í™˜ì„± ë³´ì¥)
    
    ì´ í•¨ìˆ˜ëŠ” ì›ì‹œ CSV ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , ì‹œê³„ì—´ ëª¨ë¸ì— ì í•©í•œ í˜•íƒœë¡œ
    ì „ì²˜ë¦¬í•¨. ìºì‹œ ê¸°ëŠ¥ì„ í†µí•´ ì¬ì‹œì‘ ì‹œ ì‹œê°„ì„ ì ˆì•½í•¨.
    
    Args:
        data_path: CSV íŒŒì¼ ê²½ë¡œ
        checkpoint_manager: ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
        force_reload: ìºì‹œ ë¬´ì‹œí•˜ê³  ë‹¤ì‹œ ë¡œë“œí• ì§€ ì—¬ë¶€
        
    Returns:
        ì „ì²˜ë¦¬ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    
    # ìºì‹œëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    if not force_reload:
        cached_data = checkpoint_manager.load_data()
        if cached_data:
            logger.info("âœ… ìºì‹œëœ ë°ì´í„° ì‚¬ìš©")
            return cached_data
    
    logger.info(f"ğŸ“‚ ë°ì´í„° ìƒˆë¡œ ë¡œë”©: {data_path}")
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ
        logger.info("ğŸ“Š [1/8] CSV íŒŒì¼ ë¡œë”© ì¤‘...")
        data = pd.read_csv(data_path)
        logger.info(f"   âœ“ ì›ë³¸ ë°ì´í„° í¬ê¸°: {data.shape}")
        
        # 2. ì‹œê°„ ë³€í™˜ - ë¬¸ìì—´ í˜•íƒœì˜ ì‹œê°„ì„ datetime ê°ì²´ë¡œ ë³€í™˜
        logger.info("ğŸ•’ [2/8] ì‹œê°„ ë°ì´í„° ë³€í™˜ ì¤‘...")
        data['CURRTIME'] = pd.to_datetime(data['CURRTIME'], format='%Y%m%d%H%M')
        data['TIME'] = pd.to_datetime(data['TIME'], format='%Y%m%d%H%M')
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        data = data[['CURRTIME', 'TOTALCNT', 'TIME']]
        data.set_index('CURRTIME', inplace=True)
        
        # 3. ë‚ ì§œ í•„í„°ë§ - í•™ìŠµ ê¸°ê°„ ì„¤ì •
        logger.info("ğŸ“… [3/8] ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ ì¤‘...")
        start_date = pd.to_datetime('2024-02-01 00:00:00')
        end_date = pd.to_datetime('2024-07-27 23:59:59')
        data = data[(data['TIME'] >= start_date) & (data['TIME'] <= end_date)]
        logger.info(f"   âœ“ í•„í„°ë§ í›„ ë°ì´í„°: {data.shape}")
        
        # 4. ì´ìƒì¹˜ ì œê±° - ë¹„ì •ìƒì ì¸ ë²”ìœ„ì˜ ë°ì´í„° ì œê±°
        logger.info("ğŸ” [4/8] ì´ìƒì¹˜ ì œê±° ì¤‘...")
        before_outlier = len(data)
        # ë¬¼ë¥˜ëŸ‰ì´ 800 ë¯¸ë§Œì´ê±°ë‚˜ 2500 ì´ˆê³¼ì¸ ê²½ìš° ì œê±°
        data = data[(data['TOTALCNT'] >= 800) & (data['TOTALCNT'] <= 2500)]
        logger.info(f"   âœ“ ì œê±°ëœ ì´ìƒì¹˜: {before_outlier - len(data)}ê°œ")
        
        # 5. FUTURE ë° ë¼ë²¨ ìƒì„±
        logger.info("ğŸ¯ [5/8] íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")
        # 10ë¶„ í›„ì˜ ê°’ì„ íƒ€ê²Ÿìœ¼ë¡œ ì„¤ì • (shift(-10))
        data['FUTURE'] = data['TOTALCNT'].shift(-10)
        # 1400 ì´ìƒì¸ ê²½ìš°ë¥¼ ê¸‰ì¦(spike)ìœ¼ë¡œ ë¼ë²¨ë§
        data['spike_label'] = (data['FUTURE'] >= 1400).astype(int)
        data.dropna(inplace=True)
        
        logger.info(f"   âœ“ ì „ì²˜ë¦¬ ì™„ë£Œ: {data.shape}")
        logger.info(f"   âœ“ 1400+ ë¹„ìœ¨: {data['spike_label'].mean():.2%}")
        logger.info(f"   âœ“ 1400+ ê°œìˆ˜: {data['spike_label'].sum()}ê°œ")
        
        # 6. íŠ¹ì§• ìƒì„± - ëª¨ë¸ì´ í•™ìŠµí•  ì¶”ê°€ íŠ¹ì§•ë“¤ì„ ìƒì„±
        logger.info("âš™ï¸ [6/8] íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì¤‘...")
        
        # ì´ë™í‰ê·  - ë‹¨ê¸°, ì¤‘ê¸°, ì¥ê¸° íŠ¸ë Œë“œ íŒŒì•…
        logger.info("   ì´ë™í‰ê·  ê³„ì‚°...")
        data['MA_10'] = data['TOTALCNT'].rolling(10, min_periods=1).mean()  # 10ë¶„ ì´ë™í‰ê· 
        data['MA_30'] = data['TOTALCNT'].rolling(30, min_periods=1).mean()  # 30ë¶„ ì´ë™í‰ê· 
        data['MA_60'] = data['TOTALCNT'].rolling(60, min_periods=1).mean()  # 60ë¶„ ì´ë™í‰ê· 
        
        # í‘œì¤€í¸ì°¨ - ë³€ë™ì„± íŒŒì•…
        logger.info("   í‘œì¤€í¸ì°¨ ê³„ì‚°...")
        data['STD_10'] = data['TOTALCNT'].rolling(10, min_periods=1).std()  # 10ë¶„ í‘œì¤€í¸ì°¨
        data['STD_30'] = data['TOTALCNT'].rolling(30, min_periods=1).std()  # 30ë¶„ í‘œì¤€í¸ì°¨
        
        # ë³€í™”ìœ¨ - ê¸‰ê²©í•œ ë³€í™” ê°ì§€
        logger.info("   ë³€í™”ìœ¨ ê³„ì‚°...")
        data['change_rate'] = data['TOTALCNT'].pct_change()      # 1ë¶„ ë³€í™”ìœ¨
        data['change_rate_10'] = data['TOTALCNT'].pct_change(10) # 10ë¶„ ë³€í™”ìœ¨
        
        # ì‹œê°„ íŠ¹ì§• - ì‹œê°„ëŒ€ë³„ íŒ¨í„´ í•™ìŠµ
        logger.info("   ì‹œê°„ íŠ¹ì§• ì¶”ì¶œ...")
        data['hour'] = data.index.hour           # ì‹œê°„ (0-23)
        data['dayofweek'] = data.index.dayofweek # ìš”ì¼ (0-6)
        data['is_weekend'] = (data.index.dayofweek >= 5).astype(int)  # ì£¼ë§ ì—¬ë¶€
        data['trend'] = data['MA_10'] - data['MA_30']  # ë‹¨ê¸°-ì¤‘ê¸° íŠ¸ë Œë“œ
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        data.fillna(method='ffill', inplace=True)  # ì „ë°© ì±„ìš°ê¸°
        data.fillna(0, inplace=True)               # ë‚¨ì€ ê²°ì¸¡ê°’ì€ 0ìœ¼ë¡œ
        
        # 7. ìŠ¤ì¼€ì¼ë§ - ëª¨ë“  íŠ¹ì§•ì„ ë¹„ìŠ·í•œ ë²”ìœ„ë¡œ ì •ê·œí™”
        logger.info("ğŸ“ [7/8] ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì¤‘...")
        scaler = RobustScaler()  # ì´ìƒì¹˜ì— ê°•ê±´í•œ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©
        feature_cols = ['TOTALCNT', 'MA_10', 'MA_30', 'MA_60', 'STD_10', 'STD_30',
                       'change_rate', 'change_rate_10', 'hour', 'dayofweek', 
                       'is_weekend', 'trend']
        
        # íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ëª¨ë‘ ìŠ¤ì¼€ì¼ë§
        data[feature_cols + ['FUTURE']] = scaler.fit_transform(data[feature_cols + ['FUTURE']])
        
        # 8. ì‹œí€€ìŠ¤ ìƒì„± - LSTM ì…ë ¥ì„ ìœ„í•œ ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±
        logger.info("ğŸ”„ [8/8] ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì¤‘...")
        SEQ_LENGTH = 50  # ê³¼ê±° 50ë¶„ì˜ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡
        X, y, spike_labels = [], [], []
        
        total_sequences = len(data) - SEQ_LENGTH
        logger.info(f"   ì´ {total_sequences}ê°œ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        
        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ì‹œí€€ìŠ¤ ìƒì„±
        for i in range(total_sequences):
            if i % 5000 == 0:
                logger.info(f"   ì§„í–‰ë¥ : {(i/total_sequences)*100:.1f}%")
            
            # ì…ë ¥: ië¶€í„° i+SEQ_LENGTHê¹Œì§€ì˜ íŠ¹ì§•ë“¤
            X.append(data[feature_cols].iloc[i:i+SEQ_LENGTH].values)
            # íƒ€ê²Ÿ: i+SEQ_LENGTH ì‹œì ì˜ ë¯¸ë˜ê°’
            y.append(data['FUTURE'].iloc[i+SEQ_LENGTH])
            # ê¸‰ì¦ ë¼ë²¨
            spike_labels.append(data['spike_label'].iloc[i+SEQ_LENGTH])
        
        X, y, spike_labels = np.array(X), np.array(y), np.array(spike_labels)
        
        # ë°ì´í„° ë¶„í•  - í•™ìŠµ(70%), ê²€ì¦(15%), í…ŒìŠ¤íŠ¸(15%)
        logger.info("ğŸ“Š ë°ì´í„°ì…‹ ë¶„í•  ì¤‘...")
        train_size = int(0.7 * len(X))
        val_size = int(0.15 * len(X))
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì •ë¦¬
        data_dict = {
            'X_train': X[:train_size],
            'y_train': y[:train_size],
            'spike_train': spike_labels[:train_size],
            'X_val': X[train_size:train_size+val_size],
            'y_val': y[train_size:train_size+val_size],
            'spike_val': spike_labels[train_size:train_size+val_size],
            'X_test': X[train_size+val_size:],
            'y_test': y[train_size+val_size:],
            'spike_test': spike_labels[train_size+val_size:],
            'scaler': scaler,
            'feature_cols': feature_cols,
            'input_shape': (SEQ_LENGTH, len(feature_cols))
        }
        
        # ìºì‹œ ì €ì¥
        checkpoint_manager.save_data(data_dict)
        
        logger.info("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
        return data_dict
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise

# ===================================
# 7. ëª¨ë¸ ì •ì˜ (ê°œì„ ëœ ë²„ì „)
# ===================================

def build_improved_lstm(input_shape):
    """
    ê°œì„ ëœ LSTM ëª¨ë¸ (1400+ ì˜ˆì¸¡ ê°•í™”)
    
    ì—¬ëŸ¬ LSTM ë ˆì´ì–´ë¥¼ ìŠ¤íƒí•˜ê³ , ë“œë¡­ì•„ì›ƒê³¼ ì •ê·œí™”ë¥¼ ì ìš©í•˜ì—¬
    ê³¼ì í•©ì„ ë°©ì§€í•˜ë©´ì„œë„ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„
    
    Args:
        input_shape: ì…ë ¥ ë°ì´í„°ì˜ í˜•íƒœ (ì‹œí€€ìŠ¤ ê¸¸ì´, íŠ¹ì§• ìˆ˜)
        
    Returns:
        ì»´íŒŒì¼ë˜ì§€ ì•Šì€ ëª¨ë¸
    """
    model = Sequential([
        Input(shape=input_shape),
        # ì²« ë²ˆì§¸ LSTM ë ˆì´ì–´ - 128 ìœ ë‹›, L1/L2 ì •ê·œí™”ë¡œ ê³¼ì í•© ë°©ì§€
        LSTM(128, return_sequences=True, kernel_regularizer=l1_l2(l1=0.005, l2=0.005)),
        Dropout(0.4),  # 40% ë“œë¡­ì•„ì›ƒìœ¼ë¡œ ê³¼ì í•© ë°©ì§€
        BatchNormalization(),  # ë°°ì¹˜ ì •ê·œí™”ë¡œ í•™ìŠµ ì•ˆì •í™”
        # ë‘ ë²ˆì§¸ LSTM ë ˆì´ì–´ - 64 ìœ ë‹›
        LSTM(64, return_sequences=True),
        Dropout(0.4),
        # ì„¸ ë²ˆì§¸ LSTM ë ˆì´ì–´ - 32 ìœ ë‹›, ë§ˆì§€ë§‰ ë ˆì´ì–´ì´ë¯€ë¡œ return_sequences=False
        LSTM(32, return_sequences=False),
        Dropout(0.3),
        # Dense ë ˆì´ì–´ë“¤
        Dense(32, activation='relu'),
        Dropout(0.2),
        Dense(1)  # ìµœì¢… ì¶œë ¥ (ì˜ˆì¸¡ê°’ 1ê°œ)
    ])
    return model

def build_improved_gru(input_shape):
    """
    ê°œì„ ëœ GRU ëª¨ë¸
    
    GRUëŠ” LSTMì˜ ê°„ì†Œí™”ëœ ë²„ì „ìœ¼ë¡œ, ë” ë¹ ë¥¸ í•™ìŠµì´ ê°€ëŠ¥í•˜ë©´ì„œë„
    ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì„
    """
    model = Sequential([
        Input(shape=input_shape),
        GRU(128, return_sequences=True, kernel_regularizer=l1_l2(l1=0.005, l2=0.005)),
        Dropout(0.4),
        GRU(64, return_sequences=True),
        Dropout(0.4),
        GRU(32, return_sequences=False),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dropout(0.2),
        Dense(1)
    ])
    return model

def build_improved_cnn_lstm(input_shape):
    """
    ê°œì„ ëœ CNN-LSTM í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
    
    CNNìœ¼ë¡œ ë¡œì»¬ íŒ¨í„´ì„ ì¶”ì¶œí•˜ê³ , LSTMìœ¼ë¡œ ì‹œê³„ì—´ ì˜ì¡´ì„±ì„ í•™ìŠµí•˜ëŠ”
    í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°. ê¸‰ê²©í•œ ë³€í™”ë¥¼ ê°ì§€í•˜ëŠ”ë° íš¨ê³¼ì 
    """
    model = Sequential([
        Input(shape=input_shape),
        # CNN ë ˆì´ì–´ë“¤ - ë¡œì»¬ íŒ¨í„´ ì¶”ì¶œ
        Conv1D(64, 3, activation='relu', padding='same'),
        BatchNormalization(),
        Conv1D(64, 3, activation='relu', padding='same'),
        MaxPooling1D(2),  # ë‹¤ìš´ìƒ˜í”Œë§
        Dropout(0.3),
        # LSTM ë ˆì´ì–´ë“¤ - ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ
        LSTM(64, return_sequences=True),
        Dropout(0.4),
        LSTM(32, return_sequences=False),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dropout(0.2),
        Dense(1)
    ])
    return model

def build_improved_spike_detector(input_shape):
    """
    ê°•í™”ëœ ê¸‰ë³€ ê°ì§€ê¸°
    
    1400+ ë¬¼ë¥˜ëŸ‰ ë°œìƒì„ ì˜ˆì¸¡í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ ëª¨ë¸
    CNNê³¼ ì–‘ë°©í–¥ LSTMì„ ê²°í•©í•˜ì—¬ ê¸‰ê²©í•œ ë³€í™”ì˜ ì „ì¡°ë¥¼ ê°ì§€
    """
    model = Sequential([
        Input(shape=input_shape),
        # CNN ë ˆì´ì–´ - ê¸‰ê²©í•œ ë³€í™” íŒ¨í„´ ê°ì§€
        Conv1D(64, 3, activation='relu', padding='same'),
        BatchNormalization(),
        Conv1D(64, 3, activation='relu', padding='same'),
        MaxPooling1D(2),
        Dropout(0.3),
        # ì–‘ë°©í–¥ LSTM - ê³¼ê±°ì™€ ë¯¸ë˜ ì»¨í…ìŠ¤íŠ¸ ëª¨ë‘ í™œìš©
        Bidirectional(LSTM(64, return_sequences=True)),
        Dropout(0.4),
        Bidirectional(LSTM(32, return_sequences=False)),
        Dropout(0.3),
        # Dense ë ˆì´ì–´ë“¤
        Dense(64, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dropout(0.2),
        Dense(1, activation='sigmoid')  # ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™”
    ])
    return model

# ===================================
# 8. í•™ìŠµ í•¨ìˆ˜ (ê°€ì¤‘ ì†ì‹¤ ì ìš©)
# ===================================

def train_model_with_resume(model, model_name, data_dict, checkpoint_manager,
                           epochs=50, batch_size=128, resume=False, use_augmentation=True):
    """
    ì¬ì‹œì‘ ê°€ëŠ¥í•œ í•™ìŠµ í•¨ìˆ˜ (ê°€ì¤‘ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©)
    
    ì´ í•¨ìˆ˜ëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ë©°, ì¤‘ë‹¨ëœ ê²½ìš° ì´ì–´ì„œ í•™ìŠµí•  ìˆ˜ ìˆëŠ”
    ê¸°ëŠ¥ì„ ì œê³µí•¨. 1400+ ì˜ˆì¸¡ì„ ìœ„í•´ ê°€ì¤‘ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•¨.
    
    Args:
        model: í•™ìŠµí•  ëª¨ë¸
        model_name: ëª¨ë¸ ì´ë¦„
        data_dict: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        checkpoint_manager: ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
        epochs: ì´ ì—í­ ìˆ˜
        batch_size: ë°°ì¹˜ í¬ê¸°
        resume: ì¬ì‹œì‘ ì—¬ë¶€
        use_augmentation: ë°ì´í„° ì¦ê°• ì‚¬ìš© ì—¬ë¶€
        
    Returns:
        í•™ìŠµëœ ëª¨ë¸
    """
    
    # ë°ì´í„° ì–¸íŒ©
    X_train = data_dict['X_train']
    y_train = data_dict['y_train']
    spike_train = data_dict['spike_train']
    X_val = data_dict['X_val']
    y_val = data_dict['y_val']
    
    # ê¸‰ë³€ ê°ì§€ê¸°ì¸ ê²½ìš° íƒ€ê²Ÿì„ spike ë¼ë²¨ë¡œ ë³€ê²½
    if 'spike' in model_name:
        y_train = spike_train
        y_val = data_dict['spike_val']
        
        # ê¸‰ë³€ ê°ì§€ê¸°ëŠ” í•­ìƒ ë°ì´í„° ì¦ê°• ì‚¬ìš©
        if use_augmentation:
            X_train, y_train, _ = augment_high_value_data(X_train, data_dict['y_train'], spike_train)
    else:
        # ì¼ë°˜ ëª¨ë¸ë„ ë°ì´í„° ì¦ê°• ì˜µì…˜ ì ìš©
        if use_augmentation:
            X_train, y_train, spike_train = augment_high_value_data(X_train, y_train, spike_train)
    
    # ì¬ì‹œì‘ ì²˜ë¦¬ - ì´ì „ì— ì¤‘ë‹¨ëœ ì—í­ë¶€í„° ì‹œì‘
    start_epoch = 0
    if resume:
        start_epoch = checkpoint_manager.get_latest_epoch(model_name)
        if start_epoch > 0:
            logger.info(f"ğŸ“‚ {model_name} Epoch {start_epoch}ë¶€í„° ì¬ì‹œì‘")
            checkpoint_manager.load_model_weights(model, model_name, start_epoch)
    
    # ì´ë¯¸ ì™„ë£Œëœ ê²½ìš° ìŠ¤í‚µ
    if start_epoch >= epochs:
        logger.info(f"âœ… {model_name} ì´ë¯¸ ì™„ë£Œë¨")
        return model
    
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸš€ {model_name} í•™ìŠµ ì‹œì‘ (Epoch {start_epoch+1}/{epochs})")
    logger.info(f"{'='*60}")
    
    # ëª¨ë¸ ì»´íŒŒì¼
    if 'spike' in model_name:
        # ê¸‰ë³€ ê°ì§€ê¸°ëŠ” ì´ì§„ ë¶„ë¥˜ì´ë¯€ë¡œ binary_crossentropy ì‚¬ìš©
        model.compile(
            optimizer=Adam(learning_rate=0.0001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
    else:
        # ì¼ë°˜ ëª¨ë¸ì€ ê°€ì¤‘ MSE ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©
        weighted_loss = create_weighted_mse(spike_threshold=0.5)
        model.compile(
            optimizer=Adam(learning_rate=0.0001),
            loss=weighted_loss,  # 1400+ ê°’ì— 10ë°° ê°€ì¤‘ì¹˜
            metrics=['mae']
        )
    
    # ì½œë°± ì„¤ì •
    callbacks = [
        # ì²´í¬í¬ì¸íŠ¸ ì½œë°± - ì§„í–‰ ìƒí™© ì €ì¥
        CheckpointCallback(checkpoint_manager, model_name),
        # ì¡°ê¸° ì¢…ë£Œ - 15 ì—í­ ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ ì¤‘ë‹¨
        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),
        # í•™ìŠµë¥  ê°ì†Œ - 7 ì—í­ ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ í•™ìŠµë¥ ì„ ì ˆë°˜ìœ¼ë¡œ
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7, verbose=1),
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        ModelCheckpoint(f'{checkpoint_manager.models_dir}/{model_name}_best.h5', 
                       save_best_only=True, verbose=0)
    ]
    
    # í•™ìŠµ ì‹¤í–‰
    try:
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            initial_epoch=start_epoch,  # ì¬ì‹œì‘ ì‹œ ì´ì „ ì—í­ë¶€í„° ì‹œì‘
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        # ì™„ë£Œ ìƒíƒœ ì €ì¥
        state = {
            'model_progress': {
                model_name: {
                    'completed': True,
                    'final_epoch': epochs,
                    'completed_time': datetime.now().isoformat()
                }
            }
        }
        checkpoint_manager.save_state(state)
        
    except KeyboardInterrupt:
        # ì‚¬ìš©ìê°€ ì¤‘ë‹¨í•œ ê²½ìš°
        logger.warning(f"\nâš ï¸ {model_name} í•™ìŠµ ì¤‘ë‹¨ë¨. ì¬ì‹œì‘ ê°€ëŠ¥!")
        state = {
            'interrupted': True,
            'interrupted_model': model_name,
            'interrupted_time': datetime.now().isoformat()
        }
        checkpoint_manager.save_state(state)
        raise
        
    except Exception as e:
        logger.error(f"âŒ {model_name} í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise
    
    return model

# ===================================
# 9. ê°•í™”ëœ ì•™ìƒë¸” ì˜ˆì¸¡
# ===================================

def enhanced_ensemble_predict(models, spike_detector, X_test):
    """
    1400+ ì˜ˆì¸¡ ê°•í™”ëœ ì•™ìƒë¸”
    
    ê¸‰ë³€ ê°ì§€ê¸°ì˜ ì˜ˆì¸¡ì„ ê¸°ë°˜ìœ¼ë¡œ ê° ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ê³ ,
    1400+ ì˜ˆì¸¡ ì‹œ ë¶€ìŠ¤íŒ…ì„ ì ìš©í•˜ì—¬ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚´
    
    Args:
        models: í•™ìŠµëœ ëª¨ë¸ë“¤ì˜ ë”•ì…”ë„ˆë¦¬
        spike_detector: ê¸‰ë³€ ê°ì§€ ëª¨ë¸
        X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
        
    Returns:
        ì•™ìƒë¸” ì˜ˆì¸¡, ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡, ê¸‰ë³€ í™•ë¥ 
    """
    
    logger.info("ğŸ”® ì•™ìƒë¸” ì˜ˆì¸¡ ì‹œì‘...")
    
    # ê¸‰ë³€ í™•ë¥  ì˜ˆì¸¡ (0~1 ì‚¬ì´ì˜ ê°’)
    spike_probs = spike_detector.predict(X_test, verbose=0).flatten()
    
    # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ ì €ì¥
    predictions = {}
    for name, model in models.items():
        predictions[name] = model.predict(X_test, verbose=0).flatten()
    
    # ë™ì  ê°€ì¤‘ì¹˜ ì•™ìƒë¸”
    ensemble_pred = np.zeros(len(X_test))
    
    for i in range(len(X_test)):
        # ê¸‰ë³€ í™•ë¥ ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ì™€ ë¶€ìŠ¤íŒ… ê³„ìˆ˜ ê²°ì •
        if spike_probs[i] > 0.7:  # ë†’ì€ í™•ì‹ ë„ (70% ì´ìƒ)
            # CNN-LSTMì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬ (ê¸‰ë³€ ê°ì§€ì— íš¨ê³¼ì )
            weights = {'lstm': 0.15, 'gru': 0.15, 'cnn_lstm': 0.7}
            boost_factor = 1.20  # 20% ì¦í­
        elif spike_probs[i] > 0.5:  # ì¤‘ê°„ í™•ì‹ ë„ (50-70%)
            weights = {'lstm': 0.2, 'gru': 0.2, 'cnn_lstm': 0.6}
            boost_factor = 1.15  # 15% ì¦í­
        elif spike_probs[i] > 0.3:  # ì•½í•œ ì‹ í˜¸ (30-50%)
            weights = {'lstm': 0.3, 'gru': 0.3, 'cnn_lstm': 0.4}
            boost_factor = 1.08  # 8% ì¦í­
        else:  # ì •ìƒ ë²”ìœ„ (30% ë¯¸ë§Œ)
            # LSTMê³¼ GRUì— ë†’ì€ ê°€ì¤‘ì¹˜ (ì•ˆì •ì ì¸ ì˜ˆì¸¡)
            weights = {'lstm': 0.4, 'gru': 0.35, 'cnn_lstm': 0.25}
            boost_factor = 1.0  # ì¦í­ ì—†ìŒ
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        for name, weight in weights.items():
            if name in predictions:
                ensemble_pred[i] += weight * predictions[name][i]
        
        # ë¶€ìŠ¤íŒ… ì ìš© (1400+ ì˜ˆì¸¡ ê°•í™”)
        ensemble_pred[i] *= boost_factor
    
    logger.info(f"   âœ… 1400+ ì˜ˆìƒ ê°œìˆ˜: {np.sum(spike_probs > 0.5)}")
    
    return ensemble_pred, predictions, spike_probs

# ===================================
# 10. ë©”ì¸ ì‹¤í–‰
# ===================================

def main(resume=False, reset=False):
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    
    ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•˜ë©°, ë°ì´í„° ë¡œë“œë¶€í„° ëª¨ë¸ í•™ìŠµ,
    í‰ê°€, ì‹œê°í™”ê¹Œì§€ ëª¨ë“  ê³¼ì •ì„ ìˆ˜í–‰í•¨
    
    Args:
        resume: ì´ì „ í•™ìŠµì„ ì¬ê°œí• ì§€ ì—¬ë¶€
        reset: ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  ì²˜ìŒë¶€í„° ì‹œì‘í• ì§€ ì—¬ë¶€
    """
    
    # ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    checkpoint_manager = UltimateCheckpointManager()
    
    # ë¦¬ì…‹ ì²˜ë¦¬ - ëª¨ë“  ì´ì „ ë°ì´í„° ì‚­ì œ
    if reset:
        if os.path.exists(checkpoint_manager.checkpoint_dir):
            shutil.rmtree(checkpoint_manager.checkpoint_dir)
            logger.info("ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™”ë¨")
        checkpoint_manager = UltimateCheckpointManager()
    
    # ì¬ì‹œì‘ ìƒíƒœ í™•ì¸
    if resume:
        state = checkpoint_manager.load_state()
        if state:
            logger.info("="*60)
            logger.info("ğŸ“‚ ì´ì „ í•™ìŠµ ì¬ê°œ")
            if 'interrupted_model' in state:
                logger.info(f"   ì¤‘ë‹¨ëœ ëª¨ë¸: {state['interrupted_model']}")
            logger.info("="*60)
        else:
            logger.info("âš ï¸ ì €ì¥ëœ ìƒíƒœ ì—†ìŒ. ì²˜ìŒë¶€í„° ì‹œì‘")
            resume = False
    
    logger.info("="*60)
    logger.info("ğŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ v5.0 - 1400+ ê°•í™”")
    logger.info("="*60)
    
    # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    data_dict = load_and_preprocess_data(
        'data/20240201_TO_202507281705.csv',
        checkpoint_manager,
        force_reload=not resume  # ì¬ì‹œì‘ì´ë©´ ìºì‹œ ì‚¬ìš©
    )
    
    input_shape = data_dict['input_shape']
    logger.info(f"ğŸ“Š ì…ë ¥ Shape: {input_shape}")
    
    # ëª¨ë¸ ì •ì˜ - 4ê°œì˜ ëª¨ë¸ì„ ì‚¬ìš©
    model_configs = [
        ('lstm', build_improved_lstm),
        ('gru', build_improved_gru),
        ('cnn_lstm', build_improved_cnn_lstm),
        ('spike_detector', build_improved_spike_detector)
    ]
    
    models = {}  # ì¼ë°˜ ì˜ˆì¸¡ ëª¨ë¸ë“¤
    spike_model = None  # ê¸‰ë³€ ê°ì§€ ëª¨ë¸
    
    # ê° ëª¨ë¸ í•™ìŠµ
    for model_name, build_func in model_configs:
        try:
            # ì™„ë£Œëœ ëª¨ë¸ì€ ìŠ¤í‚µ
            state = checkpoint_manager.load_state()
            if state and 'model_progress' in state:
                if state['model_progress'].get(model_name, {}).get('completed', False):
                    logger.info(f"âœ… {model_name} ì´ë¯¸ ì™„ë£Œë¨. ê±´ë„ˆëœ€")
                    # ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
                    model = build_func(input_shape)
                    model.load_weights(f'{checkpoint_manager.models_dir}/{model_name}_best.h5')
                    if model_name != 'spike_detector':
                        models[model_name] = model
                    else:
                        spike_model = model
                    continue
            
            # ëª¨ë¸ ë¹Œë“œ
            model = build_func(input_shape)
            logger.info(f"ğŸ”¨ {model_name} ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
            # í•™ìŠµ ìˆ˜í–‰
            epochs = 40 if model_name == 'spike_detector' else 60  # ê¸‰ë³€ ê°ì§€ê¸°ëŠ” ë” ì ì€ ì—í­
            model = train_model_with_resume(
                model, model_name, data_dict, checkpoint_manager,
                epochs=epochs, batch_size=128, resume=resume,
                use_augmentation=True  # ë°ì´í„° ì¦ê°• ì‚¬ìš©
            )
            
            # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
            if model_name != 'spike_detector':
                models[model_name] = model
            else:
                spike_model = model
                
        except KeyboardInterrupt:
            logger.warning("\nâš ï¸ í•™ìŠµ ì¤‘ë‹¨. python model_v5_ultimate.py --resume ë¡œ ì¬ì‹œì‘ ê°€ëŠ¥")
            return
        except Exception as e:
            logger.error(f"âŒ ì˜¤ë¥˜: {str(e)}")
            logger.info("python model_v5_ultimate.py --resume ë¡œ ì¬ì‹œì‘ ì‹œë„")
            return
    
    # ===================================
    # í‰ê°€ ë‹¨ê³„
    # ===================================
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š ëª¨ë¸ í‰ê°€")
    logger.info("="*60)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    X_test = data_dict['X_test']
    y_test = data_dict['y_test']
    spike_test = data_dict['spike_test']
    scaler = data_dict['scaler']
    feature_cols = data_dict['feature_cols']
    
    # ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰
    ensemble_pred, individual_preds, spike_probs = enhanced_ensemble_predict(
        models, spike_model, X_test
    )
    
    # ì—­ë³€í™˜ - ìŠ¤ì¼€ì¼ëœ ê°’ì„ ì›ë˜ ê°’ìœ¼ë¡œ ë³€í™˜
    y_test_original = scaler.inverse_transform(
        np.column_stack([np.zeros((len(y_test), len(feature_cols))), y_test])
    )[:, -1]
    
    ensemble_original = scaler.inverse_transform(
        np.column_stack([np.zeros((len(ensemble_pred), len(feature_cols))), ensemble_pred])
    )[:, -1]
    
    # ì „ì²´ ì„±ëŠ¥ í‰ê°€
    mae = mean_absolute_error(y_test_original, ensemble_original)
    rmse = np.sqrt(mean_squared_error(y_test_original, ensemble_original))
    r2 = r2_score(y_test_original, ensemble_original)
    
    logger.info(f"ğŸ“ˆ ì „ì²´ ì„±ëŠ¥:")
    logger.info(f"   MAE: {mae:.2f}")
    logger.info(f"   RMSE: {rmse:.2f}")
    logger.info(f"   RÂ²: {r2:.4f}")
    
    # 1400+ êµ¬ê°„ ì„±ëŠ¥ í‰ê°€
    high_mask = y_test_original >= 1400
    if high_mask.sum() > 0:
        mae_high = mean_absolute_error(y_test_original[high_mask], ensemble_original[high_mask])
        rmse_high = np.sqrt(mean_squared_error(y_test_original[high_mask], ensemble_original[high_mask]))
        
        logger.info(f"\nğŸ¯ 1400+ ì„±ëŠ¥:")
        logger.info(f"   ê°œìˆ˜: {high_mask.sum()}ê°œ")
        logger.info(f"   MAE: {mae_high:.2f}")
        logger.info(f"   RMSE: {rmse_high:.2f}")
        
        # ì˜ˆì¸¡ ì„±ê³µë¥  ê³„ì‚°
        pred_high = ensemble_original >= 1400
        precision = np.sum((pred_high) & (high_mask)) / np.sum(pred_high) if np.sum(pred_high) > 0 else 0
        recall = np.sum((pred_high) & (high_mask)) / np.sum(high_mask)
        
        logger.info(f"   Precision: {precision:.2%}")
        logger.info(f"   Recall: {recall:.2%}")
    
    # ê¸‰ë³€ ê°ì§€ ì •í™•ë„
    spike_acc = accuracy_score(spike_test, spike_probs > 0.5)
    logger.info(f"\nğŸ” ê¸‰ë³€ ê°ì§€ ì •í™•ë„: {spike_acc:.2%}")
    
    # ===================================
    # ì‹œê°í™”
    # ===================================
    
    logger.info("\nğŸ“Š ê²°ê³¼ ì‹œê°í™” ìƒì„± ì¤‘...")
    
    # 1. ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(20, 10))
    
    # ìƒë‹¨: ì „ì²´ ì˜ˆì¸¡ ë¹„êµ
    plt.subplot(2, 1, 1)
    plt.plot(y_test_original[:300], label='ì‹¤ì œê°’', color='blue', linewidth=2)
    plt.plot(ensemble_original[:300], label='ì˜ˆì¸¡ê°’', color='red', alpha=0.7, linewidth=1.5)
    plt.axhline(y=1400, color='green', linestyle='--', alpha=0.5, label='1400 ì„ê³„ê°’')
    plt.title(f'ì˜ˆì¸¡ ê²°ê³¼ (ì „ì²´ MAE: {mae:.2f}, 1400+ MAE: {mae_high:.2f})', fontsize=14)
    plt.ylabel('ë¬¼ë¥˜ëŸ‰', fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # í•˜ë‹¨: 1400+ êµ¬ê°„ ì§‘ì¤‘ ì‹œê°í™”
    plt.subplot(2, 1, 2)
    high_points = np.where(y_test_original[:300] >= 1400)[0]
    plt.scatter(high_points, y_test_original[high_points], color='blue', s=50, label='ì‹¤ì œ 1400+', zorder=5)
    plt.scatter(high_points, ensemble_original[high_points], color='red', s=30, label='ì˜ˆì¸¡ 1400+', zorder=4)
    plt.plot(y_test_original[:300], color='blue', alpha=0.3, linewidth=1)
    plt.plot(ensemble_original[:300], color='red', alpha=0.3, linewidth=1)
    plt.axhline(y=1400, color='green', linestyle='--', alpha=0.5)
    plt.title('1400+ êµ¬ê°„ ì˜ˆì¸¡ ì„±ëŠ¥', fontsize=14)
    plt.xlabel('ì‹œê°„ (ë¶„)', fontsize=12)
    plt.ylabel('ë¬¼ë¥˜ëŸ‰', fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results_v5.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # 2. ì˜¤ì°¨ ë¶„í¬ ë¶„ì„
    plt.figure(figsize=(15, 5))
    
    # ì „ì²´ ì˜¤ì°¨ ë¶„í¬
    plt.subplot(1, 3, 1)
    errors = y_test_original - ensemble_original
    plt.hist(errors, bins=50, color='blue', alpha=0.7, edgecolor='black')
    plt.title(f'ì „ì²´ ì˜¤ì°¨ ë¶„í¬\ní‰ê· : {np.mean(errors):.2f}, í‘œì¤€í¸ì°¨: {np.std(errors):.2f}')
    plt.xlabel('ì˜¤ì°¨')
    plt.ylabel('ë¹ˆë„')
    
    # 1400+ ì˜¤ì°¨ ë¶„í¬
    plt.subplot(1, 3, 2)
    if high_mask.sum() > 0:
        high_errors = y_test_original[high_mask] - ensemble_original[high_mask]
        plt.hist(high_errors, bins=30, color='red', alpha=0.7, edgecolor='black')
        plt.title(f'1400+ ì˜¤ì°¨ ë¶„í¬\ní‰ê· : {np.mean(high_errors):.2f}')
        plt.xlabel('ì˜¤ì°¨')
        plt.ylabel('ë¹ˆë„')
    
    # ê¸‰ë³€ ê°ì§€ í™•ë¥  ë¶„í¬
    plt.subplot(1, 3, 3)
    plt.hist(spike_probs, bins=30, color='green', alpha=0.7, edgecolor='black')
    plt.axvline(x=0.5, color='red', linestyle='--', label='ì„ê³„ê°’')
    plt.title('ê¸‰ë³€ ê°ì§€ í™•ë¥  ë¶„í¬')
    plt.xlabel('í™•ë¥ ')
    plt.ylabel('ë¹ˆë„')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('error_distribution_v5.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ===================================
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    # ===================================
    
    logger.info("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
    
    os.makedirs('models_v5', exist_ok=True)
    
    # ê° ëª¨ë¸ ì €ì¥
    for name, model in models.items():
        path = f'models_v5/{name}_final.h5'
        model.save(path)
        logger.info(f"   âœ… {name}: {path}")
    
    # ê¸‰ë³€ ê°ì§€ê¸° ì €ì¥
    spike_model.save('models_v5/spike_detector_final.h5')
    logger.info(f"   âœ… spike_detector: models_v5/spike_detector_final.h5")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    joblib.dump(scaler, 'models_v5/scaler.pkl')
    logger.info(f"   âœ… scaler: models_v5/scaler.pkl")
    
    # ì„¤ì • ì •ë³´ ì €ì¥
    config = {
        'version': 'v5.0',
        'features': feature_cols,
        'seq_length': 50,
        'models': list(models.keys()),
        'performance': {
            'mae': float(mae),
            'rmse': float(rmse),
            'r2': float(r2),
            'mae_1400+': float(mae_high) if high_mask.sum() > 0 else None,
            'spike_accuracy': float(spike_acc)
        },
        'timestamp': datetime.now().isoformat()
    }
    
    with open('models_v5/config.json', 'w') as f:
        json.dump(config, f, indent=4)
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ‰ ì™„ë£Œ!")
    logger.info(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥:")
    logger.info(f"   ì „ì²´ MAE: {mae:.2f}")
    logger.info(f"   1400+ MAE: {mae_high:.2f}" if high_mask.sum() > 0 else "   1400+ ë°ì´í„° ì—†ìŒ")
    logger.info("="*60)

# ===================================
# 11. ì‹¤í–‰ ì§„ì…ì 
# ===================================

if __name__ == "__main__":
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì„œ ì„¤ì •
    parser = argparse.ArgumentParser(description='ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ v5.0 - 1400+ ê°•í™”')
    parser.add_argument('--resume', action='store_true', help='ì´ì „ í•™ìŠµ ì¬ê°œ')
    parser.add_argument('--reset', action='store_true', help='ì´ˆê¸°í™” í›„ ì‹œì‘')
    
    args = parser.parse_args()
    
    try:
        # ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
        main(resume=args.resume, reset=args.reset)
    except Exception as e:
        # ì¹˜ëª…ì  ì˜¤ë¥˜ ì²˜ë¦¬
        logger.error(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")
        logger.error(traceback.format_exc())
        print("\nâŒ ì˜¤ë¥˜ ë°œìƒ! --resume ì˜µì…˜ìœ¼ë¡œ ì¬ì‹œì‘ ê°€ëŠ¥")