"""
ExtremeNet V6.5 - ÏÉà Îç∞Ïù¥ÌÑ∞ ÌèâÍ∞Ä Î∞è ÏòàÏ∏° Í≤∞Í≥º Ï†ÄÏû•
==================================================
data/20250732_to20250806.csv Îç∞Ïù¥ÌÑ∞ ÌèâÍ∞Ä
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
import os
import pickle
from datetime import datetime, timedelta

warnings.filterwarnings('ignore')

print("="*80)
print("üîÆ ExtremeNet V6.5 - ÏÉà Îç∞Ïù¥ÌÑ∞ ÌèâÍ∞Ä")
print("üìä ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞: data/20250732_to20250806.csv")
print("="*80)

# ========================================
# Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÌÅ¥ÎûòÏä§
# ========================================
class DataEvaluator:
    def __init__(self, model_path='models_final/extremenet_v65_final.keras',
                 scaler_path='scalers_final/'):
        """ÌèâÍ∞ÄÍ∏∞ Ï¥àÍ∏∞Ìôî"""
        
        # Î™®Îç∏ Î°úÎìú
        print("\nüì¶ Î™®Îç∏ Î°úÎî© Ï§ë...")
        try:
            # custom_objects Ï∂îÍ∞Ä
            custom_objects = {
                'weighted_mae_signal': self.weighted_mae_signal,
                'weighted_mae': self.weighted_mae
            }
            self.model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)
            print(f"‚úÖ Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ: {model_path}")
        except Exception as e:
            print(f"‚ùå Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}")
            # ÎåÄÏ≤¥ Í≤ΩÎ°ú ÏãúÎèÑ
            alt_paths = ['models/extremenet_v65_fixed_final.keras', 
                        'models/best_extremenet_fixed.keras']
            for alt_path in alt_paths:
                if os.path.exists(alt_path):
                    try:
                        self.model = tf.keras.models.load_model(alt_path, custom_objects=custom_objects)
                        print(f"‚úÖ ÎåÄÏ≤¥ Î™®Îç∏ Î°úÎìú: {alt_path}")
                        break
                    except:
                        continue
        
        # Ïä§ÏºÄÏùºÎü¨ Î°úÎìú
        print("üì¶ Ïä§ÏºÄÏùºÎü¨ Î°úÎî© Ï§ë...")
        try:
            with open(f'{scaler_path}scaler_X.pkl', 'rb') as f:
                self.scaler_X = pickle.load(f)
            with open(f'{scaler_path}scaler_y.pkl', 'rb') as f:
                self.scaler_y = pickle.load(f)
            print("‚úÖ Ïä§ÏºÄÏùºÎü¨ Î°úÎìú ÏÑ±Í≥µ")
        except:
            # ÎåÄÏ≤¥ Í≤ΩÎ°ú
            try:
                with open('scalers/scaler_X.pkl', 'rb') as f:
                    self.scaler_X = pickle.load(f)
                with open('scalers/scaler_y.pkl', 'rb') as f:
                    self.scaler_y = pickle.load(f)
                print("‚úÖ ÎåÄÏ≤¥ Ïä§ÏºÄÏùºÎü¨ Î°úÎìú")
            except:
                print("‚ö†Ô∏è Ïä§ÏºÄÏùºÎü¨ ÏóÜÏùå, ÏÉàÎ°ú ÏÉùÏÑ±")
                self.scaler_X = StandardScaler()
                self.scaler_y = MinMaxScaler()
        
        # ÌäπÏÑ± Ïª¨Îüº Ï†ïÏùò
        self.feature_columns = [
            'TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M14AM10A', 'M14AM16',
            'RATIO', 'GOLDEN', 'SIGNAL_ZONE', 'PRE_SIGNAL', 'DANGER_ZONE',
            'PATTERN', 'TREND',
            'MA_5', 'MA_10', 'MA_20',
            'STD_5', 'STD_10', 'STD_20',
            'M14B_MA_5', 'M14B_MA_10', 'M14B_MA_20',
            'CHANGE_1', 'CHANGE_5', 'CHANGE_10',
            'M14B_CHANGE_1', 'M14B_CHANGE_5', 'M14B_CHANGE_10',
            'RISE_COUNT'
        ]
        
        self.seq_len = 100
        self.pred_len = 10
        
    def weighted_mae_signal(self, y_true, y_pred):
        """Ïª§Ïä§ÌÖÄ ÏÜêÏã§Ìï®Ïàò"""
        weights = tf.ones_like(y_true)
        signal_mask = tf.logical_and(y_true >= 0.65, y_true <= 0.75)
        weights = tf.where(signal_mask, 5.0, weights)
        danger_mask = y_true > 0.75
        weights = tf.where(danger_mask, 10.0, weights)
        mae = tf.abs(y_true - y_pred)
        weighted_mae = mae * weights
        return tf.reduce_mean(weighted_mae)
    
    def weighted_mae(self, y_true, y_pred):
        """Ïª§Ïä§ÌÖÄ ÏÜêÏã§Ìï®Ïàò (ÎåÄÏ≤¥)"""
        return self.weighted_mae_signal(y_true, y_pred)
        
    def load_and_process_data(self, filepath):
        """Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨"""
        print(f"\nüìÇ Îç∞Ïù¥ÌÑ∞ Î°úÎî©: {filepath}")
        
        # CSV Î°úÎìú
        df = pd.read_csv(filepath)
        print(f"‚úÖ ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞: {len(df):,}Ìñâ")
        
        # ÏãúÍ∞Ñ Ï†ïÎ†¨
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                       format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        # 0Í∞í Ï†úÍ±∞
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        print(f"‚úÖ Ïú†Ìö® Îç∞Ïù¥ÌÑ∞: {len(df):,}Ìñâ")
        
        # Íµ¨Í∞Ñ ÌÜµÍ≥Ñ
        normal = (df['TOTALCNT'] < 1400).sum()
        caution = ((df['TOTALCNT'] >= 1400) & (df['TOTALCNT'] <= 1650)).sum()
        signal = ((df['TOTALCNT'] >= 1651) & (df['TOTALCNT'] <= 1699)).sum()
        danger = (df['TOTALCNT'] >= 1700).sum()
        
        print(f"\nüìä Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨:")
        print(f"  Ï†ïÏÉÅ(~1400): {normal:,}Í∞ú ({normal/len(df)*100:.1f}%)")
        print(f"  Ï£ºÏùò(1400~1650): {caution:,}Í∞ú ({caution/len(df)*100:.1f}%)")
        print(f"  Ïã†Ìò∏(1651~1699): {signal:,}Í∞ú ({signal/len(df)*100:.1f}%)")
        print(f"  ÏúÑÌóò(1700+): {danger:,}Í∞ú ({danger/len(df)*100:.1f}%)")
        
        return df
    
    def create_features(self, df):
        """ÌäπÏÑ± ÏÉùÏÑ±"""
        print("\n‚öôÔ∏è ÌäπÏÑ± ÏÉùÏÑ± Ï§ë...")
        
        # Í∏∞Î≥∏ ÌäπÏÑ±
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(int)
        
        # Íµ¨Í∞Ñ ÌäπÏÑ±
        df['SIGNAL_ZONE'] = ((df['TOTALCNT'] >= 1651) & (df['TOTALCNT'] <= 1699)).astype(int)
        df['PRE_SIGNAL'] = ((df['TOTALCNT'] >= 1600) & (df['TOTALCNT'] < 1651)).astype(int)
        df['DANGER_ZONE'] = (df['TOTALCNT'] >= 1700).astype(int)
        
        # Ïù¥ÎèôÌèâÍ∑†
        for w in [5, 10, 20]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
            df[f'M14B_MA_{w}'] = df['M14AM14B'].rolling(w, min_periods=1).mean()
        
        # Î≥ÄÌôîÏú®
        for lag in [1, 5, 10]:
            df[f'CHANGE_{lag}'] = df['TOTALCNT'].diff(lag).fillna(0)
            df[f'M14B_CHANGE_{lag}'] = df['M14AM14B'].diff(lag).fillna(0)
        
        # Ïó∞ÏÜç ÏÉÅÏäπ
        df['RISE'] = (df['TOTALCNT'] > df['TOTALCNT'].shift(1)).astype(int)
        df['RISE_COUNT'] = df['RISE'].rolling(10, min_periods=1).sum()
        
        # Ìå®ÌÑ¥
        df['PATTERN'] = 0
        signal_mask = ((df['TOTALCNT'] >= 1651) & (df['TOTALCNT'] <= 1699))
        high_m14b = df['M14AM14B'].rolling(100, min_periods=1).mean() > 380
        df.loc[signal_mask, 'PATTERN'] = 1
        df.loc[high_m14b, 'PATTERN'] = 2
        
        # Ï∂îÏÑ∏
        df['TREND'] = 1  # Í∏∞Î≥∏Í∞í Î≥¥Ìï©
        for i in range(20, len(df)):
            recent = df['TOTALCNT'].iloc[i-20:i].values
            if len(recent) > 1:
                slope = np.polyfit(range(len(recent)), recent, 1)[0]
                if slope > 5:
                    df.loc[i, 'TREND'] = 2  # ÏÉÅÏäπ
                elif slope < -5:
                    df.loc[i, 'TREND'] = 0  # ÌïòÎùΩ
        
        print(f"‚úÖ ÌäπÏÑ± ÏÉùÏÑ± ÏôÑÎ£å: {len(self.feature_columns)}Í∞ú")
        return df
    
    def predict_sequences(self, df):
        """ÏãúÌÄÄÏä§ Îã®ÏúÑÎ°ú ÏòàÏ∏°"""
        print("\nüîÆ ÏòàÏ∏° ÏãúÏûë...")
        
        results = []
        
        # 100Î∂Ñ ÏãúÌÄÄÏä§Î°ú 10Î∂Ñ ÌõÑ ÏòàÏ∏°
        for i in range(len(df) - self.seq_len - self.pred_len + 1):
            # 100Î∂Ñ Îç∞Ïù¥ÌÑ∞
            seq_data = df.iloc[i:i+self.seq_len]
            
            # ÌòÑÏû¨ ÏãúÍ∞Ñ
            current_time = seq_data['CURRTIME'].iloc[-1]
            
            # 10Î∂Ñ ÌõÑ ÏãúÍ∞Ñ
            target_time = current_time + timedelta(minutes=10)
            
            # Ïã§Ï†úÍ∞í (10Î∂Ñ ÌõÑ)
            if i+self.seq_len+self.pred_len-1 < len(df):
                actual_value = df['TOTALCNT'].iloc[i+self.seq_len+self.pred_len-1]
            else:
                actual_value = np.nan
            
            # ÌäπÏÑ± Ï∂îÏ∂ú
            X = seq_data[self.feature_columns].values
            
            # Ïä§ÏºÄÏùºÎßÅ
            try:
                X_scaled = self.scaler_X.transform(X.reshape(-1, len(self.feature_columns)))
                X_scaled = X_scaled.reshape(1, self.seq_len, len(self.feature_columns))
            except:
                # Ïä§ÏºÄÏùºÎü¨Í∞Ä ÌïôÏäµÎêòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞
                X_scaled = X.reshape(1, self.seq_len, len(self.feature_columns))
            
            # ÏòàÏ∏°
            preds = self.model.predict(X_scaled, verbose=0)
            y_pred_scaled = preds[0][0, 0]
            
            # Ïó≠Ïä§ÏºÄÏùºÎßÅ
            try:
                y_pred = self.scaler_y.inverse_transform([[y_pred_scaled]])[0, 0]
            except:
                # Ïä§ÏºÄÏùºÎü¨Í∞Ä ÏóÜÎäî Í≤ΩÏö∞ ÎåÄÎûµÏ†Å Î≥ÄÌôò
                y_pred = y_pred_scaled * 900 + 900
            
            # ÌòÑÏû¨Í∞í
            current_value = seq_data['TOTALCNT'].iloc[-1]
            
            # ÏùºÎ≥Ñ ÏµúÎåÄÍ∞í Í≥ÑÏÇ∞ (ÎãπÏùº ÏµúÎåÄÍ∞í)
            current_date = current_time.date()
            day_data = df[df['CURRTIME'].dt.date == current_date]
            if len(day_data) > 0:
                daily_max = day_data['TOTALCNT'].max()
            else:
                daily_max = current_value
            
            # Í≤∞Í≥º Ï†ÄÏû•
            results.append({
                'DateTime': current_time.strftime('%Y-%m-%d %H:%M'),
                'TargetDateTime': target_time.strftime('%Y-%m-%d %H:%M'),
                'CurrentValue': current_value,
                'ActualValue': actual_value,
                'PredictedValue': y_pred,
                'DailyMax': daily_max,
                'Error': abs(actual_value - y_pred) if not np.isnan(actual_value) else np.nan
            })
            
            # ÏßÑÌñâ ÏÉÅÌô© Ï∂úÎ†•
            if (i+1) % 100 == 0:
                print(f"  ÏßÑÌñâ: {i+1}/{len(df)-self.seq_len-self.pred_len+1}")
        
        print(f"‚úÖ ÏòàÏ∏° ÏôÑÎ£å: {len(results)}Í∞ú")
        return pd.DataFrame(results)
    
    def evaluate_results(self, results_df):
        """ÌèâÍ∞Ä Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞"""
        print("\nüìä ÌèâÍ∞Ä Î©îÌä∏Î¶≠:")
        
        # NaN Ï†úÍ±∞
        valid_results = results_df.dropna(subset=['ActualValue', 'PredictedValue'])
        
        if len(valid_results) > 0:
            mae = mean_absolute_error(valid_results['ActualValue'], 
                                     valid_results['PredictedValue'])
            rmse = np.sqrt(mean_squared_error(valid_results['ActualValue'], 
                                             valid_results['PredictedValue']))
            r2 = r2_score(valid_results['ActualValue'], 
                         valid_results['PredictedValue'])
            
            print(f"  MAE: {mae:.2f}")
            print(f"  RMSE: {rmse:.2f}")
            print(f"  R¬≤: {r2:.4f}")
            
            # Íµ¨Í∞ÑÎ≥Ñ ÌèâÍ∞Ä
            signal_mask = (valid_results['ActualValue'] >= 1651) & \
                         (valid_results['ActualValue'] <= 1699)
            danger_mask = valid_results['ActualValue'] >= 1700
            
            if signal_mask.sum() > 0:
                signal_mae = mean_absolute_error(
                    valid_results[signal_mask]['ActualValue'],
                    valid_results[signal_mask]['PredictedValue']
                )
                print(f"\nüü† Ïã†Ìò∏Íµ¨Í∞Ñ(1651~1699) MAE: {signal_mae:.2f}")
            
            if danger_mask.sum() > 0:
                danger_mae = mean_absolute_error(
                    valid_results[danger_mask]['ActualValue'],
                    valid_results[danger_mask]['PredictedValue']
                )
                print(f"üî¥ ÏúÑÌóòÍµ¨Í∞Ñ(1700+) MAE: {danger_mae:.2f}")
        else:
            print("‚ö†Ô∏è Ïú†Ìö®Ìïú ÏòàÏ∏° Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§.")

# ========================================
# Î©îÏù∏ Ïã§Ìñâ
# ========================================
def main():
    # ÌèâÍ∞ÄÍ∏∞ ÏÉùÏÑ±
    evaluator = DataEvaluator()
    
    # Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú
    data_path = 'data/20250732_to20250806.csv'
    
    # ÎåÄÏ≤¥ Í≤ΩÎ°ú
    if not os.path.exists(data_path):
        alt_paths = ['20250732_to20250806.csv', 
                    'data/20250807_DATA.CSV',
                    '20250807_DATA.CSV']
        for alt in alt_paths:
            if os.path.exists(alt):
                data_path = alt
                print(f"üìå ÎåÄÏ≤¥ Í≤ΩÎ°ú ÏÇ¨Ïö©: {data_path}")
                break
    
    if not os.path.exists(data_path):
        # ÌÖåÏä§Ìä∏Ïö© ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
        print("‚ö†Ô∏è Îç∞Ïù¥ÌÑ∞ ÌååÏùºÏù¥ ÏóÜÏñ¥ÏÑú ÏÉòÌîå Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.")
        
        # Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏùºÎ∂Ä Ï∂îÏ∂ú
        original_path = 'data/20240201_TO_202507281705.csv'
        if not os.path.exists(original_path):
            original_path = '20240201_TO_202507281705.csv'
        
        if os.path.exists(original_path):
            df_original = pd.read_csv(original_path)
            # ÎßàÏßÄÎßâ 1000Í∞ú ÌñâÏùÑ ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Î°ú ÏÇ¨Ïö©
            df_test = df_original.tail(1000)
            df_test.to_csv('test_data.csv', index=False)
            data_path = 'test_data.csv'
            print(f"‚úÖ ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±: {data_path}")
    
    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï≤òÎ¶¨
    df = evaluator.load_and_process_data(data_path)
    df = evaluator.create_features(df)
    
    # ÏòàÏ∏° ÏàòÌñâ
    results_df = evaluator.predict_sequences(df)
    
    # ÌèâÍ∞Ä
    evaluator.evaluate_results(results_df)
    
    # Í≤∞Í≥º Ï†ÄÏû•
    output_file = 'prediction_results.csv'
    
    # Ï†ÄÏû•Ìï† Ïª¨Îüº ÏÑ†ÌÉù
    save_columns = ['DateTime', 'TargetDateTime', 'DailyMax', 'PredictedValue']
    if 'ActualValue' in results_df.columns:
        # Ïã§Ï†úÍ∞íÏù¥ ÏûàÏúºÎ©¥ Ìè¨Ìï®
        save_columns = ['DateTime', 'TargetDateTime', 'DailyMax', 'ActualValue', 'PredictedValue', 'Error']
    
    results_df[save_columns].to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nüíæ Í≤∞Í≥º Ï†ÄÏû•: {output_file}")
    
    # ÏÉòÌîå Ï∂úÎ†•
    print("\nüìã ÏòàÏ∏° Í≤∞Í≥º ÏÉòÌîå (Ï≤òÏùå 10Í∞ú):")
    print(results_df[save_columns].head(10))
    
    # ÏöîÏïΩ ÌÜµÍ≥Ñ
    print("\nüìä ÏòàÏ∏°Í∞í ÏöîÏïΩ:")
    print(f"  ÌèâÍ∑†: {results_df['PredictedValue'].mean():.2f}")
    print(f"  ÌëúÏ§ÄÌé∏Ï∞®: {results_df['PredictedValue'].std():.2f}")
    print(f"  ÏµúÏÜå: {results_df['PredictedValue'].min():.2f}")
    print(f"  ÏµúÎåÄ: {results_df['PredictedValue'].max():.2f}")
    
    if 'Error' in results_df.columns:
        valid_errors = results_df['Error'].dropna()
        if len(valid_errors) > 0:
            print(f"\nüìä Ïò§Ï∞® Î∂ÑÌè¨:")
            print(f"  ÌèâÍ∑† Ïò§Ï∞®: {valid_errors.mean():.2f}")
            print(f"  Ïò§Ï∞® < 50: {(valid_errors < 50).sum()/len(valid_errors)*100:.1f}%")
            print(f"  Ïò§Ï∞® < 100: {(valid_errors < 100).sum()/len(valid_errors)*100:.1f}%")
    
    print("\n‚úÖ ÌèâÍ∞Ä ÏôÑÎ£å!")
    return results_df

if __name__ == "__main__":
    results = main()