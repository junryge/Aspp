# -*- coding: utf-8 -*-
"""
4ê°œ ëª¨ë¸ë§Œ ì‚¬ìš©í•˜ì—¬ í‰ê°€ (1651-1682 êµ¬ê°„ 4% ìƒí–¥ ë³´ì •)
RandomForest, ExtraTrees, GradientBoosting, MLP
"""

import pandas as pd
import numpy as np
import joblib
import os
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("4ê°œ ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ (1651-1682 êµ¬ê°„ ë³´ì • ì ìš©)")
print("="*80)

# ì„¤ì •
model_dir = 'regression_classification_models_280to10'
data_file = 'data/20250731_to_20250806.csv'
output_file = 'evaluation_results_4models_adjusted.csv'

# ì‚¬ìš©í•  ëª¨ë¸ (4ê°œë§Œ)
MODEL_NAMES = ['RandomForest', 'ExtraTrees', 'GradientBoosting', 'MLP']

# ì´ìƒì‹ í˜¸ êµ¬ê°„ ë³´ì • í•¨ìˆ˜
def adjust_anomaly_prediction(prediction, current_totalcnt):
    """
    1651-1682 êµ¬ê°„ ê·¼ì²˜ì—ì„œ ì˜ˆì¸¡ê°’ì„ 4% ìƒí–¥ ì¡°ì •
    """
    # í˜„ì¬ê°’ì´ 1600-1700 ë²”ìœ„ì´ê³  ì˜ˆì¸¡ê°’ì´ 1500-1700 ë²”ìœ„ì¼ ë•Œ
    if 1600 <= current_totalcnt <= 1700 and 1500 <= prediction <= 1700:
        # 4% ìƒí–¥ ì¡°ì •
        adjusted = prediction * 1.04
        
        # íŠ¹íˆ 1651-1682 êµ¬ê°„ìœ¼ë¡œ ì˜ˆì¸¡ë  ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë©´ ë” ë³´ì •
        if 1640 <= prediction <= 1690:
            adjusted = prediction * 1.045  # 4.5% ìƒí–¥
            
        return adjusted
    
    # ì˜ˆì¸¡ê°’ì´ ì´ë¯¸ 1651-1682 êµ¬ê°„ì´ë©´ ì•½ê°„ë§Œ ìƒí–¥
    elif 1651 <= prediction <= 1682:
        return prediction * 1.02  # 2% ì¶”ê°€ ìƒí–¥
    
    return prediction

# ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜
def create_sequences_for_evaluation(data, seq_length=280, pred_horizon=10):
    """í‰ê°€ìš© 280ë¶„ ì‹œí€€ìŠ¤ ìƒì„±"""
    
    print(f"ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
    
    feature_cols = ['M14AM14B', 'M14AM10A', 'M14AM16', 'M14AM14BSUM', 
                   'M14AM10ASUM', 'M14AM16SUM', 'M14BM14A', 'M10AM14A', 'M16M14A', 'TOTALCNT']
    
    # íŒŒìƒ ë³€ìˆ˜ ìƒì„±
    data = data.copy()
    data['ratio_M14B_M14A'] = np.clip(data['M14AM14B'] / (data['M14AM10A'] + 1), 0, 1000)
    data['ratio_M14B_M16'] = np.clip(data['M14AM14B'] / (data['M14AM16'] + 1), 0, 1000)
    data['totalcnt_change'] = data['TOTALCNT'].diff().fillna(0)
    data['totalcnt_pct_change'] = np.clip(data['TOTALCNT'].pct_change().fillna(0), -10, 10)
    
    data = data.replace([np.inf, -np.inf], 0).fillna(0)
    
    X_list = []
    y_reg_list = []
    time_info_list = []
    
    n_sequences = len(data) - seq_length - pred_horizon + 1
    
    for i in range(n_sequences):
        if i % 500 == 0:
            print(f"  {i}/{n_sequences}", end='\r')
        
        start_idx = i
        end_idx = i + seq_length
        seq_data = data.iloc[start_idx:end_idx]
        
        features = []
        
        for col in feature_cols:
            values = seq_data[col].values
            
            # ê¸°ë³¸ í†µê³„
            features.extend([
                np.mean(values),
                np.std(values) if len(values) > 1 else 0,
                np.min(values),
                np.max(values),
                np.percentile(values, 25),
                np.percentile(values, 50),
                np.percentile(values, 75),
                values[-1],
                values[-1] - values[0],
                np.mean(values[-60:]),
                np.max(values[-60:]),
                np.mean(values[-30:]),
                np.max(values[-30:]),
            ])
            
            if col == 'TOTALCNT':
                # êµ¬ê°„ ê´€ë ¨ íŠ¹ì§•
                features.append(np.sum((values >= 1650) & (values < 1700)))
                features.append(np.sum(values >= 1700))
                features.append(np.max(values[-20:]))
                features.append(np.sum(values < 1400))
                features.append(np.sum((values >= 1400) & (values < 1700)))
                features.append(np.sum(values >= 1700))
                features.append(np.sum((values >= 1651) & (values <= 1682)))
                
                anomaly_values = values[(values >= 1651) & (values <= 1682)]
                features.append(np.max(anomaly_values) if len(anomaly_values) > 0 else 0)
                
                normal_vals = values[values < 1400]
                check_vals = values[(values >= 1400) & (values < 1700)]
                danger_vals = values[values >= 1700]
                
                features.append(np.mean(normal_vals) if len(normal_vals) > 0 else 0)
                features.append(np.mean(check_vals) if len(check_vals) > 0 else 0)
                features.append(np.mean(danger_vals) if len(danger_vals) > 0 else 0)
                
                try:
                    x = np.arange(len(values))
                    slope, _ = np.polyfit(x, values, 1)
                    features.append(np.clip(slope, -100, 100))
                except:
                    features.append(0)
                
                try:
                    recent_slope = np.polyfit(np.arange(60), values[-60:], 1)[0]
                    features.append(np.clip(recent_slope, -100, 100))
                except:
                    features.append(0)
        
        last_idx = end_idx - 1
        features.extend([
            np.clip(data['ratio_M14B_M14A'].iloc[last_idx], 0, 1000),
            np.clip(data['ratio_M14B_M16'].iloc[last_idx], 0, 1000),
            np.clip(data['totalcnt_change'].iloc[last_idx], -1000, 1000),
            np.clip(data['totalcnt_pct_change'].iloc[last_idx], -10, 10),
        ])
        
        target_idx = end_idx + pred_horizon - 1
        if target_idx < len(data):
            time_info = {
                'current_idx': end_idx - 1,
                'target_idx': target_idx,
                'current_totalcnt': data['TOTALCNT'].iloc[end_idx - 1],  # í˜„ì¬ê°’ ì¶”ê°€
                'seq_max': np.max(seq_data['TOTALCNT'].values),
                'seq_min': np.min(seq_data['TOTALCNT'].values),
            }
            
            X_list.append(features)
            y_reg_list.append(data['TOTALCNT'].iloc[target_idx])
            time_info_list.append(time_info)
    
    X = np.nan_to_num(np.array(X_list), nan=0.0, posinf=1000.0, neginf=-1000.0)
    
    print(f"\nâœ“ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {len(X)}ê°œ")
    return X, np.array(y_reg_list), time_info_list

# ë©”ì¸ ì‹¤í–‰
print(f"\n1. ë°ì´í„° ë¡œë”©: {data_file}")
df = pd.read_csv(data_file)
print(f"   ë°ì´í„° í¬ê¸°: {len(df):,}í–‰")

# DateTime ìƒì„±
base_date = datetime(2025, 7, 31, 0, 0)
df['DateTime'] = [base_date + timedelta(minutes=i) for i in range(len(df))]

print(f"\n2. ì‹œí€€ìŠ¤ ìƒì„± (280ë¶„ â†’ 10ë¶„ í›„)")
X, y_true, time_info = create_sequences_for_evaluation(df, seq_length=280, pred_horizon=10)

print(f"\n3. ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”©")
scaler = joblib.load(os.path.join(model_dir, 'scaler.pkl'))
X_scaled = scaler.transform(X)

print(f"\n4. 4ê°œ ëª¨ë¸ ì˜ˆì¸¡ (1651-1682 êµ¬ê°„ ë³´ì • ì ìš©)")
print("-"*60)
predictions = {}
adjusted_predictions = {}

for name in MODEL_NAMES:
    print(f"   {name} ì˜ˆì¸¡...", end='')
    model = joblib.load(os.path.join(model_dir, f'{name}_regression_model.pkl'))
    
    # ê¸°ë³¸ ì˜ˆì¸¡
    raw_pred = model.predict(X_scaled)
    
    # 1651-1682 êµ¬ê°„ ë³´ì • ì ìš©
    adjusted_pred = []
    for i in range(len(raw_pred)):
        current_val = time_info[i]['current_totalcnt']
        adj_val = adjust_anomaly_prediction(raw_pred[i], current_val)
        adjusted_pred.append(adj_val)
    
    adjusted_pred = np.array(adjusted_pred)
    
    predictions[name] = raw_pred
    adjusted_predictions[name] = adjusted_pred
    
    # ì„±ëŠ¥ ë¹„êµ
    mae_raw = np.mean(np.abs(raw_pred - y_true))
    mae_adj = np.mean(np.abs(adjusted_pred - y_true))
    
    print(f" MAE: {mae_raw:.2f} â†’ {mae_adj:.2f} (ë³´ì • í›„)")
    
    # 1651-1682 êµ¬ê°„ ì„±ëŠ¥ í™•ì¸
    anomaly_mask = (y_true >= 1651) & (y_true <= 1682)
    if np.sum(anomaly_mask) > 0:
        anomaly_mae_raw = np.mean(np.abs(raw_pred[anomaly_mask] - y_true[anomaly_mask]))
        anomaly_mae_adj = np.mean(np.abs(adjusted_pred[anomaly_mask] - y_true[anomaly_mask]))
        print(f"      1651-1682 êµ¬ê°„ MAE: {anomaly_mae_raw:.2f} â†’ {anomaly_mae_adj:.2f}")

print("-"*60)

print(f"\n5. ê²°ê³¼ CSV ìƒì„±")
results = []

for i in range(len(X)):
    current_time = df['DateTime'].iloc[time_info[i]['current_idx']]
    target_time = df['DateTime'].iloc[time_info[i]['target_idx']]
    
    row = {
        'ë‚ ì§œ': current_time.strftime('%Y-%m-%d %H:%M'),
        'íƒ€ì¼“ë‚ ì§œ': target_time.strftime('%Y-%m-%d %H:%M'),
        'ì‹¤ì œê°’': round(y_true[i], 0),
        'ì‹œí€€ìŠ¤MAX': round(time_info[i]['seq_max'], 0),
        'ì‹œí€€ìŠ¤MIN': round(time_info[i]['seq_min'], 0),
    }
    
    # 4ê°œ ëª¨ë¸ ì˜ˆì¸¡ê°’ (ë³´ì • ì ìš©)
    for name in MODEL_NAMES:
        row[f'{name}_ì˜ˆì¸¡'] = round(adjusted_predictions[name][i], 0)
        row[f'{name}_ì˜¤ì°¨'] = round(adjusted_predictions[name][i] - y_true[i], 0)
        row[f'{name}_ì˜¤ì°¨ìœ¨(%)'] = round(abs(adjusted_predictions[name][i] - y_true[i]) / y_true[i] * 100, 2)
    
    results.append(row)

results_df = pd.DataFrame(results)
results_df.to_csv(output_file, index=False, encoding='utf-8-sig')

print(f"\nâœ… ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {output_file}")
print(f"   ì´ {len(results_df):,}ê°œ ì˜ˆì¸¡ ìƒì„±")

# ì„±ëŠ¥ ìš”ì•½
print(f"\n[ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½ - ë³´ì • ì ìš©]")
print("="*80)
print(f"{'ëª¨ë¸':20} {'MAE':>10} {'RMSE':>10} {'1700+ì •í™•ë„':>12} {'1651-1682ì •í™•ë„':>15}")
print("-"*80)

for name in MODEL_NAMES:
    errors = adjusted_predictions[name] - y_true
    mae = np.mean(np.abs(errors))
    rmse = np.sqrt(np.mean(errors**2))
    
    # 1700+ ì˜ˆì¸¡ ì •í™•ë„
    danger_mask = y_true >= 1700
    if np.sum(danger_mask) > 0:
        danger_acc = np.mean(np.abs(errors[danger_mask]) < 50) * 100  # ì˜¤ì°¨ 50 ì´ë‚´ë¥¼ ì •í™•ìœ¼ë¡œ ë´„
    else:
        danger_acc = 0
    
    # 1651-1682 êµ¬ê°„ ì •í™•ë„
    anomaly_mask = (y_true >= 1651) & (y_true <= 1682)
    if np.sum(anomaly_mask) > 0:
        anomaly_acc = np.mean(np.abs(errors[anomaly_mask]) < 30) * 100  # ì˜¤ì°¨ 30 ì´ë‚´ë¥¼ ì •í™•ìœ¼ë¡œ ë´„
    else:
        anomaly_acc = 0
    
    print(f"{name:20} {mae:10.2f} {rmse:10.2f} {danger_acc:11.1f}% {anomaly_acc:14.1f}%")

print("="*80)

# 1651-1682 êµ¬ê°„ ìƒì„¸ ë¶„ì„
print(f"\n[1651-1682 ì´ìƒì‹ í˜¸ êµ¬ê°„ ìƒì„¸ ë¶„ì„]")
print("-"*80)
anomaly_mask = (y_true >= 1651) & (y_true <= 1682)
anomaly_count = np.sum(anomaly_mask)

if anomaly_count > 0:
    print(f"ì´ìƒì‹ í˜¸ êµ¬ê°„ ìƒ˜í”Œ ìˆ˜: {anomaly_count}ê°œ")
    print(f"\nëª¨ë¸ë³„ ì´ìƒì‹ í˜¸ êµ¬ê°„ ì˜ˆì¸¡ ì„±ëŠ¥:")
    
    for name in MODEL_NAMES:
        anomaly_pred = adjusted_predictions[name][anomaly_mask]
        anomaly_true = y_true[anomaly_mask]
        
        mae = np.mean(np.abs(anomaly_pred - anomaly_true))
        rmse = np.sqrt(np.mean((anomaly_pred - anomaly_true)**2))
        
        # ì˜ˆì¸¡ì´ 1651-1682 êµ¬ê°„ì— ë“¤ì–´ê°„ ë¹„ìœ¨
        correct_range = np.sum((anomaly_pred >= 1651) & (anomaly_pred <= 1682))
        range_accuracy = correct_range / anomaly_count * 100
        
        print(f"\n{name}:")
        print(f"  - MAE: {mae:.2f}")
        print(f"  - RMSE: {rmse:.2f}")
        print(f"  - êµ¬ê°„ ë‚´ ì˜ˆì¸¡ ë¹„ìœ¨: {range_accuracy:.1f}%")
        
        # ìƒ˜í”Œ ì¶œë ¥ (ì²˜ìŒ 5ê°œ)
        if anomaly_count > 0:
            print(f"  - ì˜ˆì¸¡ ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):")
            for j in range(min(5, anomaly_count)):
                actual = anomaly_true[j]
                pred = anomaly_pred[j]
                error = pred - actual
                print(f"    ì‹¤ì œ: {actual:.0f} â†’ ì˜ˆì¸¡: {pred:.0f} (ì˜¤ì°¨: {error:+.0f})")
else:
    print("ì´ìƒì‹ í˜¸ êµ¬ê°„ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")

# ìµœì¢… ê¶Œì¥ì‚¬í•­
print(f"\n[ìµœì¢… ê¶Œì¥ì‚¬í•­]")
print("="*80)
print("âœ… 1651-1682 êµ¬ê°„ 4% ìƒí–¥ ë³´ì • ì ìš© ì™„ë£Œ")
print("âœ… 4ê°œ í•µì‹¬ ëª¨ë¸ë§Œ ì‚¬ìš© (RandomForest, ExtraTrees, GradientBoosting, MLP)")
print("âœ… Ridge, ElasticNet ì œì™¸")
print(f"\nğŸ“Š ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (ë³´ì • í›„ ê¸°ì¤€):")

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
best_mae = float('inf')
best_model = ""
for name in MODEL_NAMES:
    mae = np.mean(np.abs(adjusted_predictions[name] - y_true))
    if mae < best_mae:
        best_mae = mae
        best_model = name

print(f"   â†’ {best_model} (MAE: {best_mae:.2f})")
print("="*80)