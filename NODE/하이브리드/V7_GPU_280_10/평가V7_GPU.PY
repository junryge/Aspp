# -*- coding: utf-8 -*-
"""
ì¶”ì„¸ ê¸°ë°˜ ë™ì  ë³´ì • ì‹œìŠ¤í…œ
ì‹œí€€ìŠ¤ ë§ˆì§€ë§‰ 100ê°œ ë°ì´í„°ì˜ ì¶”ì„¸ì— ë”°ë¼ 3-5% ì°¨ë“± ë³´ì •
"""

import pandas as pd
import numpy as np
import joblib
import os
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("ì¶”ì„¸ ê¸°ë°˜ ë™ì  ë³´ì • ì‹œìŠ¤í…œ (ìƒìŠ¹ 5% / ë³´í•© 4% / í•˜ë½ 3%)")
print("="*80)

# ì„¤ì •
model_dir = 'regression_classification_models_280to10'
data_file = 'data/20250731_to_20250806.csv'
output_file = 'evaluation_trend_based.csv'

# ì‚¬ìš©í•  ëª¨ë¸ (4ê°œ)
MODEL_NAMES = ['RandomForest', 'ExtraTrees', 'GradientBoosting', 'MLP']

def calculate_trend(sequence_last_100):
    """
    ë§ˆì§€ë§‰ 100ê°œ ë°ì´í„°ì˜ ì¶”ì„¸ ê³„ì‚°
    ìƒìŠ¹(+1), ë³´í•©(0), í•˜ë½(-1) ë°˜í™˜
    """
    if len(sequence_last_100) < 100:
        sequence_last_100 = sequence_last_100[-min(len(sequence_last_100), 100):]
    
    # 1. ì„ í˜• íšŒê·€ë¡œ ì¶”ì„¸ ê³„ì‚°
    x = np.arange(len(sequence_last_100))
    slope, _ = np.polyfit(x, sequence_last_100, 1)
    
    # 2. ì´ë™í‰ê·  ë¹„êµ
    if len(sequence_last_100) >= 50:
        first_half_avg = np.mean(sequence_last_100[:50])
        second_half_avg = np.mean(sequence_last_100[50:])
        ma_trend = second_half_avg - first_half_avg
    else:
        ma_trend = 0
    
    # 3. ìµœê·¼ 20ê°œ vs ì´ì „ 20ê°œ ë¹„êµ
    if len(sequence_last_100) >= 40:
        recent_20_avg = np.mean(sequence_last_100[-20:])
        prev_20_avg = np.mean(sequence_last_100[-40:-20])
        short_trend = recent_20_avg - prev_20_avg
    else:
        short_trend = 0
    
    # ì¢…í•© íŒë‹¨
    # slopeê°€ ì–‘ìˆ˜ì´ê³  ì´ë™í‰ê· ë„ ìƒìŠ¹ â†’ ìƒìŠ¹ ì¶”ì„¸
    if slope > 0.5 and ma_trend > 10:
        return 1  # ìƒìŠ¹
    # slopeê°€ ìŒìˆ˜ì´ê³  ì´ë™í‰ê· ë„ í•˜ë½ â†’ í•˜ë½ ì¶”ì„¸
    elif slope < -0.5 and ma_trend < -10:
        return -1  # í•˜ë½
    # ê·¸ ì™¸ëŠ” ë³´í•©
    else:
        return 0  # ë³´í•©

def trend_based_correction(prediction, current_totalcnt, sequence_last_100, seq_max):
    """
    ì¶”ì„¸ ê¸°ë°˜ ë™ì  ë³´ì •
    """
    # ì¶”ì„¸ ê³„ì‚°
    trend = calculate_trend(sequence_last_100)
    
    # ì¶”ì„¸ì— ë”°ë¥¸ ë³´ì •ë¥  ê²°ì •
    if trend == 1:  # ìƒìŠ¹ ì¶”ì„¸
        base_correction = 1.05  # 5%
        trend_name = "ìƒìŠ¹"
    elif trend == -1:  # í•˜ë½ ì¶”ì„¸
        base_correction = 1.03  # 3%
        trend_name = "í•˜ë½"
    else:  # ë³´í•©
        base_correction = 1.04  # 4%
        trend_name = "ë³´í•©"
    
    # 1651-1682 êµ¬ê°„ (ê¸°ì¡´ ìš”êµ¬ì‚¬í•­)
    if 1651 <= current_totalcnt <= 1682:
        adjusted = prediction * base_correction
        return adjusted, trend_name
    
    # 1680-1750 êµ¬ê°„ (ê°•í™” ë³´ì •)
    if 1680 <= current_totalcnt <= 1750:
        if prediction < 1600:
            # ê·¹ì‹¬í•œ ê³¼ì†Œì˜ˆì¸¡ - ì¶”ì„¸ ê´€ê³„ì—†ì´ ê°•ë ¥ ë³´ì •
            adjusted = prediction * 1.15
            # ì¶”ì„¸ê°€ ìƒìŠ¹ì´ë©´ ì¶”ê°€ ë³´ì •
            if trend == 1:
                adjusted *= 1.03
        elif 1600 <= prediction < 1680:
            # ì¤‘ê°„ ê³¼ì†Œì˜ˆì¸¡ - ì¶”ì„¸ ê¸°ë°˜ ë³´ì • ê°•í™”
            if trend == 1:
                adjusted = prediction * 1.12  # ìƒìŠ¹ ì‹œ 12%
            elif trend == -1:
                adjusted = prediction * 1.08  # í•˜ë½ ì‹œ 8%
            else:
                adjusted = prediction * 1.10  # ë³´í•© ì‹œ 10%
        else:
            # ì ì • ì˜ˆì¸¡ - ê¸°ë³¸ ì¶”ì„¸ ë³´ì •
            adjusted = prediction * base_correction
        
        return adjusted, trend_name
    
    # 1700+ êµ¬ê°„
    if current_totalcnt >= 1700:
        if prediction < 1650:
            # ê³¼ì†Œì˜ˆì¸¡ - ì¶”ì„¸ ë°˜ì˜ ê°•í™”
            if trend == 1:
                adjusted = prediction * 1.13
            else:
                adjusted = prediction * 1.10
        elif prediction < 1700:
            # ì•½ê°„ ë‚®ì€ ì˜ˆì¸¡ - ì¶”ì„¸ ê¸°ë°˜
            adjusted = prediction * (base_correction + 0.03)
        else:
            # ì ì • ì˜ˆì¸¡ - ê°€ë²¼ìš´ ë³´ì •
            adjusted = prediction * base_correction
        
        return adjusted, trend_name
    
    # ì¼ë°˜ êµ¬ê°„
    if current_totalcnt >= 1600 and prediction < current_totalcnt * 0.95:
        # í˜„ì¬ê°’ë³´ë‹¤ 5% ì´ìƒ ë‚®ê²Œ ì˜ˆì¸¡ ì‹œ ì¶”ì„¸ ë³´ì •
        return prediction * base_correction, trend_name
    
    return prediction, "ì—†ìŒ"

def create_sequences_for_evaluation(data, seq_length=280, pred_horizon=10):
    """í‰ê°€ìš© 280ë¶„ ì‹œí€€ìŠ¤ ìƒì„± (ë§ˆì§€ë§‰ 100ê°œ ë°ì´í„° ì¶”ê°€)"""
    
    print(f"ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
    
    feature_cols = ['M14AM14B', 'M14AM10A', 'M14AM16', 'M14AM14BSUM', 
                   'M14AM10ASUM', 'M14AM16SUM', 'M14BM14A', 'M10AM14A', 'M16M14A', 'TOTALCNT']
    
    # íŒŒìƒ ë³€ìˆ˜ ìƒì„±
    data = data.copy()
    data['ratio_M14B_M14A'] = np.clip(data['M14AM14B'] / (data['M14AM10A'] + 1), 0, 1000)
    data['ratio_M14B_M16'] = np.clip(data['M14AM14B'] / (data['M14AM16'] + 1), 0, 1000)
    data['totalcnt_change'] = data['TOTALCNT'].diff().fillna(0)
    data['totalcnt_pct_change'] = np.clip(data['TOTALCNT'].pct_change().fillna(0), -10, 10)
    
    data = data.replace([np.inf, -np.inf], 0).fillna(0)
    
    X_list = []
    y_reg_list = []
    time_info_list = []
    
    n_sequences = len(data) - seq_length - pred_horizon + 1
    
    for i in range(n_sequences):
        if i % 500 == 0:
            print(f"  {i}/{n_sequences}", end='\r')
        
        start_idx = i
        end_idx = i + seq_length
        seq_data = data.iloc[start_idx:end_idx]
        
        features = []
        
        for col in feature_cols:
            values = seq_data[col].values
            
            # ê¸°ë³¸ í†µê³„
            features.extend([
                np.mean(values),
                np.std(values) if len(values) > 1 else 0,
                np.min(values),
                np.max(values),
                np.percentile(values, 25),
                np.percentile(values, 50),
                np.percentile(values, 75),
                values[-1],
                values[-1] - values[0],
                np.mean(values[-60:]),
                np.max(values[-60:]),
                np.mean(values[-30:]),
                np.max(values[-30:]),
            ])
            
            if col == 'TOTALCNT':
                # êµ¬ê°„ ê´€ë ¨ íŠ¹ì§•
                features.append(np.sum((values >= 1650) & (values < 1700)))
                features.append(np.sum(values >= 1700))
                features.append(np.max(values[-20:]))
                features.append(np.sum(values < 1400))
                features.append(np.sum((values >= 1400) & (values < 1700)))
                features.append(np.sum(values >= 1700))
                features.append(np.sum((values >= 1651) & (values <= 1682)))
                
                anomaly_values = values[(values >= 1651) & (values <= 1682)]
                features.append(np.max(anomaly_values) if len(anomaly_values) > 0 else 0)
                
                normal_vals = values[values < 1400]
                check_vals = values[(values >= 1400) & (values < 1700)]
                danger_vals = values[values >= 1700]
                
                features.append(np.mean(normal_vals) if len(normal_vals) > 0 else 0)
                features.append(np.mean(check_vals) if len(check_vals) > 0 else 0)
                features.append(np.mean(danger_vals) if len(danger_vals) > 0 else 0)
                
                try:
                    x = np.arange(len(values))
                    slope, _ = np.polyfit(x, values, 1)
                    features.append(np.clip(slope, -100, 100))
                except:
                    features.append(0)
                
                try:
                    recent_slope = np.polyfit(np.arange(60), values[-60:], 1)[0]
                    features.append(np.clip(recent_slope, -100, 100))
                except:
                    features.append(0)
        
        last_idx = end_idx - 1
        features.extend([
            np.clip(data['ratio_M14B_M14A'].iloc[last_idx], 0, 1000),
            np.clip(data['ratio_M14B_M16'].iloc[last_idx], 0, 1000),
            np.clip(data['totalcnt_change'].iloc[last_idx], -1000, 1000),
            np.clip(data['totalcnt_pct_change'].iloc[last_idx], -10, 10),
        ])
        
        target_idx = end_idx + pred_horizon - 1
        if target_idx < len(data):
            # ë§ˆì§€ë§‰ 100ê°œ ë°ì´í„° ì €ì¥
            last_100_data = seq_data['TOTALCNT'].values[-100:]
            
            time_info = {
                'current_idx': end_idx - 1,
                'target_idx': target_idx,
                'current_totalcnt': data['TOTALCNT'].iloc[end_idx - 1],
                'seq_max': np.max(seq_data['TOTALCNT'].values),
                'seq_min': np.min(seq_data['TOTALCNT'].values),
                'last_100': last_100_data,  # ì¶”ì„¸ ë¶„ì„ìš©
            }
            
            X_list.append(features)
            y_reg_list.append(data['TOTALCNT'].iloc[target_idx])
            time_info_list.append(time_info)
    
    X = np.nan_to_num(np.array(X_list), nan=0.0, posinf=1000.0, neginf=-1000.0)
    
    print(f"\nâœ“ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {len(X)}ê°œ")
    return X, np.array(y_reg_list), time_info_list

# ë©”ì¸ ì‹¤í–‰
print(f"\n1. ë°ì´í„° ë¡œë”©: {data_file}")
df = pd.read_csv(data_file)
print(f"   ë°ì´í„° í¬ê¸°: {len(df):,}í–‰")

# DateTime ìƒì„±
base_date = datetime(2025, 7, 31, 0, 0)
df['DateTime'] = [base_date + timedelta(minutes=i) for i in range(len(df))]

print(f"\n2. ì‹œí€€ìŠ¤ ìƒì„± (280ë¶„ â†’ 10ë¶„ í›„)")
X, y_true, time_info = create_sequences_for_evaluation(df)

print(f"\n3. ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”©")
scaler = joblib.load(os.path.join(model_dir, 'scaler.pkl'))

# íŠ¹ì§• ê°œìˆ˜ ë§ì¶”ê¸°
expected_features = scaler.n_features_in_
if X.shape[1] < expected_features:
    X = np.hstack([X, np.zeros((X.shape[0], expected_features - X.shape[1]))])
elif X.shape[1] > expected_features:
    X = X[:, :expected_features]

X_scaled = scaler.transform(X)

print(f"\n4. ì¶”ì„¸ ê¸°ë°˜ ë™ì  ë³´ì • ì˜ˆì¸¡")
print("-"*80)

predictions = {}
corrected_predictions = {}
trend_stats = {'ìƒìŠ¹': 0, 'ë³´í•©': 0, 'í•˜ë½': 0, 'ì—†ìŒ': 0}

for name in MODEL_NAMES:
    print(f"\n{name}:")
    model = joblib.load(os.path.join(model_dir, f'{name}_regression_model.pkl'))
    
    # ê¸°ë³¸ ì˜ˆì¸¡
    raw_pred = model.predict(X_scaled)
    
    # ì¶”ì„¸ ê¸°ë°˜ ë³´ì • ì ìš©
    corrected_pred = []
    for i in range(len(raw_pred)):
        current_val = time_info[i]['current_totalcnt']
        seq_last_100 = time_info[i]['last_100']
        seq_max = time_info[i]['seq_max']
        
        corrected_val, trend_type = trend_based_correction(
            raw_pred[i], current_val, seq_last_100, seq_max
        )
        corrected_pred.append(corrected_val)
        
        if name == 'GradientBoosting':  # í•œ ëª¨ë¸ë§Œ í†µê³„
            trend_stats[trend_type] += 1
    
    corrected_pred = np.array(corrected_pred)
    
    predictions[name] = raw_pred
    corrected_predictions[name] = corrected_pred
    
    # ì „ì²´ ì„±ëŠ¥
    mae_raw = np.mean(np.abs(raw_pred - y_true))
    mae_corrected = np.mean(np.abs(corrected_pred - y_true))
    print(f"  ì „ì²´ MAE: {mae_raw:.2f} â†’ {mae_corrected:.2f}")
    
    # 1680+ êµ¬ê°„ ì„±ëŠ¥
    high_mask = y_true >= 1680
    if np.sum(high_mask) > 0:
        high_mae_raw = np.mean(np.abs(raw_pred[high_mask] - y_true[high_mask]))
        high_mae_corrected = np.mean(np.abs(corrected_pred[high_mask] - y_true[high_mask]))
        print(f"  1680+ MAE: {high_mae_raw:.2f} â†’ {high_mae_corrected:.2f}")

# ì¶”ì„¸ í†µê³„
print(f"\nì¶”ì„¸ ë¶„í¬:")
for trend_type, count in trend_stats.items():
    if count > 0:
        print(f"  {trend_type}: {count}ê°œ ({count/len(y_true)*100:.1f}%)")

print("-"*80)

print(f"\n5. ê²°ê³¼ CSV ìƒì„±")
results = []

for i in range(len(X)):
    current_time = df['DateTime'].iloc[time_info[i]['current_idx']]
    target_time = df['DateTime'].iloc[time_info[i]['target_idx']]
    
    # ì¶”ì„¸ ê³„ì‚°
    trend = calculate_trend(time_info[i]['last_100'])
    trend_name = {1: "ìƒìŠ¹", 0: "ë³´í•©", -1: "í•˜ë½"}[trend]
    
    row = {
        'ë‚ ì§œ': current_time.strftime('%Y-%m-%d %H:%M'),
        'íƒ€ì¼“ë‚ ì§œ': target_time.strftime('%Y-%m-%d %H:%M'),
        'ì‹¤ì œê°’': round(y_true[i], 0),
        'ì‹œí€€ìŠ¤MAX': round(time_info[i]['seq_max'], 0),
        'ì‹œí€€ìŠ¤MIN': round(time_info[i]['seq_min'], 0),
        'ì¶”ì„¸': trend_name,
    }
    
    # 4ê°œ ëª¨ë¸ ì˜ˆì¸¡ (ë³´ì • ì ìš©)
    for name in MODEL_NAMES:
        row[f'{name}_ì˜ˆì¸¡'] = round(corrected_predictions[name][i], 0)
    
    results.append(row)

results_df = pd.DataFrame(results)
results_df.to_csv(output_file, index=False, encoding='utf-8-sig')

print(f"\nâœ… ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {output_file}")

# ì„±ëŠ¥ ìš”ì•½
print(f"\n[ìµœì¢… ì„±ëŠ¥ ìš”ì•½]")
print("="*80)
print(f"{'ëª¨ë¸':20} {'ì „ì²´ MAE':>10} {'1680+ MAE':>12}")
print("-"*60)

for name in MODEL_NAMES:
    mae = np.mean(np.abs(corrected_predictions[name] - y_true))
    
    high_mask = y_true >= 1680
    if np.sum(high_mask) > 0:
        high_mae = np.mean(np.abs(corrected_predictions[name][high_mask] - y_true[high_mask]))
    else:
        high_mae = 0
    
    print(f"{name:20} {mae:10.2f} {high_mae:12.2f}")

print("="*80)

# ë¬¸ì œ ìƒ˜í”Œ ë¶„ì„
print(f"\n[1680+ êµ¬ê°„ ì˜ˆì¸¡ ìƒ˜í”Œ]")
print("-"*100)
problem_samples = results_df[results_df['ì‹¤ì œê°’'] >= 1680].head(15)

if len(problem_samples) > 0:
    display_cols = ['ë‚ ì§œ', 'ì‹¤ì œê°’', 'ì¶”ì„¸', 'RandomForest_ì˜ˆì¸¡', 
                   'ExtraTrees_ì˜ˆì¸¡', 'GradientBoosting_ì˜ˆì¸¡', 'MLP_ì˜ˆì¸¡']
    print(problem_samples[display_cols].to_string(index=False))
    
    # í‰ê·  ì˜¤ì°¨ ê³„ì‚°
    print(f"\nê° ëª¨ë¸ í‰ê·  ì˜¤ì°¨ (1680+ êµ¬ê°„):")
    for name in MODEL_NAMES:
        errors = problem_samples[f'{name}_ì˜ˆì¸¡'] - problem_samples['ì‹¤ì œê°’']
        print(f"  {name}: {errors.mean():.1f}")

print(f"\nğŸ’¡ ì¶”ì„¸ ê¸°ë°˜ ë³´ì • íš¨ê³¼:")
print("  ìƒìŠ¹ ì¶”ì„¸ â†’ 5% ë³´ì •")
print("  ë³´í•© ì¶”ì„¸ â†’ 4% ë³´ì •")
print("  í•˜ë½ ì¶”ì„¸ â†’ 3% ë³´ì •")