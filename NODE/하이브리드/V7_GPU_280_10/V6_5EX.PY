"""
ğŸ”¥ EXTREME_NET ì™„ì „ ìˆ˜ì • ë²„ì „ - RÂ² ë¬¸ì œ í•´ê²° + UU1/UU2 íŒ¨í„´
===========================================================
ëª¨ë“  ê¸°ëŠ¥ í¬í•¨, RÂ² ë¬¸ì œ í•´ê²°
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model, Input, backend as K
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import *
from tensorflow.keras.regularizers import l2
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix
import warnings
import os
import pickle
import json
from datetime import datetime

warnings.filterwarnings('ignore')
tf.random.set_seed(42)
np.random.seed(42)

# GPU ì„¤ì •
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    print(f"ğŸ”§ GPU {len(gpus)}ê°œ ë°œê²¬")
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    strategy = tf.distribute.MirroredStrategy()
else:
    print("âš ï¸ GPU ì—†ìŒ, CPU ì‚¬ìš©")
    strategy = tf.distribute.get_strategy()

print("="*80)
print("ğŸ”¥ EXTREME_NET ì™„ì „ ìˆ˜ì • ë²„ì „")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print("="*80)

# ====================================
# ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­
# ====================================
def r2_keras(y_true, y_pred):
    """RÂ² ë©”íŠ¸ë¦­ for Keras"""
    SS_res = K.sum(K.square(y_true - y_pred))
    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))
    return (1 - SS_res/(SS_tot + K.epsilon()))

# ====================================
# UU1/UU2 íŒ¨í„´ ê°ì§€
# ====================================
class PatternDetector:
    """UU1/UU2 íŒ¨í„´ ê°ì§€"""
    
    def detect_pattern(self, df):
        """íŒ¨í„´ ê°ì§€"""
        totalcnt_max = df['TOTALCNT'].max()
        totalcnt_mean = df['TOTALCNT'].mean()
        m14b_mean = df['M14AM14B'].mean()
        m14b_max = df['M14AM14B'].max()
        high_cases = len(df[df['TOTALCNT'] >= 1682])
        m14b_350_ratio = (df['M14AM14B'] > 350).sum() / len(df)
        
        # íŒ¨í„´ íŒì •
        if high_cases > 5 and m14b_mean > 380:
            pattern = "UU2"
            confidence = 0.95
        elif m14b_350_ratio > 0.3:
            pattern = "UU2"
            confidence = 0.90
        else:
            pattern = "UU1"
            confidence = 0.85
        
        return pattern, confidence
    
    def create_pattern_features(self, X_seq):
        """íŒ¨í„´ íŠ¹ì§• ìƒì„±"""
        # X_seq shape: (samples, timesteps, features)
        # TOTALCNTê°€ ì²« ë²ˆì§¸ íŠ¹ì§•ì´ë¼ê³  ê°€ì •
        totalcnt = X_seq[:, :, 0]
        m14b = X_seq[:, :, 1] if X_seq.shape[2] > 1 else np.zeros_like(totalcnt)
        
        pattern_features = []
        for i in range(len(X_seq)):
            # ê° ì‹œí€€ìŠ¤ì˜ íŒ¨í„´ íŠ¹ì§•
            tc_mean = np.mean(totalcnt[i])
            tc_max = np.max(totalcnt[i])
            tc_trend = (totalcnt[i][-1] - totalcnt[i][0]) / 100
            
            m14b_mean = np.mean(m14b[i])
            m14b_max = np.max(m14b[i])
            
            # UU2 ê°€ëŠ¥ì„± ì ìˆ˜
            uu2_score = 0
            if m14b_mean > 300:
                uu2_score += 0.5
            if tc_max > 1600:
                uu2_score += 0.5
            
            pattern_features.append([tc_trend, uu2_score, m14b_mean/300])
        
        return np.array(pattern_features, dtype=np.float32)

# ====================================
# ìˆ˜ì •ëœ EXTREME_NET ëª¨ë¸
# ====================================
def build_extreme_net_fixed(input_shape, pattern_shape=(3,)):
    """ìˆ˜ì •ëœ EXTREME_NET - ë” ì•ˆì •ì ì¸ êµ¬ì¡°"""
    
    # ë©”ì¸ ì…ë ¥
    main_input = Input(shape=input_shape, name='main_input')
    pattern_input = Input(shape=pattern_shape, name='pattern_input')
    
    # LSTM ê²½ë¡œ (ì •ê·œí™” ì¶”ê°€)
    x = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.001))(main_input)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    x = LSTM(32, kernel_regularizer=l2(0.001))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    # íŒ¨í„´ ì²˜ë¦¬
    p = Dense(8, activation='relu')(pattern_input)
    
    # ê²°í•©
    combined = Concatenate()([x, p])
    
    # Dense ë ˆì´ì–´
    combined = Dense(32, activation='relu', kernel_regularizer=l2(0.001))(combined)
    combined = BatchNormalization()(combined)
    combined = Dropout(0.3)(combined)
    combined = Dense(16, activation='relu')(combined)
    
    # ì¶œë ¥
    out_reg = Dense(1, name='regression')(combined)
    out_cls = Dense(3, activation='softmax', name='classification')(combined)
    
    model = Model([main_input, pattern_input], [out_reg, out_cls], name='ExtremeNetFixed')
    return model

# ====================================
# ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤
# ====================================
class DataProcessor:
    def __init__(self, seq_len=100, pred_len=10):
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.scaler_X = RobustScaler()  # ì´ìƒì¹˜ì— ê°•ê±´í•œ ìŠ¤ì¼€ì¼ëŸ¬
        self.scaler_y = RobustScaler()
        self.pattern_detector = PatternDetector()
        self.feature_columns = None
        
    def prepare_data(self, df):
        """ë°ì´í„° ì¤€ë¹„"""
        print("\nğŸ“Š ë°ì´í„° ì¤€ë¹„...")
        
        # 0ê°’ ì œê±°
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        
        # íŒ¨í„´ ê°ì§€
        pattern, confidence = self.pattern_detector.detect_pattern(df)
        print(f"  íŒ¨í„´: {pattern} (ì‹ ë¢°ë„: {confidence:.1%})")
        
        # í•„ìˆ˜ ì»¬ëŸ¼
        base_columns = ['TOTALCNT', 'M14AM14B', 'M14AM10A', 'M14AM14BSUM', 'M14AM16']
        self.feature_columns = [col for col in base_columns if col in df.columns]
        
        # íŠ¹ì§• ìƒì„±
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        df['MA_10'] = df['TOTALCNT'].rolling(10, min_periods=1).mean()
        df['MA_30'] = df['TOTALCNT'].rolling(30, min_periods=1).mean()
        df['STD_10'] = df['TOTALCNT'].rolling(10, min_periods=1).std().fillna(0)
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        
        # ì—°ì† ìƒìŠ¹ ì¹´ìš´íŠ¸
        df['CONSECUTIVE_RISE'] = 0
        consecutive = 0
        for i in range(1, len(df)):
            if df.loc[i, 'TOTALCNT'] > df.loc[i-1, 'TOTALCNT']:
                consecutive += 1
            else:
                consecutive = 0
            df.loc[i, 'CONSECUTIVE_RISE'] = consecutive
        
        # UU1/UU2 ì‹ í˜¸
        df['UU2_SIGNAL'] = ((df['M14AM14B'] > 350) | (df['TOTALCNT'] > 1682)).astype(float)
        
        # ìµœì¢… íŠ¹ì§•
        all_features = self.feature_columns + [
            'RATIO', 'GOLDEN', 'MA_10', 'MA_30', 'STD_10', 
            'CHANGE_1', 'CHANGE_10', 'CONSECUTIVE_RISE', 'UU2_SIGNAL'
        ]
        all_features = [col for col in all_features if col in df.columns]
        
        print(f"  íŠ¹ì§• ê°œìˆ˜: {len(all_features)}")
        
        # ë°ì´í„° ì¶”ì¶œ
        X = df[all_features].values.astype(np.float32)
        y = df['TOTALCNT'].values.astype(np.float32)
        
        # 3êµ¬ê°„ ë ˆë²¨
        y_cls = np.zeros(len(y), dtype=np.int32)
        y_cls[y >= 1400] = 1
        y_cls[y >= 1700] = 2
        
        # í†µê³„
        print(f"  ë°ì´í„° í¬ê¸°: {X.shape}")
        print(f"  TOTALCNT ë²”ìœ„: {y.min():.0f} ~ {y.max():.0f}")
        print(f"  TOTALCNT í‰ê· : {y.mean():.0f} Â± {y.std():.0f}")
        
        return X, y, y_cls, all_features
    
    def create_sequences(self, X, y, y_cls):
        """ì‹œí€€ìŠ¤ ìƒì„±"""
        print("\nğŸ”„ ì‹œí€€ìŠ¤ ìƒì„±...")
        
        sequences_X = []
        sequences_y = []
        sequences_y_cls = []
        
        # NaN ì²´í¬
        X = np.nan_to_num(X, 0)
        
        for i in range(len(X) - self.seq_len - self.pred_len + 1):
            seq_x = X[i:i + self.seq_len]
            target_idx = i + self.seq_len + self.pred_len - 1
            
            if target_idx < len(y):
                sequences_X.append(seq_x)
                sequences_y.append(y[target_idx])
                sequences_y_cls.append(y_cls[target_idx])
        
        sequences_X = np.array(sequences_X, dtype=np.float32)
        sequences_y = np.array(sequences_y, dtype=np.float32)
        sequences_y_cls = np.array(sequences_y_cls, dtype=np.int32)
        
        print(f"  ì‹œí€€ìŠ¤ ìˆ˜: {len(sequences_X)}")
        
        # íŒ¨í„´ íŠ¹ì§• ìƒì„±
        pattern_features = self.pattern_detector.create_pattern_features(sequences_X)
        
        # ë¶„í• 
        n = len(sequences_X)
        train_size = int(n * 0.7)
        val_size = int(n * 0.15)
        
        # í›ˆë ¨ ë°ì´í„°
        X_train = sequences_X[:train_size]
        X_train_pattern = pattern_features[:train_size]
        y_train = sequences_y[:train_size]
        y_train_cls = sequences_y_cls[:train_size]
        
        # ê²€ì¦ ë°ì´í„°
        X_val = sequences_X[train_size:train_size + val_size]
        X_val_pattern = pattern_features[train_size:train_size + val_size]
        y_val = sequences_y[train_size:train_size + val_size]
        y_val_cls = sequences_y_cls[train_size:train_size + val_size]
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        X_test = sequences_X[train_size + val_size:]
        X_test_pattern = pattern_features[train_size + val_size:]
        y_test = sequences_y[train_size + val_size:]
        y_test_cls = sequences_y_cls[train_size + val_size:]
        
        # ìŠ¤ì¼€ì¼ë§ (ì¤‘ìš”!)
        print("\nâš™ï¸ ìŠ¤ì¼€ì¼ë§...")
        
        # X ìŠ¤ì¼€ì¼ë§
        n_samples, n_timesteps, n_features = X_train.shape
        X_train_2d = X_train.reshape(-1, n_features)
        X_train_scaled_2d = self.scaler_X.fit_transform(X_train_2d)
        X_train_scaled = X_train_scaled_2d.reshape(n_samples, n_timesteps, n_features)
        
        X_val_scaled = self.scaler_X.transform(X_val.reshape(-1, n_features)).reshape(X_val.shape)
        X_test_scaled = self.scaler_X.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape)
        
        # y ìŠ¤ì¼€ì¼ë§ (íšŒê·€ íƒ€ê²Ÿë§Œ)
        y_train_scaled = self.scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = self.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        y_test_scaled = self.scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        print(f"  Train: {X_train_scaled.shape}")
        print(f"  Val: {X_val_scaled.shape}")
        print(f"  Test: {X_test_scaled.shape}")
        
        # ìŠ¤ì¼€ì¼ë§ ê²€ì¦
        print(f"\n  ìŠ¤ì¼€ì¼ë§ ê²€ì¦:")
        print(f"    X_train ë²”ìœ„: [{X_train_scaled.min():.3f}, {X_train_scaled.max():.3f}]")
        print(f"    y_train ë²”ìœ„: [{y_train_scaled.min():.3f}, {y_train_scaled.max():.3f}]")
        
        # 3ê°œì˜ íŠœí”Œë¡œ ë¬¶ì–´ì„œ ë°˜í™˜
        train_data = ([X_train_scaled, X_train_pattern], y_train_scaled, y_train_cls, y_train)
        val_data = ([X_val_scaled, X_val_pattern], y_val_scaled, y_val_cls, y_val)
        test_data = ([X_test_scaled, X_test_pattern], y_test_scaled, y_test_cls, y_test)
        
        return train_data, val_data, test_data
    
    def save_all(self, path='models/'):
        """ëª¨ë“  ì„¤ì • ì €ì¥"""
        os.makedirs(path, exist_ok=True)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        with open(f'{path}scaler_X.pkl', 'wb') as f:
            pickle.dump(self.scaler_X, f)
        with open(f'{path}scaler_y.pkl', 'wb') as f:
            pickle.dump(self.scaler_y, f)
        
        # ì„¤ì • ì €ì¥
        config = {
            'seq_len': int(self.seq_len),
            'pred_len': int(self.pred_len),
            'feature_columns': self.feature_columns
        }
        with open(f'{path}config.json', 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"âœ… ì„¤ì • ì €ì¥: {path}")

# ====================================
# í•™ìŠµ í•¨ìˆ˜
# ====================================
def train_model(model, train_data, val_data, test_data, processor):
    """ëª¨ë¸ í•™ìŠµ"""
    # ë°ì´í„° ì–¸íŒ¨í‚¹
    X_train, y_train_scaled, y_train_cls, y_train_orig = train_data
    X_val, y_val_scaled, y_val_cls, y_val_orig = val_data
    X_test, y_test_scaled, y_test_cls, y_test_orig = test_data
    
    print("\n" + "="*60)
    print("ğŸ¯ EXTREME_NET í•™ìŠµ ì‹œì‘")
    print("="*60)
    
    with strategy.scope():
        # ëª¨ë¸ ì»´íŒŒì¼
        model.compile(
            optimizer=Adam(learning_rate=0.0005),  # í•™ìŠµë¥  ë‚®ì¶¤
            loss={
                'regression': 'mse',  # MAE ëŒ€ì‹  MSE
                'classification': 'sparse_categorical_crossentropy'
            },
            loss_weights={
                'regression': 0.8,  # íšŒê·€ì— ë” ê°€ì¤‘ì¹˜
                'classification': 0.2
            },
            metrics={
                'regression': [r2_keras, 'mae'],
                'classification': ['accuracy']
            }
        )
    
    # ì½œë°±
    callbacks = [
        EarlyStopping(
            monitor='val_regression_loss',  # íšŒê·€ ì†ì‹¤ ëª¨ë‹ˆí„°ë§
            patience=20,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_regression_loss',
            factor=0.5,
            patience=10,
            min_lr=1e-6,
            verbose=1
        ),
        ModelCheckpoint(
            'models/best_model.keras',
            monitor='val_regression_r2_keras',  # RÂ² ëª¨ë‹ˆí„°ë§
            mode='max',
            save_best_only=True,
            verbose=1
        )
    ]
    
    # í•™ìŠµ
    batch_size = 32
    
    history = model.fit(
        X_train,
        {'regression': y_train_scaled, 'classification': y_train_cls},
        validation_data=(
            X_val,
            {'regression': y_val_scaled, 'classification': y_val_cls}
        ),
        epochs=100,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=1
    )
    
    # í‰ê°€
    print("\n" + "="*60)
    print("ğŸ“Š ì„±ëŠ¥ í‰ê°€")
    print("="*60)
    
    # ì˜ˆì¸¡
    preds = model.predict(X_test, batch_size=batch_size, verbose=0)
    y_pred_scaled = preds[0].flatten()
    y_cls_pred = np.argmax(preds[1], axis=1)
    
    # ì—­ë³€í™˜ (ì¤‘ìš”!)
    y_pred = processor.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    mae = mean_absolute_error(y_test_orig, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred))
    r2 = r2_score(y_test_orig, y_pred)
    
    # RÂ² ë””ë²„ê¹…
    print(f"\nğŸ” RÂ² ë””ë²„ê¹…:")
    print(f"  y_test ë²”ìœ„: {y_test_orig.min():.0f} ~ {y_test_orig.max():.0f}")
    print(f"  y_pred ë²”ìœ„: {y_pred.min():.0f} ~ {y_pred.max():.0f}")
    print(f"  y_test í‰ê· : {y_test_orig.mean():.0f}")
    print(f"  y_pred í‰ê· : {y_pred.mean():.0f}")
    print(f"  y_test í‘œì¤€í¸ì°¨: {y_test_orig.std():.0f}")
    print(f"  y_pred í‘œì¤€í¸ì°¨: {y_pred.std():.0f}")
    
    # ìƒê´€ê³„ìˆ˜
    correlation = np.corrcoef(y_test_orig, y_pred)[0, 1]
    print(f"  ìƒê´€ê³„ìˆ˜: {correlation:.4f}")
    
    print(f"\nğŸ“ˆ íšŒê·€ ì„±ëŠ¥:")
    print(f"  MAE: {mae:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  RÂ²: {r2:.4f}")
    
    # ë¶„ë¥˜ ì„±ëŠ¥
    acc = accuracy_score(y_test_cls, y_cls_pred)
    print(f"\nğŸ“Š ë¶„ë¥˜ ì„±ëŠ¥:")
    print(f"  ì •í™•ë„: {acc:.4f}")
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs('models', exist_ok=True)
    model.save('models/extreme_net_final.keras')
    processor.save_all()
    
    return {
        'MAE': mae,
        'RMSE': rmse,
        'R2': r2,
        'Accuracy': acc,
        'Correlation': correlation
    }

# ====================================
# ì‹¤ì‹œê°„ ì˜ˆì¸¡
# ====================================
class RealtimePredictor:
    def __init__(self, model_path='models/extreme_net_final.keras'):
        self.model = tf.keras.models.load_model(
            model_path,
            custom_objects={'r2_keras': r2_keras}
        )
        self.processor = DataProcessor()
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        with open('models/scaler_X.pkl', 'rb') as f:
            self.processor.scaler_X = pickle.load(f)
        with open('models/scaler_y.pkl', 'rb') as f:
            self.processor.scaler_y = pickle.load(f)
        
        print("âœ… ì˜ˆì¸¡ê¸° ì¤€ë¹„ ì™„ë£Œ")
    
    def predict(self, data_100min):
        """100ë¶„ ë°ì´í„°ë¡œ ì˜ˆì¸¡"""
        if len(data_100min) != 100:
            raise ValueError(f"100ë¶„ ë°ì´í„° í•„ìš” (í˜„ì¬: {len(data_100min)})")
        
        # íŒ¨í„´ ê°ì§€
        pattern, confidence = self.processor.pattern_detector.detect_pattern(data_100min)
        
        # íŠ¹ì§• ì¤€ë¹„
        X = data_100min[self.processor.feature_columns].values
        X = X.reshape(1, 100, -1)
        
        # íŒ¨í„´ íŠ¹ì§•
        pattern_features = self.processor.pattern_detector.create_pattern_features(X)
        
        # ìŠ¤ì¼€ì¼ë§
        X_scaled = self.processor.scaler_X.transform(X.reshape(-1, X.shape[2])).reshape(X.shape)
        
        # ì˜ˆì¸¡
        preds = self.model.predict([X_scaled, pattern_features], verbose=0)
        y_pred_scaled = preds[0][0, 0]
        y_cls_probs = preds[1][0]
        
        # ì—­ë³€í™˜
        y_pred = self.processor.scaler_y.inverse_transform([[y_pred_scaled]])[0, 0]
        
        # íŒ¨í„´ ê¸°ë°˜ ì¡°ì •
        current = data_100min['TOTALCNT'].iloc[-1]
        m14b = data_100min['M14AM14B'].iloc[-1]
        m14a = data_100min['M14AM10A'].iloc[-1]
        
        if pattern == "UU1" and m14b > 300 and m14a < 80:
            y_pred *= 1.1  # í™©ê¸ˆ íŒ¨í„´
        elif pattern == "UU2":
            y_pred *= 0.98  # ë³´ìˆ˜ì 
        
        return {
            'prediction': float(y_pred),
            'current': float(current),
            'change': float(y_pred - current),
            'pattern': pattern,
            'confidence': confidence,
            'level_probs': y_cls_probs.tolist()
        }

# ====================================
# ë©”ì¸ ì‹¤í–‰
# ====================================
def main():
    print("\n" + "="*80)
    print("ğŸš€ EXTREME_NET í•™ìŠµ ì‹œì‘")
    print("="*80)
    
    # ë°ì´í„° ì°¾ê¸°
    data_paths = [
        '/mnt/user-data/uploads/gs.CSV',
        'uu.csv',
        'uu2.csv',
        'data.csv'
    ]
    
    data_path = None
    for path in data_paths:
        if os.path.exists(path):
            data_path = path
            print(f"âœ… ë°ì´í„°: {path}")
            break
    
    if not data_path:
        print("âŒ ë°ì´í„° ì—†ìŒ")
        return
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(data_path)
    print(f"  ë¡œë“œ: {len(df):,}í–‰")
    
    # ë°ì´í„° ì²˜ë¦¬
    processor = DataProcessor()
    X, y, y_cls, features = processor.prepare_data(df)
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    train_data, val_data, test_data = processor.create_sequences(X, y, y_cls)
    
    # ëª¨ë¸ ìƒì„±
    input_shape = (100, len(features))
    model = build_extreme_net_fixed(input_shape)
    
    # í•™ìŠµ
    results = train_model(model, train_data, val_data, test_data, processor)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ† ìµœì¢… ì„±ëŠ¥")
    print("="*80)
    print(f"  RÂ²: {results['R2']:.4f}")
    print(f"  MAE: {results['MAE']:.2f}")
    print(f"  ì •í™•ë„: {results['Accuracy']:.4f}")
    print(f"  ìƒê´€ê³„ìˆ˜: {results['Correlation']:.4f}")
    
    # ì‹¤ì‹œê°„ í…ŒìŠ¤íŠ¸
    if len(df) >= 100:
        print("\nğŸ”® ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸...")
        predictor = RealtimePredictor()
        test_data = df.iloc[-100:].copy()
        result = predictor.predict(test_data)
        
        print(f"  íŒ¨í„´: {result['pattern']}")
        print(f"  í˜„ì¬: {result['current']:.0f}")
        print(f"  ì˜ˆì¸¡: {result['prediction']:.0f}")
        print(f"  ë³€í™”: {result['change']:+.0f}")
    
    print("\nâœ… ì™„ë£Œ!")
    return results

if __name__ == "__main__":
    results = main()