"""
ğŸ”¥ ExtremeNet V6.5 - íŒ¨í„´ í•™ìŠµ ê°•í™” ë²„ì „
============================================
âœ… ExtremeNet ë‹¨ì¼ ëª¨ë¸ ì§‘ì¤‘
âœ… ìœ„í—˜êµ¬ê°„(1651~1682) íŠ¹ë³„ í•™ìŠµ
âœ… íŒ¨í„´ ì¸ì‹ ê°•í™” (UU1/UU2)
âœ… R2 ìŒìˆ˜ ë°©ì§€ ì²˜ë¦¬
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import *
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
import os
import pickle
import json
from datetime import datetime, timedelta

warnings.filterwarnings('ignore')
tf.random.set_seed(42)
np.random.seed(42)

# GPU ì„¤ì •
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    print(f"ğŸ”§ GPU {len(gpus)}ê°œ ë°œê²¬")
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    strategy = tf.distribute.MirroredStrategy()
    device_type = 'GPU'
else:
    print("âš ï¸ GPU ì—†ìŒ, CPU ì‚¬ìš©")
    strategy = tf.distribute.get_strategy()
    device_type = 'CPU'

print("="*80)
print("ğŸ”¥ ExtremeNet V6.5 - íŒ¨í„´ í•™ìŠµ ê°•í™” ë²„ì „")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print(f"ğŸ”§ Device: {device_type}")
print("="*80)

# ========================================
# ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤ (íŒ¨í„´ ê°•í™”)
# ========================================
class PatternDataProcessor:
    def __init__(self, seq_len=100, pred_len=10):
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.feature_scaler = RobustScaler()  # ì´ìƒì¹˜ì— ê°•ê±´
        self.target_scaler = RobustScaler()
        self.feature_columns = None
        
        # êµ¬ê°„ ì •ì˜
        self.NORMAL_MAX = 1400
        self.WARNING_MAX = 1700
        self.DANGER_MIN = 1651  # ìœ„í—˜ ì‹ í˜¸ ì‹œì‘
        self.DANGER_MAX = 1682  # ìœ„í—˜ ì‹ í˜¸ ë
        
    def save_scalers(self, path='scalers_v65/'):
        """ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥"""
        os.makedirs(path, exist_ok=True)
        
        with open(f'{path}feature_scaler.pkl', 'wb') as f:
            pickle.dump(self.feature_scaler, f)
        with open(f'{path}target_scaler.pkl', 'wb') as f:
            pickle.dump(self.target_scaler, f)
            
        config = {
            'seq_len': self.seq_len,
            'pred_len': self.pred_len,
            'feature_columns': self.feature_columns
        }
        with open(f'{path}config.json', 'w') as f:
            json.dump(config, f)
            
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {path}")
    
    def load_and_process(self, filepath):
        print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {filepath}")
        df = pd.read_csv(filepath)
        print(f"âœ… ì›ë³¸: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")
        
        # 0ê°’ ì œê±°
        before = len(df)
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        print(f"âœ… 0ê°’ ì œê±°: {before - len(df):,}ê°œ")
        
        # ì‹œê°„ ì •ë ¬
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                       format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        # êµ¬ê°„ ë¶„ì„
        normal = (df['TOTALCNT'] < self.NORMAL_MAX).sum()
        warning = ((df['TOTALCNT'] >= self.NORMAL_MAX) & 
                  (df['TOTALCNT'] < self.WARNING_MAX)).sum()
        critical = (df['TOTALCNT'] >= self.WARNING_MAX).sum()
        danger_signal = ((df['TOTALCNT'] >= self.DANGER_MIN) & 
                        (df['TOTALCNT'] <= self.DANGER_MAX)).sum()
        
        print(f"\nğŸ“Š êµ¬ê°„ ë¶„í¬:")
        print(f"  ì •ìƒ(~1400): {normal:,}ê°œ ({normal/len(df)*100:.1f}%)")
        print(f"  ì£¼ì˜(1400~1699): {warning:,}ê°œ ({warning/len(df)*100:.1f}%)")
        print(f"  ì‹¬ê°(1700+): {critical:,}ê°œ ({critical/len(df)*100:.1f}%)")
        print(f"  ğŸ”¥ ìœ„í—˜ì‹ í˜¸(1651~1682): {danger_signal:,}ê°œ ({danger_signal/len(df)*100:.1f}%)")
        
        return df
    
    def detect_pattern(self, df, window=100):
        """íŒ¨í„´ ê°ì§€ (UU1/UU2)"""
        patterns = []
        
        for i in range(len(df)):
            if i < window:
                patterns.append(0)  # íŒ¨í„´ ì—†ìŒ
                continue
                
            # ìµœê·¼ window ë°ì´í„°
            recent = df.iloc[max(0, i-window):i]
            
            # íŒ¨í„´ íŒì • ê¸°ì¤€
            high_cases = (recent['TOTALCNT'] >= self.DANGER_MAX).sum()
            m14b_mean = recent['M14AM14B'].mean()
            m14b_350_ratio = (recent['M14AM14B'] > 350).mean()
            danger_zone = ((recent['TOTALCNT'] >= self.DANGER_MIN) & 
                          (recent['TOTALCNT'] <= self.DANGER_MAX)).sum()
            
            # UU2: ê³ ìœ„í—˜ íŒ¨í„´ (ë³´ìˆ˜ì )
            if high_cases > 0 and m14b_mean > 380:
                patterns.append(2)  # UU2
            elif m14b_350_ratio > 0.3:
                patterns.append(2)  # UU2
            # UU1: ê¸‰ì¦ ê°€ëŠ¥ íŒ¨í„´
            elif danger_zone > 0:  # ìœ„í—˜ ì‹ í˜¸ ê°ì§€
                patterns.append(1)  # UU1 - ê¸‰ì¦ ì˜ˆìƒ
            else:
                patterns.append(0)  # ì •ìƒ
                
        df['PATTERN'] = patterns
        
        print(f"\nğŸ“Š íŒ¨í„´ ë¶„í¬:")
        print(f"  ì •ìƒ: {(df['PATTERN'] == 0).sum():,}ê°œ")
        print(f"  UU1(ê¸‰ì¦ê°€ëŠ¥): {(df['PATTERN'] == 1).sum():,}ê°œ")
        print(f"  UU2(ê³ ìœ„í—˜): {(df['PATTERN'] == 2).sum():,}ê°œ")
        
        return df
    
    def create_features(self, df):
        print("\nâš™ï¸ íŠ¹ì„± ìƒì„±...")
        
        # íŒ¨í„´ ê°ì§€
        df = self.detect_pattern(df)
        
        # ê¸°ë³¸ íŠ¹ì„±
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        
        # ìœ„í—˜ ì‹ í˜¸ íŠ¹ì„±
        df['DANGER_SIGNAL'] = ((df['TOTALCNT'] >= self.DANGER_MIN) & 
                               (df['TOTALCNT'] <= self.DANGER_MAX)).astype(float)
        
        # ì‹œê°„ íŠ¹ì„±
        df['HOUR'] = df['CURRTIME'].dt.hour
        df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
        df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
        
        # ì´ë™ í†µê³„
        for w in [5, 10, 20, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
            
            # M14AM14B ì´ë™í‰ê· 
            df[f'M14B_MA_{w}'] = df['M14AM14B'].rolling(w, min_periods=1).mean()
        
        # ë³€í™”ìœ¨
        for lag in [1, 5, 10, 20]:
            df[f'CHANGE_{lag}'] = df['TOTALCNT'].diff(lag).fillna(0)
            df[f'M14B_CHANGE_{lag}'] = df['M14AM14B'].diff(lag).fillna(0)
        
        # ì—°ì† ìƒìŠ¹ íšŸìˆ˜
        df['CONSECUTIVE_RISE'] = 0
        for i in range(1, len(df)):
            if df.loc[i, 'TOTALCNT'] > df.loc[i-1, 'TOTALCNT']:
                df.loc[i, 'CONSECUTIVE_RISE'] = df.loc[i-1, 'CONSECUTIVE_RISE'] + 1
            else:
                df.loc[i, 'CONSECUTIVE_RISE'] = 0
        
        # íƒ€ê²Ÿ ìƒì„±
        df['TARGET'] = df['TOTALCNT'].shift(-self.pred_len)
        
        # êµ¬ê°„ ë ˆì´ë¸” (3êµ¬ê°„)
        df['LEVEL'] = 0  # ì •ìƒ
        df.loc[df['TARGET'] >= self.NORMAL_MAX, 'LEVEL'] = 1  # ì£¼ì˜
        df.loc[df['TARGET'] >= self.WARNING_MAX, 'LEVEL'] = 2  # ì‹¬ê°
        
        # ìœ„í—˜ ì‹ í˜¸ ë ˆì´ë¸” (1651~1682 êµ¬ê°„ íŠ¹ë³„ ì²˜ë¦¬)
        df['IS_DANGER'] = ((df['TARGET'] >= self.DANGER_MIN) & 
                          (df['TARGET'] <= self.DANGER_MAX)).astype(int)
        
        df = df.dropna()
        print(f"âœ… ìœ íš¨ ë°ì´í„°: {len(df)}í–‰")
        
        return df
    
    def create_sequences(self, df):
        print("\nğŸ”„ ì‹œí€€ìŠ¤ ìƒì„±...")
        
        # íŠ¹ì„± ì„ íƒ (ExtremeNetìš© í™•ì¥)
        self.feature_columns = [
            'TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M14AM10A', 'M14AM16',
            'RATIO', 'GOLDEN', 'DANGER_SIGNAL',
            'PATTERN',  # íŒ¨í„´ ì •ë³´
            'HOUR_SIN', 'HOUR_COS',
            'MA_5', 'MA_10', 'MA_20', 'MA_30',
            'STD_5', 'STD_10', 'STD_20', 'STD_30',
            'M14B_MA_5', 'M14B_MA_10', 'M14B_MA_20',
            'CHANGE_1', 'CHANGE_5', 'CHANGE_10',
            'M14B_CHANGE_1', 'M14B_CHANGE_5',
            'CONSECUTIVE_RISE'
        ]
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ
        self.feature_columns = [f for f in self.feature_columns if f in df.columns]
        print(f"  íŠ¹ì„±: {len(self.feature_columns)}ê°œ")
        
        X_data = df[self.feature_columns].values.astype(np.float32)
        y_reg = df['TARGET'].values.astype(np.float32)
        y_cls = df['LEVEL'].values.astype(np.int32)
        y_danger = df['IS_DANGER'].values.astype(np.int32)
        
        # ì¶”ì„¸ ë ˆì´ë¸” ì¶”ê°€
        y_trend = self.calculate_trend_labels(df)
        
        # ë¶„í• 
        total_samples = len(X_data) - self.seq_len
        train_end = int(total_samples * 0.7)
        val_end = int(total_samples * 0.85)
        
        # ìŠ¤ì¼€ì¼ë§
        print("  ìŠ¤ì¼€ì¼ë§...")
        self.feature_scaler.fit(X_data[:train_end + self.seq_len])
        self.target_scaler.fit(y_reg[:train_end].reshape(-1, 1))
        
        X_scaled = self.feature_scaler.transform(X_data)
        y_reg_scaled = self.target_scaler.transform(y_reg.reshape(-1, 1)).flatten()
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y_r, y_c, y_d, y_t = [], [], [], [], []
        for i in range(total_samples):
            X.append(X_scaled[i:i+self.seq_len])
            y_r.append(y_reg_scaled[i+self.seq_len-1])
            y_c.append(y_cls[i+self.seq_len-1])
            y_d.append(y_danger[i+self.seq_len-1])
            y_t.append(y_trend[i+self.seq_len-1])
        
        X = np.array(X, dtype=np.float32)
        y_r = np.array(y_r, dtype=np.float32)
        y_c = np.array(y_c, dtype=np.int32)
        y_d = np.array(y_d, dtype=np.int32)
        y_t = np.array(y_t, dtype=np.int32)
        
        # ë°ì´í„° ë¶„í• 
        X_train = X[:train_end]
        X_val = X[train_end:val_end]
        X_test = X[val_end:]
        
        y_train = (y_r[:train_end], y_c[:train_end], y_d[:train_end], y_t[:train_end])
        y_val = (y_r[train_end:val_end], y_c[train_end:val_end], 
                y_d[train_end:val_end], y_t[train_end:val_end])
        y_test = (y_r[val_end:], y_c[val_end:], y_d[val_end:], y_t[val_end:])
        
        print(f"  Train: {X_train.shape}")
        print(f"  Val: {X_val.shape}")
        print(f"  Test: {X_test.shape}")
        
        # ìœ„í—˜ êµ¬ê°„ ë¹„ìœ¨ í™•ì¸
        danger_train = (y_train[2] == 1).sum()
        danger_val = (y_val[2] == 1).sum()
        danger_test = (y_test[2] == 1).sum()
        
        print(f"\nğŸ”¥ ìœ„í—˜êµ¬ê°„(1651~1682) ìƒ˜í”Œ:")
        print(f"  Train: {danger_train}ê°œ ({danger_train/len(X_train)*100:.1f}%)")
        print(f"  Val: {danger_val}ê°œ ({danger_val/len(X_val)*100:.1f}%)")
        print(f"  Test: {danger_test}ê°œ ({danger_test/len(X_test)*100:.1f}%)")
        
        self.save_scalers()
        
        return (X_train, y_train), (X_val, y_val), (X_test, y_test)
    
    def calculate_trend_labels(self, df):
        """ì¶”ì„¸ ë ˆì´ë¸” ê³„ì‚° (0: í•˜ë½, 1: ë³´í•©, 2: ìƒìŠ¹)"""
        trends = []
        window = 20
        
        for i in range(len(df)):
            if i < window:
                trends.append(1)  # ë³´í•©
                continue
            
            # ìµœê·¼ window ê¸°ê°„ ì¶”ì„¸
            recent = df['TOTALCNT'].iloc[max(0, i-window):i].values
            if len(recent) < 2:
                trends.append(1)
                continue
                
            # ì„ í˜• íšŒê·€ ì¶”ì„¸
            x = np.arange(len(recent))
            slope = np.polyfit(x, recent, 1)[0]
            
            # ì¶”ì„¸ íŒì •
            if slope > 5:  # ìƒìŠ¹
                trends.append(2)
            elif slope < -5:  # í•˜ë½
                trends.append(0)
            else:  # ë³´í•©
                trends.append(1)
        
        return np.array(trends)

# ========================================
# ExtremeNet V6.5 ëª¨ë¸ (ê°•í™”ë²„ì „)
# ========================================
def build_extremenet_v65(input_shape):
    """ExtremeNet V6.5 - íŒ¨í„´/ì¶”ì„¸ í•™ìŠµ ê°•í™”"""
    inputs = Input(shape=input_shape)
    
    # Branch 1: LSTM ì¥ê¸° íŒ¨í„´
    lstm1 = LSTM(128, return_sequences=True)(inputs)
    lstm1 = BatchNormalization()(lstm1)
    lstm1 = Dropout(0.3)(lstm1)
    lstm2 = LSTM(64)(lstm1)
    lstm2 = BatchNormalization()(lstm2)
    
    # Branch 2: Attention ë©”ì»¤ë‹ˆì¦˜
    attn = MultiHeadAttention(num_heads=8, key_dim=16)(inputs, inputs)
    attn = LayerNormalization()(attn)
    attn = GlobalAveragePooling1D()(attn)
    
    # Branch 3: CNN ë‹¨ê¸° íŒ¨í„´ (ìœ„í—˜êµ¬ê°„ ê°ì§€)
    cnn = Conv1D(64, 5, activation='relu', padding='same')(inputs)
    cnn = BatchNormalization()(cnn)
    cnn = Conv1D(32, 3, activation='relu', padding='same')(cnn)
    cnn = GlobalMaxPooling1D()(cnn)
    
    # Branch 4: ìµœê·¼ ë°ì´í„° ì§‘ì¤‘ (ìœ„í—˜ì‹ í˜¸ ê°ì§€)
    recent = Lambda(lambda x: x[:, -30:, :])(inputs)
    recent = GRU(32)(recent)
    
    # í†µí•©
    combined = Concatenate()([lstm2, attn, cnn, recent])
    combined = BatchNormalization()(combined)
    
    # Dense layers
    x = Dense(128, activation='relu')(combined)
    x = Dropout(0.3)(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    # ì¶œë ¥ì¸µ
    out_reg = Dense(1, name='regression')(x)  # ì˜ˆì¸¡ê°’
    out_cls = Dense(3, activation='softmax', name='classification')(x)  # êµ¬ê°„
    out_danger = Dense(1, activation='sigmoid', name='danger')(x)  # ìœ„í—˜ì‹ í˜¸
    out_trend = Dense(3, activation='softmax', name='trend')(x)  # ì¶”ì„¸
    
    return Model(inputs, [out_reg, out_cls, out_danger, out_trend], name='ExtremeNet_V65')

# ========================================
# ì²´í¬í¬ì¸íŠ¸ í•™ìŠµ ì‹œìŠ¤í…œ
# ========================================
class CheckpointTrainer:
    def __init__(self, model_name='ExtremeNet_V65'):
        self.model_name = model_name
        self.checkpoint_dir = f'checkpoints_{model_name}/'
        self.history_file = f'{self.checkpoint_dir}history.json'
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
    def load_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint_path = f'{self.checkpoint_dir}{self.model_name}.keras'
        history = {'epoch': 0, 'best_loss': float('inf')}
        
        if os.path.exists(checkpoint_path):
            print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë°œê²¬! ë¡œë”©...")
            model = tf.keras.models.load_model(checkpoint_path)
            
            if os.path.exists(self.history_file):
                with open(self.history_file, 'r') as f:
                    history = json.load(f)
                print(f"  ì´ì „ í•™ìŠµ: {history['epoch']}ì—í¬í¬, ìµœê³ ì†ì‹¤: {history['best_loss']:.4f}")
        else:
            print("âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, ìƒˆë¡œ ì‹œì‘")
            model = None
            
        return model, history
    
    def save_checkpoint(self, model, epoch, best_loss):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_path = f'{self.checkpoint_dir}{self.model_name}.keras'
        model.save(checkpoint_path)
        
        history = {'epoch': epoch, 'best_loss': float(best_loss)}
        with open(self.history_file, 'w') as f:
            json.dump(history, f)
        
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: ì—í¬í¬ {epoch}, ì†ì‹¤ {best_loss:.4f}")

# ========================================
# ì»¤ìŠ¤í…€ ì½œë°± (ìœ„í—˜êµ¬ê°„ ì§‘ì¤‘)
# ========================================
class DangerZoneFocusCallback(tf.keras.callbacks.Callback):
    """ìœ„í—˜êµ¬ê°„(1651~1682) ì˜ˆì¸¡ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, X_val, y_val, processor):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val
        self.processor = processor
        
    def on_epoch_end(self, epoch, logs=None):
        # ìœ„í—˜êµ¬ê°„ ìƒ˜í”Œë§Œ
        danger_idx = np.where(self.y_val[2] == 1)[0]
        
        if len(danger_idx) > 0:
            X_danger = self.X_val[danger_idx]
            y_danger_true = self.y_val[0][danger_idx]
            
            # ì˜ˆì¸¡
            preds = self.model.predict(X_danger, verbose=0)
            y_danger_pred = preds[0].flatten()
            
            # ì—­ë³€í™˜
            y_true_orig = self.processor.target_scaler.inverse_transform(
                y_danger_true.reshape(-1, 1)).flatten()
            y_pred_orig = self.processor.target_scaler.inverse_transform(
                y_danger_pred.reshape(-1, 1)).flatten()
            
            mae = mean_absolute_error(y_true_orig, y_pred_orig)
            accuracy = np.mean(np.abs(y_true_orig - y_pred_orig) < 50)  # 50 ì´ë‚´ ì •í™•ë„
            
            print(f"\n  ğŸ”¥ ìœ„í—˜êµ¬ê°„ ì„±ëŠ¥: MAE={mae:.1f}, ì •í™•ë„={accuracy:.1%}")

# ========================================
# í•™ìŠµ ë° í‰ê°€
# ========================================
def train_extremenet(train_data, val_data, test_data, processor):
    """ExtremeNet V6.5 í•™ìŠµ"""
    
    X_train, y_train = train_data
    X_val, y_val = val_data
    X_test, y_test = test_data
    
    # ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ
    trainer = CheckpointTrainer()
    model, history = trainer.load_checkpoint()
    
    # ëª¨ë¸ ìƒì„± ë˜ëŠ” ë¡œë“œ
    with strategy.scope():
        if model is None:
            input_shape = (X_train.shape[1], X_train.shape[2])
            model = build_extremenet_v65(input_shape)
            
            model.compile(
                optimizer=Adam(0.001),
                loss={
                    'regression': 'huber',  # ì´ìƒì¹˜ì— ê°•ê±´
                    'classification': 'sparse_categorical_crossentropy',
                    'danger': 'binary_crossentropy',
                    'trend': 'sparse_categorical_crossentropy'
                },
                loss_weights={
                    'regression': 0.5,
                    'classification': 0.2,
                    'danger': 0.2,  # ìœ„í—˜êµ¬ê°„ ì¤‘ìš”
                    'trend': 0.1
                },
                metrics={
                    'regression': 'mae',
                    'classification': 'accuracy',
                    'danger': 'accuracy',
                    'trend': 'accuracy'
                }
            )
    
    print("\n" + "="*80)
    print("ğŸ”¥ ExtremeNet V6.5 í•™ìŠµ ì‹œì‘")
    print("="*80)
    
    # ì½œë°±
    callbacks = [
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        ModelCheckpoint(
            f'{trainer.checkpoint_dir}{trainer.model_name}_best.keras',
            save_best_only=True,
            monitor='val_loss',
            verbose=1
        ),
        
        # ì¡°ê¸°ì¢…ë£Œ
        EarlyStopping(
            patience=15,
            restore_best_weights=True,
            monitor='val_loss',
            verbose=1
        ),
        
        # í•™ìŠµë¥  ì¡°ì •
        ReduceLROnPlateau(
            factor=0.5,
            patience=5,
            min_lr=1e-6,
            verbose=1
        ),
        
        # ìœ„í—˜êµ¬ê°„ ëª¨ë‹ˆí„°ë§
        DangerZoneFocusCallback(X_val, y_val, processor)
    ]
    
    # ë°°ì¹˜ í¬ê¸°
    batch_size = 64 * strategy.num_replicas_in_sync if gpus else 32
    
    # í•™ìŠµ ì¬ê°œ ë˜ëŠ” ì‹œì‘
    initial_epoch = history['epoch']
    
    print(f"\nğŸ“š í•™ìŠµ ì„¤ì •:")
    print(f"  ì‹œì‘ ì—í¬í¬: {initial_epoch}")
    print(f"  ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"  ì´ ì—í¬í¬: 100")
    
    # í•™ìŠµ
    history = model.fit(
        X_train,
        {
            'regression': y_train[0],
            'classification': y_train[1],
            'danger': y_train[2],
            'trend': y_train[3]
        },
        validation_data=(
            X_val,
            {
                'regression': y_val[0],
                'classification': y_val[1],
                'danger': y_val[2],
                'trend': y_val[3]
            }
        ),
        epochs=100,
        initial_epoch=initial_epoch,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=1
    )
    
    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    best_loss = min(history.history['val_loss'])
    final_epoch = initial_epoch + len(history.history['val_loss'])
    trainer.save_checkpoint(model, final_epoch, best_loss)
    
    # í‰ê°€
    print("\n" + "="*80)
    print("ğŸ“Š ExtremeNet V6.5 ìµœì¢… í‰ê°€")
    print("="*80)
    
    # ì „ì²´ í…ŒìŠ¤íŠ¸
    preds = model.predict(X_test, verbose=0)
    y_reg_pred = preds[0].flatten()
    y_cls_pred = np.argmax(preds[1], axis=1)
    y_danger_pred = (preds[2].flatten() > 0.5).astype(int)
    y_trend_pred = np.argmax(preds[3], axis=1)
    
    # ì—­ë³€í™˜
    y_reg_true_orig = processor.target_scaler.inverse_transform(
        y_test[0].reshape(-1, 1)).flatten()
    y_reg_pred_orig = processor.target_scaler.inverse_transform(
        y_reg_pred.reshape(-1, 1)).flatten()
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    mae = mean_absolute_error(y_reg_true_orig, y_reg_pred_orig)
    rmse = np.sqrt(mean_squared_error(y_reg_true_orig, y_reg_pred_orig))
    
    # R2 (ìŒìˆ˜ ë°©ì§€)
    r2_raw = r2_score(y_reg_true_orig, y_reg_pred_orig)
    r2 = max(0.0, r2_raw)  # ìŒìˆ˜ë©´ 0ìœ¼ë¡œ
    
    # ì •í™•ë„
    cls_acc = np.mean(y_cls_pred == y_test[1])
    danger_acc = np.mean(y_danger_pred == y_test[2])
    trend_acc = np.mean(y_trend_pred == y_test[3])
    
    print(f"\nğŸ“ˆ íšŒê·€ ì„±ëŠ¥:")
    print(f"  MAE: {mae:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  RÂ² Score: {r2:.4f} (ì›ë³¸: {r2_raw:.4f})")
    
    print(f"\nğŸ¯ ë¶„ë¥˜ ì„±ëŠ¥:")
    print(f"  êµ¬ê°„ ì •í™•ë„: {cls_acc:.2%}")
    print(f"  ìœ„í—˜ì‹ í˜¸ ì •í™•ë„: {danger_acc:.2%}")
    print(f"  ì¶”ì„¸ ì •í™•ë„: {trend_acc:.2%}")
    
    # ìœ„í—˜êµ¬ê°„ë§Œ í‰ê°€
    danger_idx = np.where(y_test[2] == 1)[0]
    if len(danger_idx) > 0:
        danger_mae = mean_absolute_error(
            y_reg_true_orig[danger_idx],
            y_reg_pred_orig[danger_idx]
        )
        print(f"\nğŸ”¥ ìœ„í—˜êµ¬ê°„(1651~1682) ì„±ëŠ¥:")
        print(f"  ìƒ˜í”Œ ìˆ˜: {len(danger_idx)}ê°œ")
        print(f"  MAE: {danger_mae:.2f}")
        print(f"  ê°ì§€ìœ¨: {np.mean(y_danger_pred[danger_idx]):.1%}")
    
    # ì¶”ì„¸ë³„ ì„±ëŠ¥
    print(f"\nğŸ“Š ì¶”ì„¸ë³„ ì˜ˆì¸¡ ì„±ëŠ¥:")
    for trend, name in enumerate(['í•˜ë½', 'ë³´í•©', 'ìƒìŠ¹']):
        trend_idx = np.where(y_test[3] == trend)[0]
        if len(trend_idx) > 0:
            trend_mae = mean_absolute_error(
                y_reg_true_orig[trend_idx],
                y_reg_pred_orig[trend_idx]
            )
            print(f"  {name} ì¶”ì„¸: MAE={trend_mae:.2f} (n={len(trend_idx)})")
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs('models_v65', exist_ok=True)
    model.save('models_v65/ExtremeNet_V65_final.keras')
    print(f"\nğŸ’¾ ìµœì¢… ëª¨ë¸: models_v65/ExtremeNet_V65_final.keras")
    
    return model, {'MAE': mae, 'RMSE': rmse, 'R2': r2}

# ========================================
# ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
# ========================================
class ExtremeNetPredictor:
    def __init__(self, model_path='models_v65/ExtremeNet_V65_final.keras'):
        """ExtremeNet V6.5 ì˜ˆì¸¡ê¸°"""
        self.model = tf.keras.models.load_model(model_path)
        self.processor = PatternDataProcessor()
        self.processor.load_scalers('scalers_v65/')
        print("âœ… ExtremeNet V6.5 ì˜ˆì¸¡ê¸° ì¤€ë¹„ ì™„ë£Œ")
    
    def predict(self, data_100min):
        """100ë¶„ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡"""
        
        # íŒ¨í„´ ê°ì§€
        pattern = self.detect_current_pattern(data_100min)
        
        # ì¶”ì„¸ íŒë‹¨
        trend = self.detect_current_trend(data_100min)
        
        # íŠ¹ì„± ì¶”ì¶œ ë° ìŠ¤ì¼€ì¼ë§
        X = data_100min[self.processor.feature_columns].values
        X_scaled = self.processor.feature_scaler.transform(X)
        X_seq = X_scaled.reshape(1, 100, -1)
        
        # ì˜ˆì¸¡
        preds = self.model.predict(X_seq, verbose=0)
        
        # ê²°ê³¼ í•´ì„
        y_reg = self.processor.target_scaler.inverse_transform([[preds[0][0, 0]]])[0, 0]
        y_cls = np.argmax(preds[1][0])
        y_danger = preds[2][0, 0]
        y_trend = np.argmax(preds[3][0])
        
        # ë ˆë²¨
        levels = ['ì •ìƒ', 'ì£¼ì˜', 'ì‹¬ê°']
        trends = ['í•˜ë½', 'ë³´í•©', 'ìƒìŠ¹']
        
        # ìœ„í—˜ ì‹ í˜¸ ì²´í¬
        is_danger_zone = (1651 <= y_reg <= 1682)
        
        result = {
            'prediction': float(y_reg),
            'level': levels[y_cls],
            'danger_probability': float(y_danger),
            'trend': trends[y_trend],
            'pattern': f"UU{pattern+1}" if pattern > 0 else "ì •ìƒ",
            'is_danger_zone': is_danger_zone,
            'confidence': float(np.max(preds[1][0]))
        }
        
        # ê²½ê³  ë©”ì‹œì§€
        if is_danger_zone or y_danger > 0.7:
            result['alert'] = "âš ï¸ ìœ„í—˜ì‹ í˜¸! 1700+ ê¸‰ì¦ ê°€ëŠ¥ì„± ë†’ìŒ"
        elif y_cls == 2:
            result['alert'] = "ğŸ”´ ì‹¬ê° êµ¬ê°„ ì˜ˆìƒ"
        elif y_cls == 1:
            result['alert'] = "ğŸŸ¡ ì£¼ì˜ êµ¬ê°„ ì˜ˆìƒ"
        else:
            result['alert'] = "ğŸŸ¢ ì •ìƒ êµ¬ê°„"
        
        return result
    
    def detect_current_pattern(self, df):
        """í˜„ì¬ íŒ¨í„´ ê°ì§€"""
        high_cases = (df['TOTALCNT'] >= 1682).sum()
        m14b_mean = df['M14AM14B'].mean()
        
        if high_cases > 0 and m14b_mean > 380:
            return 2  # UU2
        elif (df['M14AM14B'] > 350).mean() > 0.3:
            return 2  # UU2
        elif ((df['TOTALCNT'] >= 1651) & (df['TOTALCNT'] <= 1682)).sum() > 0:
            return 1  # UU1
        else:
            return 0  # ì •ìƒ
    
    def detect_current_trend(self, df):
        """í˜„ì¬ ì¶”ì„¸ ê°ì§€"""
        recent = df['TOTALCNT'].tail(20).values
        if len(recent) < 2:
            return "ë³´í•©"
        
        slope = np.polyfit(np.arange(len(recent)), recent, 1)[0]
        
        if slope > 5:
            return "ìƒìŠ¹"
        elif slope < -5:
            return "í•˜ë½"
        else:
            return "ë³´í•©"

# ========================================
# ë©”ì¸ ì‹¤í–‰
# ========================================
def main():
    # ë°ì´í„° ê²½ë¡œ
    data_paths = [
        'data/20240201_TO_202507281705.csv',
        '20240201_TO_202507281705.csv'
    ]
    
    data_path = None
    for path in data_paths:
        if os.path.exists(path):
            data_path = path
            print(f"âœ… ë°ì´í„° ë°œê²¬: {path}")
            break
    
    if not data_path:
        print("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ë°ì´í„° ì²˜ë¦¬
    processor = PatternDataProcessor()
    df = processor.load_and_process(data_path)
    df = processor.create_features(df)
    
    # ë°ì´í„° ë¶„í• 
    train_data, val_data, test_data = processor.create_sequences(df)
    
    # ExtremeNet V6.5 í•™ìŠµ
    model, results = train_extremenet(train_data, val_data, test_data, processor)
    
    print("\n" + "="*80)
    print("ğŸ¯ ExtremeNet V6.5 í•™ìŠµ ì™„ë£Œ!")
    print("="*80)
    
    # ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    print("\nğŸ”® ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸")
    print("-"*50)
    
    predictor = ExtremeNetPredictor()
    
    # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ
    test_sample = df.iloc[-100:].copy()
    result = predictor.predict(test_sample)
    
    print(f"\nì˜ˆì¸¡ ê²°ê³¼:")
    print(f"  10ë¶„ í›„ ì˜ˆì¸¡ê°’: {result['prediction']:.0f}")
    print(f"  êµ¬ê°„: {result['level']}")
    print(f"  ì¶”ì„¸: {result['trend']}")
    print(f"  íŒ¨í„´: {result['pattern']}")
    print(f"  ìœ„í—˜í™•ë¥ : {result['danger_probability']:.1%}")
    print(f"  ì‹ ë¢°ë„: {result['confidence']:.1%}")
    print(f"  {result['alert']}")
    
    # ê²°ê³¼ ì €ì¥
    summary = {
        'model': 'ExtremeNet V6.5',
        'mae': results['MAE'],
        'rmse': results['RMSE'],
        'r2': results['R2'],
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open('extremenet_v65_results.json', 'w') as f:
        json.dump(summary, f, indent=2)
    
    print("\nâœ… ì™„ë£Œ! ê²°ê³¼ ì €ì¥: extremenet_v65_results.json")
    print("\nğŸ’¡ ì¤‘ë‹¨ í›„ ì¬ì‹¤í–‰ì‹œ ìë™ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ì¬ê°œë©ë‹ˆë‹¤!")
    
    return model, results

if __name__ == "__main__":
    model, results = main()