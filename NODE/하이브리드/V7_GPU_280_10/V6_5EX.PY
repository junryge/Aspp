"""
ğŸ”¥ EXTREME_NET + UU1/UU2 íŒ¨í„´ í†µí•© ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
==========================================================
100ë¶„ ë°ì´í„° â†’ 10ë¶„ í›„ ì˜ˆì¸¡ + íŒ¨í„´ ê°ì§€
TensorFlow 2.16.1 CPU/GPU ì§€ì›
UU1: ê¸‰ì¦ ê°€ëŠ¥ íŒ¨í„´ / UU2: ê³ ê°’ ìœ ì§€ íŒ¨í„´
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import *
from tensorflow.keras.losses import Huber
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix
import warnings
import os
import gc
import pickle
import json
from datetime import datetime, timedelta

warnings.filterwarnings('ignore')
tf.random.set_seed(42)
np.random.seed(42)

# ====================================
# GPU ì„¤ì •
# ====================================
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    print(f"ğŸ”§ GPU {len(gpus)}ê°œ ë°œê²¬")
    for i, gpu in enumerate(gpus):
        tf.config.experimental.set_memory_growth(gpu, True)
        print(f"  GPU:{i} ë©”ëª¨ë¦¬ ë™ì  í• ë‹¹ í™œì„±í™”")
    strategy = tf.distribute.MirroredStrategy()
    device = f'GPU x {len(gpus)}ê°œ'
else:
    print("âš ï¸ GPU ì—†ìŒ, CPU ì‚¬ìš©")
    strategy = tf.distribute.get_strategy()
    device = 'CPU'

print("="*80)
print("ğŸ”¥ EXTREME_NET + UU1/UU2 íŒ¨í„´ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print(f"ğŸ”§ Device: {device}")
print("="*80)

# ====================================
# UU1/UU2 íŒ¨í„´ ê°ì§€ í´ë˜ìŠ¤
# ====================================
class PatternDetector:
    """UU1/UU2 íŒ¨í„´ ê°ì§€ ë° ë¶„ì„"""
    
    def __init__(self):
        self.pattern_history = []
        self.pattern_stats = {}
    
    def detect_pattern(self, df):
        """UU1/UU2 íŒ¨í„´ ê°ì§€"""
        print("\nğŸ” UU1/UU2 íŒ¨í„´ ê°ì§€ ì¤‘...")
        
        # í†µê³„ ê³„ì‚°
        totalcnt_max = df['TOTALCNT'].max()
        totalcnt_mean = df['TOTALCNT'].mean()
        m14b_mean = df['M14AM14B'].mean()
        m14b_max = df['M14AM14B'].max()
        high_cases = len(df[df['TOTALCNT'] >= 1682])
        critical_cases = len(df[df['TOTALCNT'] >= 1700])
        m14b_350_ratio = (df['M14AM14B'] > 350).sum() / len(df)
        m14b_400_ratio = (df['M14AM14B'] > 400).sum() / len(df)
        
        print(f"  TOTALCNT ìµœëŒ€: {totalcnt_max:,}")
        print(f"  TOTALCNT í‰ê· : {totalcnt_mean:.0f}")
        print(f"  M14AM14B í‰ê· : {m14b_mean:.0f}")
        print(f"  M14AM14B ìµœëŒ€: {m14b_max}")
        print(f"  1682+ ì¼€ì´ìŠ¤: {high_cases}ê°œ ({high_cases/len(df)*100:.1f}%)")
        print(f"  1700+ ì¼€ì´ìŠ¤: {critical_cases}ê°œ ({critical_cases/len(df)*100:.1f}%)")
        print(f"  M14B>350 ë¹„ìœ¨: {m14b_350_ratio:.1%}")
        print(f"  M14B>400 ë¹„ìœ¨: {m14b_400_ratio:.1%}")
        
        # íŒ¨í„´ íŒì • (ê°œì„ ëœ ë¡œì§)
        if high_cases > 5 and m14b_mean > 380:
            pattern = "UU2"
            confidence = min(0.95, high_cases / 10 * 0.1 + 0.5)
            print(f"  ğŸ”¥ UU2 íŒ¨í„´ ê°ì§€: ê³ ê°’ ìœ ì§€ ìƒíƒœ (ì‹ ë¢°ë„: {confidence:.1%})")
        elif m14b_350_ratio > 0.3 and totalcnt_mean > 1500:
            pattern = "UU2"
            confidence = min(0.9, m14b_350_ratio * 2)
            print(f"  ğŸ”¥ UU2 íŒ¨í„´ ê°ì§€: M14B ë†’ìŒ (ì‹ ë¢°ë„: {confidence:.1%})")
        elif m14b_400_ratio > 0.15:
            pattern = "UU2"
            confidence = min(0.85, m14b_400_ratio * 3)
            print(f"  ğŸ”¥ UU2 íŒ¨í„´ ê°ì§€: M14B ë§¤ìš° ë†’ìŒ (ì‹ ë¢°ë„: {confidence:.1%})")
        else:
            pattern = "UU1"
            confidence = 0.7 + (1 - m14b_350_ratio) * 0.3
            print(f"  ğŸ“ˆ UU1 íŒ¨í„´ ê°ì§€: ê¸‰ì¦ ê°€ëŠ¥ (ì‹ ë¢°ë„: {confidence:.1%})")
        
        # íŒ¨í„´ í†µê³„ ì €ì¥
        self.pattern_stats = {
            'pattern': pattern,
            'confidence': confidence,
            'totalcnt_max': totalcnt_max,
            'totalcnt_mean': totalcnt_mean,
            'm14b_mean': m14b_mean,
            'm14b_max': m14b_max,
            'high_cases': high_cases,
            'critical_cases': critical_cases,
            'm14b_350_ratio': m14b_350_ratio,
            'm14b_400_ratio': m14b_400_ratio
        }
        
        return pattern, confidence
    
    def calculate_pattern_features(self, sequence_data):
        """íŒ¨í„´ ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ"""
        features = []
        
        for seq in sequence_data:
            # ì‹œí€€ìŠ¤ë³„ íŒ¨í„´ íŠ¹ì§•
            m14b_values = seq[:, 1]  # M14AM14B ì»¬ëŸ¼ (ì¸ë±ìŠ¤ 1)
            totalcnt_values = seq[:, 0]  # TOTALCNT ì»¬ëŸ¼ (ì¸ë±ìŠ¤ 0)
            
            # UU2 ê°€ëŠ¥ì„± ì ìˆ˜
            uu2_score = 0
            if np.mean(m14b_values) > 380:
                uu2_score += 0.3
            if np.max(totalcnt_values) > 1682:
                uu2_score += 0.3
            if (m14b_values > 350).sum() / len(m14b_values) > 0.3:
                uu2_score += 0.4
            
            # UU1 ê°€ëŠ¥ì„± ì ìˆ˜
            uu1_score = 1 - uu2_score
            
            # ê¸‰ì¦ ìœ„í—˜ë„
            spike_risk = 0
            if m14b_values[-1] > 300 and totalcnt_values[-1] < 1400:
                spike_risk = 0.7
            elif m14b_values[-1] > 400:
                spike_risk = 0.9
            
            features.append([uu1_score, uu2_score, spike_risk])
        
        return np.array(features)

# ====================================
# í™•ë¥  ê³„ì‚° í´ë˜ìŠ¤
# ====================================
class ProbabilityCalculator:
    """íŒ¨í„´ ê¸°ë°˜ í™•ë¥  ê³„ì‚°"""
    
    @staticmethod
    def calculate_probabilities(predicted_value, pattern, confidence=1.0):
        """ì •ìƒ/ì£¼ì˜/ì‹¬ê° í™•ë¥  ê³„ì‚°"""
        
        if pattern == "UU1":
            # UU1: ê¸‰ì¦ ê°€ëŠ¥ íŒ¨í„´
            if predicted_value < 1400:
                probs = {'ì •ìƒ': 0.85, 'ì£¼ì˜': 0.15, 'ì‹¬ê°': 0.00}
            elif 1400 <= predicted_value < 1500:
                probs = {'ì •ìƒ': 0.10, 'ì£¼ì˜': 0.85, 'ì‹¬ê°': 0.05}
            elif 1500 <= predicted_value < 1600:
                probs = {'ì •ìƒ': 0.05, 'ì£¼ì˜': 0.80, 'ì‹¬ê°': 0.15}
            elif 1600 <= predicted_value < 1700:
                probs = {'ì •ìƒ': 0.00, 'ì£¼ì˜': 0.60, 'ì‹¬ê°': 0.40}
            elif 1700 <= predicted_value < 1800:
                probs = {'ì •ìƒ': 0.00, 'ì£¼ì˜': 0.20, 'ì‹¬ê°': 0.80}
            else:
                probs = {'ì •ìƒ': 0.00, 'ì£¼ì˜': 0.05, 'ì‹¬ê°': 0.95}
        else:  # UU2
            # UU2: ê³ ê°’ ìœ ì§€ íŒ¨í„´
            if predicted_value < 1400:
                probs = {'ì •ìƒ': 0.90, 'ì£¼ì˜': 0.10, 'ì‹¬ê°': 0.00}
            elif 1400 <= predicted_value < 1600:
                probs = {'ì •ìƒ': 0.10, 'ì£¼ì˜': 0.80, 'ì‹¬ê°': 0.10}
            elif 1600 <= predicted_value < 1700:
                probs = {'ì •ìƒ': 0.00, 'ì£¼ì˜': 0.50, 'ì‹¬ê°': 0.50}
            else:
                probs = {'ì •ìƒ': 0.00, 'ì£¼ì˜': 0.20, 'ì‹¬ê°': 0.80}
        
        # ì‹ ë¢°ë„ ë°˜ì˜
        for key in probs:
            probs[key] *= confidence
        
        # ì •ê·œí™”
        total = sum(probs.values())
        if total > 0:
            for key in probs:
                probs[key] /= total
        
        return probs
    
    @staticmethod
    def pattern_based_prediction_adjustment(base_prediction, pattern, m14b, m14a, consecutive_rises):
        """íŒ¨í„´ ê¸°ë°˜ ì˜ˆì¸¡ê°’ ì¡°ì •"""
        
        ratio = m14b / (m14a + 1)
        
        if pattern == "UU1":
            # UU1: ê¸‰ì¦ ê°€ëŠ¥ íŒ¨í„´
            if m14b > 300 and m14a < 80:  # í™©ê¸ˆ íŒ¨í„´
                adjustment = 1.15
            elif consecutive_rises >= 10:
                adjustment = 1.12
            elif ratio > 4:
                adjustment = 1.10
            elif m14b >= 400:
                adjustment = 1.08
            elif m14b >= 350:
                adjustment = 1.05
            elif m14b >= 300:
                adjustment = 1.03
            else:
                adjustment = 1.02
        else:  # UU2
            # UU2: ë³´ìˆ˜ì  ì˜ˆì¸¡
            if m14b >= 450:
                adjustment = 1.02
            elif m14b >= 400:
                adjustment = 1.01
            else:
                adjustment = 0.99
        
        return base_prediction * adjustment

# ====================================
# EXTREME_NET ëª¨ë¸ ì •ì˜ (íŒ¨í„´ ì¸ì‹ ê°•í™”)
# ====================================
def build_extreme_net_with_pattern(input_shape, pattern_shape=(3,)):
    """
    EXTREME_NET - LSTM + Multi-Head Attention + Pattern Recognition
    """
    # ë©”ì¸ ì‹œí€€ìŠ¤ ì…ë ¥
    main_inputs = Input(shape=input_shape, name='main_inputs')
    # íŒ¨í„´ íŠ¹ì§• ì…ë ¥
    pattern_inputs = Input(shape=pattern_shape, name='pattern_inputs')
    
    # LSTM ë¸Œëœì¹˜ (ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ)
    lstm = LSTM(64, return_sequences=True, name='lstm1')(main_inputs)
    lstm = LSTM(32, name='lstm2')(lstm)
    
    # Attention ë¸Œëœì¹˜ (ì¤‘ìš” ì‹œì  í¬ì°©)
    attn = MultiHeadAttention(
        num_heads=4, 
        key_dim=16,
        name='multi_head_attention'
    )(main_inputs, main_inputs)
    attn = GlobalAveragePooling1D(name='global_avg_pool')(attn)
    
    # íŒ¨í„´ ì²˜ë¦¬
    pattern_dense = Dense(16, activation='relu', name='pattern_dense')(pattern_inputs)
    
    # ìœµí•©
    x = Concatenate(name='fusion')([lstm, attn, pattern_dense])
    x = BatchNormalization(name='batch_norm')(x)
    x = Dense(64, activation='relu', name='dense1')(x)
    x = Dropout(0.2, name='dropout1')(x)
    x = Dense(32, activation='relu', name='dense2')(x)
    x = Dropout(0.1, name='dropout2')(x)
    
    # ì¶œë ¥ì¸µ
    out_reg = Dense(1, name='regression')(x)  # íšŒê·€ (ë¬¼ë¥˜ëŸ‰ ì˜ˆì¸¡)
    out_cls = Dense(3, activation='softmax', name='classification')(x)  # ë¶„ë¥˜ (êµ¬ê°„)
    out_pattern = Dense(2, activation='softmax', name='pattern_output')(x)  # íŒ¨í„´ ë¶„ë¥˜ (UU1/UU2)
    
    model = Model([main_inputs, pattern_inputs], [out_reg, out_cls, out_pattern], name='ExtremeNetPattern')
    
    print("\nğŸ“Š EXTREME_NET + Pattern êµ¬ì¡°:")
    print(f"  - LSTM ë ˆì´ì–´: 64 â†’ 32")
    print(f"  - Attention Heads: 4ê°œ (key_dim=16)")
    print(f"  - Pattern Dense: 16")
    print(f"  - ìœµí•© Dense: 64 â†’ 32")
    print(f"  - Dropout: 20% â†’ 10%")
    print(f"  - ì¶œë ¥: íšŒê·€ + 3êµ¬ê°„ ë¶„ë¥˜ + UU1/UU2 íŒ¨í„´")
    
    return model

# ====================================
# ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤ (íŒ¨í„´ í†µí•©)
# ====================================
class ExtremeDataProcessorWithPattern:
    def __init__(self, seq_len=100, pred_len=10):
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.feature_scaler = StandardScaler()
        self.target_scaler = RobustScaler()
        self.pattern_detector = PatternDetector()
        self.prob_calculator = ProbabilityCalculator()
        self.feature_columns = None
        self.statistics = {}
        self.pattern_stats = {}
        
    def save_all(self, path='extreme_net_pattern_model/'):
        """ëª¨ë“  ì„¤ì • ì €ì¥"""
        os.makedirs(path, exist_ok=True)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        with open(f'{path}feature_scaler.pkl', 'wb') as f:
            pickle.dump(self.feature_scaler, f)
        with open(f'{path}target_scaler.pkl', 'wb') as f:
            pickle.dump(self.target_scaler, f)
            
        # ì„¤ì • ì €ì¥
        config = {
            'seq_len': self.seq_len,
            'pred_len': self.pred_len,
            'feature_columns': self.feature_columns,
            'statistics': self.statistics,
            'pattern_stats': self.pattern_detector.pattern_stats
        }
        with open(f'{path}config.json', 'w') as f:
            json.dump(config, f, indent=2)
            
        print(f"âœ… ëª¨ë“  ì„¤ì • ì €ì¥: {path}")
    
    def load_all(self, path='extreme_net_pattern_model/'):
        """ëª¨ë“  ì„¤ì • ë¡œë“œ"""
        with open(f'{path}feature_scaler.pkl', 'rb') as f:
            self.feature_scaler = pickle.load(f)
        with open(f'{path}target_scaler.pkl', 'rb') as f:
            self.target_scaler = pickle.load(f)
        with open(f'{path}config.json', 'r') as f:
            config = json.load(f)
            self.seq_len = config['seq_len']
            self.pred_len = config['pred_len']
            self.feature_columns = config['feature_columns']
            self.statistics = config['statistics']
            self.pattern_stats = config.get('pattern_stats', {})
        print(f"âœ… ëª¨ë“  ì„¤ì • ë¡œë“œ: {path}")
        
    def load_and_process(self, filepath):
        print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {filepath}")
        df = pd.read_csv(filepath)
        print(f"âœ… ì›ë³¸: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")
        
        # 0ê°’ ì œê±°
        before = len(df)
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        if before - len(df) > 0:
            print(f"âœ… 0ê°’ ì œê±°: {before - len(df):,}ê°œ")
        
        # ì‹œê°„ ì •ë ¬
        if 'CURRTIME' in df.columns:
            df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                           format='%Y%m%d%H%M', errors='coerce')
            df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        # ì „ì²´ ë°ì´í„° íŒ¨í„´ ê°ì§€
        pattern, confidence = self.pattern_detector.detect_pattern(df)
        
        # í†µê³„ ì €ì¥
        self.statistics = {
            'mean': float(df['TOTALCNT'].mean()),
            'std': float(df['TOTALCNT'].std()),
            'min': float(df['TOTALCNT'].min()),
            'max': float(df['TOTALCNT'].max()),
            'median': float(df['TOTALCNT'].median()),
            'pattern': pattern,
            'pattern_confidence': confidence
        }
        
        print(f"\nğŸ“Š TOTALCNT í†µê³„:")
        print(f"  í‰ê· : {self.statistics['mean']:,.0f}")
        print(f"  í‘œì¤€í¸ì°¨: {self.statistics['std']:,.0f}")
        print(f"  ìµœì†Œ: {self.statistics['min']:,}")
        print(f"  ìµœëŒ€: {self.statistics['max']:,}")
        print(f"  ì¤‘ì•™ê°’: {self.statistics['median']:,}")
        print(f"  ì£¼ìš” íŒ¨í„´: {pattern} (ì‹ ë¢°ë„: {confidence:.1%})")
        
        # êµ¬ê°„ ë¶„í¬
        normal = (df['TOTALCNT'] < 1400).sum()
        warning = ((df['TOTALCNT'] >= 1400) & (df['TOTALCNT'] < 1700)).sum()
        critical = (df['TOTALCNT'] >= 1700).sum()
        
        print(f"\nğŸ“Š 3êµ¬ê°„ ë¶„í¬:")
        print(f"  Level 0 (ì •ìƒ < 1400): {normal:,}ê°œ ({normal/len(df)*100:.1f}%)")
        print(f"  Level 1 (ì£¼ì˜ 1400-1699): {warning:,}ê°œ ({warning/len(df)*100:.1f}%)")
        print(f"  Level 2 (ìœ„í—˜ â‰¥ 1700): {critical:,}ê°œ ({critical/len(df)*100:.1f}%)")
        
        # ê¸‰ì¦ íŒ¨í„´ ê°ì§€
        spike_count = 0
        consecutive_rises = 0
        max_consecutive = 0
        
        for i in range(1, len(df)):
            if df.loc[i, 'TOTALCNT'] > df.loc[i-1, 'TOTALCNT']:
                consecutive_rises += 1
                max_consecutive = max(max_consecutive, consecutive_rises)
            else:
                consecutive_rises = 0
            
            if i >= 10 and df.loc[i, 'TOTALCNT'] - df.loc[i-10, 'TOTALCNT'] > 100:
                spike_count += 1
        
        print(f"\nâš ï¸ íŒ¨í„´ ë¶„ì„:")
        print(f"  ê¸‰ì¦ íŒ¨í„´ (10ë¶„ê°„ 100+ ì¦ê°€): {spike_count}íšŒ")
        print(f"  ìµœëŒ€ ì—°ì† ìƒìŠ¹: {max_consecutive}íšŒ")
        
        return df
    
    def create_features(self, df):
        print("\nâš™ï¸ EXTREME_NET + Patternìš© íŠ¹ì„± ìƒì„±...")
        
        # í•µì‹¬ íŠ¹ì„±
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        df['SPIKE'] = ((df['M14AM14B'] / (df['M14AM10A'] + 1)) > 4).astype(float)
        
        # UU1/UU2 íŒ¨í„´ íŠ¹ì§•
        df['UU2_SIGNAL'] = ((df['M14AM14B'] > 350) | (df['TOTALCNT'] > 1682)).astype(float)
        df['UU1_SIGNAL'] = ((df['M14AM14B'] <= 350) & (df['TOTALCNT'] < 1682)).astype(float)
        df['PATTERN_RISK'] = df['M14AM14B'].rolling(10, min_periods=1).mean() / 350  # íŒ¨í„´ ìœ„í—˜ë„
        
        # ì‹œê°„ íŠ¹ì„±
        if 'CURRTIME' in df.columns:
            df['HOUR'] = df['CURRTIME'].dt.hour
            df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
            df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
        else:
            df['HOUR_SIN'] = 0
            df['HOUR_COS'] = 1
        
        # ì´ë™í‰ê·  & ë³€ë™ì„±
        for w in [5, 10, 20, 30, 50]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
        
        # ë³€í™”ìœ¨
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_5'] = df['TOTALCNT'].diff(5).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        df['CHANGE_RATE'] = df['TOTALCNT'].pct_change(10).fillna(0) * 100
        
        # ì—°ì† ìƒìŠ¹ ì¹´ìš´íŠ¸
        df['CONSECUTIVE_RISE'] = 0
        consecutive = 0
        for i in range(1, len(df)):
            if df.loc[i, 'TOTALCNT'] > df.loc[i-1, 'TOTALCNT']:
                consecutive += 1
            else:
                consecutive = 0
            df.loc[i, 'CONSECUTIVE_RISE'] = consecutive
        
        # ì»¬ëŸ¼ë³„ íŠ¹ì„±
        for col in ['M14AM14B', 'M14AM14BSUM', 'M14AM10A', 'M14AM16']:
            if col in df.columns:
                df[f'{col}_MA10'] = df[col].rolling(10, min_periods=1).mean()
                df[f'{col}_CHANGE'] = df[col].diff(10).fillna(0)
        
        # íƒ€ê²Ÿ ìƒì„±
        df['TARGET'] = df['TOTALCNT'].shift(-self.pred_len)
        df['LEVEL'] = 0
        df.loc[df['TARGET'] >= 1400, 'LEVEL'] = 1
        df.loc[df['TARGET'] >= 1700, 'LEVEL'] = 2
        
        # ì´ìƒì‹ í˜¸ ê°ì§€
        df['ANOMALY'] = ((df['TARGET'] >= 1651) & (df['TARGET'] <= 1682)).astype(int)
        
        # íŒ¨í„´ íƒ€ê²Ÿ (UU1=0, UU2=1)
        df['PATTERN_TARGET'] = 0
        # íƒ€ê²Ÿ ì‹œì ì˜ íŒ¨í„´ íŒì •
        for i in range(len(df) - self.pred_len):
            future_window = df.iloc[i:i+self.pred_len+50]
            if len(future_window) > 0:
                future_m14b_mean = future_window['M14AM14B'].mean()
                future_high_ratio = (future_window['TOTALCNT'] > 1682).mean()
                if future_m14b_mean > 380 or future_high_ratio > 0.1:
                    df.loc[i, 'PATTERN_TARGET'] = 1
        
        df = df.dropna()
        print(f"âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(df.columns)}ê°œ ì»¬ëŸ¼")
        print(f"âœ… ìœ íš¨ ë°ì´í„°: {len(df):,}í–‰")
        
        return df
    
    def create_sequences(self, df):
        print("\nğŸ”„ EXTREME_NET + Pattern ì‹œí€€ìŠ¤ ìƒì„±...")
        
        # íŠ¹ì„± ì„ íƒ
        base_features = [
            'TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M14AM10A', 'M14AM16',
            'RATIO', 'GOLDEN', 'SPIKE',
            'UU1_SIGNAL', 'UU2_SIGNAL', 'PATTERN_RISK',
            'HOUR_SIN', 'HOUR_COS',
            'CONSECUTIVE_RISE'
        ]
        
        ma_features = [f'MA_{w}' for w in [5, 10, 20, 30, 50]]
        std_features = [f'STD_{w}' for w in [5, 10, 20, 30, 50]]
        change_features = ['CHANGE_1', 'CHANGE_5', 'CHANGE_10', 'CHANGE_RATE']
        
        col_features = []
        for col in ['M14AM14B', 'M14AM14BSUM', 'M14AM10A', 'M14AM16']:
            if f'{col}_MA10' in df.columns:
                col_features.extend([f'{col}_MA10', f'{col}_CHANGE'])
        
        self.feature_columns = base_features + ma_features + std_features + change_features + col_features
        self.feature_columns = [f for f in self.feature_columns if f in df.columns]
        
        print(f"  ì„ íƒëœ íŠ¹ì„±: {len(self.feature_columns)}ê°œ")
        
        X_data = df[self.feature_columns].values.astype(np.float32)
        y_reg = df['TARGET'].values.astype(np.float32)
        y_cls = df['LEVEL'].values.astype(np.int32)
        y_pattern = df['PATTERN_TARGET'].values.astype(np.int32)
        y_anomaly = df['ANOMALY'].values.astype(np.int32)
        
        # ë¶„í•  ë¹„ìœ¨
        total_samples = len(X_data) - self.seq_len
        train_end = int(total_samples * 0.7)
        val_end = int(total_samples * 0.85)
        
        # ìŠ¤ì¼€ì¼ë§
        print("  ìŠ¤ì¼€ì¼ë§ ì ìš©...")
        self.feature_scaler.fit(X_data[:train_end + self.seq_len])
        self.target_scaler.fit(y_reg[:train_end].reshape(-1, 1))
        
        X_scaled = self.feature_scaler.transform(X_data)
        y_reg_scaled = self.target_scaler.transform(y_reg.reshape(-1, 1)).flatten()
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, X_pattern, y_r, y_c, y_p, y_a = [], [], [], [], [], []
        for i in range(total_samples):
            seq = X_scaled[i:i+self.seq_len]
            X.append(seq)
            
            # íŒ¨í„´ íŠ¹ì§• ê³„ì‚°
            pattern_feat = self.pattern_detector.calculate_pattern_features(seq.reshape(1, self.seq_len, -1))[0]
            X_pattern.append(pattern_feat)
            
            y_r.append(y_reg_scaled[i+self.seq_len-1])
            y_c.append(y_cls[i+self.seq_len-1])
            y_p.append(y_pattern[i+self.seq_len-1])
            y_a.append(y_anomaly[i+self.seq_len-1])
        
        X = np.array(X, dtype=np.float32)
        X_pattern = np.array(X_pattern, dtype=np.float32)
        y_r = np.array(y_r, dtype=np.float32)
        y_c = np.array(y_c, dtype=np.int32)
        y_p = np.array(y_p, dtype=np.int32)
        y_a = np.array(y_a, dtype=np.int32)
        
        # ì‹œê³„ì—´ ìˆœì„œ ìœ ì§€ ë¶„í• 
        X_train = [X[:train_end], X_pattern[:train_end]]
        X_val = [X[train_end:val_end], X_pattern[train_end:val_end]]
        X_test = [X[val_end:], X_pattern[val_end:]]
        
        y_train = (y_r[:train_end], y_c[:train_end], y_p[:train_end], y_a[:train_end])
        y_val = (y_r[train_end:val_end], y_c[train_end:val_end], y_p[train_end:val_end], y_a[train_end:val_end])
        y_test = (y_r[val_end:], y_c[val_end:], y_p[val_end:], y_a[val_end:])
        
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
        print(f"  Train: {X_train[0].shape} (70%)")
        print(f"  Val: {X_val[0].shape} (15%)")
        print(f"  Test: {X_test[0].shape} (15%)")
        
        # íŒ¨í„´ ë¶„í¬ í™•ì¸
        print(f"\nğŸ“Š íŒ¨í„´ ë¶„í¬:")
        for name, y_pat in [('Train', y_train[2]), ('Val', y_val[2]), ('Test', y_test[2])]:
            uu1_count = (y_pat == 0).sum()
            uu2_count = (y_pat == 1).sum()
            print(f"  {name}: UU1={uu1_count:,}ê°œ ({uu1_count/len(y_pat)*100:.1f}%), " +
                  f"UU2={uu2_count:,}ê°œ ({uu2_count/len(y_pat)*100:.1f}%)")
        
        # ëª¨ë“  ì„¤ì • ì €ì¥
        self.save_all()
        
        return (X_train, y_train), (X_val, y_val), (X_test, y_test)

# ====================================
# EXTREME_NET + Pattern í•™ìŠµ í•¨ìˆ˜
# ====================================
def train_extreme_net_with_pattern(train_data, val_data, test_data, processor):
    X_train, y_train = train_data
    X_val, y_val = val_data
    X_test, y_test = test_data
    
    # ëª¨ë¸ ìƒì„±
    main_shape = (X_train[0].shape[1], X_train[0].shape[2])
    pattern_shape = (X_train[1].shape[1],)
    
    with strategy.scope():
        model = build_extreme_net_with_pattern(main_shape, pattern_shape)
        
        # ì»´íŒŒì¼
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss={
                'regression': 'mae',
                'classification': 'sparse_categorical_crossentropy',
                'pattern_output': 'sparse_categorical_crossentropy'
            },
            loss_weights={
                'regression': 0.5,
                'classification': 0.3,
                'pattern_output': 0.2
            },
            metrics={
                'regression': ['mae', 'mse'],
                'classification': ['accuracy'],
                'pattern_output': ['accuracy']
            }
        )
    
    print("\n" + "="*60)
    print("ğŸ¯ EXTREME_NET + Pattern í•™ìŠµ ì‹œì‘")
    print("="*60)
    
    # ì½œë°± ì„¤ì •
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=20,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=10,
            min_lr=1e-6,
            verbose=1
        ),
        ModelCheckpoint(
            'extreme_net_pattern_model/best_model.keras',
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        )
    ]
    
    # ë°°ì¹˜ í¬ê¸°
    batch_size = 64 * strategy.num_replicas_in_sync if gpus else 32
    
    # í•™ìŠµ
    print(f"\nğŸ“ˆ í•™ìŠµ ì„¤ì •:")
    print(f"  Batch Size: {batch_size}")
    print(f"  Epochs: 60 (Early Stopping)")
    print(f"  Learning Rate: 0.001 (ReduceLROnPlateau)")
    
    history = model.fit(
        X_train,
        {
            'regression': y_train[0],
            'classification': y_train[1],
            'pattern_output': y_train[2]
        },
        validation_data=(
            X_val,
            {
                'regression': y_val[0],
                'classification': y_val[1],
                'pattern_output': y_val[2]
            }
        ),
        epochs=60,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=1
    )
    
    # í‰ê°€
    print("\n" + "="*60)
    print("ğŸ“Š EXTREME_NET + Pattern ì„±ëŠ¥ í‰ê°€")
    print("="*60)
    
    # ì˜ˆì¸¡
    preds = model.predict(X_test, batch_size=batch_size, verbose=0)
    y_reg_pred = preds[0].flatten()
    y_cls_pred = np.argmax(preds[1], axis=1)
    y_pat_pred = np.argmax(preds[2], axis=1)
    
    # ì—­ë³€í™˜
    y_reg_true_orig = processor.target_scaler.inverse_transform(
        y_test[0].reshape(-1, 1)
    ).flatten()
    y_reg_pred_orig = processor.target_scaler.inverse_transform(
        y_reg_pred.reshape(-1, 1)
    ).flatten()
    
    # íŒ¨í„´ ê¸°ë°˜ ì¡°ì •
    print("\nğŸ”§ íŒ¨í„´ ê¸°ë°˜ ì˜ˆì¸¡ê°’ ì¡°ì •...")
    adjusted_predictions = []
    for i in range(len(y_reg_pred_orig)):
        pattern = 'UU2' if y_pat_pred[i] == 1 else 'UU1'
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ M14B, M14A ê°’ ì¶”ì¶œ (ì„ì‹œ ê°’ ì‚¬ìš©)
        m14b = 350  # ì‹¤ì œ êµ¬í˜„ì‹œ X_testì—ì„œ ì¶”ì¶œ
        m14a = 75
        consecutive = 5
        
        adjusted = processor.prob_calculator.pattern_based_prediction_adjustment(
            y_reg_pred_orig[i], pattern, m14b, m14a, consecutive
        )
        adjusted_predictions.append(adjusted)
    
    adjusted_predictions = np.array(adjusted_predictions)
    
    # ì¡°ì •ëœ ì˜ˆì¸¡ ë©”íŠ¸ë¦­
    mae_adjusted = mean_absolute_error(y_reg_true_orig, adjusted_predictions)
    rmse_adjusted = np.sqrt(mean_squared_error(y_reg_true_orig, adjusted_predictions))
    r2_adjusted = r2_score(y_reg_true_orig, adjusted_predictions)
    
    # ê¸°ë³¸ íšŒê·€ ë©”íŠ¸ë¦­
    mae = mean_absolute_error(y_reg_true_orig, y_reg_pred_orig)
    rmse = np.sqrt(mean_squared_error(y_reg_true_orig, y_reg_pred_orig))
    r2 = r2_score(y_reg_true_orig, y_reg_pred_orig)
    mape = np.mean(np.abs((y_reg_true_orig - y_reg_pred_orig) / y_reg_true_orig)) * 100
    
    print(f"\nğŸ“ˆ íšŒê·€ ì„±ëŠ¥ (ê¸°ë³¸):")
    print(f"  MAE: {mae:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  RÂ²: {r2:.4f}")
    print(f"  MAPE: {mape:.2f}%")
    
    print(f"\nğŸ“ˆ íšŒê·€ ì„±ëŠ¥ (íŒ¨í„´ ì¡°ì •):")
    print(f"  MAE: {mae_adjusted:.2f} ({mae_adjusted-mae:+.2f})")
    print(f"  RMSE: {rmse_adjusted:.2f} ({rmse_adjusted-rmse:+.2f})")
    print(f"  RÂ²: {r2_adjusted:.4f} ({r2_adjusted-r2:+.4f})")
    
    # íŒ¨í„´ ì˜ˆì¸¡ ì •í™•ë„
    pattern_acc = accuracy_score(y_test[2], y_pat_pred)
    print(f"\nğŸ” UU1/UU2 íŒ¨í„´ ì˜ˆì¸¡ ì •í™•ë„: {pattern_acc:.4f}")
    
    # íŒ¨í„´ë³„ ì„±ëŠ¥ ë¶„ì„
    for pat_val, pat_name in [(0, 'UU1'), (1, 'UU2')]:
        mask = (y_test[2] == pat_val)
        if mask.sum() > 0:
            pat_mae = mean_absolute_error(y_reg_true_orig[mask], adjusted_predictions[mask])
            print(f"  {pat_name} MAE: {pat_mae:.2f} (n={mask.sum()})")
    
    # ë¶„ë¥˜ ë©”íŠ¸ë¦­
    acc = accuracy_score(y_test[1], y_cls_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_test[1], y_cls_pred, average='weighted'
    )
    
    print(f"\nğŸ“Š 3êµ¬ê°„ ë¶„ë¥˜ ì„±ëŠ¥:")
    print(f"  Accuracy: {acc:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1-Score: {f1:.4f}")
    
    # êµ¬ê°„ë³„ ì„±ëŠ¥
    cm = confusion_matrix(y_test[1], y_cls_pred)
    print(f"\nğŸ“‹ Confusion Matrix:")
    print("  ì‹¤ì œ\\ì˜ˆì¸¡  Level0  Level1  Level2")
    for i in range(3):
        print(f"  Level{i}:   {cm[i, 0]:6d}  {cm[i, 1]:6d}  {cm[i, 2]:6d}")
    
    # ìœ„í—˜ ê°ì§€ ì„±ëŠ¥
    if 2 in y_test[1]:
        danger_true = (y_test[1] == 2)
        danger_pred = (y_cls_pred == 2)
        
        tp = np.sum(danger_true & danger_pred)
        fp = np.sum(~danger_true & danger_pred)
        fn = np.sum(danger_true & ~danger_pred)
        
        danger_precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        danger_recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        danger_f1 = 2 * (danger_precision * danger_recall) / (danger_precision + danger_recall) \
                    if (danger_precision + danger_recall) > 0 else 0
        
        print(f"\nâš ï¸ ìœ„í—˜ êµ¬ê°„(Level 2) ê°ì§€ ì„±ëŠ¥:")
        print(f"  Precision: {danger_precision:.4f}")
        print(f"  Recall: {danger_recall:.4f}")
        print(f"  F1-Score: {danger_f1:.4f}")
        print(f"  ì •í™• ì˜ˆì¸¡: {tp}ê°œ, ì˜¤íƒ: {fp}ê°œ, ë†“ì¹¨: {fn}ê°œ")
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs('extreme_net_pattern_model', exist_ok=True)
    model.save('extreme_net_pattern_model/extreme_net_pattern_final.keras')
    print(f"\nğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: extreme_net_pattern_model/extreme_net_pattern_final.keras")
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'MAE': mae_adjusted,
        'RMSE': rmse_adjusted,
        'R2': r2_adjusted,
        'MAPE': mape,
        'Pattern_Accuracy': pattern_acc,
        'Classification_Accuracy': acc,
        'Precision': precision,
        'Recall': recall,
        'F1_Score': f1
    }
    
    # CSV ì €ì¥
    pd.DataFrame([results]).to_csv('extreme_net_pattern_results.csv', index=False)
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: extreme_net_pattern_results.csv")
    
    return model, results, history

# ====================================
# ì‹¤ì‹œê°„ ì˜ˆì¸¡ í´ë˜ìŠ¤ (íŒ¨í„´ í†µí•©)
# ====================================
class ExtremeNetPatternPredictor:
    def __init__(self, model_path='extreme_net_pattern_model/extreme_net_pattern_final.keras'):
        """EXTREME_NET + Pattern ì‹¤ì‹œê°„ ì˜ˆì¸¡ê¸°"""
        self.model = tf.keras.models.load_model(model_path)
        self.processor = ExtremeDataProcessorWithPattern()
        self.processor.load_all('extreme_net_pattern_model/')
        print(f"âœ… EXTREME_NET + Pattern ì˜ˆì¸¡ê¸° ì¤€ë¹„ ì™„ë£Œ")
        
    def predict(self, data_100min):
        """100ë¶„ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡"""
        # ë°ì´í„° ê²€ì¦
        if len(data_100min) != 100:
            raise ValueError(f"100ë¶„ ë°ì´í„° í•„ìš” (í˜„ì¬: {len(data_100min)})")
        
        # íŒ¨í„´ ê°ì§€
        pattern, confidence = self.processor.pattern_detector.detect_pattern(data_100min)
        
        # íŠ¹ì„± ì¶”ì¶œ
        X = data_100min[self.processor.feature_columns].values
        
        # ìŠ¤ì¼€ì¼ë§
        X_scaled = self.processor.feature_scaler.transform(X)
        X_seq = X_scaled.reshape(1, 100, -1).astype(np.float32)
        
        # íŒ¨í„´ íŠ¹ì§•
        X_pattern = self.processor.pattern_detector.calculate_pattern_features(X_seq)
        
        # ì˜ˆì¸¡
        preds = self.model.predict([X_seq, X_pattern], verbose=0)
        y_reg_scaled = preds[0][0, 0]
        y_cls_probs = preds[1][0]
        y_pat_probs = preds[2][0]
        
        y_cls = np.argmax(y_cls_probs)
        y_pat = np.argmax(y_pat_probs)
        
        # ì—­ë³€í™˜
        y_reg = self.processor.target_scaler.inverse_transform([[y_reg_scaled]])[0, 0]
        
        # íŒ¨í„´ ê¸°ë°˜ ì¡°ì •
        m14b = data_100min['M14AM14B'].iloc[-1]
        m14a = data_100min['M14AM10A'].iloc[-1]
        
        # ì—°ì† ìƒìŠ¹ ê³„ì‚°
        consecutive = 0
        for i in range(len(data_100min)-1, 0, -1):
            if data_100min['TOTALCNT'].iloc[i] > data_100min['TOTALCNT'].iloc[i-1]:
                consecutive += 1
            else:
                break
        
        # ì˜ˆì¸¡ê°’ ì¡°ì •
        adjusted = self.processor.prob_calculator.pattern_based_prediction_adjustment(
            y_reg, pattern, m14b, m14a, consecutive
        )
        
        # í™•ë¥  ê³„ì‚°
        probs = self.processor.prob_calculator.calculate_probabilities(adjusted, pattern, confidence)
        
        # ë ˆë²¨ ì •ì˜
        levels = {
            0: 'ì •ìƒ (< 1400)',
            1: 'ì£¼ì˜ (1400-1699)',
            2: 'ìœ„í—˜ (â‰¥ 1700)'
        }
        
        # ìƒíƒœ íŒì •
        if adjusted >= 1700:
            status = "ğŸ”´ ì‹¬ê° - ë¬¼ë¥˜ëŸ‰ ê¸‰ì¦!"
        elif adjusted >= 1400:
            status = "ğŸŸ¡ ì£¼ì˜ - ë¬¼ë¥˜ëŸ‰ ì¦ê°€ ëŒ€ë¹„"
        else:
            status = "ğŸŸ¢ ì •ìƒ - ì•ˆì •ì "
        
        # ì´ìƒì‹ í˜¸ ê°ì§€
        is_anomaly = (1651 <= adjusted <= 1682)
        
        # ê¸‰ì¦ ì—¬ë¶€
        current = data_100min['TOTALCNT'].iloc[-1]
        is_spike = (adjusted - current > 100)
        
        return {
            'prediction': float(adjusted),
            'prediction_original': float(y_reg),
            'current': float(current),
            'change': float(adjusted - current),
            'change_rate': float((adjusted - current) / current * 100),
            'pattern': pattern,
            'pattern_confidence': confidence,
            'pattern_probs': {
                'UU1': float(y_pat_probs[0]),
                'UU2': float(y_pat_probs[1])
            },
            'level': y_cls,
            'level_name': levels[y_cls],
            'level_probs': {
                'ì •ìƒ': float(probs['ì •ìƒ']),
                'ì£¼ì˜': float(probs['ì£¼ì˜']),
                'ì‹¬ê°': float(probs['ì‹¬ê°'])
            },
            'status': status,
            'confidence': float(np.max(y_cls_probs)),
            'is_anomaly': is_anomaly,
            'is_spike': is_spike,
            'consecutive_rises': consecutive,
            'm14b_m14a_ratio': float(m14b / (m14a + 1)),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

# ====================================
# ë©”ì¸ ì‹¤í–‰
# ====================================
def main():
    print("\n" + "="*80)
    print("ğŸ”¥ EXTREME_NET + UU1/UU2 íŒ¨í„´ í•™ìŠµ ì‹œì‘")
    print("="*80)
    
    # ë°ì´í„° ì°¾ê¸°
    data_paths = [
        '/mnt/user-data/uploads/gs.CSV',
        'data/20240201_TO_202507281705.csv',
        'data/20250731_to20250806.csv',
        'data/20250807_DATA.CSV',
        'uu.csv',
        'uu2.csv',
        'data.csv'
    ]
    
    data_path = None
    for path in data_paths:
        if os.path.exists(path):
            data_path = path
            print(f"âœ… ë°ì´í„° ë°œê²¬: {path}")
            break
    
    if not data_path:
        print("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("ë‹¤ìŒ ê²½ë¡œ ì¤‘ í•˜ë‚˜ì— ë°ì´í„°ë¥¼ ë°°ì¹˜í•˜ì„¸ìš”:")
        for p in data_paths:
            print(f"  - {p}")
        return
    
    # ë°ì´í„° ì²˜ë¦¬
    processor = ExtremeDataProcessorWithPattern()
    df = processor.load_and_process(data_path)
    df = processor.create_features(df)
    train_data, val_data, test_data = processor.create_sequences(df)
    
    # EXTREME_NET + Pattern í•™ìŠµ
    model, results, history = train_extreme_net_with_pattern(
        train_data, val_data, test_data, processor
    )
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ† EXTREME_NET + Pattern ìµœì¢… ì„±ëŠ¥")
    print("="*80)
    
    print("\nğŸ“Š í•µì‹¬ ì§€í‘œ:")
    print(f"  ì˜ˆì¸¡ ì •í™•ë„ (RÂ²): {results['R2']:.4f}")
    print(f"  í‰ê·  ì˜¤ì°¨ (MAE): {results['MAE']:.2f}")
    print(f"  3êµ¬ê°„ ì •í™•ë„: {results['Classification_Accuracy']:.4f}")
    print(f"  íŒ¨í„´ ì˜ˆì¸¡ ì •í™•ë„: {results['Pattern_Accuracy']:.4f}")
    
    # ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    print("\n" + "="*80)
    print("ğŸ”® ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ (UU1/UU2 íŒ¨í„´ ì ìš©)")
    print("="*80)
    
    predictor = ExtremeNetPatternPredictor()
    
    # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ
    if len(df) >= 100:
        test_sample = df.iloc[-100:].copy()
        result = predictor.predict(test_sample)
        
        print(f"\nğŸ“ˆ ì˜ˆì¸¡ ê²°ê³¼:")
        print(f"  íŒ¨í„´: {result['pattern']} (ì‹ ë¢°ë„: {result['pattern_confidence']:.1%})")
        print(f"  í˜„ì¬: {result['current']:.0f}")
        print(f"  10ë¶„ í›„ ì˜ˆì¸¡: {result['prediction']:.0f}")
        print(f"  ë³€í™”ëŸ‰: {result['change']:+.0f} ({result['change_rate']:+.1f}%)")
        print(f"  ìƒíƒœ: {result['status']}")
        
        print(f"\n  í™•ë¥  ë¶„í¬:")
        print(f"    ğŸŸ¢ ì •ìƒ: {result['level_probs']['ì •ìƒ']:.1%}")
        print(f"    ğŸŸ¡ ì£¼ì˜: {result['level_probs']['ì£¼ì˜']:.1%}")
        print(f"    ğŸ”´ ì‹¬ê°: {result['level_probs']['ì‹¬ê°']:.1%}")
        
        print(f"\n  íŒ¨í„´ í™•ë¥ :")
        print(f"    UU1 (ê¸‰ì¦ ê°€ëŠ¥): {result['pattern_probs']['UU1']:.1%}")
        print(f"    UU2 (ê³ ê°’ ìœ ì§€): {result['pattern_probs']['UU2']:.1%}")
        
        print(f"\n  ì¶”ê°€ ì •ë³´:")
        print(f"    ì—°ì† ìƒìŠ¹: {result['consecutive_rises']}íšŒ")
        print(f"    M14B/M14A ë¹„ìœ¨: {result['m14b_m14a_ratio']:.2f}")
        
        if result['is_spike']:
            print(f"    âš ï¸ ê¸‰ì¦ ê²½ê³ !")
        if result['is_anomaly']:
            print(f"    âš ï¸ ì´ìƒì‹ í˜¸ ê°ì§€ (1651-1682)")
    
    print("\nâœ… EXTREME_NET + UU1/UU2 íŒ¨í„´ í•™ìŠµ ì™„ë£Œ!")
    print("="*80)
    
    # Patch Time Series Transformer ì•ˆë‚´
    print("\n" + "="*80)
    print("ğŸ“Œ Patch Time Series Transformer ì•ˆë‚´")
    print("="*80)
    print("100ë§Œê°œ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ë©´ ì•Œë ¤ì£¼ì„¸ìš”!")
    print("PatchTST ëª¨ë¸ë¡œ ë” ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("="*80)
    
    return results

if __name__ == "__main__":
    results = main()