"""
ğŸ”¥ ExtremeNet V7.0 - ìœ„í—˜êµ¬ê°„(1700+) ì˜ˆì¸¡ ê°•í™” ë²„ì „
================================================================
í•µì‹¬ ê°œì„ ì‚¬í•­:
1. ì „ì´ íŒ¨í„´ í•™ìŠµ: 1651~1699 â†’ 1700+ ì „ì´ íŠ¹ë³„ í•™ìŠµ
2. ì‹œê³„ì—´ ì—°ì†ì„± ê°•í™”: Transformer + Temporal Convolution
3. ìœ„í—˜êµ¬ê°„ ì „ìš© ë¸Œëœì¹˜: 1700+ íŠ¹í™” ì˜ˆì¸¡
4. íŒ¨í„´ ë§ˆì´ë‹: ìœ„í—˜ ì „ì¡° íŒ¨í„´ ìë™ íƒì§€
5. ì•™ìƒë¸” ì˜ˆì¸¡: ë‹¤ì¤‘ ì‹œì  ì˜ˆì¸¡ í›„ í†µí•©
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import *
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
import os
import pickle
import json
from datetime import datetime

warnings.filterwarnings('ignore')
tf.random.set_seed(42)
np.random.seed(42)

# GPU ì„¤ì •
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    strategy = tf.distribute.MirroredStrategy()
else:
    strategy = tf.distribute.get_strategy()

print("="*80)
print("ğŸ”¥ ExtremeNet V7.0 - ìœ„í—˜êµ¬ê°„ ì˜ˆì¸¡ ê°•í™”")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print("="*80)

# ========================================
# ê°œì„ ëœ ë°ì´í„° ì²˜ë¦¬
# ========================================
class EnhancedDataProcessor:
    def __init__(self):
        self.seq_len = 100
        self.pred_len = 10
        
        # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ëŸ¬ (RobustScaler ì¶”ê°€ - ì´ìƒì¹˜ì— ê°•í•¨)
        self.scaler_X = StandardScaler()
        self.scaler_y = RobustScaler()  # MinMaxScaler ëŒ€ì‹  RobustScaler
        
        # êµ¬ê°„ ì •ì˜
        self.NORMAL_MAX = 1400
        self.CAUTION_MAX = 1650
        self.SIGNAL_MIN = 1651
        self.SIGNAL_MAX = 1699
        self.DANGER_MIN = 1700
        
    def create_advanced_features(self, df):
        """ê³ ê¸‰ íŠ¹ì„± ìƒì„± - ì „ì´ íŒ¨í„´ í¬ì°©"""
        print("\nâš™ï¸ ê³ ê¸‰ íŠ¹ì„± ìƒì„± ì¤‘...")
        
        # ê¸°ë³¸ íŠ¹ì„±
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(int)
        
        # êµ¬ê°„ íŠ¹ì„±
        df['SIGNAL_ZONE'] = ((df['TOTALCNT'] >= self.SIGNAL_MIN) & 
                             (df['TOTALCNT'] <= self.SIGNAL_MAX)).astype(int)
        df['PRE_SIGNAL'] = ((df['TOTALCNT'] >= 1600) & 
                            (df['TOTALCNT'] < self.SIGNAL_MIN)).astype(int)
        df['DANGER_ZONE'] = (df['TOTALCNT'] >= self.DANGER_MIN).astype(int)
        
        # â­ ì „ì´ íŒ¨í„´ íŠ¹ì„± (ìƒˆë¡œ ì¶”ê°€)
        df['NEAR_SIGNAL'] = ((df['TOTALCNT'] >= 1630) & 
                             (df['TOTALCNT'] < self.SIGNAL_MIN)).astype(int)
        df['SIGNAL_TO_DANGER_RISK'] = ((df['TOTALCNT'] >= 1680) & 
                                       (df['TOTALCNT'] < self.DANGER_MIN)).astype(int)
        
        # ì‹ í˜¸êµ¬ê°„ ì§€ì† ì‹œê°„
        df['SIGNAL_DURATION'] = 0
        signal_count = 0
        for i in range(len(df)):
            if df['SIGNAL_ZONE'].iloc[i] == 1:
                signal_count += 1
            else:
                signal_count = 0
            df.loc[i, 'SIGNAL_DURATION'] = signal_count
        
        # ì´ë™í‰ê·  (ë” ë§ì€ ìœˆë„ìš°)
        for w in [3, 5, 10, 20, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
            df[f'M14B_MA_{w}'] = df['M14AM14B'].rolling(w, min_periods=1).mean()
        
        # ë³€í™”ìœ¨ (ë” ì„¸ë°€í•˜ê²Œ)
        for lag in [1, 3, 5, 10, 15]:
            df[f'CHANGE_{lag}'] = df['TOTALCNT'].diff(lag).fillna(0)
            df[f'M14B_CHANGE_{lag}'] = df['M14AM14B'].diff(lag).fillna(0)
            
            # ë³€í™”ìœ¨ì˜ ë³€í™”ìœ¨ (ê°€ì†ë„)
            df[f'ACCEL_{lag}'] = df[f'CHANGE_{lag}'].diff(1).fillna(0)
        
        # ì—°ì† ìƒìŠ¹
        df['RISE'] = (df['TOTALCNT'] > df['TOTALCNT'].shift(1)).astype(int)
        df['RISE_COUNT'] = df['RISE'].rolling(10, min_periods=1).sum()
        df['RISE_STREAK'] = df.groupby((df['RISE'] != df['RISE'].shift()).cumsum())['RISE'].cumsum()
        
        # ê¸‰ì¦ ì‹ í˜¸ (10ë¶„ì— 50 ì´ìƒ ì¦ê°€)
        df['SPIKE_10'] = (df['CHANGE_10'] > 50).astype(int)
        df['SPIKE_5'] = (df['CHANGE_5'] > 30).astype(int)
        
        # ë³¼ë¦°ì € ë°´ë“œ
        for w in [20, 30]:
            ma = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            std = df['TOTALCNT'].rolling(w, min_periods=1).std()
            df[f'BB_UPPER_{w}'] = ma + (2 * std)
            df[f'BB_LOWER_{w}'] = ma - (2 * std)
            df[f'BB_WIDTH_{w}'] = df[f'BB_UPPER_{w}'] - df[f'BB_LOWER_{w}']
            df[f'BB_POSITION_{w}'] = (df['TOTALCNT'] - df[f'BB_LOWER_{w}']) / (df[f'BB_WIDTH_{w}'] + 1)
        
        # RSI (ìƒëŒ€ê°•ë„ì§€ìˆ˜)
        def calculate_rsi(series, period=14):
            delta = series.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()
            rs = gain / (loss + 1e-10)
            rsi = 100 - (100 / (1 + rs))
            return rsi
        
        df['RSI_14'] = calculate_rsi(df['TOTALCNT'])
        df['RSI_7'] = calculate_rsi(df['TOTALCNT'], 7)
        
        # MACD
        exp1 = df['TOTALCNT'].ewm(span=12, adjust=False).mean()
        exp2 = df['TOTALCNT'].ewm(span=26, adjust=False).mean()
        df['MACD'] = exp1 - exp2
        df['MACD_SIGNAL'] = df['MACD'].ewm(span=9, adjust=False).mean()
        df['MACD_DIFF'] = df['MACD'] - df['MACD_SIGNAL']
        
        # íŒ¨í„´ ì¸ì‹
        df['PATTERN'] = 0
        
        # íŒ¨í„´ 1: ì‹ í˜¸êµ¬ê°„
        signal_mask = ((df['TOTALCNT'] >= self.SIGNAL_MIN) & 
                      (df['TOTALCNT'] <= self.SIGNAL_MAX))
        df.loc[signal_mask, 'PATTERN'] = 1
        
        # íŒ¨í„´ 2: ê¸‰ìƒìŠ¹ ì¤‘
        rapid_rise = (df['CHANGE_5'] > 30) & (df['RISE_COUNT'] > 7)
        df.loc[rapid_rise, 'PATTERN'] = 2
        
        # íŒ¨í„´ 3: ìœ„í—˜ ì„ë°•
        danger_imminent = (df['TOTALCNT'] >= 1680) & (df['RSI_14'] > 70)
        df.loc[danger_imminent, 'PATTERN'] = 3
        
        # ì¶”ì„¸ (ë” ì •êµí•˜ê²Œ)
        df['TREND'] = 1
        df['TREND_STRENGTH'] = 0
        
        for i in range(30, len(df)):
            recent_30 = df['TOTALCNT'].iloc[i-30:i].values
            recent_10 = df['TOTALCNT'].iloc[i-10:i].values
            
            if len(recent_30) > 1:
                # ì¥ê¸° ì¶”ì„¸
                slope_30 = np.polyfit(range(len(recent_30)), recent_30, 1)[0]
                # ë‹¨ê¸° ì¶”ì„¸
                slope_10 = np.polyfit(range(len(recent_10)), recent_10, 1)[0]
                
                # ì¶”ì„¸ íŒë‹¨
                if slope_10 > 5:
                    df.loc[i, 'TREND'] = 2  # ìƒìŠ¹
                    df.loc[i, 'TREND_STRENGTH'] = slope_10
                elif slope_10 < -5:
                    df.loc[i, 'TREND'] = 0  # í•˜ë½
                    df.loc[i, 'TREND_STRENGTH'] = slope_10
                else:
                    df.loc[i, 'TREND'] = 1  # ë³´í•©
                    
                # ì¶”ì„¸ ì „í™˜ ê°ì§€
                if i > 40:
                    prev_slope = np.polyfit(range(10), df['TOTALCNT'].iloc[i-20:i-10].values, 1)[0]
                    if prev_slope < 0 and slope_10 > 5:
                        df.loc[i, 'TREND_REVERSAL'] = 1  # í•˜ë½ì—ì„œ ìƒìŠ¹ ì „í™˜
                    elif prev_slope > 0 and slope_10 < -5:
                        df.loc[i, 'TREND_REVERSAL'] = -1  # ìƒìŠ¹ì—ì„œ í•˜ë½ ì „í™˜
                    else:
                        df.loc[i, 'TREND_REVERSAL'] = 0
        
        # íŠ¹ì„± ì»¬ëŸ¼ ëª©ë¡
        self.feature_columns = [
            'TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M14AM10A', 'M14AM16',
            'RATIO', 'GOLDEN', 
            'SIGNAL_ZONE', 'PRE_SIGNAL', 'DANGER_ZONE',
            'NEAR_SIGNAL', 'SIGNAL_TO_DANGER_RISK',  # ì „ì´ íŠ¹ì„±
            'SIGNAL_DURATION',
            'PATTERN', 'TREND', 'TREND_STRENGTH',
            'MA_3', 'MA_5', 'MA_10', 'MA_20', 'MA_30',
            'STD_3', 'STD_5', 'STD_10', 'STD_20', 'STD_30',
            'M14B_MA_3', 'M14B_MA_5', 'M14B_MA_10', 'M14B_MA_20', 'M14B_MA_30',
            'CHANGE_1', 'CHANGE_3', 'CHANGE_5', 'CHANGE_10', 'CHANGE_15',
            'M14B_CHANGE_1', 'M14B_CHANGE_3', 'M14B_CHANGE_5', 'M14B_CHANGE_10', 'M14B_CHANGE_15',
            'ACCEL_1', 'ACCEL_3', 'ACCEL_5', 'ACCEL_10', 'ACCEL_15',
            'RISE_COUNT', 'RISE_STREAK',
            'SPIKE_10', 'SPIKE_5',
            'BB_UPPER_20', 'BB_LOWER_20', 'BB_WIDTH_20', 'BB_POSITION_20',
            'RSI_14', 'RSI_7',
            'MACD', 'MACD_SIGNAL', 'MACD_DIFF'
        ]
        
        # NaN ì²˜ë¦¬
        df = df.fillna(0)
        
        print(f"âœ… ê³ ê¸‰ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(self.feature_columns)}ê°œ")
        return df
    
    def create_transition_sequences(self, df):
        """ì „ì´ íŒ¨í„´ ì¤‘ì‹¬ ì‹œí€€ìŠ¤ ìƒì„± - ìƒìŠ¹/í•˜ë½ êµ¬ë¶„"""
        print("\nğŸ”„ ì „ì´ íŒ¨í„´ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        
        X_data = df[self.feature_columns].values
        y_data = df['TOTALCNT'].shift(-self.pred_len).values
        
        X, y = [], []
        transition_X, transition_y = [], []  # ì „ì´ íŒ¨í„´
        danger_X, danger_y = [], []  # ìœ„í—˜ íŒ¨í„´
        
        # ìƒìŠ¹/í•˜ë½ êµ¬ë¶„ ì¹´ìš´í„°
        rise_signal_to_danger = 0
        rise_in_danger = 0
        fall_from_danger = 0
        
        for i in range(len(df) - self.seq_len - self.pred_len):
            seq_X = X_data[i:i+self.seq_len]
            target = y_data[i+self.seq_len-1]
            
            if np.isnan(target):
                continue
                
            current = df['TOTALCNT'].iloc[i+self.seq_len-1]
            
            X.append(seq_X)
            y.append(target)
            
            # â­ ìƒìŠ¹/í•˜ë½ êµ¬ë¶„
            is_rising = target > current  # ìƒìŠ¹ ì¤‘
            change_amount = target - current
            
            # ì „ì´ íŒ¨í„´ íƒì§€ ë° ì˜¤ë²„ìƒ˜í”Œë§
            
            # 1. ì‹ í˜¸â†’ìœ„í—˜ ìƒìŠ¹ ì „ì´ (ê°€ì¥ ì¤‘ìš”!)
            if 1650 <= current <= 1699 and target >= 1700 and is_rising:
                # ìƒìŠ¹í•˜ì—¬ ìœ„í—˜êµ¬ê°„ ì§„ì… - 20ë°° ì˜¤ë²„ìƒ˜í”Œë§
                for _ in range(20):
                    transition_X.append(seq_X)
                    transition_y.append(target)
                rise_signal_to_danger += 1
                print(f"  ğŸš¨ ìƒìŠ¹ ì „ì´: {current:.0f} â†— {target:.0f} (+{change_amount:.0f})")
            
            # 2. ìœ„í—˜êµ¬ê°„ ë‚´ ìƒìŠ¹ ì§€ì†
            elif current >= 1700 and target >= 1700 and is_rising:
                # ìœ„í—˜êµ¬ê°„ì—ì„œ ê³„ì† ìƒìŠ¹ - 15ë°°
                for _ in range(15):
                    danger_X.append(seq_X)
                    danger_y.append(target)
                rise_in_danger += 1
            
            # 3. ìœ„í—˜êµ¬ê°„ì—ì„œ í•˜ë½ (ì•ˆì •í™”)
            elif current >= 1700 and target < current:
                # í•˜ë½ ì¤‘ - 3ë°°ë§Œ (ëœ ì¤‘ìš”)
                for _ in range(3):
                    transition_X.append(seq_X)
                    transition_y.append(target)
                fall_from_danger += 1
            
            # 4. ê¸‰ìƒìŠ¹ íŒ¨í„´ (50 ì´ìƒ)
            elif change_amount >= 50 and is_rising:
                # ì–´ëŠ êµ¬ê°„ì´ë“  ê¸‰ìƒìŠ¹ - 12ë°°
                for _ in range(12):
                    transition_X.append(seq_X)
                    transition_y.append(target)
            
            # 5. ì‹ í˜¸êµ¬ê°„ ë‚´ ìƒìŠ¹
            elif 1651 <= current <= 1699 and is_rising and change_amount >= 20:
                # ì‹ í˜¸êµ¬ê°„ ë‚´ ìƒìŠ¹ - 8ë°°
                for _ in range(8):
                    transition_X.append(seq_X)
                    transition_y.append(target)
            
            # 6. 1680 ì´ìƒì—ì„œ ìƒìŠ¹ (ìœ„í—˜ ì„ë°•)
            elif current >= 1680 and is_rising:
                # ìœ„í—˜ ì„ë°• ìƒìŠ¹ - 10ë°°
                for _ in range(10):
                    danger_X.append(seq_X)
                    danger_y.append(target)
        
        # ì˜¤ë²„ìƒ˜í”Œë§ ë°ì´í„° ì¶”ê°€
        if transition_X:
            X.extend(transition_X)
            y.extend(transition_y)
            print(f"  ğŸ”¥ ì „ì´ íŒ¨í„´ {len(transition_X)}ê°œ ì¶”ê°€")
        
        if danger_X:
            X.extend(danger_X)
            y.extend(danger_y)
            print(f"  ğŸ”¥ ìœ„í—˜ íŒ¨í„´ {len(danger_X)}ê°œ ì¶”ê°€")
        
        print(f"\n  ğŸ“Š íŒ¨í„´ í†µê³„:")
        print(f"    ì‹ í˜¸â†’ìœ„í—˜ ìƒìŠ¹ ì „ì´: {rise_signal_to_danger}ê±´")
        print(f"    ìœ„í—˜êµ¬ê°„ ë‚´ ìƒìŠ¹: {rise_in_danger}ê±´")
        print(f"    ìœ„í—˜êµ¬ê°„ì—ì„œ í•˜ë½: {fall_from_danger}ê±´")
        
        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.float32)
        
        print(f"  ì´ ì‹œí€€ìŠ¤: {X.shape[0]}ê°œ")
        
        # ì…”í”Œ
        shuffle_idx = np.arange(len(X))
        np.random.shuffle(shuffle_idx)
        X = X[shuffle_idx]
        y = y[shuffle_idx]
        
        return self.split_and_scale(X, y)
    
    def split_and_scale(self, X, y):
        """ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§"""
        # ë¶„í• 
        train_size = int(len(X) * 0.7)
        val_size = int(len(X) * 0.15)
        
        X_train = X[:train_size]
        y_train = y[:train_size]
        X_val = X[train_size:train_size+val_size]
        y_val = y[train_size:train_size+val_size]
        X_test = X[train_size+val_size:]
        y_test = y[train_size+val_size:]
        
        # ìŠ¤ì¼€ì¼ë§
        X_train_flat = X_train.reshape(-1, X_train.shape[-1])
        self.scaler_X.fit(X_train_flat)
        
        X_train_scaled = self.scaler_X.transform(X_train_flat).reshape(X_train.shape)
        X_val_scaled = self.scaler_X.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)
        X_test_scaled = self.scaler_X.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)
        
        self.scaler_y.fit(y_train.reshape(-1, 1))
        y_train_scaled = self.scaler_y.transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = self.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        y_test_scaled = self.scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        # êµ¬ê°„ë³„ í†µê³„
        self._print_statistics(y_train, y_val, y_test)
        
        # ë ˆì´ë¸” ìƒì„±
        y_train_level = self._create_level_labels(y_train)
        y_val_level = self._create_level_labels(y_val)
        y_test_level = self._create_level_labels(y_test)
        
        y_train_danger = (y_train >= self.DANGER_MIN).astype(int)
        y_val_danger = (y_val >= self.DANGER_MIN).astype(int)
        y_test_danger = (y_test >= self.DANGER_MIN).astype(int)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        os.makedirs('scalers_v7', exist_ok=True)
        with open('scalers_v7/scaler_X.pkl', 'wb') as f:
            pickle.dump(self.scaler_X, f)
        with open('scalers_v7/scaler_y.pkl', 'wb') as f:
            pickle.dump(self.scaler_y, f)
        
        return (X_train_scaled, y_train_scaled, y_train, y_train_level, y_train_danger), \
               (X_val_scaled, y_val_scaled, y_val, y_val_level, y_val_danger), \
               (X_test_scaled, y_test_scaled, y_test, y_test_level, y_test_danger)
    
    def _create_level_labels(self, y):
        """êµ¬ê°„ ë ˆì´ë¸” ìƒì„±"""
        labels = np.zeros(len(y), dtype=int)
        labels[(y >= self.NORMAL_MAX) & (y <= self.CAUTION_MAX)] = 1
        labels[(y >= self.SIGNAL_MIN) & (y <= self.SIGNAL_MAX)] = 2
        labels[y >= self.DANGER_MIN] = 3
        return labels
    
    def _print_statistics(self, y_train, y_val, y_test):
        """í†µê³„ ì¶œë ¥"""
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
        print(f"  Train: {len(y_train):,}ê°œ")
        print(f"  Val: {len(y_val):,}ê°œ")
        print(f"  Test: {len(y_test):,}ê°œ")
        
        for name, data in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:
            signal = ((data >= self.SIGNAL_MIN) & (data <= self.SIGNAL_MAX)).sum()
            danger = (data >= self.DANGER_MIN).sum()
            print(f"\n{name} - ì‹ í˜¸: {signal}ê°œ ({signal/len(data)*100:.1f}%), "
                  f"ìœ„í—˜: {danger}ê°œ ({danger/len(data)*100:.1f}%)")

# ========================================
# ê°œì„ ëœ ëª¨ë¸ ì•„í‚¤í…ì²˜
# ========================================
class TransformerBlock(layers.Layer):
    """Transformer ë¸”ë¡ - ì‹œê³„ì—´ ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ"""
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super().__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

class TemporalConvBlock(layers.Layer):
    """Temporal Convolution - ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‹œê°„ íŒ¨í„´"""
    def __init__(self, filters, kernel_sizes=[3, 5, 7]):
        super().__init__()
        self.convs = []
        for k in kernel_sizes:
            self.convs.append(Conv1D(filters, k, padding='same', activation='relu'))
        self.batch_norm = BatchNormalization()
        self.dropout = Dropout(0.2)
        
    def call(self, inputs, training):
        outputs = []
        for conv in self.convs:
            outputs.append(conv(inputs))
        x = tf.concat(outputs, axis=-1)
        x = self.batch_norm(x)
        return self.dropout(x, training=training)

def build_extremenet_v7(input_shape, num_features):
    """ExtremeNet V7.0 - ìœ„í—˜êµ¬ê°„ ì˜ˆì¸¡ ê°•í™” ëª¨ë¸"""
    
    inputs = Input(shape=input_shape, name='input')
    
    # ========== 1. Feature Extraction ==========
    # Temporal Convolution (ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŒ¨í„´)
    temporal_conv = TemporalConvBlock(64)(inputs)
    
    # ========== 2. ë‹¤ì¤‘ ê²½ë¡œ ì²˜ë¦¬ ==========
    
    # ê²½ë¡œ 1: Transformer (ì¥ê¸° ì˜ì¡´ì„±)
    transformer = TransformerBlock(input_shape[-1], 8, 128)(inputs)
    transformer = TransformerBlock(input_shape[-1], 8, 128)(transformer)
    transformer_pool = GlobalAveragePooling1D()(transformer)
    
    # ê²½ë¡œ 2: Bidirectional LSTM (ì–‘ë°©í–¥ ì‹œê³„ì—´)
    lstm = Bidirectional(LSTM(128, return_sequences=True))(temporal_conv)
    lstm = Bidirectional(LSTM(64))(lstm)
    lstm = Dropout(0.3)(lstm)
    
    # ê²½ë¡œ 3: CNN + Attention (ì¤‘ìš” íŒ¨í„´)
    cnn = Conv1D(128, 5, activation='relu')(inputs)
    cnn = BatchNormalization()(cnn)
    cnn = Conv1D(64, 3, activation='relu')(cnn)
    attention = MultiHeadAttention(num_heads=4, key_dim=64)(cnn, cnn)
    cnn_pool = GlobalMaxPooling1D()(attention)
    
    # ê²½ë¡œ 4: GRU (ìµœê·¼ íŒ¨í„´)
    recent = Lambda(lambda x: x[:, -30:, :])(inputs)
    gru = GRU(64, return_sequences=True)(recent)
    gru = GRU(32)(gru)
    
    # ê²½ë¡œ 5: í†µê³„ íŠ¹ì„± (ì§ì ‘ ì¶”ì¶œ)
    stats = Lambda(lambda x: tf.concat([
        tf.reduce_mean(x, axis=1, keepdims=True),
        tf.reduce_max(x, axis=1, keepdims=True),
        tf.reduce_min(x, axis=1, keepdims=True),
        tf.math.reduce_std(x, axis=1, keepdims=True)
    ], axis=1))(inputs)
    stats = Flatten()(stats)
    
    # ========== 3. íŠ¹ì„± í†µí•© ==========
    merged = Concatenate()([transformer_pool, lstm, cnn_pool, gru, stats])
    merged = BatchNormalization()(merged)
    merged = Dropout(0.3)(merged)
    
    # ========== 4. ì˜ˆì¸¡ í—¤ë“œ ==========
    
    # ê³µí†µ íŠ¹ì„±
    x = Dense(256, activation='relu')(merged)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    # ë©”ì¸ ì˜ˆì¸¡ (íšŒê·€)
    main_branch = Dense(64, activation='relu')(x)
    output_reg = Dense(1, name='regression')(main_branch)
    
    # êµ¬ê°„ ë¶„ë¥˜ (4êµ¬ê°„)
    class_branch = Dense(32, activation='relu')(x)
    output_cls = Dense(4, activation='softmax', name='classification')(class_branch)
    
    # ìœ„í—˜ ê°ì§€ (ì´ì§„ ë¶„ë¥˜)
    danger_branch = Dense(32, activation='relu')(x)
    output_danger = Dense(1, activation='sigmoid', name='danger')(danger_branch)
    
    model = Model(inputs, [output_reg, output_cls, output_danger], 
                  name='ExtremeNet_V7')
    
    return model

# ========================================
# ê°œì„ ëœ ì†ì‹¤í•¨ìˆ˜
# ========================================
class AdaptiveLoss(tf.keras.losses.Loss):
    """ì ì‘í˜• ì†ì‹¤í•¨ìˆ˜ - êµ¬ê°„ë³„ ë™ì  ê°€ì¤‘ì¹˜"""
    
    def __init__(self, processor):
        super().__init__()
        self.processor = processor
        
    def call(self, y_true, y_pred):
        # ê¸°ë³¸ MAE
        mae = tf.abs(y_true - y_pred)
        
        # ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
        weights = tf.ones_like(y_true)
        
        # ê°’ ë²”ìœ„ë¥¼ ì‹¤ì œ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜ (ì—­ë³€í™˜ ì‹œë®¬ë ˆì´ì…˜)
        y_true_rescaled = y_true * 1000 + 1000  # ëŒ€ëµì  ë³€í™˜
        
        # ì‹ í˜¸êµ¬ê°„ (1651~1699)
        signal_mask = tf.logical_and(
            y_true_rescaled >= 1651,
            y_true_rescaled <= 1699
        )
        weights = tf.where(signal_mask, 5.0, weights)
        
        # ìœ„í—˜êµ¬ê°„ (1700+)
        danger_mask = y_true_rescaled >= 1700
        weights = tf.where(danger_mask, 10.0, weights)
        
        # ì „ì´ êµ¬ê°„ (1680~1720) íŠ¹ë³„ ê°€ì¤‘ì¹˜
        transition_mask = tf.logical_and(
            y_true_rescaled >= 1680,
            y_true_rescaled <= 1720
        )
        weights = tf.where(transition_mask, 15.0, weights)
        
        # ì˜¤ì°¨ê°€ í° ê²½ìš° ì¶”ê°€ íŒ¨ë„í‹°
        large_error_mask = mae > 0.1  # ìŠ¤ì¼€ì¼ëœ ê°’ ê¸°ì¤€
        weights = tf.where(large_error_mask, weights * 1.5, weights)
        
        weighted_mae = mae * weights
        return tf.reduce_mean(weighted_mae)

# ========================================
# í•™ìŠµ ê´€ë¦¬
# ========================================
class TrainingManager:
    def __init__(self, processor):
        self.processor = processor
        self.best_score = float('inf')
        
    def train(self, train_data, val_data, test_data):
        """ëª¨ë¸ í•™ìŠµ"""
        X_train, y_train_scaled, y_train, y_train_level, y_train_danger = train_data
        X_val, y_val_scaled, y_val, y_val_level, y_val_danger = val_data
        X_test, y_test_scaled, y_test, y_test_level, y_test_danger = test_data
        
        # ëª¨ë¸ ìƒì„±
        with strategy.scope():
            model = build_extremenet_v7(
                input_shape=(X_train.shape[1], X_train.shape[2]),
                num_features=X_train.shape[2]
            )
            
            # ë‹¤ì¤‘ ì˜µí‹°ë§ˆì´ì € (í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§)
            initial_lr = 0.001
            lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
                initial_lr, 1000, alpha=0.1
            )
            
            model.compile(
                optimizer=Adam(learning_rate=lr_schedule),
                loss={
                    'regression': AdaptiveLoss(self.processor),
                    'classification': 'sparse_categorical_crossentropy',
                    'danger': 'binary_crossentropy'
                },
                loss_weights={
                    'regression': 0.5,
                    'classification': 0.2,
                    'danger': 0.3  # ìœ„í—˜ ê°ì§€ ì¤‘ìš”ë„ ì¦ê°€
                },
                metrics={
                    'regression': 'mae',
                    'classification': 'accuracy',
                    'danger': ['accuracy', tf.keras.metrics.AUC(name='auc')]
                }
            )
        
        print("\n" + "="*80)
        print("ğŸ”¥ ExtremeNet V7.0 í•™ìŠµ ì‹œì‘")
        print("="*80)
        
        # ì½œë°±
        callbacks = [
            ModelCheckpoint(
                'models_v7/best_model.keras',
                save_best_only=True,
                monitor='val_regression_mae',
                mode='min',
                verbose=1
            ),
            EarlyStopping(
                patience=30,
                restore_best_weights=True,
                monitor='val_regression_mae',
                mode='min',
                verbose=1
            ),
            ReduceLROnPlateau(
                factor=0.5,
                patience=10,
                min_lr=1e-7,
                monitor='val_regression_mae',
                mode='min',
                verbose=1
            ),
            LambdaCallback(
                on_epoch_end=lambda epoch, logs: 
                self._evaluate_danger_zone(model, X_val, y_val, epoch)
            )
        ]
        
        # í•™ìŠµ
        os.makedirs('models_v7', exist_ok=True)
        
        history = model.fit(
            X_train,
            {
                'regression': y_train_scaled,
                'classification': y_train_level,
                'danger': y_train_danger
            },
            validation_data=(
                X_val,
                {
                    'regression': y_val_scaled,
                    'classification': y_val_level,
                    'danger': y_val_danger
                }
            ),
            epochs=150,  # ë” ë§ì€ ì—í¬í¬
            batch_size=64,
            callbacks=callbacks,
            verbose=1
        )
        
        # ìµœì¢… í‰ê°€
        self._final_evaluation(model, X_test, y_test_scaled, y_test)
        
        # ëª¨ë¸ ì €ì¥
        model.save('models_v7/extremenet_v7_final.keras')
        print(f"\nğŸ’¾ ìµœì¢… ëª¨ë¸: models_v7/extremenet_v7_final.keras")
        
        return model, history
    
    def _evaluate_danger_zone(self, model, X_val, y_val, epoch):
        """ìœ„í—˜êµ¬ê°„ í‰ê°€"""
        if epoch % 5 == 0:
            preds = model.predict(X_val, verbose=0)
            y_pred = self.processor.scaler_y.inverse_transform(
                preds[0].reshape(-1, 1)
            ).flatten()
            
            # ìœ„í—˜êµ¬ê°„ í‰ê°€
            danger_idx = y_val >= 1700
            if danger_idx.sum() > 0:
                danger_mae = mean_absolute_error(y_val[danger_idx], y_pred[danger_idx])
                print(f"\n  ğŸ”´ ì—í¬í¬ {epoch} - ìœ„í—˜êµ¬ê°„ MAE: {danger_mae:.2f}")
                
                # ì „ì´ íŒ¨í„´ í‰ê°€
                transition_idx = (y_val >= 1680) & (y_val <= 1720)
                if transition_idx.sum() > 0:
                    trans_mae = mean_absolute_error(y_val[transition_idx], y_pred[transition_idx])
                    print(f"  ğŸ¯ ì „ì´êµ¬ê°„(1680~1720) MAE: {trans_mae:.2f}")
    
    def _final_evaluation(self, model, X_test, y_test_scaled, y_test):
        """ìµœì¢… í‰ê°€"""
        print("\n" + "="*80)
        print("ğŸ“Š ìµœì¢… í‰ê°€")
        print("="*80)
        
        preds = model.predict(X_test, verbose=0)
        y_pred = self.processor.scaler_y.inverse_transform(
            preds[0].reshape(-1, 1)
        ).flatten()
        
        # ì „ì²´ ë©”íŠ¸ë¦­
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)
        
        print(f"\nğŸ“ˆ ì „ì²´ ì„±ëŠ¥:")
        print(f"  MAE: {mae:.2f}")
        print(f"  RMSE: {rmse:.2f}")
        print(f"  RÂ²: {r2:.4f}")
        
        # êµ¬ê°„ë³„ ìƒì„¸ í‰ê°€
        for name, min_val, max_val in [
            ("ì‹ í˜¸êµ¬ê°„(1651~1699)", 1651, 1699),
            ("ì „ì´êµ¬ê°„(1680~1720)", 1680, 1720),
            ("ìœ„í—˜êµ¬ê°„(1700+)", 1700, float('inf'))
        ]:
            if max_val == float('inf'):
                idx = y_test >= min_val
            else:
                idx = (y_test >= min_val) & (y_test <= max_val)
            
            if idx.sum() > 0:
                zone_mae = mean_absolute_error(y_test[idx], y_pred[idx])
                zone_acc = np.mean(np.abs(y_test[idx] - y_pred[idx]) < 30)
                
                print(f"\n{'ğŸŸ ' if 'ì‹ í˜¸' in name else 'ğŸ”´'} {name}:")
                print(f"  ìƒ˜í”Œ: {idx.sum()}ê°œ")
                print(f"  MAE: {zone_mae:.2f}")
                print(f"  ì •í™•ë„(Â±30): {zone_acc*100:.1f}%")

# ========================================
# ê°œì„ ëœ ì‹¤ì‹œê°„ ì˜ˆì¸¡
# ========================================
class EnhancedPredictor:
    """ìƒìŠ¹/í•˜ë½ êµ¬ë¶„ ê°•í™” ì˜ˆì¸¡ê¸°"""
    
    def __init__(self, model_path='models_v7/extremenet_v7_final.keras'):
        print("\nğŸš€ ì˜ˆì¸¡ê¸° ë¡œë”© ì¤‘...")
        self.model = tf.keras.models.load_model(
            model_path,
            custom_objects={'AdaptiveLoss': AdaptiveLoss}
        )
        
        with open('scalers_v7/scaler_X.pkl', 'rb') as f:
            self.scaler_X = pickle.load(f)
        with open('scalers_v7/scaler_y.pkl', 'rb') as f:
            self.scaler_y = pickle.load(f)
        
        self.processor = EnhancedDataProcessor()
        print("âœ… ì˜ˆì¸¡ê¸° ì¤€ë¹„ ì™„ë£Œ")
    
    def predict_with_trend(self, df_100min):
        """100ë¶„ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡ - ìƒìŠ¹/í•˜ë½ ìƒì„¸ ë¶„ì„"""
        
        # í˜„ì¬ê°’
        current_value = df_100min['TOTALCNT'].iloc[-1]
        recent_values = df_100min['TOTALCNT'].tail(10).values
        
        # ë‹¤ì¤‘ ì‹œê°„ëŒ€ ì¶”ì„¸ ë¶„ì„
        trend_5min = self._calculate_trend(df_100min['TOTALCNT'].tail(5).values)
        trend_10min = self._calculate_trend(df_100min['TOTALCNT'].tail(10).values)
        trend_20min = self._calculate_trend(df_100min['TOTALCNT'].tail(20).values)
        
        # íŠ¹ì„± ìƒì„±
        df_features = self.processor.create_advanced_features(df_100min)
        X = df_features[self.processor.feature_columns].values
        X_scaled = self.scaler_X.transform(X.reshape(-1, len(self.processor.feature_columns)))
        X_scaled = X_scaled.reshape(1, 100, len(self.processor.feature_columns))
        
        # ì˜ˆì¸¡
        preds = self.model.predict(X_scaled, verbose=0)
        y_pred_scaled = preds[0][0, 0]
        y_pred = self.scaler_y.inverse_transform([[y_pred_scaled]])[0, 0]
        
        # ë³€í™”ëŸ‰ ê³„ì‚°
        change = y_pred - current_value
        change_rate = (change / current_value) * 100
        
        # ìœ„í—˜ë„ ì ìˆ˜ (0~100)
        danger_prob = float(preds[2][0, 0]) * 100
        
        # ìƒì„¸ ì§„ë‹¨
        diagnosis = self._diagnose_situation(
            current_value, y_pred, change, 
            trend_5min, trend_10min, trend_20min,
            danger_prob
        )
        
        return {
            'current': float(current_value),
            'prediction': float(y_pred),
            'change': float(change),
            'change_rate': float(change_rate),
            'trend_5min': trend_5min,
            'trend_10min': trend_10min,
            'trend_20min': trend_20min,
            'danger_probability': danger_prob,
            'level': diagnosis['level'],
            'status': diagnosis['status'],
            'message': diagnosis['message'],
            'action': diagnosis['action'],
            'risk_score': diagnosis['risk_score']
        }
    
    def _calculate_trend(self, values):
        """ì¶”ì„¸ ê³„ì‚°"""
        if len(values) < 2:
            return {'direction': 'flat', 'strength': 0}
        
        slope = np.polyfit(range(len(values)), values, 1)[0]
        
        if slope > 5:
            direction = 'rising'
            strength = min(slope / 10, 10)  # 0~10 ìŠ¤ì¼€ì¼
        elif slope < -5:
            direction = 'falling'
            strength = min(abs(slope) / 10, 10)
        else:
            direction = 'flat'
            strength = 0
            
        return {'direction': direction, 'strength': strength}
    
    def _diagnose_situation(self, current, predicted, change, 
                           trend_5, trend_10, trend_20, danger_prob):
        """ìƒí™© ì§„ë‹¨ - ìƒìŠ¹/í•˜ë½ êµ¬ë¶„"""
        
        diagnosis = {
            'level': '',
            'status': '',
            'message': '',
            'action': '',
            'risk_score': 0
        }
        
        # í˜„ì¬ êµ¬ê°„ íŒì •
        if current < 1400:
            current_zone = 'normal'
        elif current < 1651:
            current_zone = 'caution'
        elif current < 1700:
            current_zone = 'signal'
        else:
            current_zone = 'danger'
        
        # ì˜ˆì¸¡ êµ¬ê°„ íŒì •
        if predicted < 1400:
            pred_zone = 'normal'
        elif predicted < 1651:
            pred_zone = 'caution'
        elif predicted < 1700:
            pred_zone = 'signal'
        else:
            pred_zone = 'danger'
        
        # ìƒìŠ¹/í•˜ë½ ì¢…í•© íŒë‹¨
        is_rising = change > 0
        is_accelerating = (trend_5['strength'] > trend_10['strength'] and 
                          trend_5['direction'] == 'rising')
        
        # â­ í•µì‹¬ ì¼€ì´ìŠ¤ë³„ ì§„ë‹¨
        
        # ì¼€ì´ìŠ¤ 1: ì‹ í˜¸â†’ìœ„í—˜ ìƒìŠ¹ ì „ì´ (ê°€ì¥ ìœ„í—˜!)
        if current_zone == 'signal' and pred_zone == 'danger' and is_rising:
            diagnosis['level'] = 'ğŸ”´ğŸ”´ğŸ”´ ê·¹ë„ìœ„í—˜'
            diagnosis['status'] = 'ê¸‰ì¦ ì§„í–‰'
            diagnosis['message'] = f'1700+ ëŒíŒŒ ì˜ˆìƒ! í˜„ì¬ {current:.0f} â†’ {predicted:.0f} (+{change:.0f})'
            diagnosis['action'] = 'ğŸš¨ ì¦‰ì‹œ ë¹„ìƒëŒ€ì‘! ë¬¼ë¥˜ ê¸‰ì¦ ì„ë°•'
            diagnosis['risk_score'] = 100
        
        # ì¼€ì´ìŠ¤ 2: ìœ„í—˜êµ¬ê°„ ë‚´ ê³„ì† ìƒìŠ¹
        elif current_zone == 'danger' and pred_zone == 'danger' and is_rising:
            diagnosis['level'] = 'ğŸ”´ğŸ”´ ë§¤ìš°ìœ„í—˜'
            diagnosis['status'] = 'ìœ„í—˜ ìƒìŠ¹'
            diagnosis['message'] = f'ìœ„í—˜êµ¬ê°„ ë‚´ ìƒìŠ¹ ì§€ì† {current:.0f} â†’ {predicted:.0f}'
            diagnosis['action'] = 'âš ï¸ ê¸´ê¸‰ëŒ€ì‘ ìœ ì§€'
            diagnosis['risk_score'] = 90
        
        # ì¼€ì´ìŠ¤ 3: ìœ„í—˜êµ¬ê°„ì—ì„œ í•˜ë½ (ì•ˆì •í™”)
        elif current_zone == 'danger' and not is_rising:
            diagnosis['level'] = 'ğŸŸ  ìœ„í—˜ì™„í™”'
            diagnosis['status'] = 'í•˜ë½ ì „í™˜'
            diagnosis['message'] = f'í”¼í¬ í†µê³¼, í•˜ë½ ì¤‘ {current:.0f} â†’ {predicted:.0f} ({change:.0f})'
            diagnosis['action'] = 'ğŸ“‰ ëª¨ë‹ˆí„°ë§ ì§€ì†, ì•ˆì •í™” í™•ì¸'
            diagnosis['risk_score'] = 60
        
        # ì¼€ì´ìŠ¤ 4: ì‹ í˜¸êµ¬ê°„ ë‚´ ê¸‰ìƒìŠ¹
        elif current_zone == 'signal' and is_rising and change >= 30:
            diagnosis['level'] = 'ğŸŸ ğŸŸ  ì£¼ì˜'
            diagnosis['status'] = 'ê¸‰ìƒìŠ¹ ê°ì§€'
            diagnosis['message'] = f'ì‹ í˜¸êµ¬ê°„ ê¸‰ìƒìŠ¹ {current:.0f} â†’ {predicted:.0f} (+{change:.0f})'
            diagnosis['action'] = 'âš ï¸ ì‚¬ì „ ëŒ€ì‘ ì¤€ë¹„'
            diagnosis['risk_score'] = 75
        
        # ì¼€ì´ìŠ¤ 5: ê°€ì† ìƒìŠ¹ (ì–´ëŠ êµ¬ê°„ì´ë“ )
        elif is_accelerating and change >= 20:
            diagnosis['level'] = 'ğŸŸ¡ğŸŸ¡ ê²½ê³„'
            diagnosis['status'] = 'ê°€ì† ìƒìŠ¹'
            diagnosis['message'] = f'ìƒìŠ¹ ê°€ì†ë„ ì¦ê°€ ì¤‘ (+{change:.0f})'
            diagnosis['action'] = 'ğŸ“ˆ ë©´ë°€í•œ ëª¨ë‹ˆí„°ë§ í•„ìš”'
            diagnosis['risk_score'] = 65
        
        # ì¼€ì´ìŠ¤ 6: 1680 ì´ìƒì—ì„œ ìƒìŠ¹ (ìœ„í—˜ ì„ë°•)
        elif current >= 1680 and is_rising:
            diagnosis['level'] = 'ğŸŸ ğŸŸ ğŸŸ  ê³ ìœ„í—˜'
            diagnosis['status'] = 'ìœ„í—˜ ì„ë°•'
            diagnosis['message'] = f'1700 ì„ë°•! {current:.0f} â†’ {predicted:.0f}'
            diagnosis['action'] = 'ğŸ”” ëŒ€ì‘íŒ€ ëŒ€ê¸°'
            diagnosis['risk_score'] = 85
        
        # ì¼€ì´ìŠ¤ 7: ì•ˆì • êµ¬ê°„
        else:
            if pred_zone == 'normal':
                diagnosis['level'] = 'ğŸŸ¢ ì •ìƒ'
                diagnosis['status'] = 'ì•ˆì •'
                diagnosis['message'] = f'ì •ìƒ ë²”ìœ„ ìœ ì§€ {current:.0f} â†’ {predicted:.0f}'
                diagnosis['action'] = 'âœ“ ì •ìƒ ìš´ì˜'
                diagnosis['risk_score'] = 20
            else:
                diagnosis['level'] = 'ğŸŸ¡ ì£¼ì˜'
                diagnosis['status'] = 'ê´€ì°° í•„ìš”'
                diagnosis['message'] = f'ë³€ë™ ê´€ì°° {current:.0f} â†’ {predicted:.0f}'
                diagnosis['action'] = 'ğŸ‘€ ì§€ì† ëª¨ë‹ˆí„°ë§'
                diagnosis['risk_score'] = 40
        
        # ìœ„í—˜ë„ ì ìˆ˜ ë³´ì •
        if danger_prob > 70:
            diagnosis['risk_score'] = min(diagnosis['risk_score'] + 10, 100)
        
        return diagnosis
def main():
    # ë°ì´í„° ë¡œë“œ
    data_path = None
    for path in ['data/20240201_TO_202507281705.csv', 
                 '20240201_TO_202507281705.csv']:
        if os.path.exists(path):
            data_path = path
            break
    
    if not data_path:
        print("âŒ ë°ì´í„° íŒŒì¼ ì—†ìŒ!")
        return None
    
    # ë°ì´í„° ì²˜ë¦¬
    processor = EnhancedDataProcessor()
    
    df = pd.read_csv(data_path)
    df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                   format='%Y%m%d%H%M', errors='coerce')
    df = df.sort_values('CURRTIME').reset_index(drop=True)
    df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
    
    print(f"âœ… ë°ì´í„°: {len(df):,}ê°œ ë¡œë“œ")
    
    # ê³ ê¸‰ íŠ¹ì„± ìƒì„±
    df = processor.create_advanced_features(df)
    
    # ì „ì´ íŒ¨í„´ ì‹œí€€ìŠ¤ ìƒì„±
    train_data, val_data, test_data = processor.create_transition_sequences(df)
    
    # í•™ìŠµ
    trainer = TrainingManager(processor)
    model, history = trainer.train(train_data, val_data, test_data)
    
    print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
    print("ğŸ”¥ ExtremeNet V7.0 - ìœ„í—˜êµ¬ê°„ ì˜ˆì¸¡ ê°•í™” ì™„ë£Œ!")
    
    return model, history

if __name__ == "__main__":
    model, history = main()