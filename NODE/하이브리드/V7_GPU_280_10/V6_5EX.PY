"""
ğŸ”¥ EXTREME_NET ì™„ì „ì²´ - ëª¨ë“  ê¸°ëŠ¥ í¬í•¨ (RÂ² ë¬¸ì œ í•´ê²°)
================================================
íŒ¨í„´ ì¡°ì • í•˜ë“œì½”ë”© ë¬¸ì œ ìˆ˜ì • ì™„ë£Œ!
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model, Input, backend as K
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import *
from tensorflow.keras.regularizers import l2
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix
import warnings
import os
import pickle
import json
from datetime import datetime

warnings.filterwarnings('ignore')
tf.random.set_seed(42)
np.random.seed(42)

print("="*80)
print("ğŸ”¥ EXTREME_NET ì™„ì „ì²´ - ëª¨ë“  ê¸°ëŠ¥ í¬í•¨ (ìˆ˜ì • ë²„ì „)")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")

# GPU ì„¤ì •
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    print(f"ğŸ”§ GPU {len(gpus)}ê°œ ë°œê²¬")
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    print(f"  GPU ë©”ëª¨ë¦¬ ë™ì  í• ë‹¹ ì„¤ì •")

print("="*80)

# ====================================
# ìˆ˜ì •ëœ RÂ² ë©”íŠ¸ë¦­
# ====================================
class R2Score(tf.keras.metrics.Metric):
    """ì˜¬ë°”ë¥¸ RÂ² ë©”íŠ¸ë¦­"""
    def __init__(self, name='r2_score', **kwargs):
        super().__init__(name=name, **kwargs)
        self.ss_res = self.add_weight(name='ss_res', initializer='zeros')
        self.ss_tot = self.add_weight(name='ss_tot', initializer='zeros')
        self.count = self.add_weight(name='count', initializer='zeros')
        self.y_sum = self.add_weight(name='y_sum', initializer='zeros')
        
    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(tf.reshape(y_true, [-1]), tf.float32)
        y_pred = tf.cast(tf.reshape(y_pred, [-1]), tf.float32)
        
        batch_size = tf.cast(tf.shape(y_true)[0], tf.float32)
        self.count.assign_add(batch_size)
        self.y_sum.assign_add(tf.reduce_sum(y_true))
        y_mean = self.y_sum / self.count
        
        ss_res = tf.reduce_sum(tf.square(y_true - y_pred))
        ss_tot = tf.reduce_sum(tf.square(y_true - y_mean))
        
        self.ss_res.assign_add(ss_res)
        self.ss_tot.assign_add(ss_tot)
    
    def result(self):
        return 1 - (self.ss_res / (self.ss_tot + 1e-7))
    
    def reset_state(self):
        self.ss_res.assign(0)
        self.ss_tot.assign(0)
        self.count.assign(0)
        self.y_sum.assign(0)

# ====================================
# UU1/UU2 íŒ¨í„´ ê°ì§€ í´ë˜ìŠ¤
# ====================================
class PatternDetector:
    """UU1/UU2 íŒ¨í„´ ê°ì§€"""
    
    def __init__(self):
        self.pattern_stats = {}
    
    def detect_pattern(self, df):
        """íŒ¨í„´ ê°ì§€"""
        print("\nğŸ” UU1/UU2 íŒ¨í„´ ê°ì§€ ì¤‘...")
        
        totalcnt_max = df['TOTALCNT'].max()
        totalcnt_mean = df['TOTALCNT'].mean()
        m14b_mean = df['M14AM14B'].mean() if 'M14AM14B' in df.columns else 0
        m14b_max = df['M14AM14B'].max() if 'M14AM14B' in df.columns else 0
        high_cases = len(df[df['TOTALCNT'] >= 1682])
        critical_cases = len(df[df['TOTALCNT'] >= 1700])
        m14b_350_ratio = (df['M14AM14B'] > 350).mean() if 'M14AM14B' in df.columns else 0
        
        print(f"  TOTALCNT ìµœëŒ€: {totalcnt_max:,}")
        print(f"  TOTALCNT í‰ê· : {totalcnt_mean:.0f}")
        print(f"  M14AM14B í‰ê· : {m14b_mean:.0f}")
        print(f"  1682+ ì¼€ì´ìŠ¤: {high_cases}ê°œ")
        print(f"  1700+ ì¼€ì´ìŠ¤: {critical_cases}ê°œ")
        
        # íŒ¨í„´ íŒì •
        if high_cases > 5 and m14b_mean > 380:
            pattern = "UU2"
            confidence = 0.95
            print(f"  ğŸ”¥ UU2 íŒ¨í„´: ê³ ê°’ ìœ ì§€ (ì‹ ë¢°ë„: {confidence:.1%})")
        elif m14b_350_ratio > 0.3:
            pattern = "UU2"
            confidence = 0.90
            print(f"  ğŸ”¥ UU2 íŒ¨í„´: M14B ë†’ìŒ (ì‹ ë¢°ë„: {confidence:.1%})")
        else:
            pattern = "UU1"
            confidence = 0.85
            print(f"  ğŸ“ˆ UU1 íŒ¨í„´: ê¸‰ì¦ ê°€ëŠ¥ (ì‹ ë¢°ë„: {confidence:.1%})")
        
        self.pattern_stats = {
            'pattern': pattern,
            'confidence': float(confidence),
            'totalcnt_max': float(totalcnt_max),
            'm14b_mean': float(m14b_mean),
            'high_cases': int(high_cases)
        }
        
        return pattern, confidence
    
    def create_pattern_features(self, X_seq):
        """íŒ¨í„´ íŠ¹ì§• ìƒì„±"""
        features = []
        
        for i in range(len(X_seq)):
            seq = X_seq[i]
            totalcnt = seq[:, 0]
            m14b = seq[:, 1] if seq.shape[1] > 1 else np.zeros_like(totalcnt)
            
            # UU2 ì ìˆ˜
            uu2_score = 0
            if np.mean(m14b) > 350:
                uu2_score += 0.5
            if np.max(totalcnt) > 1682:
                uu2_score += 0.5
            
            # UU1 ì ìˆ˜
            uu1_score = 1 - uu2_score
            
            # ê¸‰ì¦ ìœ„í—˜
            spike_risk = 0
            if m14b[-1] > 300 and totalcnt[-1] < 1400:
                spike_risk = 0.7
            elif m14b[-1] > 400:
                spike_risk = 0.9
            
            features.append([uu1_score, uu2_score, spike_risk])
        
        return np.array(features, dtype=np.float32)

# ====================================
# í™•ë¥  ê³„ì‚° í´ë˜ìŠ¤
# ====================================
class ProbabilityCalculator:
    """íŒ¨í„´ ê¸°ë°˜ í™•ë¥  ê³„ì‚°"""
    
    @staticmethod
    def calculate_probabilities(predicted_value, pattern, confidence=1.0):
        """ì •ìƒ/ì£¼ì˜/ì‹¬ê° í™•ë¥ """
        
        if pattern == "UU1":
            if predicted_value < 1400:
                probs = {'ì •ìƒ': 0.85, 'ì£¼ì˜': 0.15, 'ì‹¬ê°': 0.00}
            elif 1400 <= predicted_value < 1500:
                probs = {'ì •ìƒ': 0.10, 'ì£¼ì˜': 0.85, 'ì‹¬ê°': 0.05}
            elif 1500 <= predicted_value < 1600:
                probs = {'ì •ìƒ': 0.05, 'ì£¼ì˜': 0.80, 'ì‹¬ê°': 0.15}
            elif 1600 <= predicted_value < 1700:
                probs = {'ì •ìƒ': 0.00, 'ì£¼ì˜': 0.60, 'ì‹¬ê°': 0.40}
            elif 1700 <= predicted_value < 1800:
                probs = {'ì •ìƒ': 0.00, 'ì£¼ì˜': 0.20, 'ì‹¬ê°': 0.80}
            else:
                probs = {'ì •ìƒ': 0.00, 'ì£¼ì˜': 0.05, 'ì‹¬ê°': 0.95}
        else:  # UU2
            if predicted_value < 1400:
                probs = {'ì •ìƒ': 0.90, 'ì£¼ì˜': 0.10, 'ì‹¬ê°': 0.00}
            elif 1400 <= predicted_value < 1600:
                probs = {'ì •ìƒ': 0.10, 'ì£¼ì˜': 0.80, 'ì‹¬ê°': 0.10}
            elif 1600 <= predicted_value < 1700:
                probs = {'ì •ìƒ': 0.00, 'ì£¼ì˜': 0.50, 'ì‹¬ê°': 0.50}
            else:
                probs = {'ì •ìƒ': 0.00, 'ì£¼ì˜': 0.20, 'ì‹¬ê°': 0.80}
        
        # ì‹ ë¢°ë„ ë°˜ì˜
        for key in probs:
            probs[key] *= confidence
        
        # ì •ê·œí™”
        total = sum(probs.values())
        if total > 0:
            for key in probs:
                probs[key] /= total
        
        return probs
    
    @staticmethod
    def pattern_based_adjustment(base_pred, pattern, m14b, m14a, consecutive):
        """íŒ¨í„´ ê¸°ë°˜ ì˜ˆì¸¡ ì¡°ì • - ê³¼ë„í•œ ì¡°ì • ë°©ì§€"""
        
        ratio = m14b / (m14a + 1)
        
        # ì¡°ì • ë¹„ìœ¨ì„ ì¤„ì„ (ê¸°ì¡´ 1.15 â†’ 1.03 ë“±)
        if pattern == "UU1":
            if m14b > 300 and m14a < 80:  # í™©ê¸ˆ íŒ¨í„´!
                adjustment = 1.03  # ê¸°ì¡´ 1.15ì—ì„œ ê°ì†Œ
            elif consecutive >= 10:
                adjustment = 1.02  # ê¸°ì¡´ 1.12ì—ì„œ ê°ì†Œ
            elif ratio > 4:
                adjustment = 1.01  # ê¸°ì¡´ 1.10ì—ì„œ ê°ì†Œ
            elif m14b >= 400:
                adjustment = 1.01
            elif m14b >= 350:
                adjustment = 1.00
            else:
                adjustment = 1.00
        else:  # UU2
            if m14b >= 450:
                adjustment = 1.00
            elif m14b >= 400:
                adjustment = 1.00
            else:
                adjustment = 0.99
        
        return base_pred * adjustment

# ====================================
# EXTREME_NET ëª¨ë¸ (Attention í¬í•¨!)
# ====================================
def create_extreme_net_model(input_shape, pattern_shape=(3,)):
    """EXTREME_NET ì™„ì „ì²´ ëª¨ë¸"""
    
    # ì…ë ¥
    main_input = Input(shape=input_shape, name='main_input')
    pattern_input = Input(shape=pattern_shape, name='pattern_input')
    
    # LSTM ë¸Œëœì¹˜
    x = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.001))(main_input)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    x = LSTM(32, kernel_regularizer=l2(0.001))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    
    # Attention ë¸Œëœì¹˜ (ì¤‘ìš”!)
    attention = MultiHeadAttention(num_heads=4, key_dim=16)(main_input, main_input)
    attention = GlobalAveragePooling1D()(attention)
    
    # íŒ¨í„´ ì²˜ë¦¬
    p = Dense(16, activation='relu')(pattern_input)
    p = Dropout(0.1)(p)
    
    # ìœµí•©
    combined = Concatenate()([x, attention, p])
    combined = BatchNormalization()(combined)
    combined = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(combined)
    combined = Dropout(0.2)(combined)
    combined = Dense(32, activation='relu')(combined)
    combined = Dropout(0.1)(combined)
    
    # ì¶œë ¥ (3ê°œ!)
    out_reg = Dense(1, name='regression')(combined)  # íšŒê·€
    out_cls = Dense(3, activation='softmax', name='classification')(combined)  # 3êµ¬ê°„
    out_pattern = Dense(2, activation='softmax', name='pattern_output')(combined)  # UU1/UU2
    
    model = Model(
        [main_input, pattern_input], 
        [out_reg, out_cls, out_pattern],
        name='ExtremeNetComplete'
    )
    
    # ì»´íŒŒì¼ (ìˆ˜ì •ëœ RÂ² ë©”íŠ¸ë¦­ ì‚¬ìš©)
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss={
            'regression': 'mse',
            'classification': 'sparse_categorical_crossentropy',
            'pattern_output': 'sparse_categorical_crossentropy'
        },
        loss_weights={
            'regression': 0.7,  # íšŒê·€ì— ë” ì§‘ì¤‘
            'classification': 0.2,
            'pattern_output': 0.1
        },
        metrics={
            'regression': [R2Score(), 'mae'],
            'classification': ['accuracy'],
            'pattern_output': ['accuracy']
        }
    )
    
    print("\nğŸ“Š EXTREME_NET êµ¬ì¡°:")
    print("  - LSTM: 64 â†’ 32")
    print("  - Attention: 4 heads (key_dim=16)")
    print("  - Pattern Dense: 16")
    print("  - ìœµí•© Dense: 64 â†’ 32")
    print("  - ì¶œë ¥: íšŒê·€ + 3êµ¬ê°„ + UU1/UU2")
    print(f"  - ì´ íŒŒë¼ë¯¸í„°: {model.count_params():,}ê°œ")
    
    return model

# ====================================
# ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤ (ì™„ì „ì²´)
# ====================================
class DataProcessor:
    def __init__(self, seq_len=100, pred_len=10):
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.pattern_detector = PatternDetector()
        self.prob_calculator = ProbabilityCalculator()
        self.feature_columns = None
        self.original_data = None  # ì›ë³¸ ë°ì´í„° ì €ì¥ìš©
        
    def prepare_data(self, df):
        """ë°ì´í„° ì¤€ë¹„"""
        print("\nğŸ“Š ë°ì´í„° ì¤€ë¹„...")
        
        # ì›ë³¸ ë°ì´í„° ì €ì¥ (ë‚˜ì¤‘ì— ì‹¤ì œ ê°’ ì¶”ì¶œìš©)
        self.original_data = df.copy()
        
        # 0ê°’ ì œê±°
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        
        # ì´ìƒì¹˜ ì œê±° (RÂ² ê°œì„ ìš©)
        df = df[df['TOTALCNT'] < 5000].reset_index(drop=True)
        
        # íŒ¨í„´ ê°ì§€
        pattern, confidence = self.pattern_detector.detect_pattern(df)
        
        # í•„ìˆ˜ ì»¬ëŸ¼
        base_cols = ['TOTALCNT', 'M14AM14B', 'M14AM10A', 'M14AM14BSUM', 'M14AM16']
        self.feature_columns = [c for c in base_cols if c in df.columns]
        
        # í™©ê¸ˆ íŒ¨í„´!
        df['GOLDEN'] = 0
        if 'M14AM14B' in df.columns and 'M14AM10A' in df.columns:
            df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
            golden_count = df['GOLDEN'].sum()
            if golden_count > 0:
                print(f"  ğŸ† í™©ê¸ˆíŒ¨í„´: {golden_count}ê°œ")
        
        # ë¹„ìœ¨
        df['RATIO'] = 1
        if 'M14AM14B' in df.columns and 'M14AM10A' in df.columns:
            df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        
        # Spike ê°ì§€
        df['SPIKE'] = (df['RATIO'] > 4).astype(float)
        spike_count = df['SPIKE'].sum()
        if spike_count > 0:
            print(f"  âš¡ Spike ê°ì§€: {spike_count}ê°œ")
        
        # UU1/UU2 ì‹ í˜¸
        df['UU2_SIGNAL'] = 0
        df['UU1_SIGNAL'] = 1
        if 'M14AM14B' in df.columns:
            df['UU2_SIGNAL'] = ((df['M14AM14B'] > 350) | (df['TOTALCNT'] > 1682)).astype(float)
            df['UU1_SIGNAL'] = 1 - df['UU2_SIGNAL']
        
        # ì´ìƒì‹ í˜¸ ê°ì§€ (1651-1682)
        df['ANOMALY'] = ((df['TOTALCNT'] >= 1651) & (df['TOTALCNT'] <= 1682)).astype(float)
        anomaly_count = df['ANOMALY'].sum()
        if anomaly_count > 0:
            print(f"  âš ï¸ ì´ìƒì‹ í˜¸(1651-1682): {anomaly_count}ê°œ")
        
        # ì´ë™í‰ê· 
        for w in [5, 10, 20, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
        
        # ë³€í™”ìœ¨
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        df['CHANGE_RATE'] = df['TOTALCNT'].pct_change(10).fillna(0) * 100
        
        # ì—°ì† ìƒìŠ¹ ì¹´ìš´íŠ¸
        df['CONSECUTIVE_RISE'] = 0
        consecutive = 0
        max_consecutive = 0
        for i in range(1, len(df)):
            if df.loc[i, 'TOTALCNT'] > df.loc[i-1, 'TOTALCNT']:
                consecutive += 1
                max_consecutive = max(max_consecutive, consecutive)
            else:
                consecutive = 0
            df.loc[i, 'CONSECUTIVE_RISE'] = consecutive
        
        if max_consecutive > 0:
            print(f"  ğŸ“ˆ ìµœëŒ€ ì—°ì†ìƒìŠ¹: {max_consecutive}íšŒ")
        
        # ìµœì¢… íŠ¹ì§•
        all_features = self.feature_columns + [
            'GOLDEN', 'RATIO', 'SPIKE',
            'UU1_SIGNAL', 'UU2_SIGNAL', 'ANOMALY',
            'MA_5', 'MA_10', 'MA_20', 'MA_30',
            'STD_5', 'STD_10', 'STD_20', 'STD_30',
            'CHANGE_1', 'CHANGE_10', 'CHANGE_RATE',
            'CONSECUTIVE_RISE'
        ]
        all_features = [c for c in all_features if c in df.columns]
        self.feature_columns = all_features
        
        print(f"  íŠ¹ì§• ê°œìˆ˜: {len(all_features)}")
        
        X = df[all_features].values.astype(np.float32)
        y = df['TOTALCNT'].values.astype(np.float32)
        
        # 3êµ¬ê°„
        y_cls = np.zeros(len(y), dtype=np.int32)
        y_cls[y >= 1400] = 1
        y_cls[y >= 1700] = 2
        
        print(f"  ë°ì´í„°: {X.shape}")
        print(f"  TOTALCNT: {y.min():.0f} ~ {y.max():.0f}")
        
        # êµ¬ê°„ ë¶„í¬
        for i in range(3):
            count = (y_cls == i).sum()
            names = ['ì •ìƒ(<1400)', 'ì£¼ì˜(1400-1699)', 'ìœ„í—˜(â‰¥1700)']
            print(f"  {names[i]}: {count}ê°œ ({count/len(y)*100:.1f}%)")
        
        return X, y, y_cls
    
    def create_sequences(self, X, y, y_cls):
        """ì‹œí€€ìŠ¤ ìƒì„±"""
        print("\nğŸ”„ ì‹œí€€ìŠ¤ ìƒì„±...")
        
        # NaN ì²˜ë¦¬
        X = np.nan_to_num(X, 0)
        
        sequences_X = []
        sequences_y = []
        sequences_y_cls = []
        sequences_orig = []  # ì›ë³¸ ë°ì´í„° ì¸ë±ìŠ¤ ì €ì¥
        
        for i in range(len(X) - self.seq_len - self.pred_len + 1):
            seq_x = X[i:i + self.seq_len]
            target_idx = i + self.seq_len + self.pred_len - 1
            
            if target_idx < len(y):
                sequences_X.append(seq_x)
                sequences_y.append(y[target_idx])
                sequences_y_cls.append(y_cls[target_idx])
                sequences_orig.append(X[i + self.seq_len - 1])  # ë§ˆì§€ë§‰ ì‹œì ì˜ ì›ë³¸ íŠ¹ì§• ì €ì¥
        
        sequences_X = np.array(sequences_X, dtype=np.float32)
        sequences_y = np.array(sequences_y, dtype=np.float32)
        sequences_y_cls = np.array(sequences_y_cls, dtype=np.int32)
        sequences_orig = np.array(sequences_orig, dtype=np.float32)
        
        print(f"  ì‹œí€€ìŠ¤ ìˆ˜: {len(sequences_X)}")
        
        # íŒ¨í„´ íŠ¹ì§•
        pattern_features = self.pattern_detector.create_pattern_features(sequences_X)
        
        # íŒ¨í„´ íƒ€ê²Ÿ (UU1=0, UU2=1)
        pattern_targets = np.zeros(len(sequences_X), dtype=np.int32)
        for i in range(len(sequences_X)):
            seq = sequences_X[i]
            m14b_mean = np.mean(seq[:, 1]) if seq.shape[1] > 1 else 0
            tc_max = np.max(seq[:, 0])
            
            if m14b_mean > 350 or tc_max > 1682:
                pattern_targets[i] = 1  # UU2
        
        # ë¶„í• 
        n = len(sequences_X)
        train_size = int(n * 0.7)
        val_size = int(n * 0.15)
        
        # Train
        X_train = sequences_X[:train_size]
        X_train_pattern = pattern_features[:train_size]
        X_train_orig = sequences_orig[:train_size]
        y_train = sequences_y[:train_size]
        y_train_cls = sequences_y_cls[:train_size]
        y_train_pattern = pattern_targets[:train_size]
        
        # Val
        X_val = sequences_X[train_size:train_size + val_size]
        X_val_pattern = pattern_features[train_size:train_size + val_size]
        X_val_orig = sequences_orig[train_size:train_size + val_size]
        y_val = sequences_y[train_size:train_size + val_size]
        y_val_cls = sequences_y_cls[train_size:train_size + val_size]
        y_val_pattern = pattern_targets[train_size:train_size + val_size]
        
        # Test
        X_test = sequences_X[train_size + val_size:]
        X_test_pattern = pattern_features[train_size + val_size:]
        X_test_orig = sequences_orig[train_size + val_size:]
        y_test = sequences_y[train_size + val_size:]
        y_test_cls = sequences_y_cls[train_size + val_size:]
        y_test_pattern = pattern_targets[train_size + val_size:]
        
        # ìŠ¤ì¼€ì¼ë§ (ì¤‘ìš”!)
        print("\nâš™ï¸ ìŠ¤ì¼€ì¼ë§...")
        
        # X ìŠ¤ì¼€ì¼ë§
        n_samples, n_timesteps, n_features = X_train.shape
        X_train_2d = X_train.reshape(-1, n_features)
        X_train_scaled_2d = self.scaler_X.fit_transform(X_train_2d)
        X_train_scaled = X_train_scaled_2d.reshape(n_samples, n_timesteps, n_features)
        
        X_val_scaled = self.scaler_X.transform(X_val.reshape(-1, n_features)).reshape(X_val.shape)
        X_test_scaled = self.scaler_X.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape)
        
        # y ìŠ¤ì¼€ì¼ë§
        y_train_scaled = self.scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = self.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        y_test_scaled = self.scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        print(f"  Train: {X_train_scaled.shape}")
        print(f"  Val: {X_val_scaled.shape}")
        print(f"  Test: {X_test_scaled.shape}")
        
        # ìŠ¤ì¼€ì¼ë§ ê²€ì¦
        print(f"\n  ìŠ¤ì¼€ì¼ë§ ê²€ì¦:")
        print(f"    X_train ë²”ìœ„: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]")
        print(f"    y_train ë²”ìœ„: [{y_train_scaled.min():.2f}, {y_train_scaled.max():.2f}]")
        print(f"    y_train í‰ê· : {y_train_scaled.mean():.2f} Â± {y_train_scaled.std():.2f}")
        
        # íŒ¨í„´ ë¶„í¬
        print(f"\nğŸ“Š íŒ¨í„´ ë¶„í¬:")
        for name, y_pat in [('Train', y_train_pattern), ('Val', y_val_pattern), ('Test', y_test_pattern)]:
            uu1 = (y_pat == 0).sum()
            uu2 = (y_pat == 1).sum()
            print(f"  {name}: UU1={uu1}ê°œ ({uu1/len(y_pat)*100:.1f}%), UU2={uu2}ê°œ ({uu2/len(y_pat)*100:.1f}%)")
        
        # ë°˜í™˜ (ì›ë³¸ íŠ¹ì§• ì¶”ê°€!)
        train_data = ([X_train_scaled, X_train_pattern], y_train_scaled, y_train_cls, y_train_pattern, y_train, X_train_orig)
        val_data = ([X_val_scaled, X_val_pattern], y_val_scaled, y_val_cls, y_val_pattern, y_val, X_val_orig)
        test_data = ([X_test_scaled, X_test_pattern], y_test_scaled, y_test_cls, y_test_pattern, y_test, X_test_orig)
        
        return train_data, val_data, test_data
    
    def save_all(self, path='models/'):
        """ëª¨ë“  ì„¤ì • ì €ì¥"""
        os.makedirs(path, exist_ok=True)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        with open(f'{path}scaler_X.pkl', 'wb') as f:
            pickle.dump(self.scaler_X, f)
        with open(f'{path}scaler_y.pkl', 'wb') as f:
            pickle.dump(self.scaler_y, f)
        
        # ì„¤ì • ì €ì¥
        config = {
            'seq_len': int(self.seq_len),
            'pred_len': int(self.pred_len),
            'feature_columns': self.feature_columns,
            'pattern_stats': self.pattern_detector.pattern_stats
        }
        with open(f'{path}config.json', 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"âœ… ëª¨ë“  ì„¤ì • ì €ì¥: {path}")

# ====================================
# ìˆ˜ì •ëœ í•™ìŠµ í•¨ìˆ˜ (í•µì‹¬!)
# ====================================
def train_model(model, train_data, val_data, test_data, processor):
    """ëª¨ë¸ í•™ìŠµ - íŒ¨í„´ ì¡°ì • ë¬¸ì œ í•´ê²°"""
    # 6ê°œ ìš”ì†Œ ì–¸íŒ¨í‚¹ (ì›ë³¸ íŠ¹ì§• í¬í•¨)
    X_train, y_train_scaled, y_train_cls, y_train_pattern, y_train_orig, X_train_orig = train_data
    X_val, y_val_scaled, y_val_cls, y_val_pattern, y_val_orig, X_val_orig = val_data
    X_test, y_test_scaled, y_test_cls, y_test_pattern, y_test_orig, X_test_orig = test_data
    
    print("\n" + "="*60)
    print("ğŸ¯ EXTREME_NET í•™ìŠµ ì‹œì‘")
    print("="*60)
    
    # ì½œë°±
    callbacks = [
        EarlyStopping(
            monitor='val_regression_r2_score',
            mode='max',
            patience=20,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=10,
            min_lr=1e-6,
            verbose=1
        ),
        ModelCheckpoint(
            'models/best_extreme_net.keras',
            monitor='val_regression_r2_score',
            mode='max',
            save_best_only=True,
            verbose=1
        )
    ]
    
    # ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ ê³ ë ¤)
    batch_size = 32 if gpus else 16
    
    # í•™ìŠµ
    print(f"\nğŸ“ˆ í•™ìŠµ ì„¤ì •:")
    print(f"  Batch Size: {batch_size}")
    print(f"  Epochs: 100 (Early Stopping)")
    print(f"  Learning Rate: 0.001 (ReduceLROnPlateau)")
    
    history = model.fit(
        X_train,
        {
            'regression': y_train_scaled,
            'classification': y_train_cls,
            'pattern_output': y_train_pattern
        },
        validation_data=(
            X_val,
            {
                'regression': y_val_scaled,
                'classification': y_val_cls,
                'pattern_output': y_val_pattern
            }
        ),
        epochs=100,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=1
    )
    
    # í‰ê°€
    print("\n" + "="*60)
    print("ğŸ“Š EXTREME_NET ì„±ëŠ¥ í‰ê°€")
    print("="*60)
    
    # ì˜ˆì¸¡
    preds = model.predict(X_test, batch_size=batch_size, verbose=0)
    y_pred_scaled = preds[0].flatten()
    y_cls_pred = np.argmax(preds[1], axis=1)
    y_pat_pred = np.argmax(preds[2], axis=1)
    
    # ì—­ë³€í™˜ (ì¤‘ìš”!)
    y_pred = processor.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    
    # =========================================
    # ìˆ˜ì •ëœ íŒ¨í„´ ê¸°ë°˜ ì¡°ì • (í•µì‹¬ ìˆ˜ì •!)
    # =========================================
    print("\nğŸ”§ íŒ¨í„´ ê¸°ë°˜ ì˜ˆì¸¡ ì¡°ì • (ìˆ˜ì •ë¨)...")
    adjusted = []
    golden_detected = 0
    
    for i in range(len(y_pred)):
        pattern = 'UU2' if y_pat_pred[i] == 1 else 'UU1'
        
        # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ê°’ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ!)
        try:
            # X_testì—ì„œ ìŠ¤ì¼€ì¼ëœ ê°’ ê°€ì ¸ì˜¤ê¸°
            last_timestep = X_test[0][i, -1, :]  # ë§ˆì§€ë§‰ ì‹œì 
            
            # ì—­ìŠ¤ì¼€ì¼ë§ (ì¤‘ìš”!)
            unscaled = processor.scaler_X.inverse_transform(
                last_timestep.reshape(1, -1)
            )[0]
            
            # feature_columnsì—ì„œ ì¸ë±ìŠ¤ ì°¾ê¸°
            feature_list = processor.feature_columns
            
            if 'M14AM14B' in feature_list:
                m14b_idx = feature_list.index('M14AM14B')
                m14b = unscaled[m14b_idx]
            else:
                m14b = 250
            
            if 'M14AM10A' in feature_list:
                m14a_idx = feature_list.index('M14AM10A')
                m14a = unscaled[m14a_idx]
            else:
                m14a = 80
            
            if 'CONSECUTIVE_RISE' in feature_list:
                cons_idx = feature_list.index('CONSECUTIVE_RISE')
                consecutive = int(unscaled[cons_idx])
            else:
                consecutive = 0
                
        except Exception as e:
            # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’ (ë°ì´í„° í‰ê· ê°’ ì‚¬ìš©)
            m14b = 250
            m14a = 80
            consecutive = 0
        
        # í™©ê¸ˆ íŒ¨í„´ ì²´í¬ (ì‹¤ì œ ê°’ìœ¼ë¡œ!)
        if m14b > 300 and m14a < 80:
            golden_detected += 1
        
        # ì¡°ì • (ê³¼ë„í•˜ì§€ ì•Šê²Œ)
        adj = processor.prob_calculator.pattern_based_adjustment(
            y_pred[i], pattern, m14b, m14a, consecutive
        )
        adjusted.append(adj)
    
    adjusted = np.array(adjusted)
    
    if golden_detected > 0:
        print(f"  ğŸ† ì‹¤ì œ í™©ê¸ˆíŒ¨í„´ ê°ì§€: {golden_detected}ê°œ")
    
    # ì¡°ì • ì „í›„ ë¹„êµ
    mae_before = mean_absolute_error(y_test_orig, y_pred)
    mae_after = mean_absolute_error(y_test_orig, adjusted)
    
    print(f"\n  ì¡°ì • íš¨ê³¼:")
    print(f"    ì¡°ì • ì „ MAE: {mae_before:.2f}")
    print(f"    ì¡°ì • í›„ MAE: {mae_after:.2f}")
    
    # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
    mae = mae_after
    rmse = np.sqrt(mean_squared_error(y_test_orig, adjusted))
    r2 = r2_score(y_test_orig, adjusted)
    
    # íŒ¨í„´ ì •í™•ë„
    pattern_acc = accuracy_score(y_test_pattern, y_pat_pred)
    
    # 3êµ¬ê°„ ì •í™•ë„
    cls_acc = accuracy_score(y_test_cls, y_cls_pred)
    
    # ì´ìƒì‹ í˜¸ ê°ì§€ (1651-1682)
    anomaly_true = ((y_test_orig >= 1651) & (y_test_orig <= 1682))
    anomaly_pred = ((adjusted >= 1651) & (adjusted <= 1682))
    anomaly_detected = np.sum(anomaly_true & anomaly_pred)
    
    # ìœ„í—˜êµ¬ê°„ ê°ì§€ (â‰¥1700)
    danger_true = (y_test_cls == 2)
    danger_pred = (y_cls_pred == 2)
    if danger_true.sum() > 0:
        tp = np.sum(danger_true & danger_pred)
        fp = np.sum(~danger_true & danger_pred)
        fn = np.sum(danger_true & ~danger_pred)
        danger_precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        danger_recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        danger_f1 = 2 * (danger_precision * danger_recall) / (danger_precision + danger_recall) \
                    if (danger_precision + danger_recall) > 0 else 0
    else:
        danger_precision = danger_recall = danger_f1 = 0
    
    print(f"\nğŸ“ˆ íšŒê·€ ì„±ëŠ¥:")
    print(f"  MAE: {mae:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  RÂ²: {r2:.4f}")
    
    print(f"\nğŸ” íŒ¨í„´ ì˜ˆì¸¡:")
    print(f"  UU1/UU2 ì •í™•ë„: {pattern_acc:.4f}")
    
    print(f"\nğŸ“Š 3êµ¬ê°„ ë¶„ë¥˜:")
    print(f"  ì •í™•ë„: {cls_acc:.4f}")
    
    if danger_true.sum() > 0:
        print(f"\nâš ï¸ ìœ„í—˜êµ¬ê°„(â‰¥1700) ê°ì§€:")
        print(f"  Precision: {danger_precision:.4f}")
        print(f"  Recall: {danger_recall:.4f}")
        print(f"  F1-Score: {danger_f1:.4f}")
    
    if anomaly_true.sum() > 0:
        print(f"\nâš ï¸ ì´ìƒì‹ í˜¸(1651-1682) ê°ì§€:")
        print(f"  ê°ì§€: {anomaly_detected}/{anomaly_true.sum()}ê°œ")
    
    # RÂ² ê²€ì¦
    if r2 < 0:
        print(f"\nâš ï¸ RÂ² ì—¬ì „íˆ ìŒìˆ˜: {r2:.4f}")
        print("  ë””ë²„ê¹… ì •ë³´:")
        print(f"    ì˜ˆì¸¡ê°’ ë²”ìœ„: {adjusted.min():.0f} ~ {adjusted.max():.0f}")
        print(f"    ì‹¤ì œê°’ ë²”ìœ„: {y_test_orig.min():.0f} ~ {y_test_orig.max():.0f}")
        print(f"    ì˜ˆì¸¡ í‰ê· : {adjusted.mean():.0f}")
        print(f"    ì‹¤ì œ í‰ê· : {y_test_orig.mean():.0f}")
        correlation = np.corrcoef(y_test_orig, adjusted)[0, 1]
        print(f"    ìƒê´€ê³„ìˆ˜: {correlation:.4f}")
    elif r2 > 0.8:
        print(f"\nâœ… ìš°ìˆ˜í•œ RÂ² ì„±ëŠ¥: {r2:.4f}")
    else:
        print(f"\nğŸ“Š RÂ² ì •ìƒ: {r2:.4f}")
    
    # ì €ì¥
    os.makedirs('models', exist_ok=True)
    model.save('models/extreme_net_complete.keras')
    processor.save_all()
    
    print("\nğŸ’¾ ëª¨ë¸ ë° ì„¤ì • ì €ì¥ ì™„ë£Œ")
    
    return {
        'MAE': mae,
        'RMSE': rmse,
        'R2': r2,
        'PatternAcc': pattern_acc,
        'ClassAcc': cls_acc,
        'DangerF1': danger_f1
    }

# ====================================
# ì‹¤ì‹œê°„ ì˜ˆì¸¡ í´ë˜ìŠ¤
# ====================================
class RealtimePredictor:
    def __init__(self, model_path='models/extreme_net_complete.keras'):
        """ì‹¤ì‹œê°„ ì˜ˆì¸¡ê¸°"""
        # ì»¤ìŠ¤í…€ ê°ì²´ ë“±ë¡
        custom_objects = {'R2Score': R2Score}
        
        self.model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)
        self.processor = DataProcessor()
        
        # ì„¤ì • ë¡œë“œ
        with open('models/scaler_X.pkl', 'rb') as f:
            self.processor.scaler_X = pickle.load(f)
        with open('models/scaler_y.pkl', 'rb') as f:
            self.processor.scaler_y = pickle.load(f)
        with open('models/config.json', 'r') as f:
            config = json.load(f)
            self.processor.feature_columns = config['feature_columns']
        
        print("âœ… ì‹¤ì‹œê°„ ì˜ˆì¸¡ê¸° ì¤€ë¹„ ì™„ë£Œ")
    
    def predict(self, data_100min):
        """100ë¶„ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡"""
        if len(data_100min) != 100:
            raise ValueError(f"100ë¶„ ë°ì´í„° í•„ìš” (í˜„ì¬: {len(data_100min)})")
        
        # ë°ì´í„° ì¤€ë¹„ (prepare_dataì™€ ë™ì¼í•œ ì²˜ë¦¬)
        df = data_100min.copy()
        
        # íŠ¹ì§• ìƒì„±
        df['GOLDEN'] = 0
        if 'M14AM14B' in df.columns and 'M14AM10A' in df.columns:
            df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        
        df['RATIO'] = 1
        if 'M14AM14B' in df.columns and 'M14AM10A' in df.columns:
            df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        
        df['SPIKE'] = (df['RATIO'] > 4).astype(float)
        
        df['UU2_SIGNAL'] = 0
        df['UU1_SIGNAL'] = 1
        if 'M14AM14B' in df.columns:
            df['UU2_SIGNAL'] = ((df['M14AM14B'] > 350) | (df['TOTALCNT'] > 1682)).astype(float)
            df['UU1_SIGNAL'] = 1 - df['UU2_SIGNAL']
        
        df['ANOMALY'] = ((df['TOTALCNT'] >= 1651) & (df['TOTALCNT'] <= 1682)).astype(float)
        
        # ì´ë™í‰ê· 
        for w in [5, 10, 20, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
        
        # ë³€í™”ìœ¨
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        df['CHANGE_RATE'] = df['TOTALCNT'].pct_change(10).fillna(0) * 100
        
        # ì—°ì† ìƒìŠ¹ ì¹´ìš´íŠ¸
        df['CONSECUTIVE_RISE'] = 0
        consecutive = 0
        for i in range(1, len(df)):
            if df.loc[i, 'TOTALCNT'] > df.loc[i-1, 'TOTALCNT']:
                consecutive += 1
            else:
                consecutive = 0
            df.loc[i, 'CONSECUTIVE_RISE'] = consecutive
        
        # íŒ¨í„´ ê°ì§€
        pattern, confidence = self.processor.pattern_detector.detect_pattern(df)
        
        # íŠ¹ì§• ì¤€ë¹„
        X = df[self.processor.feature_columns].values
        X = X.reshape(1, 100, -1)
        
        # íŒ¨í„´ íŠ¹ì§•
        pattern_features = self.processor.pattern_detector.create_pattern_features(X)
        
        # ìŠ¤ì¼€ì¼ë§
        n_features = X.shape[2]
        X_scaled = self.processor.scaler_X.transform(X.reshape(-1, n_features)).reshape(X.shape)
        
        # ì˜ˆì¸¡
        preds = self.model.predict([X_scaled, pattern_features], verbose=0)
        y_pred_scaled = preds[0][0, 0]
        y_cls_probs = preds[1][0]
        y_pat_probs = preds[2][0]
        
        # ì—­ë³€í™˜
        y_pred = self.processor.scaler_y.inverse_transform([[y_pred_scaled]])[0, 0]
        
        # í˜„ì¬ê°’ê³¼ íŒ¨í„´ ì¡°ì •
        current = df['TOTALCNT'].iloc[-1]
        m14b = df['M14AM14B'].iloc[-1] if 'M14AM14B' in df.columns else 0
        m14a = df['M14AM10A'].iloc[-1] if 'M14AM10A' in df.columns else 0
        consecutive = df['CONSECUTIVE_RISE'].iloc[-1]
        
        # íŒ¨í„´ ê¸°ë°˜ ì¡°ì • (ìˆ˜ì •ëœ ë²„ì „)
        adjusted = self.processor.prob_calculator.pattern_based_adjustment(
            y_pred, pattern, m14b, m14a, consecutive
        )
        
        # í™•ë¥  ê³„ì‚°
        probs = self.processor.prob_calculator.calculate_probabilities(adjusted, pattern, confidence)
        
        # ìƒíƒœ íŒì •
        if adjusted >= 1700:
            status = "ğŸ”´ ìœ„í—˜ - ë¬¼ë¥˜ëŸ‰ ê¸‰ì¦!"
        elif adjusted >= 1400:
            status = "ğŸŸ¡ ì£¼ì˜ - ë¬¼ë¥˜ëŸ‰ ì¦ê°€"
        else:
            status = "ğŸŸ¢ ì •ìƒ - ì•ˆì •ì "
        
        # ì´ìƒì‹ í˜¸ ì²´í¬
        is_anomaly = (1651 <= adjusted <= 1682)
        
        # í™©ê¸ˆíŒ¨í„´ ì²´í¬
        is_golden = (m14b > 300 and m14a < 80)
        
        return {
            'prediction': float(adjusted),
            'current': float(current),
            'change': float(adjusted - current),
            'change_rate': float((adjusted - current) / current * 100) if current > 0 else 0,
            'pattern': pattern,
            'pattern_confidence': confidence,
            'pattern_probs': {
                'UU1': float(y_pat_probs[0]),
                'UU2': float(y_pat_probs[1])
            },
            'level_probs': probs,
            'status': status,
            'is_anomaly': is_anomaly,
            'is_golden': is_golden,
            'consecutive_rises': int(consecutive),
            'm14b_m14a_ratio': float(m14b / (m14a + 1)),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

# ====================================
# ë©”ì¸ ì‹¤í–‰
# ====================================
def main():
    print("\nğŸš€ EXTREME_NET ì™„ì „ì²´ ì‹œì‘ (ìˆ˜ì • ë²„ì „)")
    
    # ë°ì´í„° ì°¾ê¸°
    data_paths = [
        '/mnt/user-data/uploads/gs.CSV',
        'data/20240201_TO_202507281705.csv',
        'uu.csv',
        'uu2.csv',
        'data.csv'
    ]
    
    data_path = None
    for path in data_paths:
        if os.path.exists(path):
            data_path = path
            print(f"âœ… ë°ì´í„°: {path}")
            break
    
    if not data_path:
        print("âŒ ë°ì´í„° ì—†ìŒ")
        return
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(data_path)
    print(f"  ë¡œë“œ: {len(df):,}í–‰")
    
    # ì²˜ë¦¬
    processor = DataProcessor()
    X, y, y_cls = processor.prepare_data(df)
    train_data, val_data, test_data = processor.create_sequences(X, y, y_cls)
    
    # ëª¨ë¸ ìƒì„±
    input_shape = (100, X.shape[1])
    pattern_shape = (3,)
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ Strategy
    if gpus:
        strategy = tf.distribute.MirroredStrategy()
        print("ğŸ”§ GPU Strategy ì‚¬ìš©")
        with strategy.scope():
            model = create_extreme_net_model(input_shape, pattern_shape)
    else:
        print("ğŸ”§ CPU ëª¨ë“œ")
        model = create_extreme_net_model(input_shape, pattern_shape)
    
    # í•™ìŠµ
    results = train_model(model, train_data, val_data, test_data, processor)
    
    # ìµœì¢… ê²°ê³¼
    print("\n" + "="*60)
    print("ğŸ† EXTREME_NET ìµœì¢… ê²°ê³¼")
    print("="*60)
    print(f"RÂ²: {results['R2']:.4f}")
    print(f"MAE: {results['MAE']:.2f}")
    print(f"íŒ¨í„´ ì •í™•ë„: {results['PatternAcc']:.4f}")
    print(f"3êµ¬ê°„ ì •í™•ë„: {results['ClassAcc']:.4f}")
    
    # ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    if len(df) >= 100:
        print("\nğŸ”® ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸...")
        predictor = RealtimePredictor()
        test_sample = df.iloc[-100:].copy()
        result = predictor.predict(test_sample)
        
        print(f"\nğŸ“ˆ ì˜ˆì¸¡ ê²°ê³¼:")
        print(f"  íŒ¨í„´: {result['pattern']} (ì‹ ë¢°ë„: {result['pattern_confidence']:.1%})")
        print(f"  í˜„ì¬: {result['current']:.0f}")
        print(f"  10ë¶„ í›„: {result['prediction']:.0f}")
        print(f"  ë³€í™”: {result['change']:+.0f} ({result['change_rate']:+.1f}%)")
        print(f"  ìƒíƒœ: {result['status']}")
        
        if result['is_golden']:
            print(f"  ğŸ† í™©ê¸ˆíŒ¨í„´ ê°ì§€!")
        if result['is_anomaly']:
            print(f"  âš ï¸ ì´ìƒì‹ í˜¸ ê°ì§€ (1651-1682)")
        if result['consecutive_rises'] > 5:
            print(f"  ğŸ“ˆ ì—°ì† ìƒìŠ¹: {result['consecutive_rises']}íšŒ")
    
    print("\nâœ… EXTREME_NET ì™„ì „ì²´ ì™„ë£Œ!")
    return results

if __name__ == "__main__":
    results = main()