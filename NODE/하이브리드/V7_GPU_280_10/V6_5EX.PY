"""
üî• ExtremeNet V7.0 - ÏúÑÌóòÍµ¨Í∞Ñ(1700+) ÏòàÏ∏° Í∞ïÌôî Î≤ÑÏ†Ñ
================================================================
ÌïµÏã¨ Í∞úÏÑ†ÏÇ¨Ìï≠:
1. Ï†ÑÏù¥ Ìå®ÌÑ¥ ÌïôÏäµ: 1651~1699 ‚Üí 1700+ Ï†ÑÏù¥ ÌäπÎ≥Ñ ÌïôÏäµ
2. ÏãúÍ≥ÑÏó¥ Ïó∞ÏÜçÏÑ± Í∞ïÌôî: Transformer + Temporal Convolution
3. ÏúÑÌóòÍµ¨Í∞Ñ Ï†ÑÏö© Î∏åÎûúÏπò: 1700+ ÌäπÌôî ÏòàÏ∏°
4. Ìå®ÌÑ¥ ÎßàÏù¥Îãù: ÏúÑÌóò Ï†ÑÏ°∞ Ìå®ÌÑ¥ ÏûêÎèô ÌÉêÏßÄ
5. ÏïôÏÉÅÎ∏î ÏòàÏ∏°: Îã§Ï§ë ÏãúÏ†ê ÏòàÏ∏° ÌõÑ ÌÜµÌï©
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import *
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
import os
import pickle
import json
from datetime import datetime

warnings.filterwarnings('ignore')
tf.random.set_seed(42)
np.random.seed(42)

# GPU ÏÑ§Ï†ï
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    strategy = tf.distribute.MirroredStrategy()
else:
    strategy = tf.distribute.get_strategy()

print("="*80)
print("üî• ExtremeNet V7.0 - ÏúÑÌóòÍµ¨Í∞Ñ ÏòàÏ∏° Í∞ïÌôî")
print(f"üì¶ TensorFlow: {tf.__version__}")
print("="*80)

# ========================================
# Í∞úÏÑ†Îêú Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
# ========================================
class EnhancedDataProcessor:
    def __init__(self):
        self.seq_len = 100
        self.pred_len = 10
        
        # Îã§Ï§ë Ïä§ÏºÄÏùºÎü¨ (RobustScaler Ï∂îÍ∞Ä - Ïù¥ÏÉÅÏπòÏóê Í∞ïÌï®)
        self.scaler_X = StandardScaler()
        self.scaler_y = RobustScaler()  # MinMaxScaler ÎåÄÏã† RobustScaler
        
        # Íµ¨Í∞Ñ Ï†ïÏùò
        self.NORMAL_MAX = 1400
        self.CAUTION_MAX = 1650
        self.SIGNAL_MIN = 1651
        self.SIGNAL_MAX = 1699
        self.DANGER_MIN = 1700
        
    def create_advanced_features(self, df):
        """Í≥†Í∏â ÌäπÏÑ± ÏÉùÏÑ± - Ï†ÑÏù¥ Ìå®ÌÑ¥ Ìè¨Ï∞©"""
        print("\n‚öôÔ∏è Í≥†Í∏â ÌäπÏÑ± ÏÉùÏÑ± Ï§ë...")
        
        # Í∏∞Î≥∏ ÌäπÏÑ±
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(int)
        
        # Íµ¨Í∞Ñ ÌäπÏÑ±
        df['SIGNAL_ZONE'] = ((df['TOTALCNT'] >= self.SIGNAL_MIN) & 
                             (df['TOTALCNT'] <= self.SIGNAL_MAX)).astype(int)
        df['PRE_SIGNAL'] = ((df['TOTALCNT'] >= 1600) & 
                            (df['TOTALCNT'] < self.SIGNAL_MIN)).astype(int)
        df['DANGER_ZONE'] = (df['TOTALCNT'] >= self.DANGER_MIN).astype(int)
        
        # ‚≠ê Ï†ÑÏù¥ Ìå®ÌÑ¥ ÌäπÏÑ± (ÏÉàÎ°ú Ï∂îÍ∞Ä)
        df['NEAR_SIGNAL'] = ((df['TOTALCNT'] >= 1630) & 
                             (df['TOTALCNT'] < self.SIGNAL_MIN)).astype(int)
        df['SIGNAL_TO_DANGER_RISK'] = ((df['TOTALCNT'] >= 1680) & 
                                       (df['TOTALCNT'] < self.DANGER_MIN)).astype(int)
        
        # Ïã†Ìò∏Íµ¨Í∞Ñ ÏßÄÏÜç ÏãúÍ∞Ñ
        df['SIGNAL_DURATION'] = 0
        signal_count = 0
        for i in range(len(df)):
            if df['SIGNAL_ZONE'].iloc[i] == 1:
                signal_count += 1
            else:
                signal_count = 0
            df.loc[i, 'SIGNAL_DURATION'] = signal_count
        
        # Ïù¥ÎèôÌèâÍ∑† (Îçî ÎßéÏùÄ ÏúàÎèÑÏö∞)
        for w in [3, 5, 10, 20, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
            df[f'M14B_MA_{w}'] = df['M14AM14B'].rolling(w, min_periods=1).mean()
        
        # Î≥ÄÌôîÏú® (Îçî ÏÑ∏Î∞ÄÌïòÍ≤å)
        for lag in [1, 3, 5, 10, 15]:
            df[f'CHANGE_{lag}'] = df['TOTALCNT'].diff(lag).fillna(0)
            df[f'M14B_CHANGE_{lag}'] = df['M14AM14B'].diff(lag).fillna(0)
            
            # Î≥ÄÌôîÏú®Ïùò Î≥ÄÌôîÏú® (Í∞ÄÏÜçÎèÑ)
            df[f'ACCEL_{lag}'] = df[f'CHANGE_{lag}'].diff(1).fillna(0)
        
        # Ïó∞ÏÜç ÏÉÅÏäπ
        df['RISE'] = (df['TOTALCNT'] > df['TOTALCNT'].shift(1)).astype(int)
        df['RISE_COUNT'] = df['RISE'].rolling(10, min_periods=1).sum()
        df['RISE_STREAK'] = df.groupby((df['RISE'] != df['RISE'].shift()).cumsum())['RISE'].cumsum()
        
        # Í∏âÏ¶ù Ïã†Ìò∏ (10Î∂ÑÏóê 50 Ïù¥ÏÉÅ Ï¶ùÍ∞Ä)
        df['SPIKE_10'] = (df['CHANGE_10'] > 50).astype(int)
        df['SPIKE_5'] = (df['CHANGE_5'] > 30).astype(int)
        
        # Î≥ºÎ¶∞Ï†Ä Î∞¥Îìú
        for w in [20, 30]:
            ma = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            std = df['TOTALCNT'].rolling(w, min_periods=1).std()
            df[f'BB_UPPER_{w}'] = ma + (2 * std)
            df[f'BB_LOWER_{w}'] = ma - (2 * std)
            df[f'BB_WIDTH_{w}'] = df[f'BB_UPPER_{w}'] - df[f'BB_LOWER_{w}']
            df[f'BB_POSITION_{w}'] = (df['TOTALCNT'] - df[f'BB_LOWER_{w}']) / (df[f'BB_WIDTH_{w}'] + 1)
        
        # RSI (ÏÉÅÎåÄÍ∞ïÎèÑÏßÄÏàò)
        def calculate_rsi(series, period=14):
            delta = series.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()
            rs = gain / (loss + 1e-10)
            rsi = 100 - (100 / (1 + rs))
            return rsi
        
        df['RSI_14'] = calculate_rsi(df['TOTALCNT'])
        df['RSI_7'] = calculate_rsi(df['TOTALCNT'], 7)
        
        # MACD
        exp1 = df['TOTALCNT'].ewm(span=12, adjust=False).mean()
        exp2 = df['TOTALCNT'].ewm(span=26, adjust=False).mean()
        df['MACD'] = exp1 - exp2
        df['MACD_SIGNAL'] = df['MACD'].ewm(span=9, adjust=False).mean()
        df['MACD_DIFF'] = df['MACD'] - df['MACD_SIGNAL']
        
        # Ìå®ÌÑ¥ Ïù∏Ïãù
        df['PATTERN'] = 0
        
        # Ìå®ÌÑ¥ 1: Ïã†Ìò∏Íµ¨Í∞Ñ
        signal_mask = ((df['TOTALCNT'] >= self.SIGNAL_MIN) & 
                      (df['TOTALCNT'] <= self.SIGNAL_MAX))
        df.loc[signal_mask, 'PATTERN'] = 1
        
        # Ìå®ÌÑ¥ 2: Í∏âÏÉÅÏäπ Ï§ë
        rapid_rise = (df['CHANGE_5'] > 30) & (df['RISE_COUNT'] > 7)
        df.loc[rapid_rise, 'PATTERN'] = 2
        
        # Ìå®ÌÑ¥ 3: ÏúÑÌóò ÏûÑÎ∞ï
        danger_imminent = (df['TOTALCNT'] >= 1680) & (df['RSI_14'] > 70)
        df.loc[danger_imminent, 'PATTERN'] = 3
        
        # Ï∂îÏÑ∏ (Îçî Ï†ïÍµêÌïòÍ≤å)
        df['TREND'] = 1
        df['TREND_STRENGTH'] = 0
        
        for i in range(30, len(df)):
            recent_30 = df['TOTALCNT'].iloc[i-30:i].values
            recent_10 = df['TOTALCNT'].iloc[i-10:i].values
            
            if len(recent_30) > 1:
                # Ïû•Í∏∞ Ï∂îÏÑ∏
                slope_30 = np.polyfit(range(len(recent_30)), recent_30, 1)[0]
                # Îã®Í∏∞ Ï∂îÏÑ∏
                slope_10 = np.polyfit(range(len(recent_10)), recent_10, 1)[0]
                
                # Ï∂îÏÑ∏ ÌåêÎã®
                if slope_10 > 5:
                    df.loc[i, 'TREND'] = 2  # ÏÉÅÏäπ
                    df.loc[i, 'TREND_STRENGTH'] = slope_10
                elif slope_10 < -5:
                    df.loc[i, 'TREND'] = 0  # ÌïòÎùΩ
                    df.loc[i, 'TREND_STRENGTH'] = slope_10
                else:
                    df.loc[i, 'TREND'] = 1  # Î≥¥Ìï©
                    
                # Ï∂îÏÑ∏ Ï†ÑÌôò Í∞êÏßÄ
                if i > 40:
                    prev_slope = np.polyfit(range(10), df['TOTALCNT'].iloc[i-20:i-10].values, 1)[0]
                    if prev_slope < 0 and slope_10 > 5:
                        df.loc[i, 'TREND_REVERSAL'] = 1  # ÌïòÎùΩÏóêÏÑú ÏÉÅÏäπ Ï†ÑÌôò
                    elif prev_slope > 0 and slope_10 < -5:
                        df.loc[i, 'TREND_REVERSAL'] = -1  # ÏÉÅÏäπÏóêÏÑú ÌïòÎùΩ Ï†ÑÌôò
                    else:
                        df.loc[i, 'TREND_REVERSAL'] = 0
        
        # ÌäπÏÑ± Ïª¨Îüº Î™©Î°ù
        self.feature_columns = [
            'TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M14AM10A', 'M14AM16',
            'RATIO', 'GOLDEN', 
            'SIGNAL_ZONE', 'PRE_SIGNAL', 'DANGER_ZONE',
            'NEAR_SIGNAL', 'SIGNAL_TO_DANGER_RISK',  # Ï†ÑÏù¥ ÌäπÏÑ±
            'SIGNAL_DURATION',
            'PATTERN', 'TREND', 'TREND_STRENGTH',
            'MA_3', 'MA_5', 'MA_10', 'MA_20', 'MA_30',
            'STD_3', 'STD_5', 'STD_10', 'STD_20', 'STD_30',
            'M14B_MA_3', 'M14B_MA_5', 'M14B_MA_10', 'M14B_MA_20', 'M14B_MA_30',
            'CHANGE_1', 'CHANGE_3', 'CHANGE_5', 'CHANGE_10', 'CHANGE_15',
            'M14B_CHANGE_1', 'M14B_CHANGE_3', 'M14B_CHANGE_5', 'M14B_CHANGE_10', 'M14B_CHANGE_15',
            'ACCEL_1', 'ACCEL_3', 'ACCEL_5', 'ACCEL_10', 'ACCEL_15',
            'RISE_COUNT', 'RISE_STREAK',
            'SPIKE_10', 'SPIKE_5',
            'BB_UPPER_20', 'BB_LOWER_20', 'BB_WIDTH_20', 'BB_POSITION_20',
            'RSI_14', 'RSI_7',
            'MACD', 'MACD_SIGNAL', 'MACD_DIFF'
        ]
        
        # NaN Ï≤òÎ¶¨
        df = df.fillna(0)
        
        print(f"‚úÖ Í≥†Í∏â ÌäπÏÑ± ÏÉùÏÑ± ÏôÑÎ£å: {len(self.feature_columns)}Í∞ú")
        return df
    
    def create_transition_sequences(self, df):
        """Ï†ÑÏù¥ Ìå®ÌÑ¥ Ï§ëÏã¨ ÏãúÌÄÄÏä§ ÏÉùÏÑ± - ÏÉÅÏäπ/ÌïòÎùΩ Íµ¨Î∂Ñ"""
        print("\nüîÑ Ï†ÑÏù¥ Ìå®ÌÑ¥ ÏãúÌÄÄÏä§ ÏÉùÏÑ± Ï§ë...")
        
        X_data = df[self.feature_columns].values
        y_data = df['TOTALCNT'].shift(-self.pred_len).values
        
        X, y = [], []
        transition_X, transition_y = [], []  # Ï†ÑÏù¥ Ìå®ÌÑ¥
        danger_X, danger_y = [], []  # ÏúÑÌóò Ìå®ÌÑ¥
        
        # ÏÉÅÏäπ/ÌïòÎùΩ Íµ¨Î∂Ñ Ïπ¥Ïö¥ÌÑ∞
        rise_signal_to_danger = 0
        rise_in_danger = 0
        fall_from_danger = 0
        
        for i in range(len(df) - self.seq_len - self.pred_len):
            seq_X = X_data[i:i+self.seq_len]
            target = y_data[i+self.seq_len-1]
            
            if np.isnan(target):
                continue
                
            current = df['TOTALCNT'].iloc[i+self.seq_len-1]
            
            X.append(seq_X)
            y.append(target)
            
            # ‚≠ê ÏÉÅÏäπ/ÌïòÎùΩ Íµ¨Î∂Ñ
            is_rising = target > current  # ÏÉÅÏäπ Ï§ë
            change_amount = target - current
            
            # Ï†ÑÏù¥ Ìå®ÌÑ¥ ÌÉêÏßÄ Î∞è Ïò§Î≤ÑÏÉòÌîåÎßÅ
            
            # 1. Ïã†Ìò∏‚ÜíÏúÑÌóò ÏÉÅÏäπ Ï†ÑÏù¥ (Í∞ÄÏû• Ï§ëÏöî!)
            if 1650 <= current <= 1699 and target >= 1700 and is_rising:
                # ÏÉÅÏäπÌïòÏó¨ ÏúÑÌóòÍµ¨Í∞Ñ ÏßÑÏûÖ - 20Î∞∞ Ïò§Î≤ÑÏÉòÌîåÎßÅ
                for _ in range(20):
                    transition_X.append(seq_X)
                    transition_y.append(target)
                rise_signal_to_danger += 1
                print(f"  üö® ÏÉÅÏäπ Ï†ÑÏù¥: {current:.0f} ‚Üó {target:.0f} (+{change_amount:.0f})")
            
            # 2. ÏúÑÌóòÍµ¨Í∞Ñ ÎÇ¥ ÏÉÅÏäπ ÏßÄÏÜç
            elif current >= 1700 and target >= 1700 and is_rising:
                # ÏúÑÌóòÍµ¨Í∞ÑÏóêÏÑú Í≥ÑÏÜç ÏÉÅÏäπ - 15Î∞∞
                for _ in range(15):
                    danger_X.append(seq_X)
                    danger_y.append(target)
                rise_in_danger += 1
            
            # 3. ÏúÑÌóòÍµ¨Í∞ÑÏóêÏÑú ÌïòÎùΩ (ÏïàÏ†ïÌôî)
            elif current >= 1700 and target < current:
                # ÌïòÎùΩ Ï§ë - 3Î∞∞Îßå (Îçú Ï§ëÏöî)
                for _ in range(3):
                    transition_X.append(seq_X)
                    transition_y.append(target)
                fall_from_danger += 1
            
            # 4. Í∏âÏÉÅÏäπ Ìå®ÌÑ¥ (50 Ïù¥ÏÉÅ)
            elif change_amount >= 50 and is_rising:
                # Ïñ¥Îäê Íµ¨Í∞ÑÏù¥Îì† Í∏âÏÉÅÏäπ - 12Î∞∞
                for _ in range(12):
                    transition_X.append(seq_X)
                    transition_y.append(target)
            
            # 5. Ïã†Ìò∏Íµ¨Í∞Ñ ÎÇ¥ ÏÉÅÏäπ
            elif 1651 <= current <= 1699 and is_rising and change_amount >= 20:
                # Ïã†Ìò∏Íµ¨Í∞Ñ ÎÇ¥ ÏÉÅÏäπ - 8Î∞∞
                for _ in range(8):
                    transition_X.append(seq_X)
                    transition_y.append(target)
            
            # 6. 1680 Ïù¥ÏÉÅÏóêÏÑú ÏÉÅÏäπ (ÏúÑÌóò ÏûÑÎ∞ï)
            elif current >= 1680 and is_rising:
                # ÏúÑÌóò ÏûÑÎ∞ï ÏÉÅÏäπ - 10Î∞∞
                for _ in range(10):
                    danger_X.append(seq_X)
                    danger_y.append(target)
        
        # Ïò§Î≤ÑÏÉòÌîåÎßÅ Îç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä
        if transition_X:
            X.extend(transition_X)
            y.extend(transition_y)
            print(f"  üî• Ï†ÑÏù¥ Ìå®ÌÑ¥ {len(transition_X)}Í∞ú Ï∂îÍ∞Ä")
        
        if danger_X:
            X.extend(danger_X)
            y.extend(danger_y)
            print(f"  üî• ÏúÑÌóò Ìå®ÌÑ¥ {len(danger_X)}Í∞ú Ï∂îÍ∞Ä")
        
        print(f"\n  üìä Ìå®ÌÑ¥ ÌÜµÍ≥Ñ:")
        print(f"    Ïã†Ìò∏‚ÜíÏúÑÌóò ÏÉÅÏäπ Ï†ÑÏù¥: {rise_signal_to_danger}Í±¥")
        print(f"    ÏúÑÌóòÍµ¨Í∞Ñ ÎÇ¥ ÏÉÅÏäπ: {rise_in_danger}Í±¥")
        print(f"    ÏúÑÌóòÍµ¨Í∞ÑÏóêÏÑú ÌïòÎùΩ: {fall_from_danger}Í±¥")
        
        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.float32)
        
        print(f"  Ï¥ù ÏãúÌÄÄÏä§: {X.shape[0]}Í∞ú")
        
        # ÏÖîÌîå
        shuffle_idx = np.arange(len(X))
        np.random.shuffle(shuffle_idx)
        X = X[shuffle_idx]
        y = y[shuffle_idx]
        
        return self.split_and_scale(X, y)
    
    def split_and_scale(self, X, y):
        """Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† Î∞è Ïä§ÏºÄÏùºÎßÅ"""
        # Î∂ÑÌï†
        train_size = int(len(X) * 0.7)
        val_size = int(len(X) * 0.15)
        
        X_train = X[:train_size]
        y_train = y[:train_size]
        X_val = X[train_size:train_size+val_size]
        y_val = y[train_size:train_size+val_size]
        X_test = X[train_size+val_size:]
        y_test = y[train_size+val_size:]
        
        # Ïä§ÏºÄÏùºÎßÅ
        X_train_flat = X_train.reshape(-1, X_train.shape[-1])
        self.scaler_X.fit(X_train_flat)
        
        X_train_scaled = self.scaler_X.transform(X_train_flat).reshape(X_train.shape)
        X_val_scaled = self.scaler_X.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)
        X_test_scaled = self.scaler_X.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)
        
        self.scaler_y.fit(y_train.reshape(-1, 1))
        y_train_scaled = self.scaler_y.transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = self.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        y_test_scaled = self.scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        # Íµ¨Í∞ÑÎ≥Ñ ÌÜµÍ≥Ñ
        self._print_statistics(y_train, y_val, y_test)
        
        # Î†àÏù¥Î∏î ÏÉùÏÑ±
        y_train_level = self._create_level_labels(y_train)
        y_val_level = self._create_level_labels(y_val)
        y_test_level = self._create_level_labels(y_test)
        
        y_train_danger = (y_train >= self.DANGER_MIN).astype(int)
        y_val_danger = (y_val >= self.DANGER_MIN).astype(int)
        y_test_danger = (y_test >= self.DANGER_MIN).astype(int)
        
        # Ïä§ÏºÄÏùºÎü¨ Ï†ÄÏû•
        os.makedirs('scalers_v7', exist_ok=True)
        with open('scalers_v7/scaler_X.pkl', 'wb') as f:
            pickle.dump(self.scaler_X, f)
        with open('scalers_v7/scaler_y.pkl', 'wb') as f:
            pickle.dump(self.scaler_y, f)
        
        return (X_train_scaled, y_train_scaled, y_train, y_train_level, y_train_danger), \
               (X_val_scaled, y_val_scaled, y_val, y_val_level, y_val_danger), \
               (X_test_scaled, y_test_scaled, y_test, y_test_level, y_test_danger)
    
    def _create_level_labels(self, y):
        """Íµ¨Í∞Ñ Î†àÏù¥Î∏î ÏÉùÏÑ±"""
        labels = np.zeros(len(y), dtype=int)
        labels[(y >= self.NORMAL_MAX) & (y <= self.CAUTION_MAX)] = 1
        labels[(y >= self.SIGNAL_MIN) & (y <= self.SIGNAL_MAX)] = 2
        labels[y >= self.DANGER_MIN] = 3
        return labels
    
    def _print_statistics(self, y_train, y_val, y_test):
        """ÌÜµÍ≥Ñ Ï∂úÎ†•"""
        print(f"\nüìä Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†:")
        print(f"  Train: {len(y_train):,}Í∞ú")
        print(f"  Val: {len(y_val):,}Í∞ú")
        print(f"  Test: {len(y_test):,}Í∞ú")
        
        for name, data in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:
            signal = ((data >= self.SIGNAL_MIN) & (data <= self.SIGNAL_MAX)).sum()
            danger = (data >= self.DANGER_MIN).sum()
            print(f"\n{name} - Ïã†Ìò∏: {signal}Í∞ú ({signal/len(data)*100:.1f}%), "
                  f"ÏúÑÌóò: {danger}Í∞ú ({danger/len(data)*100:.1f}%)")

# ========================================
# Í∞úÏÑ†Îêú Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò
# ========================================
class TransformerBlock(layers.Layer):
    """Transformer Î∏îÎ°ù - ÏãúÍ≥ÑÏó¥ Ïû•Í∏∞ ÏùòÏ°¥ÏÑ± ÌïôÏäµ"""
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super().__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

class TemporalConvBlock(layers.Layer):
    """Temporal Convolution - Îã§Ï§ë Ïä§ÏºÄÏùº ÏãúÍ∞Ñ Ìå®ÌÑ¥"""
    def __init__(self, filters, kernel_sizes=[3, 5, 7]):
        super().__init__()
        self.convs = []
        for k in kernel_sizes:
            self.convs.append(Conv1D(filters, k, padding='same', activation='relu'))
        self.batch_norm = BatchNormalization()
        self.dropout = Dropout(0.2)
        
    def call(self, inputs, training):
        outputs = []
        for conv in self.convs:
            outputs.append(conv(inputs))
        x = tf.concat(outputs, axis=-1)
        x = self.batch_norm(x)
        return self.dropout(x, training=training)

def build_extremenet_v7(input_shape, num_features):
    """ExtremeNet V7.0 - ÏúÑÌóòÍµ¨Í∞Ñ ÏòàÏ∏° Í∞ïÌôî Î™®Îç∏"""
    
    inputs = Input(shape=input_shape, name='input')
    
    # ========== 1. Feature Extraction ==========
    # Temporal Convolution (Îã§Ï§ë Ïä§ÏºÄÏùº Ìå®ÌÑ¥)
    temporal_conv = TemporalConvBlock(64)(inputs)
    
    # ========== 2. Îã§Ï§ë Í≤ΩÎ°ú Ï≤òÎ¶¨ ==========
    
    # Í≤ΩÎ°ú 1: Transformer (Ïû•Í∏∞ ÏùòÏ°¥ÏÑ±)
    transformer = TransformerBlock(input_shape[-1], 8, 128)(inputs)
    transformer = TransformerBlock(input_shape[-1], 8, 128)(transformer)
    transformer_pool = GlobalAveragePooling1D()(transformer)
    
    # Í≤ΩÎ°ú 2: Bidirectional LSTM (ÏñëÎ∞©Ìñ• ÏãúÍ≥ÑÏó¥)
    lstm = Bidirectional(LSTM(128, return_sequences=True))(temporal_conv)
    lstm = Bidirectional(LSTM(64))(lstm)
    lstm = Dropout(0.3)(lstm)
    
    # Í≤ΩÎ°ú 3: CNN + Attention (Ï§ëÏöî Ìå®ÌÑ¥)
    cnn = Conv1D(128, 5, activation='relu')(inputs)
    cnn = BatchNormalization()(cnn)
    cnn = Conv1D(64, 3, activation='relu')(cnn)
    attention = MultiHeadAttention(num_heads=4, key_dim=64)(cnn, cnn)
    cnn_pool = GlobalMaxPooling1D()(attention)
    
    # Í≤ΩÎ°ú 4: GRU (ÏµúÍ∑º Ìå®ÌÑ¥)
    recent = Lambda(lambda x: x[:, -30:, :])(inputs)
    gru = GRU(64, return_sequences=True)(recent)
    gru = GRU(32)(gru)
    
    # Í≤ΩÎ°ú 5: ÌÜµÍ≥Ñ ÌäπÏÑ± (ÏßÅÏ†ë Ï∂îÏ∂ú)
    stats = Lambda(lambda x: tf.concat([
        tf.reduce_mean(x, axis=1, keepdims=True),
        tf.reduce_max(x, axis=1, keepdims=True),
        tf.reduce_min(x, axis=1, keepdims=True),
        tf.math.reduce_std(x, axis=1, keepdims=True)
    ], axis=1))(inputs)
    stats = Flatten()(stats)
    
    # ========== 3. ÌäπÏÑ± ÌÜµÌï© ==========
    merged = Concatenate()([transformer_pool, lstm, cnn_pool, gru, stats])
    merged = BatchNormalization()(merged)
    merged = Dropout(0.3)(merged)
    
    # ========== 4. ÏòàÏ∏° Ìó§Îìú ==========
    
    # Í≥µÌÜµ ÌäπÏÑ±
    x = Dense(256, activation='relu')(merged)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    # Î©îÏù∏ ÏòàÏ∏° (ÌöåÍ∑Ä)
    main_branch = Dense(64, activation='relu')(x)
    output_reg = Dense(1, name='regression')(main_branch)
    
    # Íµ¨Í∞Ñ Î∂ÑÎ•ò (4Íµ¨Í∞Ñ)
    class_branch = Dense(32, activation='relu')(x)
    output_cls = Dense(4, activation='softmax', name='classification')(class_branch)
    
    # ÏúÑÌóò Í∞êÏßÄ (Ïù¥ÏßÑ Î∂ÑÎ•ò)
    danger_branch = Dense(32, activation='relu')(x)
    output_danger = Dense(1, activation='sigmoid', name='danger')(danger_branch)
    
    model = Model(inputs, [output_reg, output_cls, output_danger], 
                  name='ExtremeNet_V7')
    
    return model

# ========================================
# Í∞úÏÑ†Îêú ÏÜêÏã§Ìï®Ïàò
# ========================================
class AdaptiveLoss(tf.keras.losses.Loss):
    """Ï†ÅÏùëÌòï ÏÜêÏã§Ìï®Ïàò - Íµ¨Í∞ÑÎ≥Ñ ÎèôÏ†Å Í∞ÄÏ§ëÏπò"""
    
    def __init__(self, processor):
        super().__init__()
        self.processor = processor
        
    def call(self, y_true, y_pred):
        # Í∏∞Î≥∏ MAE
        mae = tf.abs(y_true - y_pred)
        
        # ÎèôÏ†Å Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞
        weights = tf.ones_like(y_true)
        
        # Í∞í Î≤îÏúÑÎ•º Ïã§Ï†ú Ïä§ÏºÄÏùºÎ°ú Î≥ÄÌôò (Ïó≠Î≥ÄÌôò ÏãúÎÆ¨Î†àÏù¥ÏÖò)
        y_true_rescaled = y_true * 1000 + 1000  # ÎåÄÎûµÏ†Å Î≥ÄÌôò
        
        # Ïã†Ìò∏Íµ¨Í∞Ñ (1651~1699)
        signal_mask = tf.logical_and(
            y_true_rescaled >= 1651,
            y_true_rescaled <= 1699
        )
        weights = tf.where(signal_mask, 5.0, weights)
        
        # ÏúÑÌóòÍµ¨Í∞Ñ (1700+)
        danger_mask = y_true_rescaled >= 1700
        weights = tf.where(danger_mask, 10.0, weights)
        
        # Ï†ÑÏù¥ Íµ¨Í∞Ñ (1680~1720) ÌäπÎ≥Ñ Í∞ÄÏ§ëÏπò
        transition_mask = tf.logical_and(
            y_true_rescaled >= 1680,
            y_true_rescaled <= 1720
        )
        weights = tf.where(transition_mask, 15.0, weights)
        
        # Ïò§Ï∞®Í∞Ä ÌÅ∞ Í≤ΩÏö∞ Ï∂îÍ∞Ä Ìå®ÎÑêÌã∞
        large_error_mask = mae > 0.1  # Ïä§ÏºÄÏùºÎêú Í∞í Í∏∞Ï§Ä
        weights = tf.where(large_error_mask, weights * 1.5, weights)
        
        weighted_mae = mae * weights
        return tf.reduce_mean(weighted_mae)

# ========================================
# ÌïôÏäµ Í¥ÄÎ¶¨
# ========================================
class TrainingManager:
    def __init__(self, processor):
        self.processor = processor
        self.best_score = float('inf')
        
    def train(self, train_data, val_data, test_data):
        """Î™®Îç∏ ÌïôÏäµ"""
        X_train, y_train_scaled, y_train, y_train_level, y_train_danger = train_data
        X_val, y_val_scaled, y_val, y_val_level, y_val_danger = val_data
        X_test, y_test_scaled, y_test, y_test_level, y_test_danger = test_data
        
        # Î™®Îç∏ ÏÉùÏÑ±
        with strategy.scope():
            model = build_extremenet_v7(
                input_shape=(X_train.shape[1], X_train.shape[2]),
                num_features=X_train.shape[2]
            )
            
            # Îã§Ï§ë ÏòµÌã∞ÎßàÏù¥Ï†Ä (ÌïôÏäµÎ•† Ïä§ÏºÄÏ§ÑÎßÅ)
            initial_lr = 0.001
            lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
                initial_lr, 1000, alpha=0.1
            )
            
            model.compile(
                optimizer=Adam(learning_rate=lr_schedule),
                loss={
                    'regression': AdaptiveLoss(self.processor),
                    'classification': 'sparse_categorical_crossentropy',
                    'danger': 'binary_crossentropy'
                },
                loss_weights={
                    'regression': 0.5,
                    'classification': 0.2,
                    'danger': 0.3  # ÏúÑÌóò Í∞êÏßÄ Ï§ëÏöîÎèÑ Ï¶ùÍ∞Ä
                },
                metrics={
                    'regression': 'mae',
                    'classification': 'accuracy',
                    'danger': ['accuracy', tf.keras.metrics.AUC(name='auc')]
                }
            )
        
        print("\n" + "="*80)
        print("üî• ExtremeNet V7.0 ÌïôÏäµ ÏãúÏûë")
        print("="*80)
        
        # ÏΩúÎ∞±
        callbacks = [
            ModelCheckpoint(
                'models_v7/best_model.keras',
                save_best_only=True,
                monitor='val_regression_mae',
                mode='min',
                verbose=1
            ),
            EarlyStopping(
                patience=30,
                restore_best_weights=True,
                monitor='val_regression_mae',
                mode='min',
                verbose=1
            ),
            ReduceLROnPlateau(
                factor=0.5,
                patience=10,
                min_lr=1e-7,
                monitor='val_regression_mae',
                mode='min',
                verbose=1
            ),
            LambdaCallback(
                on_epoch_end=lambda epoch, logs: 
                self._evaluate_danger_zone(model, X_val, y_val, epoch)
            )
        ]
        
        # ÌïôÏäµ
        os.makedirs('models_v7', exist_ok=True)
        
        history = model.fit(
            X_train,
            {
                'regression': y_train_scaled,
                'classification': y_train_level,
                'danger': y_train_danger
            },
            validation_data=(
                X_val,
                {
                    'regression': y_val_scaled,
                    'classification': y_val_level,
                    'danger': y_val_danger
                }
            ),
            epochs=150,  # Îçî ÎßéÏùÄ ÏóêÌè¨ÌÅ¨
            batch_size=64,
            callbacks=callbacks,
            verbose=1
        )
        
        # ÏµúÏ¢Ö ÌèâÍ∞Ä
        self._final_evaluation(model, X_test, y_test_scaled, y_test)
        
        # Î™®Îç∏ Ï†ÄÏû•
        model.save('models_v7/extremenet_v7_final.keras')
        print(f"\nüíæ ÏµúÏ¢Ö Î™®Îç∏: models_v7/extremenet_v7_final.keras")
        
        return model, history
    
    def _evaluate_danger_zone(self, model, X_val, y_val, epoch):
        """ÏúÑÌóòÍµ¨Í∞Ñ ÌèâÍ∞Ä"""
        if epoch % 5 == 0:
            preds = model.predict(X_val, verbose=0)
            y_pred = self.processor.scaler_y.inverse_transform(
                preds[0].reshape(-1, 1)
            ).flatten()
            
            # ÏúÑÌóòÍµ¨Í∞Ñ ÌèâÍ∞Ä
            danger_idx = y_val >= 1700
            if danger_idx.sum() > 0:
                danger_mae = mean_absolute_error(y_val[danger_idx], y_pred[danger_idx])
                print(f"\n  üî¥ ÏóêÌè¨ÌÅ¨ {epoch} - ÏúÑÌóòÍµ¨Í∞Ñ MAE: {danger_mae:.2f}")
                
                # Ï†ÑÏù¥ Ìå®ÌÑ¥ ÌèâÍ∞Ä
                transition_idx = (y_val >= 1680) & (y_val <= 1720)
                if transition_idx.sum() > 0:
                    trans_mae = mean_absolute_error(y_val[transition_idx], y_pred[transition_idx])
                    print(f"  üéØ Ï†ÑÏù¥Íµ¨Í∞Ñ(1680~1720) MAE: {trans_mae:.2f}")
    
    def _final_evaluation(self, model, X_test, y_test_scaled, y_test):
        """ÏµúÏ¢Ö ÌèâÍ∞Ä"""
        print("\n" + "="*80)
        print("üìä ÏµúÏ¢Ö ÌèâÍ∞Ä")
        print("="*80)
        
        preds = model.predict(X_test, verbose=0)
        y_pred = self.processor.scaler_y.inverse_transform(
            preds[0].reshape(-1, 1)
        ).flatten()
        
        # Ï†ÑÏ≤¥ Î©îÌä∏Î¶≠
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)
        
        print(f"\nüìà Ï†ÑÏ≤¥ ÏÑ±Îä•:")
        print(f"  MAE: {mae:.2f}")
        print(f"  RMSE: {rmse:.2f}")
        print(f"  R¬≤: {r2:.4f}")
        
        # Íµ¨Í∞ÑÎ≥Ñ ÏÉÅÏÑ∏ ÌèâÍ∞Ä
        for name, min_val, max_val in [
            ("Ïã†Ìò∏Íµ¨Í∞Ñ(1651~1699)", 1651, 1699),
            ("Ï†ÑÏù¥Íµ¨Í∞Ñ(1680~1720)", 1680, 1720),
            ("ÏúÑÌóòÍµ¨Í∞Ñ(1700+)", 1700, float('inf'))
        ]:
            if max_val == float('inf'):
                idx = y_test >= min_val
            else:
                idx = (y_test >= min_val) & (y_test <= max_val)
            
            if idx.sum() > 0:
                zone_mae = mean_absolute_error(y_test[idx], y_pred[idx])
                zone_acc = np.mean(np.abs(y_test[idx] - y_pred[idx]) < 30)
                
                print(f"\n{'üü†' if 'Ïã†Ìò∏' in name else 'üî¥'} {name}:")
                print(f"  ÏÉòÌîå: {idx.sum()}Í∞ú")
                print(f"  MAE: {zone_mae:.2f}")
                print(f"  Ï†ïÌôïÎèÑ(¬±30): {zone_acc*100:.1f}%")

# ========================================
# Í∞úÏÑ†Îêú Ïã§ÏãúÍ∞Ñ ÏòàÏ∏°
# ========================================
class EnhancedPredictor:
    """ÏÉÅÏäπ/ÌïòÎùΩ Íµ¨Î∂Ñ Í∞ïÌôî ÏòàÏ∏°Í∏∞"""
    
    def __init__(self, model_path='models_v7/extremenet_v7_final.keras'):
        print("\nüöÄ ÏòàÏ∏°Í∏∞ Î°úÎî© Ï§ë...")
        self.model = tf.keras.models.load_model(
            model_path,
            custom_objects={'AdaptiveLoss': AdaptiveLoss}
        )
        
        with open('scalers_v7/scaler_X.pkl', 'rb') as f:
            self.scaler_X = pickle.load(f)
        with open('scalers_v7/scaler_y.pkl', 'rb') as f:
            self.scaler_y = pickle.load(f)
        
        self.processor = EnhancedDataProcessor()
        print("‚úÖ ÏòàÏ∏°Í∏∞ Ï§ÄÎπÑ ÏôÑÎ£å")
    
    def predict_with_trend(self, df_100min):
        """100Î∂Ñ Îç∞Ïù¥ÌÑ∞Î°ú 10Î∂Ñ ÌõÑ ÏòàÏ∏° - ÏÉÅÏäπ/ÌïòÎùΩ ÏÉÅÏÑ∏ Î∂ÑÏÑù"""
        
        # ÌòÑÏû¨Í∞í
        current_value = df_100min['TOTALCNT'].iloc[-1]
        recent_values = df_100min['TOTALCNT'].tail(10).values
        
        # Îã§Ï§ë ÏãúÍ∞ÑÎåÄ Ï∂îÏÑ∏ Î∂ÑÏÑù
        trend_5min = self._calculate_trend(df_100min['TOTALCNT'].tail(5).values)
        trend_10min = self._calculate_trend(df_100min['TOTALCNT'].tail(10).values)
        trend_20min = self._calculate_trend(df_100min['TOTALCNT'].tail(20).values)
        
        # ÌäπÏÑ± ÏÉùÏÑ±
        df_features = self.processor.create_advanced_features(df_100min)
        X = df_features[self.processor.feature_columns].values
        X_scaled = self.scaler_X.transform(X.reshape(-1, len(self.processor.feature_columns)))
        X_scaled = X_scaled.reshape(1, 100, len(self.processor.feature_columns))
        
        # ÏòàÏ∏°
        preds = self.model.predict(X_scaled, verbose=0)
        y_pred_scaled = preds[0][0, 0]
        y_pred = self.scaler_y.inverse_transform([[y_pred_scaled]])[0, 0]
        
        # Î≥ÄÌôîÎüâ Í≥ÑÏÇ∞
        change = y_pred - current_value
        change_rate = (change / current_value) * 100
        
        # ÏúÑÌóòÎèÑ Ï†êÏàò (0~100)
        danger_prob = float(preds[2][0, 0]) * 100
        
        # ÏÉÅÏÑ∏ ÏßÑÎã®
        diagnosis = self._diagnose_situation(
            current_value, y_pred, change, 
            trend_5min, trend_10min, trend_20min,
            danger_prob
        )
        
        return {
            'current': float(current_value),
            'prediction': float(y_pred),
            'change': float(change),
            'change_rate': float(change_rate),
            'trend_5min': trend_5min,
            'trend_10min': trend_10min,
            'trend_20min': trend_20min,
            'danger_probability': danger_prob,
            'level': diagnosis['level'],
            'status': diagnosis['status'],
            'message': diagnosis['message'],
            'action': diagnosis['action'],
            'risk_score': diagnosis['risk_score']
        }
    
    def _calculate_trend(self, values):
        """Ï∂îÏÑ∏ Í≥ÑÏÇ∞"""
        if len(values) < 2:
            return {'direction': 'flat', 'strength': 0}
        
        slope = np.polyfit(range(len(values)), values, 1)[0]
        
        if slope > 5:
            direction = 'rising'
            strength = min(slope / 10, 10)  # 0~10 Ïä§ÏºÄÏùº
        elif slope < -5:
            direction = 'falling'
            strength = min(abs(slope) / 10, 10)
        else:
            direction = 'flat'
            strength = 0
            
        return {'direction': direction, 'strength': strength}
    
    def _diagnose_situation(self, current, predicted, change, 
                           trend_5, trend_10, trend_20, danger_prob):
        """ÏÉÅÌô© ÏßÑÎã® - ÏÉÅÏäπ/ÌïòÎùΩ Íµ¨Î∂Ñ"""
        
        diagnosis = {
            'level': '',
            'status': '',
            'message': '',
            'action': '',
            'risk_score': 0
        }
        
        # ÌòÑÏû¨ Íµ¨Í∞Ñ ÌåêÏ†ï
        if current < 1400:
            current_zone = 'normal'
        elif current < 1651:
            current_zone = 'caution'
        elif current < 1700:
            current_zone = 'signal'
        else:
            current_zone = 'danger'
        
        # ÏòàÏ∏° Íµ¨Í∞Ñ ÌåêÏ†ï
        if predicted < 1400:
            pred_zone = 'normal'
        elif predicted < 1651:
            pred_zone = 'caution'
        elif predicted < 1700:
            pred_zone = 'signal'
        else:
            pred_zone = 'danger'
        
        # ÏÉÅÏäπ/ÌïòÎùΩ Ï¢ÖÌï© ÌåêÎã®
        is_rising = change > 0
        is_accelerating = (trend_5['strength'] > trend_10['strength'] and 
                          trend_5['direction'] == 'rising')
        
        # ‚≠ê ÌïµÏã¨ ÏºÄÏù¥Ïä§Î≥Ñ ÏßÑÎã®
        
        # ÏºÄÏù¥Ïä§ 1: Ïã†Ìò∏‚ÜíÏúÑÌóò ÏÉÅÏäπ Ï†ÑÏù¥ (Í∞ÄÏû• ÏúÑÌóò!)
        if current_zone == 'signal' and pred_zone == 'danger' and is_rising:
            diagnosis['level'] = 'üî¥üî¥üî¥ Í∑πÎèÑÏúÑÌóò'
            diagnosis['status'] = 'Í∏âÏ¶ù ÏßÑÌñâ'
            diagnosis['message'] = f'1700+ ÎèåÌåå ÏòàÏÉÅ! ÌòÑÏû¨ {current:.0f} ‚Üí {predicted:.0f} (+{change:.0f})'
            diagnosis['action'] = 'üö® Ï¶âÏãú ÎπÑÏÉÅÎåÄÏùë! Î¨ºÎ•ò Í∏âÏ¶ù ÏûÑÎ∞ï'
            diagnosis['risk_score'] = 100
        
        # ÏºÄÏù¥Ïä§ 2: ÏúÑÌóòÍµ¨Í∞Ñ ÎÇ¥ Í≥ÑÏÜç ÏÉÅÏäπ
        elif current_zone == 'danger' and pred_zone == 'danger' and is_rising:
            diagnosis['level'] = 'üî¥üî¥ Îß§Ïö∞ÏúÑÌóò'
            diagnosis['status'] = 'ÏúÑÌóò ÏÉÅÏäπ'
            diagnosis['message'] = f'ÏúÑÌóòÍµ¨Í∞Ñ ÎÇ¥ ÏÉÅÏäπ ÏßÄÏÜç {current:.0f} ‚Üí {predicted:.0f}'
            diagnosis['action'] = '‚ö†Ô∏è Í∏¥Í∏âÎåÄÏùë Ïú†ÏßÄ'
            diagnosis['risk_score'] = 90
        
        # ÏºÄÏù¥Ïä§ 3: ÏúÑÌóòÍµ¨Í∞ÑÏóêÏÑú ÌïòÎùΩ (ÏïàÏ†ïÌôî)
        elif current_zone == 'danger' and not is_rising:
            diagnosis['level'] = 'üü† ÏúÑÌóòÏôÑÌôî'
            diagnosis['status'] = 'ÌïòÎùΩ Ï†ÑÌôò'
            diagnosis['message'] = f'ÌîºÌÅ¨ ÌÜµÍ≥º, ÌïòÎùΩ Ï§ë {current:.0f} ‚Üí {predicted:.0f} ({change:.0f})'
            diagnosis['action'] = 'üìâ Î™®ÎãàÌÑ∞ÎßÅ ÏßÄÏÜç, ÏïàÏ†ïÌôî ÌôïÏù∏'
            diagnosis['risk_score'] = 60
        
        # ÏºÄÏù¥Ïä§ 4: Ïã†Ìò∏Íµ¨Í∞Ñ ÎÇ¥ Í∏âÏÉÅÏäπ
        elif current_zone == 'signal' and is_rising and change >= 30:
            diagnosis['level'] = 'üü†üü† Ï£ºÏùò'
            diagnosis['status'] = 'Í∏âÏÉÅÏäπ Í∞êÏßÄ'
            diagnosis['message'] = f'Ïã†Ìò∏Íµ¨Í∞Ñ Í∏âÏÉÅÏäπ {current:.0f} ‚Üí {predicted:.0f} (+{change:.0f})'
            diagnosis['action'] = '‚ö†Ô∏è ÏÇ¨Ï†Ñ ÎåÄÏùë Ï§ÄÎπÑ'
            diagnosis['risk_score'] = 75
        
        # ÏºÄÏù¥Ïä§ 5: Í∞ÄÏÜç ÏÉÅÏäπ (Ïñ¥Îäê Íµ¨Í∞ÑÏù¥Îì†)
        elif is_accelerating and change >= 20:
            diagnosis['level'] = 'üü°üü° Í≤ΩÍ≥Ñ'
            diagnosis['status'] = 'Í∞ÄÏÜç ÏÉÅÏäπ'
            diagnosis['message'] = f'ÏÉÅÏäπ Í∞ÄÏÜçÎèÑ Ï¶ùÍ∞Ä Ï§ë (+{change:.0f})'
            diagnosis['action'] = 'üìà Î©¥Î∞ÄÌïú Î™®ÎãàÌÑ∞ÎßÅ ÌïÑÏöî'
            diagnosis['risk_score'] = 65
        
        # ÏºÄÏù¥Ïä§ 6: 1680 Ïù¥ÏÉÅÏóêÏÑú ÏÉÅÏäπ (ÏúÑÌóò ÏûÑÎ∞ï)
        elif current >= 1680 and is_rising:
            diagnosis['level'] = 'üü†üü†üü† Í≥†ÏúÑÌóò'
            diagnosis['status'] = 'ÏúÑÌóò ÏûÑÎ∞ï'
            diagnosis['message'] = f'1700 ÏûÑÎ∞ï! {current:.0f} ‚Üí {predicted:.0f}'
            diagnosis['action'] = 'üîî ÎåÄÏùëÌåÄ ÎåÄÍ∏∞'
            diagnosis['risk_score'] = 85
        
        # ÏºÄÏù¥Ïä§ 7: ÏïàÏ†ï Íµ¨Í∞Ñ
        else:
            if pred_zone == 'normal':
                diagnosis['level'] = 'üü¢ Ï†ïÏÉÅ'
                diagnosis['status'] = 'ÏïàÏ†ï'
                diagnosis['message'] = f'Ï†ïÏÉÅ Î≤îÏúÑ Ïú†ÏßÄ {current:.0f} ‚Üí {predicted:.0f}'
                diagnosis['action'] = '‚úì Ï†ïÏÉÅ Ïö¥ÏòÅ'
                diagnosis['risk_score'] = 20
            else:
                diagnosis['level'] = 'üü° Ï£ºÏùò'
                diagnosis['status'] = 'Í¥ÄÏ∞∞ ÌïÑÏöî'
                diagnosis['message'] = f'Î≥ÄÎèô Í¥ÄÏ∞∞ {current:.0f} ‚Üí {predicted:.0f}'
                diagnosis['action'] = 'üëÄ ÏßÄÏÜç Î™®ÎãàÌÑ∞ÎßÅ'
                diagnosis['risk_score'] = 40
        
        # ÏúÑÌóòÎèÑ Ï†êÏàò Î≥¥Ï†ï
        if danger_prob > 70:
            diagnosis['risk_score'] = min(diagnosis['risk_score'] + 10, 100)
        
        return diagnosis
def main():
    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    data_path = None
    for path in ['data/20240201_TO_202507281705.csv', 
                 '20240201_TO_202507281705.csv']:
        if os.path.exists(path):
            data_path = path
            break
    
    if not data_path:
        print("‚ùå Îç∞Ïù¥ÌÑ∞ ÌååÏùº ÏóÜÏùå!")
        return None
    
    # Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
    processor = EnhancedDataProcessor()
    
    df = pd.read_csv(data_path)
    df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                   format='%Y%m%d%H%M', errors='coerce')
    df = df.sort_values('CURRTIME').reset_index(drop=True)
    df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
    
    print(f"‚úÖ Îç∞Ïù¥ÌÑ∞: {len(df):,}Í∞ú Î°úÎìú")
    
    # Í≥†Í∏â ÌäπÏÑ± ÏÉùÏÑ±
    df = processor.create_advanced_features(df)
    
    # Ï†ÑÏù¥ Ìå®ÌÑ¥ ÏãúÌÄÄÏä§ ÏÉùÏÑ±
    train_data, val_data, test_data = processor.create_transition_sequences(df)
    
    # ÌïôÏäµ
    trainer = TrainingManager(processor)
    model, history = trainer.train(train_data, val_data, test_data)
    
    print("\n‚úÖ ÌïôÏäµ ÏôÑÎ£å!")
    print("üî• ExtremeNet V7.0 - ÏúÑÌóòÍµ¨Í∞Ñ ÏòàÏ∏° Í∞ïÌôî ÏôÑÎ£å!")
    
    return model, history

if __name__ == "__main__":
    model, history = main()