"""
üî• ExtremeNet V6.5 - Ìå®ÌÑ¥ ÌïôÏäµ Í∞ïÌôî Î≤ÑÏ†Ñ
============================================
‚úÖ ExtremeNet Îã®Ïùº Î™®Îç∏ ÏßëÏ§ë
‚úÖ ÏúÑÌóòÍµ¨Í∞Ñ(1651~1682) ÌäπÎ≥Ñ ÌïôÏäµ
‚úÖ Ìå®ÌÑ¥ Ïù∏Ïãù Í∞ïÌôî (UU1/UU2)
‚úÖ R2 ÏùåÏàò Î∞©ÏßÄ Ï≤òÎ¶¨
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import *
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
import os
import pickle
import json
from datetime import datetime, timedelta

warnings.filterwarnings('ignore')
tf.random.set_seed(42)
np.random.seed(42)

# GPU ÏÑ§Ï†ï
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    print(f"üîß GPU {len(gpus)}Í∞ú Î∞úÍ≤¨")
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    strategy = tf.distribute.MirroredStrategy()
    device_type = 'GPU'
else:
    print("‚ö†Ô∏è GPU ÏóÜÏùå, CPU ÏÇ¨Ïö©")
    strategy = tf.distribute.get_strategy()
    device_type = 'CPU'

print("="*80)
print("üî• ExtremeNet V6.5 - Ìå®ÌÑ¥ ÌïôÏäµ Í∞ïÌôî Î≤ÑÏ†Ñ")
print(f"üì¶ TensorFlow: {tf.__version__}")
print(f"üîß Device: {device_type}")
print("="*80)

# ========================================
# Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÌÅ¥ÎûòÏä§ (Ìå®ÌÑ¥ Í∞ïÌôî)
# ========================================
class PatternDataProcessor:
    def __init__(self, seq_len=100, pred_len=10):
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.feature_scaler = RobustScaler()  # Ïù¥ÏÉÅÏπòÏóê Í∞ïÍ±¥
        self.target_scaler = RobustScaler()
        self.feature_columns = None
        
        # Íµ¨Í∞Ñ Ï†ïÏùò
        self.NORMAL_MAX = 1400
        self.WARNING_MAX = 1700
        self.DANGER_MIN = 1651  # ÏúÑÌóò Ïã†Ìò∏ ÏãúÏûë
        self.DANGER_MAX = 1682  # ÏúÑÌóò Ïã†Ìò∏ ÎÅù
        
    def save_scalers(self, path='scalers_v65/'):
        """Ïä§ÏºÄÏùºÎü¨ Ï†ÄÏû•"""
        os.makedirs(path, exist_ok=True)
        
        with open(f'{path}feature_scaler.pkl', 'wb') as f:
            pickle.dump(self.feature_scaler, f)
        with open(f'{path}target_scaler.pkl', 'wb') as f:
            pickle.dump(self.target_scaler, f)
            
        config = {
            'seq_len': self.seq_len,
            'pred_len': self.pred_len,
            'feature_columns': self.feature_columns
        }
        with open(f'{path}config.json', 'w') as f:
            json.dump(config, f)
            
        print(f"‚úÖ Ïä§ÏºÄÏùºÎü¨ Ï†ÄÏû•: {path}")
    
    def load_and_process(self, filepath):
        print(f"\nüìÇ Îç∞Ïù¥ÌÑ∞ Î°úÎî©: {filepath}")
        df = pd.read_csv(filepath)
        print(f"‚úÖ ÏõêÎ≥∏: {df.shape[0]:,}Ìñâ √ó {df.shape[1]}Ïó¥")
        
        # 0Í∞í Ï†úÍ±∞
        before = len(df)
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        print(f"‚úÖ 0Í∞í Ï†úÍ±∞: {before - len(df):,}Í∞ú")
        
        # ÏãúÍ∞Ñ Ï†ïÎ†¨
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                       format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        # Íµ¨Í∞Ñ Î∂ÑÏÑù
        normal = (df['TOTALCNT'] < self.NORMAL_MAX).sum()
        warning = ((df['TOTALCNT'] >= self.NORMAL_MAX) & 
                  (df['TOTALCNT'] < self.WARNING_MAX)).sum()
        critical = (df['TOTALCNT'] >= self.WARNING_MAX).sum()
        danger_signal = ((df['TOTALCNT'] >= self.DANGER_MIN) & 
                        (df['TOTALCNT'] <= self.DANGER_MAX)).sum()
        
        print(f"\nüìä Íµ¨Í∞Ñ Î∂ÑÌè¨:")
        print(f"  Ï†ïÏÉÅ(~1400): {normal:,}Í∞ú ({normal/len(df)*100:.1f}%)")
        print(f"  Ï£ºÏùò(1400~1699): {warning:,}Í∞ú ({warning/len(df)*100:.1f}%)")
        print(f"  Ïã¨Í∞Å(1700+): {critical:,}Í∞ú ({critical/len(df)*100:.1f}%)")
        print(f"  üî• ÏúÑÌóòÏã†Ìò∏(1651~1682): {danger_signal:,}Í∞ú ({danger_signal/len(df)*100:.1f}%)")
        
        return df
    
    def detect_pattern(self, df, window=100):
        """Ìå®ÌÑ¥ Í∞êÏßÄ (UU1/UU2)"""
        patterns = []
        
        for i in range(len(df)):
            if i < window:
                patterns.append(0)  # Ìå®ÌÑ¥ ÏóÜÏùå
                continue
                
            # ÏµúÍ∑º window Îç∞Ïù¥ÌÑ∞
            recent = df.iloc[max(0, i-window):i]
            
            # Ìå®ÌÑ¥ ÌåêÏ†ï Í∏∞Ï§Ä
            high_cases = (recent['TOTALCNT'] >= self.DANGER_MAX).sum()
            m14b_mean = recent['M14AM14B'].mean()
            m14b_350_ratio = (recent['M14AM14B'] > 350).mean()
            danger_zone = ((recent['TOTALCNT'] >= self.DANGER_MIN) & 
                          (recent['TOTALCNT'] <= self.DANGER_MAX)).sum()
            
            # UU2: Í≥†ÏúÑÌóò Ìå®ÌÑ¥ (Î≥¥ÏàòÏ†Å)
            if high_cases > 0 and m14b_mean > 380:
                patterns.append(2)  # UU2
            elif m14b_350_ratio > 0.3:
                patterns.append(2)  # UU2
            # UU1: Í∏âÏ¶ù Í∞ÄÎä• Ìå®ÌÑ¥
            elif danger_zone > 0:  # ÏúÑÌóò Ïã†Ìò∏ Í∞êÏßÄ
                patterns.append(1)  # UU1 - Í∏âÏ¶ù ÏòàÏÉÅ
            else:
                patterns.append(0)  # Ï†ïÏÉÅ
                
        df['PATTERN'] = patterns
        
        print(f"\nüìä Ìå®ÌÑ¥ Î∂ÑÌè¨:")
        print(f"  Ï†ïÏÉÅ: {(df['PATTERN'] == 0).sum():,}Í∞ú")
        print(f"  UU1(Í∏âÏ¶ùÍ∞ÄÎä•): {(df['PATTERN'] == 1).sum():,}Í∞ú")
        print(f"  UU2(Í≥†ÏúÑÌóò): {(df['PATTERN'] == 2).sum():,}Í∞ú")
        
        return df
    
    def create_features(self, df):
        print("\n‚öôÔ∏è ÌäπÏÑ± ÏÉùÏÑ±...")
        
        # Ìå®ÌÑ¥ Í∞êÏßÄ
        df = self.detect_pattern(df)
        
        # Í∏∞Î≥∏ ÌäπÏÑ±
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        
        # ÏúÑÌóò Ïã†Ìò∏ ÌäπÏÑ±
        df['DANGER_SIGNAL'] = ((df['TOTALCNT'] >= self.DANGER_MIN) & 
                               (df['TOTALCNT'] <= self.DANGER_MAX)).astype(float)
        
        # ÏãúÍ∞Ñ ÌäπÏÑ±
        df['HOUR'] = df['CURRTIME'].dt.hour
        df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
        df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
        
        # Ïù¥Îèô ÌÜµÍ≥Ñ
        for w in [5, 10, 20, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
            
            # M14AM14B Ïù¥ÎèôÌèâÍ∑†
            df[f'M14B_MA_{w}'] = df['M14AM14B'].rolling(w, min_periods=1).mean()
        
        # Î≥ÄÌôîÏú®
        for lag in [1, 5, 10, 20]:
            df[f'CHANGE_{lag}'] = df['TOTALCNT'].diff(lag).fillna(0)
            df[f'M14B_CHANGE_{lag}'] = df['M14AM14B'].diff(lag).fillna(0)
        
        # Ïó∞ÏÜç ÏÉÅÏäπ ÌöüÏàò
        df['CONSECUTIVE_RISE'] = 0
        for i in range(1, len(df)):
            if df.loc[i, 'TOTALCNT'] > df.loc[i-1, 'TOTALCNT']:
                df.loc[i, 'CONSECUTIVE_RISE'] = df.loc[i-1, 'CONSECUTIVE_RISE'] + 1
            else:
                df.loc[i, 'CONSECUTIVE_RISE'] = 0
        
        # ÌÉÄÍ≤ü ÏÉùÏÑ±
        df['TARGET'] = df['TOTALCNT'].shift(-self.pred_len)
        
        # Íµ¨Í∞Ñ Î†àÏù¥Î∏î (3Íµ¨Í∞Ñ)
        df['LEVEL'] = 0  # Ï†ïÏÉÅ
        df.loc[df['TARGET'] >= self.NORMAL_MAX, 'LEVEL'] = 1  # Ï£ºÏùò
        df.loc[df['TARGET'] >= self.WARNING_MAX, 'LEVEL'] = 2  # Ïã¨Í∞Å
        
        # ÏúÑÌóò Ïã†Ìò∏ Î†àÏù¥Î∏î (1651~1682 Íµ¨Í∞Ñ ÌäπÎ≥Ñ Ï≤òÎ¶¨)
        df['IS_DANGER'] = ((df['TARGET'] >= self.DANGER_MIN) & 
                          (df['TARGET'] <= self.DANGER_MAX)).astype(int)
        
        df = df.dropna()
        print(f"‚úÖ Ïú†Ìö® Îç∞Ïù¥ÌÑ∞: {len(df)}Ìñâ")
        
        return df
    
    def create_sequences(self, df):
        print("\nüîÑ ÏãúÌÄÄÏä§ ÏÉùÏÑ±...")
        
        # ÌäπÏÑ± ÏÑ†ÌÉù (ExtremeNetÏö© ÌôïÏû•)
        self.feature_columns = [
            'TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M14AM10A', 'M14AM16',
            'RATIO', 'GOLDEN', 'DANGER_SIGNAL',
            'PATTERN',  # Ìå®ÌÑ¥ Ï†ïÎ≥¥
            'HOUR_SIN', 'HOUR_COS',
            'MA_5', 'MA_10', 'MA_20', 'MA_30',
            'STD_5', 'STD_10', 'STD_20', 'STD_30',
            'M14B_MA_5', 'M14B_MA_10', 'M14B_MA_20',
            'CHANGE_1', 'CHANGE_5', 'CHANGE_10',
            'M14B_CHANGE_1', 'M14B_CHANGE_5',
            'CONSECUTIVE_RISE'
        ]
        
        # ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ïª¨ÎüºÎßå
        self.feature_columns = [f for f in self.feature_columns if f in df.columns]
        print(f"  ÌäπÏÑ±: {len(self.feature_columns)}Í∞ú")
        
        X_data = df[self.feature_columns].values.astype(np.float32)
        y_reg = df['TARGET'].values.astype(np.float32)
        y_cls = df['LEVEL'].values.astype(np.int32)
        y_danger = df['IS_DANGER'].values.astype(np.int32)
        
        # Ï∂îÏÑ∏ Î†àÏù¥Î∏î Ï∂îÍ∞Ä
        y_trend = self.calculate_trend_labels(df)
        
        # Î∂ÑÌï†
        total_samples = len(X_data) - self.seq_len
        train_end = int(total_samples * 0.7)
        val_end = int(total_samples * 0.85)
        
        # Ïä§ÏºÄÏùºÎßÅ
        print("  Ïä§ÏºÄÏùºÎßÅ...")
        self.feature_scaler.fit(X_data[:train_end + self.seq_len])
        self.target_scaler.fit(y_reg[:train_end].reshape(-1, 1))
        
        X_scaled = self.feature_scaler.transform(X_data)
        y_reg_scaled = self.target_scaler.transform(y_reg.reshape(-1, 1)).flatten()
        
        # ÏãúÌÄÄÏä§ ÏÉùÏÑ±
        X, y_r, y_c, y_d, y_t = [], [], [], [], []
        for i in range(total_samples):
            X.append(X_scaled[i:i+self.seq_len])
            y_r.append(y_reg_scaled[i+self.seq_len-1])
            y_c.append(y_cls[i+self.seq_len-1])
            y_d.append(y_danger[i+self.seq_len-1])
            y_t.append(y_trend[i+self.seq_len-1])
        
        X = np.array(X, dtype=np.float32)
        y_r = np.array(y_r, dtype=np.float32)
        y_c = np.array(y_c, dtype=np.int32)
        y_d = np.array(y_d, dtype=np.int32)
        y_t = np.array(y_t, dtype=np.int32)
        
        # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†
        X_train = X[:train_end]
        X_val = X[train_end:val_end]
        X_test = X[val_end:]
        
        y_train = (y_r[:train_end], y_c[:train_end], y_d[:train_end], y_t[:train_end])
        y_val = (y_r[train_end:val_end], y_c[train_end:val_end], 
                y_d[train_end:val_end], y_t[train_end:val_end])
        y_test = (y_r[val_end:], y_c[val_end:], y_d[val_end:], y_t[val_end:])
        
        print(f"  Train: {X_train.shape}")
        print(f"  Val: {X_val.shape}")
        print(f"  Test: {X_test.shape}")
        
        # ÏúÑÌóò Íµ¨Í∞Ñ ÎπÑÏú® ÌôïÏù∏
        danger_train = (y_train[2] == 1).sum()
        danger_val = (y_val[2] == 1).sum()
        danger_test = (y_test[2] == 1).sum()
        
        print(f"\nüî• ÏúÑÌóòÍµ¨Í∞Ñ(1651~1682) ÏÉòÌîå:")
        print(f"  Train: {danger_train}Í∞ú ({danger_train/len(X_train)*100:.1f}%)")
        print(f"  Val: {danger_val}Í∞ú ({danger_val/len(X_val)*100:.1f}%)")
        print(f"  Test: {danger_test}Í∞ú ({danger_test/len(X_test)*100:.1f}%)")
        
        self.save_scalers()
        
        return (X_train, y_train), (X_val, y_val), (X_test, y_test)
    
    def calculate_trend_labels(self, df):
        """Ï∂îÏÑ∏ Î†àÏù¥Î∏î Í≥ÑÏÇ∞ (0: ÌïòÎùΩ, 1: Î≥¥Ìï©, 2: ÏÉÅÏäπ)"""
        trends = []
        window = 20
        
        for i in range(len(df)):
            if i < window:
                trends.append(1)  # Î≥¥Ìï©
                continue
            
            # ÏµúÍ∑º window Í∏∞Í∞Ñ Ï∂îÏÑ∏
            recent = df['TOTALCNT'].iloc[max(0, i-window):i].values
            if len(recent) < 2:
                trends.append(1)
                continue
                
            # ÏÑ†Ìòï ÌöåÍ∑Ä Ï∂îÏÑ∏
            x = np.arange(len(recent))
            slope = np.polyfit(x, recent, 1)[0]
            
            # Ï∂îÏÑ∏ ÌåêÏ†ï
            if slope > 5:  # ÏÉÅÏäπ
                trends.append(2)
            elif slope < -5:  # ÌïòÎùΩ
                trends.append(0)
            else:  # Î≥¥Ìï©
                trends.append(1)
        
        return np.array(trends)

# ========================================
# ExtremeNet V6.5 Î™®Îç∏ (Í∞ïÌôîÎ≤ÑÏ†Ñ)
# ========================================
def build_extremenet_v65(input_shape):
    """ExtremeNet V6.5 - Ìå®ÌÑ¥/Ï∂îÏÑ∏ ÌïôÏäµ Í∞ïÌôî"""
    inputs = Input(shape=input_shape)
    
    # Branch 1: LSTM Ïû•Í∏∞ Ìå®ÌÑ¥
    lstm1 = LSTM(128, return_sequences=True)(inputs)
    lstm1 = BatchNormalization()(lstm1)
    lstm1 = Dropout(0.3)(lstm1)
    lstm2 = LSTM(64)(lstm1)
    lstm2 = BatchNormalization()(lstm2)
    
    # Branch 2: Attention Î©îÏª§ÎãàÏ¶ò
    attn = MultiHeadAttention(num_heads=8, key_dim=16)(inputs, inputs)
    attn = LayerNormalization()(attn)
    attn = GlobalAveragePooling1D()(attn)
    
    # Branch 3: CNN Îã®Í∏∞ Ìå®ÌÑ¥ (ÏúÑÌóòÍµ¨Í∞Ñ Í∞êÏßÄ)
    cnn = Conv1D(64, 5, activation='relu', padding='same')(inputs)
    cnn = BatchNormalization()(cnn)
    cnn = Conv1D(32, 3, activation='relu', padding='same')(cnn)
    cnn = GlobalMaxPooling1D()(cnn)
    
    # Branch 4: ÏµúÍ∑º Îç∞Ïù¥ÌÑ∞ ÏßëÏ§ë (ÏúÑÌóòÏã†Ìò∏ Í∞êÏßÄ)
    recent = Lambda(lambda x: x[:, -30:, :])(inputs)
    recent = GRU(32)(recent)
    
    # ÌÜµÌï©
    combined = Concatenate()([lstm2, attn, cnn, recent])
    combined = BatchNormalization()(combined)
    
    # Dense layers
    x = Dense(128, activation='relu')(combined)
    x = Dropout(0.3)(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    # Ï∂úÎ†•Ï∏µ
    out_reg = Dense(1, name='regression')(x)  # ÏòàÏ∏°Í∞í
    out_cls = Dense(3, activation='softmax', name='classification')(x)  # Íµ¨Í∞Ñ
    out_danger = Dense(1, activation='sigmoid', name='danger')(x)  # ÏúÑÌóòÏã†Ìò∏
    out_trend = Dense(3, activation='softmax', name='trend')(x)  # Ï∂îÏÑ∏
    
    return Model(inputs, [out_reg, out_cls, out_danger, out_trend], name='ExtremeNet_V65')

# ========================================
# Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌïôÏäµ ÏãúÏä§ÌÖú
# ========================================
class CheckpointTrainer:
    def __init__(self, model_name='ExtremeNet_V65'):
        self.model_name = model_name
        self.checkpoint_dir = f'checkpoints_{model_name}/'
        self.history_file = f'{self.checkpoint_dir}history.json'
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
    def load_checkpoint(self):
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú"""
        checkpoint_path = f'{self.checkpoint_dir}{self.model_name}.keras'
        history = {'epoch': 0, 'best_loss': float('inf')}
        
        if os.path.exists(checkpoint_path):
            print(f"‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∞úÍ≤¨! Î°úÎî©...")
            model = tf.keras.models.load_model(checkpoint_path)
            
            if os.path.exists(self.history_file):
                with open(self.history_file, 'r') as f:
                    history = json.load(f)
                print(f"  Ïù¥Ï†Ñ ÌïôÏäµ: {history['epoch']}ÏóêÌè¨ÌÅ¨, ÏµúÍ≥†ÏÜêÏã§: {history['best_loss']:.4f}")
        else:
            print("‚ö†Ô∏è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏóÜÏùå, ÏÉàÎ°ú ÏãúÏûë")
            model = None
            
        return model, history
    
    def save_checkpoint(self, model, epoch, best_loss):
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•"""
        checkpoint_path = f'{self.checkpoint_dir}{self.model_name}.keras'
        model.save(checkpoint_path)
        
        history = {'epoch': epoch, 'best_loss': float(best_loss)}
        with open(self.history_file, 'w') as f:
            json.dump(history, f)
        
        print(f"üíæ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•: ÏóêÌè¨ÌÅ¨ {epoch}, ÏÜêÏã§ {best_loss:.4f}")

# ========================================
# Ïª§Ïä§ÌÖÄ ÏΩúÎ∞± (ÏúÑÌóòÍµ¨Í∞Ñ ÏßëÏ§ë)
# ========================================
class DangerZoneFocusCallback(tf.keras.callbacks.Callback):
    """ÏúÑÌóòÍµ¨Í∞Ñ(1651~1682) ÏòàÏ∏° ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ"""
    
    def __init__(self, X_val, y_val, processor):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val
        self.processor = processor
        
    def on_epoch_end(self, epoch, logs=None):
        # ÏúÑÌóòÍµ¨Í∞Ñ ÏÉòÌîåÎßå
        danger_idx = np.where(self.y_val[2] == 1)[0]
        
        if len(danger_idx) > 0:
            X_danger = self.X_val[danger_idx]
            y_danger_true = self.y_val[0][danger_idx]
            
            # ÏòàÏ∏°
            preds = self.model.predict(X_danger, verbose=0)
            y_danger_pred = preds[0].flatten()
            
            # Ïó≠Î≥ÄÌôò
            y_true_orig = self.processor.target_scaler.inverse_transform(
                y_danger_true.reshape(-1, 1)).flatten()
            y_pred_orig = self.processor.target_scaler.inverse_transform(
                y_danger_pred.reshape(-1, 1)).flatten()
            
            mae = mean_absolute_error(y_true_orig, y_pred_orig)
            accuracy = np.mean(np.abs(y_true_orig - y_pred_orig) < 50)  # 50 Ïù¥ÎÇ¥ Ï†ïÌôïÎèÑ
            
            print(f"\n  üî• ÏúÑÌóòÍµ¨Í∞Ñ ÏÑ±Îä•: MAE={mae:.1f}, Ï†ïÌôïÎèÑ={accuracy:.1%}")

# ========================================
# ÌïôÏäµ Î∞è ÌèâÍ∞Ä
# ========================================
def train_extremenet(train_data, val_data, test_data, processor):
    """ExtremeNet V6.5 ÌïôÏäµ"""
    
    X_train, y_train = train_data
    X_val, y_val = val_data
    X_test, y_test = test_data
    
    # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏãúÏä§ÌÖú
    trainer = CheckpointTrainer()
    model, history = trainer.load_checkpoint()
    
    # Î™®Îç∏ ÏÉùÏÑ± ÎòêÎäî Î°úÎìú
    with strategy.scope():
        if model is None:
            input_shape = (X_train.shape[1], X_train.shape[2])
            model = build_extremenet_v65(input_shape)
            
            model.compile(
                optimizer=Adam(0.001),
                loss={
                    'regression': 'huber',  # Ïù¥ÏÉÅÏπòÏóê Í∞ïÍ±¥
                    'classification': 'sparse_categorical_crossentropy',
                    'danger': 'binary_crossentropy',
                    'trend': 'sparse_categorical_crossentropy'
                },
                loss_weights={
                    'regression': 0.5,
                    'classification': 0.2,
                    'danger': 0.2,  # ÏúÑÌóòÍµ¨Í∞Ñ Ï§ëÏöî
                    'trend': 0.1
                },
                metrics={
                    'regression': 'mae',
                    'classification': 'accuracy',
                    'danger': 'accuracy',
                    'trend': 'accuracy'
                }
            )
    
    print("\n" + "="*80)
    print("üî• ExtremeNet V6.5 ÌïôÏäµ ÏãúÏûë")
    print("="*80)
    
    # ÏΩúÎ∞±
    callbacks = [
        # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•
        ModelCheckpoint(
            f'{trainer.checkpoint_dir}{trainer.model_name}_best.keras',
            save_best_only=True,
            monitor='val_loss',
            verbose=1
        ),
        
        # Ï°∞Í∏∞Ï¢ÖÎ£å
        EarlyStopping(
            patience=15,
            restore_best_weights=True,
            monitor='val_loss',
            verbose=1
        ),
        
        # ÌïôÏäµÎ•† Ï°∞Ï†ï
        ReduceLROnPlateau(
            factor=0.5,
            patience=5,
            min_lr=1e-6,
            verbose=1
        ),
        
        # ÏúÑÌóòÍµ¨Í∞Ñ Î™®ÎãàÌÑ∞ÎßÅ
        DangerZoneFocusCallback(X_val, y_val, processor)
    ]
    
    # Î∞∞Ïπò ÌÅ¨Í∏∞
    batch_size = 64 * strategy.num_replicas_in_sync if gpus else 32
    
    # ÌïôÏäµ Ïû¨Í∞ú ÎòêÎäî ÏãúÏûë
    initial_epoch = history['epoch']
    
    print(f"\nüìö ÌïôÏäµ ÏÑ§Ï†ï:")
    print(f"  ÏãúÏûë ÏóêÌè¨ÌÅ¨: {initial_epoch}")
    print(f"  Î∞∞Ïπò ÌÅ¨Í∏∞: {batch_size}")
    print(f"  Ï¥ù ÏóêÌè¨ÌÅ¨: 100")
    
    # ÌïôÏäµ
    history = model.fit(
        X_train,
        {
            'regression': y_train[0],
            'classification': y_train[1],
            'danger': y_train[2],
            'trend': y_train[3]
        },
        validation_data=(
            X_val,
            {
                'regression': y_val[0],
                'classification': y_val[1],
                'danger': y_val[2],
                'trend': y_val[3]
            }
        ),
        epochs=100,
        initial_epoch=initial_epoch,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=1
    )
    
    # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•
    best_loss = min(history.history['val_loss'])
    final_epoch = initial_epoch + len(history.history['val_loss'])
    trainer.save_checkpoint(model, final_epoch, best_loss)
    
    # ÌèâÍ∞Ä
    print("\n" + "="*80)
    print("üìä ExtremeNet V6.5 ÏµúÏ¢Ö ÌèâÍ∞Ä")
    print("="*80)
    
    # Ï†ÑÏ≤¥ ÌÖåÏä§Ìä∏
    preds = model.predict(X_test, verbose=0)
    y_reg_pred = preds[0].flatten()
    y_cls_pred = np.argmax(preds[1], axis=1)
    y_danger_pred = (preds[2].flatten() > 0.5).astype(int)
    y_trend_pred = np.argmax(preds[3], axis=1)
    
    # Ïó≠Î≥ÄÌôò
    y_reg_true_orig = processor.target_scaler.inverse_transform(
        y_test[0].reshape(-1, 1)).flatten()
    y_reg_pred_orig = processor.target_scaler.inverse_transform(
        y_reg_pred.reshape(-1, 1)).flatten()
    
    # Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
    mae = mean_absolute_error(y_reg_true_orig, y_reg_pred_orig)
    rmse = np.sqrt(mean_squared_error(y_reg_true_orig, y_reg_pred_orig))
    
    # R2 (ÏùåÏàò Î∞©ÏßÄ)
    r2_raw = r2_score(y_reg_true_orig, y_reg_pred_orig)
    r2 = max(0.0, r2_raw)  # ÏùåÏàòÎ©¥ 0ÏúºÎ°ú
    
    # Ï†ïÌôïÎèÑ
    cls_acc = np.mean(y_cls_pred == y_test[1])
    danger_acc = np.mean(y_danger_pred == y_test[2])
    trend_acc = np.mean(y_trend_pred == y_test[3])
    
    print(f"\nüìà ÌöåÍ∑Ä ÏÑ±Îä•:")
    print(f"  MAE: {mae:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  R¬≤ Score: {r2:.4f} (ÏõêÎ≥∏: {r2_raw:.4f})")
    
    print(f"\nüéØ Î∂ÑÎ•ò ÏÑ±Îä•:")
    print(f"  Íµ¨Í∞Ñ Ï†ïÌôïÎèÑ: {cls_acc:.2%}")
    print(f"  ÏúÑÌóòÏã†Ìò∏ Ï†ïÌôïÎèÑ: {danger_acc:.2%}")
    print(f"  Ï∂îÏÑ∏ Ï†ïÌôïÎèÑ: {trend_acc:.2%}")
    
    # ÏúÑÌóòÍµ¨Í∞ÑÎßå ÌèâÍ∞Ä
    danger_idx = np.where(y_test[2] == 1)[0]
    if len(danger_idx) > 0:
        danger_mae = mean_absolute_error(
            y_reg_true_orig[danger_idx],
            y_reg_pred_orig[danger_idx]
        )
        print(f"\nüî• ÏúÑÌóòÍµ¨Í∞Ñ(1651~1682) ÏÑ±Îä•:")
        print(f"  ÏÉòÌîå Ïàò: {len(danger_idx)}Í∞ú")
        print(f"  MAE: {danger_mae:.2f}")
        print(f"  Í∞êÏßÄÏú®: {np.mean(y_danger_pred[danger_idx]):.1%}")
    
    # Ï∂îÏÑ∏Î≥Ñ ÏÑ±Îä•
    print(f"\nüìä Ï∂îÏÑ∏Î≥Ñ ÏòàÏ∏° ÏÑ±Îä•:")
    for trend, name in enumerate(['ÌïòÎùΩ', 'Î≥¥Ìï©', 'ÏÉÅÏäπ']):
        trend_idx = np.where(y_test[3] == trend)[0]
        if len(trend_idx) > 0:
            trend_mae = mean_absolute_error(
                y_reg_true_orig[trend_idx],
                y_reg_pred_orig[trend_idx]
            )
            print(f"  {name} Ï∂îÏÑ∏: MAE={trend_mae:.2f} (n={len(trend_idx)})")
    
    # Î™®Îç∏ Ï†ÄÏû•
    os.makedirs('models_v65', exist_ok=True)
    model.save('models_v65/ExtremeNet_V65_final.keras')
    print(f"\nüíæ ÏµúÏ¢Ö Î™®Îç∏: models_v65/ExtremeNet_V65_final.keras")
    
    return model, {'MAE': mae, 'RMSE': rmse, 'R2': r2}

# ========================================
# Ïã§ÏãúÍ∞Ñ ÏòàÏ∏° ÏãúÏä§ÌÖú
# ========================================
class ExtremeNetPredictor:
    def __init__(self, model_path='models_v65/ExtremeNet_V65_final.keras'):
        """ExtremeNet V6.5 ÏòàÏ∏°Í∏∞"""
        self.model = tf.keras.models.load_model(model_path)
        self.processor = PatternDataProcessor()
        self.processor.load_scalers('scalers_v65/')
        print("‚úÖ ExtremeNet V6.5 ÏòàÏ∏°Í∏∞ Ï§ÄÎπÑ ÏôÑÎ£å")
    
    def predict(self, data_100min):
        """100Î∂Ñ Îç∞Ïù¥ÌÑ∞Î°ú 10Î∂Ñ ÌõÑ ÏòàÏ∏°"""
        
        # Ìå®ÌÑ¥ Í∞êÏßÄ
        pattern = self.detect_current_pattern(data_100min)
        
        # Ï∂îÏÑ∏ ÌåêÎã®
        trend = self.detect_current_trend(data_100min)
        
        # ÌäπÏÑ± Ï∂îÏ∂ú Î∞è Ïä§ÏºÄÏùºÎßÅ
        X = data_100min[self.processor.feature_columns].values
        X_scaled = self.processor.feature_scaler.transform(X)
        X_seq = X_scaled.reshape(1, 100, -1)
        
        # ÏòàÏ∏°
        preds = self.model.predict(X_seq, verbose=0)
        
        # Í≤∞Í≥º Ìï¥ÏÑù
        y_reg = self.processor.target_scaler.inverse_transform([[preds[0][0, 0]]])[0, 0]
        y_cls = np.argmax(preds[1][0])
        y_danger = preds[2][0, 0]
        y_trend = np.argmax(preds[3][0])
        
        # Î†àÎ≤®
        levels = ['Ï†ïÏÉÅ', 'Ï£ºÏùò', 'Ïã¨Í∞Å']
        trends = ['ÌïòÎùΩ', 'Î≥¥Ìï©', 'ÏÉÅÏäπ']
        
        # ÏúÑÌóò Ïã†Ìò∏ Ï≤¥ÌÅ¨
        is_danger_zone = (1651 <= y_reg <= 1682)
        
        result = {
            'prediction': float(y_reg),
            'level': levels[y_cls],
            'danger_probability': float(y_danger),
            'trend': trends[y_trend],
            'pattern': f"UU{pattern+1}" if pattern > 0 else "Ï†ïÏÉÅ",
            'is_danger_zone': is_danger_zone,
            'confidence': float(np.max(preds[1][0]))
        }
        
        # Í≤ΩÍ≥† Î©îÏãúÏßÄ
        if is_danger_zone or y_danger > 0.7:
            result['alert'] = "‚ö†Ô∏è ÏúÑÌóòÏã†Ìò∏! 1700+ Í∏âÏ¶ù Í∞ÄÎä•ÏÑ± ÎÜíÏùå"
        elif y_cls == 2:
            result['alert'] = "üî¥ Ïã¨Í∞Å Íµ¨Í∞Ñ ÏòàÏÉÅ"
        elif y_cls == 1:
            result['alert'] = "üü° Ï£ºÏùò Íµ¨Í∞Ñ ÏòàÏÉÅ"
        else:
            result['alert'] = "üü¢ Ï†ïÏÉÅ Íµ¨Í∞Ñ"
        
        return result
    
    def detect_current_pattern(self, df):
        """ÌòÑÏû¨ Ìå®ÌÑ¥ Í∞êÏßÄ"""
        high_cases = (df['TOTALCNT'] >= 1682).sum()
        m14b_mean = df['M14AM14B'].mean()
        
        if high_cases > 0 and m14b_mean > 380:
            return 2  # UU2
        elif (df['M14AM14B'] > 350).mean() > 0.3:
            return 2  # UU2
        elif ((df['TOTALCNT'] >= 1651) & (df['TOTALCNT'] <= 1682)).sum() > 0:
            return 1  # UU1
        else:
            return 0  # Ï†ïÏÉÅ
    
    def detect_current_trend(self, df):
        """ÌòÑÏû¨ Ï∂îÏÑ∏ Í∞êÏßÄ"""
        recent = df['TOTALCNT'].tail(20).values
        if len(recent) < 2:
            return "Î≥¥Ìï©"
        
        slope = np.polyfit(np.arange(len(recent)), recent, 1)[0]
        
        if slope > 5:
            return "ÏÉÅÏäπ"
        elif slope < -5:
            return "ÌïòÎùΩ"
        else:
            return "Î≥¥Ìï©"

# ========================================
# Î©îÏù∏ Ïã§Ìñâ
# ========================================
def main():
    # Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú
    data_paths = [
        'data/20240201_TO_202507281705.csv',
        '20240201_TO_202507281705.csv'
    ]
    
    data_path = None
    for path in data_paths:
        if os.path.exists(path):
            data_path = path
            print(f"‚úÖ Îç∞Ïù¥ÌÑ∞ Î∞úÍ≤¨: {path}")
            break
    
    if not data_path:
        print("‚ùå Îç∞Ïù¥ÌÑ∞ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        return None
    
    # Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
    processor = PatternDataProcessor()
    df = processor.load_and_process(data_path)
    df = processor.create_features(df)
    
    # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†
    train_data, val_data, test_data = processor.create_sequences(df)
    
    # ExtremeNet V6.5 ÌïôÏäµ
    model, results = train_extremenet(train_data, val_data, test_data, processor)
    
    print("\n" + "="*80)
    print("üéØ ExtremeNet V6.5 ÌïôÏäµ ÏôÑÎ£å!")
    print("="*80)
    
    # Ïã§ÏãúÍ∞Ñ ÏòàÏ∏° ÌÖåÏä§Ìä∏
    print("\nüîÆ Ïã§ÏãúÍ∞Ñ ÏòàÏ∏° ÌÖåÏä§Ìä∏")
    print("-"*50)
    
    predictor = ExtremeNetPredictor()
    
    # ÌÖåÏä§Ìä∏ ÏÉòÌîå
    test_sample = df.iloc[-100:].copy()
    result = predictor.predict(test_sample)
    
    print(f"\nÏòàÏ∏° Í≤∞Í≥º:")
    print(f"  10Î∂Ñ ÌõÑ ÏòàÏ∏°Í∞í: {result['prediction']:.0f}")
    print(f"  Íµ¨Í∞Ñ: {result['level']}")
    print(f"  Ï∂îÏÑ∏: {result['trend']}")
    print(f"  Ìå®ÌÑ¥: {result['pattern']}")
    print(f"  ÏúÑÌóòÌôïÎ•†: {result['danger_probability']:.1%}")
    print(f"  Ïã†Î¢∞ÎèÑ: {result['confidence']:.1%}")
    print(f"  {result['alert']}")
    
    # Í≤∞Í≥º Ï†ÄÏû•
    summary = {
        'model': 'ExtremeNet V6.5',
        'mae': results['MAE'],
        'rmse': results['RMSE'],
        'r2': results['R2'],
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open('extremenet_v65_results.json', 'w') as f:
        json.dump(summary, f, indent=2)
    
    print("\n‚úÖ ÏôÑÎ£å! Í≤∞Í≥º Ï†ÄÏû•: extremenet_v65_results.json")
    print("\nüí° Ï§ëÎã® ÌõÑ Ïû¨Ïã§ÌñâÏãú ÏûêÎèôÏúºÎ°ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏóêÏÑú ÌïôÏäµ Ïû¨Í∞úÎê©ÎãàÎã§!")
    
    return model, results

if __name__ == "__main__":
    model, results = main()