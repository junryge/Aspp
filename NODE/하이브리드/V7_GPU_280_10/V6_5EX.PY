"""
üî• ExtremeNet V6.5 Fixed - ÏúÑÌóòÍµ¨Í∞Ñ ÏòàÏ∏° Í∞úÏÑ†Ìåê (Ïò§Î•ò ÏàòÏ†ï ÏôÑÎ£å)
================================================================
‚úÖ StandardScalerÎ°ú Î≥ÄÍ≤Ω (RobustScaler Î¨∏Ï†ú Ìï¥Í≤∞)
‚úÖ ÏúÑÌóòÍµ¨Í∞Ñ Ïò§Î≤ÑÏÉòÌîåÎßÅ
‚úÖ ÏÜêÏã§Ìï®Ïàò Í∞úÏÑ†
‚úÖ ÏúÑÌóòÍµ¨Í∞Ñ ÌäπÎ≥Ñ Í¥ÄÎ¶¨
‚úÖ Îã§Ï§ë Ï∂úÎ†• Î™®Îç∏Ïùò class_weight Ïò§Î•ò ÏàòÏ†ï (sample_weight ÏÇ¨Ïö©)
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import *
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.utils import class_weight
import warnings
import os
import pickle
import json

warnings.filterwarnings('ignore')
tf.random.set_seed(42)
np.random.seed(42)

# GPU ÏÑ§Ï†ï
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    print(f"üîß GPU {len(gpus)}Í∞ú Î∞úÍ≤¨!")
    for i, gpu in enumerate(gpus):
        tf.config.experimental.set_memory_growth(gpu, True)
    strategy = tf.distribute.MirroredStrategy()
    DEVICE = 'GPU'
    # BATCH_SIZEÎäî GPU ÏàòÏóê Îî∞Îùº ÎèôÏ†ÅÏúºÎ°ú ÏÑ§Ï†ï
    BATCH_SIZE = 64 * strategy.num_replicas_in_sync
else:
    print("‚ö†Ô∏è GPU ÏóÜÏùå, CPU ÏÇ¨Ïö©")
    strategy = tf.distribute.get_strategy()
    DEVICE = 'CPU'
    BATCH_SIZE = 32

print("="*80)
print("üî• ExtremeNet V6.5 Fixed - ÏúÑÌóòÍµ¨Í∞Ñ ÏòàÏ∏° Í∞úÏÑ†Ìåê (Ïò§Î•ò ÏàòÏ†ï ÏôÑÎ£å)")
print(f"üì¶ TensorFlow: {tf.__version__}")
print(f"üîß Device: {DEVICE}")
print("="*80)

# ========================================
# Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ (Í∞úÏÑ†Îê®)
# ========================================
class DataProcessor:
    def __init__(self):
        self.seq_len = 100
        self.pred_len = 10
        
        # ‚≠ê StandardScalerÎ°ú Î≥ÄÍ≤Ω (ÌïµÏã¨!)
        self.scaler_X = StandardScaler()
        self.scaler_y = MinMaxScaler()  # 0-1 Î≤îÏúÑÎ°ú Ï†ïÍ∑úÌôî
        
        self.feature_columns = None
        
    def load_data(self, filepath):
        """Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
        print(f"\nüìÇ Îç∞Ïù¥ÌÑ∞ Î°úÎî©: {filepath}")
        df = pd.read_csv(filepath)
        
        # ÏãúÍ∞Ñ Ï†ïÎ†¨
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                       format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        # 0Í∞í Ï†úÍ±∞
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        print(f"‚úÖ Îç∞Ïù¥ÌÑ∞: {len(df):,}Í∞ú")
        
        # Íµ¨Í∞Ñ ÌÜµÍ≥Ñ
        normal = (df['TOTALCNT'] < 1400).sum()
        warning = ((df['TOTALCNT'] >= 1400) & (df['TOTALCNT'] < 1700)).sum()
        critical = (df['TOTALCNT'] >= 1700).sum()
        danger = ((df['TOTALCNT'] >= 1651) & (df['TOTALCNT'] <= 1682)).sum()
        
        print(f"\nüìä TOTALCNT Î∂ÑÌè¨:")
        print(f"  Ï†ïÏÉÅ(~1400): {normal:,}Í∞ú ({normal/len(df)*100:.1f}%)")
        print(f"  Ï£ºÏùò(1400~1699): {warning:,}Í∞ú ({warning/len(df)*100:.1f}%)")
        print(f"  Ïã¨Í∞Å(1700+): {critical:,}Í∞ú ({critical/len(df)*100:.1f}%)")
        print(f"  üî• ÏúÑÌóòÏã†Ìò∏(1651~1682): {danger:,}Í∞ú")
        
        return df
    
    def create_features(self, df):
        """ÌäπÏÑ± ÏÉùÏÑ± (ÏµúÏ†ÅÌôî)"""
        print("\n‚öôÔ∏è ÌäπÏÑ± ÏÉùÏÑ± Ï§ë...")
        
        # Í∏∞Î≥∏ ÌäπÏÑ±
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(int)
        
        # ÏúÑÌóò Ïã†Ìò∏ Í∞ïÏ°∞
        df['DANGER_ZONE'] = ((df['TOTALCNT'] >= 1651) & 
                             (df['TOTALCNT'] <= 1682)).astype(int)
        df['PRE_DANGER'] = ((df['TOTALCNT'] >= 1600) & 
                            (df['TOTALCNT'] < 1651)).astype(int)
        
        # Ïù¥ÎèôÌèâÍ∑† (fillnaÎ°ú NaN Ï≤òÎ¶¨)
        for w in [5, 10, 20]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
            df[f'M14B_MA_{w}'] = df['M14AM14B'].rolling(w, min_periods=1).mean()
        
        # Î≥ÄÌôîÏú®
        for lag in [1, 5, 10]:
            df[f'CHANGE_{lag}'] = df['TOTALCNT'].diff(lag).fillna(0)
            df[f'M14B_CHANGE_{lag}'] = df['M14AM14B'].diff(lag).fillna(0)
        
        # Ïó∞ÏÜç ÏÉÅÏäπ (Î≤°ÌÑ∞Ìôî)
        df['RISE'] = (df['TOTALCNT'] > df['TOTALCNT'].shift(1)).astype(int)
        df['RISE_COUNT'] = df['RISE'].rolling(10, min_periods=1).sum()
        
        # Ìå®ÌÑ¥ (Í∞ÑÏÜåÌôî)
        df['PATTERN'] = 0
        danger_mask = ((df['TOTALCNT'] >= 1651) & (df['TOTALCNT'] <= 1682))
        high_m14b = df['M14AM14B'].rolling(100, min_periods=1).mean() > 380
        
        df.loc[danger_mask, 'PATTERN'] = 1  # UU1
        df.loc[high_m14b, 'PATTERN'] = 2     # UU2
        
        # Ï∂îÏÑ∏ (Î≤°ÌÑ∞Ìôî)
        df['TREND'] = 0
        for i in range(20, len(df)):
            recent = df['TOTALCNT'].iloc[i-20:i].values
            if len(recent) > 1:
                slope = np.polyfit(range(len(recent)), recent, 1)[0]
                if slope > 5:
                    df.loc[i, 'TREND'] = 2  # ÏÉÅÏäπ
                elif slope < -5:
                    df.loc[i, 'TREND'] = 0  # ÌïòÎùΩ
                else:
                    df.loc[i, 'TREND'] = 1  # Î≥¥Ìï©
        
        print(f"‚úÖ ÌäπÏÑ± ÏÉùÏÑ± ÏôÑÎ£å")
        return df
    
    def create_sequences(self, df):
        """ÏãúÌÄÄÏä§ ÏÉùÏÑ± (ÏúÑÌóòÍµ¨Í∞Ñ Ïò§Î≤ÑÏÉòÌîåÎßÅ)"""
        print("\nüîÑ ÏãúÌÄÄÏä§ ÏÉùÏÑ± Ï§ë...")
        
        # ÏÇ¨Ïö©Ìï† Ïª¨Îüº
        self.feature_columns = [
            'TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M14AM10A', 'M14AM16',
            'RATIO', 'GOLDEN', 'DANGER_ZONE', 'PRE_DANGER',
            'PATTERN', 'TREND',
            'MA_5', 'MA_10', 'MA_20',
            'STD_5', 'STD_10', 'STD_20',
            'M14B_MA_5', 'M14B_MA_10', 'M14B_MA_20',
            'CHANGE_1', 'CHANGE_5', 'CHANGE_10',
            'M14B_CHANGE_1', 'M14B_CHANGE_5', 'M14B_CHANGE_10',
            'RISE_COUNT'
        ]
        
        print(f"  ÌäπÏÑ±: {len(self.feature_columns)}Í∞ú")
        
        # Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
        X_data = df[self.feature_columns].values
        y_data = df['TOTALCNT'].shift(-self.pred_len).values
        
        # ÏãúÌÄÄÏä§ ÏÉùÏÑ±
        X, y = [], []
        danger_X, danger_y = [], []  # ÏúÑÌóòÍµ¨Í∞Ñ Îî∞Î°ú Ï†ÄÏû•
        
        for i in range(len(df) - self.seq_len - self.pred_len):
            X.append(X_data[i:i+self.seq_len])
            target = y_data[i+self.seq_len-1]
            y.append(target)
            
            # ÏúÑÌóòÍµ¨Í∞Ñ Ïò§Î≤ÑÏÉòÌîåÎßÅ
            if 1651 <= target <= 1682:
                # ÏúÑÌóòÍµ¨Í∞ÑÏùÄ 3Î∞∞ Î≥µÏ†ú
                for _ in range(3):
                    danger_X.append(X_data[i:i+self.seq_len])
                    danger_y.append(target)
        
        # ÏúÑÌóòÍµ¨Í∞Ñ Ï∂îÍ∞Ä
        if danger_X:
            X.extend(danger_X)
            y.extend(danger_y)
            print(f"  üî• ÏúÑÌóòÍµ¨Í∞Ñ {len(danger_X)}Í∞ú Ï∂îÍ∞Ä (Ïò§Î≤ÑÏÉòÌîåÎßÅ)")
        
        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.float32)
        
        # NaN Ï†úÍ±∞
        valid_idx = ~np.isnan(y)
        X = X[valid_idx]
        y = y[valid_idx]
        
        print(f"  Ï¥ù ÏãúÌÄÄÏä§: {X.shape[0]}Í∞ú")
        
        # ÏÖîÌîå (ÏãúÍ≥ÑÏó¥ ÏàúÏÑú Ïú†ÏßÄÌïòÎ©¥ÏÑú ÏúÑÌóòÍµ¨Í∞Ñ ÏÑûÍ∏∞)
        shuffle_idx = np.arange(len(X))
        np.random.shuffle(shuffle_idx)
        X = X[shuffle_idx]
        y = y[shuffle_idx]
        
        # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†
        train_size = int(len(X) * 0.7)
        val_size = int(len(X) * 0.15)
        
        X_train = X[:train_size]
        y_train = y[:train_size]
        X_val = X[train_size:train_size+val_size]
        y_val = y[train_size:train_size+val_size]
        X_test = X[train_size+val_size:]
        y_test = y[train_size+val_size:]
        
        # Ïä§ÏºÄÏùºÎßÅ
        print("  Ïä§ÏºÄÏùºÎßÅ Ï§ë...")
        
        # X Ïä§ÏºÄÏùºÎßÅ
        # ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïùò ÌòïÌÉúÎ•º 2DÎ°ú Î≥ÄÍ≤ΩÌïòÏó¨ Ïä§ÏºÄÏùºÎü¨ ÌïôÏäµ
        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])
        self.scaler_X.fit(X_train_reshaped)

        # Í∞Å Îç∞Ïù¥ÌÑ∞ÏÖãÏóê Ïä§ÏºÄÏùºÎßÅ Ï†ÅÏö©
        X_train_scaled = self.scaler_X.transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
        X_val_scaled = self.scaler_X.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)
        X_test_scaled = self.scaler_X.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)

        # y Ïä§ÏºÄÏùºÎßÅ (MinMaxScalerÎ°ú 0-1 Î≤îÏúÑ)
        self.scaler_y.fit(y_train.reshape(-1, 1))
        y_train_scaled = self.scaler_y.transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = self.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        y_test_scaled = self.scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        print(f"\nüìä Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†:")
        print(f"  Train: {len(X_train):,}Í∞ú")
        print(f"  Val: {len(X_val):,}Í∞ú")
        print(f"  Test: {len(X_test):,}Í∞ú")
        
        # ÏúÑÌóòÍµ¨Í∞Ñ ÎπÑÏú®
        danger_train = ((y_train >= 1651) & (y_train <= 1682)).sum()
        danger_val = ((y_val >= 1651) & (y_val <= 1682)).sum()
        danger_test = ((y_test >= 1651) & (y_test <= 1682)).sum()
        
        print(f"\nüî• ÏúÑÌóòÏã†Ìò∏(1651~1682) ÏÉòÌîå:")
        print(f"  Train: {danger_train}Í∞ú ({danger_train/len(X_train)*100:.1f}%)")
        print(f"  Val: {danger_val}Í∞ú ({danger_val/len(X_val)*100:.1f}%)")
        print(f"  Test: {danger_test}Í∞ú ({danger_test/len(X_test)*100:.1f}%)")
        
        # Íµ¨Í∞Ñ Î†àÏù¥Î∏î
        y_train_level = np.zeros(len(y_train), dtype=int)
        y_train_level[y_train >= 1400] = 1
        y_train_level[y_train >= 1700] = 2
        
        y_val_level = np.zeros(len(y_val), dtype=int)
        y_val_level[y_val >= 1400] = 1
        y_val_level[y_val >= 1700] = 2
        
        y_test_level = np.zeros(len(y_test), dtype=int)
        y_test_level[y_test >= 1400] = 1
        y_test_level[y_test >= 1700] = 2
        
        # ÏúÑÌóòÍµ¨Í∞Ñ Î†àÏù¥Î∏î
        y_train_danger = ((y_train >= 1651) & (y_train <= 1682)).astype(int)
        y_val_danger = ((y_val >= 1651) & (y_val <= 1682)).astype(int)
        y_test_danger = ((y_test >= 1651) & (y_test <= 1682)).astype(int)
        
        # Ïä§ÏºÄÏùºÎü¨ Ï†ÄÏû•
        os.makedirs('scalers', exist_ok=True)
        with open('scalers/scaler_X.pkl', 'wb') as f:
            pickle.dump(self.scaler_X, f)
        with open('scalers/scaler_y.pkl', 'wb') as f:
            pickle.dump(self.scaler_y, f)
        
        return (X_train_scaled, y_train_scaled, y_train, y_train_level, y_train_danger), \
               (X_val_scaled, y_val_scaled, y_val, y_val_level, y_val_danger), \
               (X_test_scaled, y_test_scaled, y_test, y_test_level, y_test_danger)

# ========================================
# ExtremeNet Î™®Îç∏ (Í∞úÏÑ†)
# ========================================
def build_extremenet(input_shape):
    """ExtremeNet V6.5 Fixed"""
    inputs = Input(shape=input_shape)
    
    # LSTM Î∏åÎûúÏπò
    lstm = LSTM(128, return_sequences=True)(inputs)
    lstm = BatchNormalization()(lstm)
    lstm = LSTM(64)(lstm)
    lstm = Dropout(0.3)(lstm)
    
    # Attention Î∏åÎûúÏπò
    attn = MultiHeadAttention(num_heads=4, key_dim=16)(inputs, inputs)
    attn = GlobalAveragePooling1D()(attn)
    
    # CNN Î∏åÎûúÏπò (ÏúÑÌóòÍµ¨Í∞Ñ Ìå®ÌÑ¥)
    cnn = Conv1D(64, 5, activation='relu')(inputs)
    cnn = BatchNormalization()(cnn)
    cnn = Conv1D(32, 3, activation='relu')(cnn)
    cnn = GlobalMaxPooling1D()(cnn)
    
    # Recent Î∏åÎûúÏπò (ÏµúÍ∑º Í∏âÎ≥Ä)
    recent = Lambda(lambda x: x[:, -20:, :])(inputs)
    recent = GRU(32)(recent)
    
    # ÌÜµÌï©
    merged = Concatenate()([lstm, attn, cnn, recent])
    merged = BatchNormalization()(merged)
    
    # Dense
    x = Dense(128, activation='relu')(merged)
    x = Dropout(0.3)(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    # Ï∂úÎ†•
    output_reg = Dense(1, name='regression')(x)
    output_cls = Dense(3, activation='softmax', name='classification')(x)
    output_danger = Dense(1, activation='sigmoid', name='danger')(x)
    
    model = Model(inputs, [output_reg, output_cls, output_danger], name='ExtremeNet_V65_Fixed')
    return model

# ========================================
# Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏
# ========================================
class CheckpointManager:
    def __init__(self, checkpoint_dir='checkpoints/'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        self.model_path = os.path.join(self.checkpoint_dir, 'extremenet_fixed.keras')
        self.history_file = os.path.join(self.checkpoint_dir, 'history.json')
        
    def save_checkpoint(self, model, epoch, loss):
        model.save(self.model_path)
        history = {'epoch': epoch, 'loss': float(loss)}
        with open(self.history_file, 'w') as f:
            json.dump(history, f)
        print(f"üíæ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•: ÏóêÌè¨ÌÅ¨ {epoch}, Í≤ΩÎ°ú: {self.model_path}")
    
    def load_checkpoint(self):
        if os.path.exists(self.model_path):
            print(f"‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∞úÍ≤¨! ÌïôÏäµ Ïû¨Í∞ú... ({self.model_path})")
            try:
                model = tf.keras.models.load_model(self.model_path, 
                                                  custom_objects={'weighted_mae': weighted_mae})
                with open(self.history_file, 'r') as f:
                    history = json.load(f)
                return model, history.get('epoch', 0)
            except Exception as e:
                print(f"‚ö†Ô∏è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú Ïã§Ìå®: {e}. ÏÉà Î™®Îç∏Î°ú ÏãúÏûëÌï©ÎãàÎã§.")
                return None, 0
        return None, 0

# ========================================
# Ïª§Ïä§ÌÖÄ ÏΩúÎ∞± (ÏúÑÌóòÍµ¨Í∞Ñ Î™®ÎãàÌÑ∞ÎßÅ)
# ========================================
class DangerZoneCallback(tf.keras.callbacks.Callback):
    def __init__(self, X_val, y_val_orig, processor):
        super().__init__()
        self.X_val = X_val
        self.y_val_orig = y_val_orig
        self.processor = processor
        
    def on_epoch_end(self, epoch, logs=None):
        # ÏúÑÌóòÍµ¨Í∞Ñ Ïù∏Îç±Ïä§
        danger_idx = ((self.y_val_orig >= 1651) & (self.y_val_orig <= 1682))
        
        if np.sum(danger_idx) > 0:
            X_danger = self.X_val[danger_idx]
            y_danger_true = self.y_val_orig[danger_idx]
            
            # ÏòàÏ∏°
            preds = self.model.predict(X_danger, verbose=0)
            y_pred_scaled = preds[0].flatten()
            y_pred = self.processor.scaler_y.inverse_transform(
                y_pred_scaled.reshape(-1, 1)).flatten()
            
            mae = mean_absolute_error(y_danger_true, y_pred)
            accuracy = np.mean(np.abs(y_danger_true - y_pred) < 50) # Ïò§Ï∞® 50 Ïù¥ÎÇ¥Î•º Ï†ïÌôïÎèÑÎ°ú Í∞ÑÏ£º
            
            print(f"\n  üî• ÏúÑÌóòÍµ¨Í∞Ñ(Í≤ÄÏ¶ùÏÖã) ÏÑ±Îä•: MAE={mae:.1f}, Ï†ïÌôïÎèÑ(¬±50)={accuracy*100:.1f}%")

# ========================================
# Ïª§Ïä§ÌÖÄ ÏÜêÏã§Ìï®Ïàò (ÏúÑÌóòÍµ¨Í∞Ñ Í∞ïÏ°∞)
# ========================================
def weighted_mae(y_true, y_pred):
    """ÏúÑÌóòÍµ¨Í∞ÑÏóê Îçî ÌÅ∞ Í∞ÄÏ§ëÏπòÎ•º Î∂ÄÏó¨ÌïòÎäî MAE"""
    # y_trueÎäî Ïù¥ÎØ∏ 0-1 ÏÇ¨Ïù¥Î°ú Ïä§ÏºÄÏùºÎßÅÎêú Í∞íÏûÖÎãàÎã§.
    # ÏúÑÌóòÍµ¨Í∞Ñ(1651~1682)Ïùò Ïä§ÏºÄÏùºÎßÅÎêú Î≤îÏúÑÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.
    # Ïù¥ Í≥ÑÏÇ∞ÏùÄ scaler_yÍ∞Ä ÌïôÏäµÎêú ÌõÑÏóêÏïº Ï†ïÌôïÌïòÏßÄÎßå, Ïó¨Í∏∞ÏÑúÎäî Í∑ºÏÇ¨ÏπòÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.
    # ÏòàÎ•º Îì§Ïñ¥, Îç∞Ïù¥ÌÑ∞ Î≤îÏúÑÍ∞Ä [800, 2000]Ïù¥ÎùºÎ©¥,
    # (1651 - 800) / (2000 - 800) = 0.709
    # (1682 - 800) / (2000 - 800) = 0.735
    # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨Ïóê Îî∞Îùº Îã¨ÎùºÏßÄÎØÄÎ°ú, Ïó¨Í∏∞ÏÑúÎäî ÎåÄÎûµÏ†ÅÏù∏ Í∞íÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.
    danger_min_scaled = 0.7 
    danger_max_scaled = 0.75
    
    # Í∞ÄÏ§ëÏπò ÌÖêÏÑú ÏÉùÏÑ± (Í∏∞Î≥∏Í∞í 1.0)
    weights = tf.ones_like(y_true)
    
    # ÏúÑÌóòÍµ¨Í∞Ñ ÎßàÏä§ÌÅ¨ ÏÉùÏÑ±
    danger_mask = tf.logical_and(
        y_true >= danger_min_scaled,
        y_true <= danger_max_scaled
    )
    
    # ÏúÑÌóòÍµ¨Í∞ÑÏóê Ìï¥ÎãπÌïòÎäî ÏÉòÌîåÏóê 3Î∞∞Ïùò Í∞ÄÏ§ëÏπò Î∂ÄÏó¨
    weights = tf.where(danger_mask, 3.0, weights)
    
    # Í∞ÄÏ§ëÏπòÍ∞Ä Ï†ÅÏö©Îêú MAE Í≥ÑÏÇ∞
    mae = tf.abs(y_true - y_pred)
    weighted_mae_val = mae * weights
    
    return tf.reduce_mean(weighted_mae_val)

# ========================================
# ÌïôÏäµ Ìï®Ïàò
# ========================================
def train_model(train_data, val_data, test_data, processor):
    """Î™®Îç∏ ÌïôÏäµ"""
    X_train, y_train_scaled, y_train_orig, y_train_level, y_train_danger = train_data
    X_val, y_val_scaled, y_val_orig, y_val_level, y_val_danger = val_data
    X_test, y_test_scaled, y_test_orig, y_test_level, y_test_danger = test_data
    
    # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í¥ÄÎ¶¨Ïûê
    cp_manager = CheckpointManager()
    model, start_epoch = cp_manager.load_checkpoint()
    
    # Î∂ÑÏÇ∞ ÌïôÏäµ Ï†ÑÎûµ Î≤îÏúÑ ÎÇ¥ÏóêÏÑú Î™®Îç∏ ÏÉùÏÑ± Î∞è Ïª¥ÌååÏùº
    with strategy.scope():
        if model is None:
            print("ÏÉà Î™®Îç∏ ÏÉùÏÑ±...")
            input_shape = (X_train.shape[1], X_train.shape[2])
            model = build_extremenet(input_shape)
            
            model.compile(
                optimizer=Adam(learning_rate=0.001),
                loss={
                    'regression': weighted_mae,  # Ïª§Ïä§ÌÖÄ ÏÜêÏã§Ìï®Ïàò
                    'classification': 'sparse_categorical_crossentropy',
                    'danger': 'binary_crossentropy'
                },
                loss_weights={
                    'regression': 0.6,
                    'classification': 0.2,
                    'danger': 0.2
                },
                metrics={
                    'regression': 'mae',
                    'classification': 'accuracy',
                    'danger': 'accuracy'
                }
            )
    
    model.summary()
    print("\n" + "="*80)
    print("üî• ExtremeNet V6.5 Fixed ÌïôÏäµ ÏãúÏûë")
    print(f"üîß Device: {DEVICE} | Global Batch: {BATCH_SIZE}")
    print(f"‚è∞ ÏãúÏûë ÏóêÌè¨ÌÅ¨: {start_epoch}")
    print("="*80)
    
    # ÏΩúÎ∞±
    callbacks = [
        ModelCheckpoint(
            'models/best_extremenet_fixed.keras',
            save_best_only=True,
            monitor='val_loss',
            verbose=1
        ),
        EarlyStopping(
            patience=20,
            monitor='val_loss',
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            factor=0.5,
            patience=7,
            min_lr=1e-6,
            monitor='val_loss',
            verbose=1
        ),
        DangerZoneCallback(X_val, y_val_orig, processor),
        LambdaCallback(
            on_epoch_end=lambda epoch, logs: 
            cp_manager.save_checkpoint(model, start_epoch + epoch + 1, logs['val_loss']) if 'val_loss' in logs else None
        )
    ]
    
    # ‚≠ê Ïò§Î•ò ÏàòÏ†ï: Îã§Ï§ë Ï∂úÎ†• Î™®Îç∏ÏùÄ class_weightÎ•º ÏßÅÏ†ë ÏßÄÏõêÌïòÏßÄ ÏïäÏäµÎãàÎã§.
    # ÎåÄÏã†, Í∞Å ÏÉòÌîåÏóê ÎåÄÌïú Í∞ÄÏ§ëÏπòÎ•º Í≥ÑÏÇ∞ÌïòÎäî sample_weightÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.
    # Ïù¥Îäî Î∂àÍ∑†ÌòïÌïú 'classification' ÏûëÏóÖÏóê Í∞ÄÏ§ëÏπòÎ•º Ìö®Í≥ºÏ†ÅÏúºÎ°ú Ï†ÅÏö©Ìï©ÎãàÎã§.
    sample_weights = class_weight.compute_sample_weight(
        class_weight='balanced',
        y=y_train_level
    )
    
    # ÌïôÏäµ
    os.makedirs('models', exist_ok=True)
    
    history = model.fit(
        X_train,
        {
            'regression': y_train_scaled,
            'classification': y_train_level,
            'danger': y_train_danger
        },
        validation_data=(
            X_val,
            {
                'regression': y_val_scaled,
                'classification': y_val_level,
                'danger': y_val_danger
            }
        ),
        epochs=100,
        initial_epoch=start_epoch,
        batch_size=BATCH_SIZE,
        callbacks=callbacks,
        sample_weight=sample_weights,  # ‚≠ê ÏàòÏ†ï: class_weight ÎåÄÏã† sample_weight ÏÇ¨Ïö©
        verbose=1
    )
    
    # ÌèâÍ∞Ä
    print("\n" + "="*80)
    print("üìä ÏµúÏ¢Ö ÌèâÍ∞Ä")
    print("="*80)
    
    # ÏòàÏ∏°
    preds = model.predict(X_test, verbose=0)
    y_pred_scaled = preds[0].flatten()
    y_pred = processor.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    
    # Î©îÌä∏Î¶≠
    mae = mean_absolute_error(y_test_orig, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred))
    r2 = r2_score(y_test_orig, y_pred)
    
    print(f"\nüìà Ï†ÑÏ≤¥ ÏÑ±Îä•:")
    print(f"  MAE: {mae:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  R¬≤: {r2:.4f}")
    
    # ÏúÑÌóòÍµ¨Í∞Ñ ÌèâÍ∞Ä
    danger_idx = ((y_test_orig >= 1651) & (y_test_orig <= 1682))
    if np.sum(danger_idx) > 0:
        danger_mae = mean_absolute_error(y_test_orig[danger_idx], y_pred[danger_idx])
        print(f"\nüî• ÏúÑÌóòÍµ¨Í∞Ñ ÏµúÏ¢Ö ÏÑ±Îä•:")
        print(f"  ÏÉòÌîå: {np.sum(danger_idx)}Í∞ú")
        print(f"  MAE: {danger_mae:.2f}")
    
    # Î™®Îç∏ Ï†ÄÏû•
    model.save('models/extremenet_v65_fixed_final.keras')
    print(f"\nüíæ ÏµúÏ¢Ö Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: models/extremenet_v65_fixed_final.keras")
    
    return model, {'MAE': mae, 'RMSE': rmse, 'R2': r2}

# ========================================
# Ïã§ÏãúÍ∞Ñ ÏòàÏ∏°
# ========================================
class Predictor:
    def __init__(self, model_path='models/extremenet_v65_fixed_final.keras'):
        print("\nüöÄ ÏòàÏ∏°Í∏∞ Î°úÎî© Ï§ë...")
        self.model = tf.keras.models.load_model(model_path,
                                               custom_objects={'weighted_mae': weighted_mae})
        with open('scalers/scaler_X.pkl', 'rb') as f:
            self.scaler_X = pickle.load(f)
        with open('scalers/scaler_y.pkl', 'rb') as f:
            self.scaler_y = pickle.load(f)
        self.feature_columns = [
            'TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M14AM10A', 'M14AM16',
            'RATIO', 'GOLDEN', 'DANGER_ZONE', 'PRE_DANGER',
            'PATTERN', 'TREND',
            'MA_5', 'MA_10', 'MA_20',
            'STD_5', 'STD_10', 'STD_20',
            'M14B_MA_5', 'M14B_MA_10', 'M14B_MA_20',
            'CHANGE_1', 'CHANGE_5', 'CHANGE_10',
            'M14B_CHANGE_1', 'M14B_CHANGE_5', 'M14B_CHANGE_10',
            'RISE_COUNT'
        ]
        print("‚úÖ ÏòàÏ∏°Í∏∞ Ï§ÄÎπÑ ÏôÑÎ£å")
    
    def predict(self, data_100min):
        if not isinstance(data_100min, pd.DataFrame) or len(data_100min) != 100:
            raise ValueError("ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Îäî 100Í∞úÏùò ÌñâÏùÑ Í∞ÄÏßÑ pandas DataFrameÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.")
            
        # ÌäπÏÑ± Ï§ÄÎπÑ
        X = data_100min[self.feature_columns].values
        X_reshaped = X.reshape(1, 100, -1)
        
        # Ïä§ÏºÄÏùºÎßÅ (Ï†ÑÏ≤¥ ÏãúÌÄÄÏä§Î•º Ìïú Î≤àÏóê Î≥ÄÌôò)
        X_flat = X_reshaped.reshape(-1, X_reshaped.shape[-1])
        X_scaled_flat = self.scaler_X.transform(X_flat)
        X_scaled = X_scaled_flat.reshape(X_reshaped.shape)
        
        # ÏòàÏ∏°
        preds = self.model.predict(X_scaled, verbose=0)
        y_pred_scaled = preds[0][0, 0]
        y_pred = self.scaler_y.inverse_transform([[y_pred_scaled]])[0, 0]
        
        # Íµ¨Í∞Ñ ÌåêÏ†ï
        if y_pred < 1400:
            level = "üü¢ Ï†ïÏÉÅ"
        elif y_pred < 1700:
            level = "üü° Ï£ºÏùò"
        else:
            level = "üî¥ Ïã¨Í∞Å"
        
        # ÏúÑÌóòÏã†Ìò∏
        alert = ""
        if 1651 <= y_pred <= 1682:
            alert = "‚ö†Ô∏è ÏúÑÌóòÏã†Ìò∏! 1700+ Í∏âÏ¶ù Í∞ÄÎä•ÏÑ±!"
        
        return {
            'prediction': float(y_pred),
            'level': level,
            'alert': alert,
            'danger_probability': float(preds[2][0, 0])
        }

# ========================================
# Î©îÏù∏ Ïã§Ìñâ
# ========================================
def main():
    # Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú ÌÉêÏÉâ
    data_path = None
    possible_paths = [
        'data/20240201_TO_202507281705.csv', 
        '20240201_TO_202507281705.csv'
    ]
    for path in possible_paths:
        if os.path.exists(path):
            data_path = path
            break
    
    if not data_path:
        print("‚ùå Îç∞Ïù¥ÌÑ∞ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§! 'data' Ìè¥Îçî ÎòêÎäî ÌòÑÏû¨ ÏúÑÏπòÏóê CSV ÌååÏùºÏùÑ ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.")
        return None, None
    
    # Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
    processor = DataProcessor()
    df = processor.load_data(data_path)
    df_features = processor.create_features(df)
    
    # ÏãúÌÄÄÏä§ ÏÉùÏÑ±
    train_data, val_data, test_data = processor.create_sequences(df_features)
    
    # Î™®Îç∏ ÌïôÏäµ
    model, results = train_model(train_data, val_data, test_data, processor)
    
    print("\n‚úÖ Î™®Îì† ÏûëÏóÖ ÏôÑÎ£å!")
    print("üî• ÏúÑÌóòÍµ¨Í∞Ñ ÏòàÏ∏°Ïù¥ Í∞úÏÑ†Îêú Î™®Îç∏ ÌïôÏäµÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§!")
    
    return model, results

if __name__ == "__main__":
    trained_model, training_results = main()
    
    # ÌïôÏäµÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏôÑÎ£åÎêòÏóàÏùÑ Í≤ΩÏö∞, ÏòàÏ∏°Í∏∞ ÏÇ¨Ïö© ÏòàÏãú
    if trained_model and os.path.exists('models/extremenet_v65_fixed_final.keras'):
        print("\n" + "="*80)
        print("‚ö° ÏòàÏ∏°Í∏∞ ÏÇ¨Ïö© ÏòàÏãú")
        print("="*80)
        
        # ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò ÎßàÏßÄÎßâ 100Î∂Ñ Îç∞Ïù¥ÌÑ∞Î•º ÏÉòÌîåÎ°ú ÏÇ¨Ïö©
        try:
            processor = DataProcessor()
            df = processor.load_data('20240201_TO_202507281705.csv')
            df_features = processor.create_features(df)
            
            # ÏòàÏ∏°ÏùÑ ÏúÑÌïú ÏµúÏã† 100Î∂Ñ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
            sample_data = df_features.tail(100)
            
            if len(sample_data) == 100:
                predictor = Predictor()
                prediction_result = predictor.predict(sample_data)
                
                print(f"üîÆ 10Î∂Ñ ÌõÑ TOTALCNT ÏòàÏ∏°: {prediction_result['prediction']:.2f}")
                print(f"üìä ÏòàÏ∏° Î†àÎ≤®: {prediction_result['level']}")
                print(f"üìà ÏúÑÌóò Í∞êÏßÄ ÌôïÎ•†: {prediction_result['danger_probability']:.2%}")
                if prediction_result['alert']:
                    print(f"üö® {prediction_result['alert']}")
            else:
                print("‚ö†Ô∏è ÏòàÏ∏° ÏòàÏãúÎ•º ÏúÑÌïú ÏÉòÌîå Îç∞Ïù¥ÌÑ∞Í∞Ä 100Í∞ú ÎØ∏ÎßåÏûÖÎãàÎã§.")

        except Exception as e:
            print(f"üí• ÏòàÏ∏° ÏòàÏãú Ïã§Ìñâ Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")

