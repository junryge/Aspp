# -*- coding: utf-8 -*-
"""
ExtraTrees ëª¨ë¸ë¡œ CSV ë°ì´í„° í‰ê°€
280ë¶„ â†’ 10ë¶„ í›„ ì˜ˆì¸¡
"""

import pandas as pd
import numpy as np
import joblib
import pickle
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import os
import warnings
warnings.filterwarnings('ignore')

def create_features_from_280min(data_280min):
    """280ë¶„ ë°ì´í„°ì—ì„œ 147ê°œ íŠ¹ì§• ì¶”ì¶œ (í•™ìŠµ ë•Œì™€ ë™ì¼)"""
    
    feature_cols = ['M14AM14B', 'M14AM10A', 'M14AM16', 'M14AM14BSUM', 
                   'M14AM10ASUM', 'M14AM16SUM', 'M14BM14A', 'M10AM14A', 
                   'M16M14A', 'TOTALCNT']
    
    # DataFrameìœ¼ë¡œ ë³€í™˜
    if isinstance(data_280min, np.ndarray):
        df = pd.DataFrame(data_280min, columns=feature_cols)
    else:
        df = data_280min
    
    # íŒŒìƒ ë³€ìˆ˜
    df['ratio_M14B_M14A'] = np.clip(df['M14AM14B'] / (df['M14AM10A'] + 1), 0, 1000)
    df['ratio_M14B_M16'] = np.clip(df['M14AM14B'] / (df['M14AM16'] + 1), 0, 1000)
    df['totalcnt_change'] = df['TOTALCNT'].diff().fillna(0)
    df['totalcnt_pct_change'] = df['TOTALCNT'].pct_change().fillna(0).clip(-10, 10)
    
    features = []
    
    # ê° ì»¬ëŸ¼ë³„ 13ê°œ íŠ¹ì§• (í•™ìŠµ ë•Œì™€ ë™ì¼)
    for col in feature_cols:
        values = df[col].values
        
        features.extend([
            np.mean(values),
            np.std(values) if len(values) > 1 else 0,
            np.min(values),
            np.max(values),
            np.percentile(values, 25),
            np.percentile(values, 50),
            np.percentile(values, 75),
            values[-1],
            values[-1] - values[0],
            np.mean(values[-60:]),
            np.max(values[-60:]),
            np.mean(values[-30:]),
            np.max(values[-30:])
        ])
        
        # TOTALCNT íŠ¹ë³„ ì²˜ë¦¬
        if col == 'TOTALCNT':
            features.append(np.sum((values >= 1650) & (values < 1700)))
            features.append(np.sum(values >= 1700))
            features.append(np.max(values[-20:]))
            features.append(np.sum(values < 1400))
            features.append(np.sum((values >= 1400) & (values < 1700)))
            features.append(np.sum(values >= 1700))
            features.append(np.sum((values >= 1651) & (values <= 1682)))
            
            anomaly_values = values[(values >= 1651) & (values <= 1682)]
            features.append(np.max(anomaly_values) if len(anomaly_values) > 0 else 0)
            
            normal_vals = values[values < 1400]
            check_vals = values[(values >= 1400) & (values < 1700)]
            danger_vals = values[values >= 1700]
            
            features.append(np.mean(normal_vals) if len(normal_vals) > 0 else 0)
            features.append(np.mean(check_vals) if len(check_vals) > 0 else 0)
            features.append(np.mean(danger_vals) if len(danger_vals) > 0 else 0)
            
            try:
                x = np.arange(len(values))
                slope, _ = np.polyfit(x, values, 1)
                features.append(np.clip(slope, -100, 100))
            except:
                features.append(0)
            
            try:
                recent_slope = np.polyfit(np.arange(60), values[-60:], 1)[0]
                features.append(np.clip(recent_slope, -100, 100))
            except:
                features.append(0)
    
    # íŒŒìƒ ë³€ìˆ˜ ì¶”ê°€
    features.extend([
        np.clip(df['ratio_M14B_M14A'].iloc[-1], 0, 1000),
        np.clip(df['ratio_M14B_M16'].iloc[-1], 0, 1000),
        np.clip(df['totalcnt_change'].iloc[-1], -1000, 1000),
        np.clip(df['totalcnt_pct_change'].iloc[-1], -10, 10),
    ])
    
    return np.array(features)

def evaluate_csv_with_extratrees(csv_file, model_path, scaler_path=None):
    """CSV íŒŒì¼ì„ ExtraTrees ëª¨ë¸ë¡œ í‰ê°€"""
    
    print("="*70)
    print("ğŸŒ² ExtraTrees ëª¨ë¸ í‰ê°€")
    print("="*70)
    
    # 1. ëª¨ë¸ ë¡œë“œ
    print("\nğŸ“ ëª¨ë¸ ë¡œë”©...")
    model = joblib.load(model_path)
    print(f"  âœ… ëª¨ë¸: {model_path}")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ë˜ëŠ” ìƒì„±
    if scaler_path and os.path.exists(scaler_path):
        scaler = joblib.load(scaler_path)
        print(f"  âœ… ìŠ¤ì¼€ì¼ëŸ¬: {scaler_path}")
    else:
        print("  âš ï¸ ìŠ¤ì¼€ì¼ëŸ¬ ì—†ìŒ - ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤")
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        scaler_fitted = False
    
    # 2. CSV ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {csv_file}")
    df = pd.read_csv(csv_file)
    print(f"  ì›ë³¸: {len(df):,}í–‰")
    
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required_cols = ['M14AM14B', 'M14AM10A', 'M14AM16', 'M14AM14BSUM', 
                     'M14AM10ASUM', 'M14AM16SUM', 'M14BM14A', 'M10AM14A', 
                     'M16M14A', 'TOTALCNT']
    
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")
        return None
    
    # 3. ì˜ˆì¸¡ ê°€ëŠ¥ ë²”ìœ„
    seq_length = 280
    pred_horizon = 10
    
    start_idx = seq_length
    end_idx = len(df) - pred_horizon
    total = end_idx - start_idx
    
    if total <= 0:
        print("âŒ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 290ë¶„ í•„ìš”)")
        return None
    
    print(f"\nğŸ”® ì˜ˆì¸¡ ì‹œì‘...")
    print(f"  ì‹œí€€ìŠ¤: 280ë¶„ â†’ 10ë¶„ í›„")
    print(f"  ì˜ˆì¸¡ ê°œìˆ˜: {total:,}ê°œ")
    
    # 4. ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì—†ìœ¼ë©´ ë°ì´í„°ë¡œ í•™ìŠµ
    if scaler_path is None or not os.path.exists(scaler_path):
        print("\nğŸ“Š ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„± ì¤‘...")
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        
        # ëª¨ë“  íŠ¹ì§• ìˆ˜ì§‘í•˜ì—¬ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
        all_features = []
        sample_size = min(1000, total)  # ìµœëŒ€ 1000ê°œ ìƒ˜í”Œë¡œ í•™ìŠµ
        
        for i in range(start_idx, min(start_idx + sample_size, end_idx)):
            seq_data = df.iloc[i-seq_length:i]
            features = create_features_from_280min(seq_data)
            all_features.append(features)
        
        all_features = np.array(all_features)
        scaler.fit(all_features)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        joblib.dump(scaler, 'generated_scaler.pkl')
        print(f"  âœ… ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„± ì™„ë£Œ! (generated_scaler.pklë¡œ ì €ì¥)")
    
    # 5. ì˜ˆì¸¡ ìˆ˜í–‰
    predictions = []
    actuals = []
    
    for i in range(start_idx, end_idx):
        if i % 100 == 0:
            print(f"  ì§„í–‰: {i-start_idx}/{total} ({(i-start_idx)/total*100:.1f}%)", end='\r')
        
        # 280ë¶„ ì‹œí€€ìŠ¤ ì¶”ì¶œ
        seq_data = df.iloc[i-seq_length:i]
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = create_features_from_280min(seq_data)
        
        # ìŠ¤ì¼€ì¼ë§
        features_scaled = scaler.transform([features])
        
        # ì˜ˆì¸¡
        pred = model.predict(features_scaled)[0]
        predictions.append(pred)
        
        # ì‹¤ì œê°’ (10ë¶„ í›„)
        actual_idx = i + pred_horizon
        if actual_idx < len(df):
            actuals.append(df.iloc[actual_idx]['TOTALCNT'])
    
    print("\nâœ… ì˜ˆì¸¡ ì™„ë£Œ!")
    
    # 5. ì„±ëŠ¥ í‰ê°€
    predictions = np.array(predictions)
    actuals = np.array(actuals)
    
    mae = mean_absolute_error(actuals, predictions)
    rmse = np.sqrt(mean_squared_error(actuals, predictions))
    r2 = r2_score(actuals, predictions)
    mape = np.mean(abs(predictions - actuals) / actuals) * 100
    
    print(f"\nğŸ“Š ì„±ëŠ¥ ì§€í‘œ:")
    print(f"  MAE: {mae:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  RÂ²: {r2:.4f}")
    print(f"  MAPE: {mape:.2f}%")
    print(f"  ì •í™•ë„: {100-mape:.2f}%")
    
    # 6. ê²°ê³¼ ì €ì¥
    results = pd.DataFrame({
        'ì‹¤ì œê°’': actuals,
        'ì˜ˆì¸¡ê°’': predictions.round(),
        'ì˜¤ì°¨': (predictions - actuals).round(),
        'ì˜¤ì°¨ìœ¨(%)': (abs(predictions - actuals) / actuals * 100).round(2)
    })
    
    # ìœ„í—˜ êµ¬ê°„ ë¶„ì„
    danger_actual = (actuals >= 1700).sum()
    danger_pred = (predictions >= 1700).sum()
    
    print(f"\nğŸ¯ ìœ„í—˜ êµ¬ê°„(1700+) ë¶„ì„:")
    print(f"  ì‹¤ì œ: {danger_actual}ê°œ ({danger_actual/len(actuals)*100:.2f}%)")
    print(f"  ì˜ˆì¸¡: {danger_pred}ê°œ ({danger_pred/len(predictions)*100:.2f}%)")
    
    # CSV ì €ì¥
    output_file = 'extratrees_evaluation_results.csv'
    results.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    csv_file = 'M14_20250916_20250918.csv'  # í‰ê°€í•  CSV
    model_path = 'regression_classification_models_280to10/ExtraTrees_model.pkl'
    scaler_path = 'regression_classification_models_280to10/scaler.pkl'  # ì—†ì–´ë„ ë¨
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print("ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•˜ì„¸ìš”!")
    elif not os.path.exists(csv_file):
        print(f"âŒ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_file}")
    else:
        # ìŠ¤ì¼€ì¼ëŸ¬ ì—†ì–´ë„ ì‹¤í–‰
        if not os.path.exists(scaler_path):
            print(f"âš ï¸ ìŠ¤ì¼€ì¼ëŸ¬ ì—†ìŒ - ìë™ ìƒì„±ë©ë‹ˆë‹¤")
            scaler_path = None
        
        # í‰ê°€ ì‹¤í–‰
        results = evaluate_csv_with_extratrees(csv_file, model_path, scaler_path)
        
        if results is not None:
            print("\nâœ… í‰ê°€ ì™„ë£Œ!")
            print(f"ì´ {len(results)}ê°œ ì˜ˆì¸¡ ìˆ˜í–‰")