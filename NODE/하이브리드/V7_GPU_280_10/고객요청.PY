# -*- coding: utf-8 -*-
"""
ExtraTrees ëª¨ë¸ ë¶„ì„ - CSV íŒŒì¼ì˜ ì‹¤ì œ ë‚ ì§œ ë°ì´í„° ì‚¬ìš©
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.figsize'] = (20, 12)

print("="*80)
print("ExtraTrees ëª¨ë¸ ë¶„ì„ - ì‹¤ì œ CSV ë°ì´í„° ë‚ ì§œ ì‚¬ìš©")
print("="*80)

# 1. ë°ì´í„° ë¡œë“œ
print("\n[1] ë°ì´í„° ë¡œë”©...")
data_file = 'data/20240201_to_202507271705.csv'
df = pd.read_csv(data_file)

print(f"âœ“ ë°ì´í„° íŒŒì¼: {data_file}")
print(f"âœ“ ì „ì²´ í–‰ ìˆ˜: {len(df):,}ê°œ")
print(f"âœ“ ì»¬ëŸ¼: {df.columns.tolist()}")

# CSV íŒŒì¼ì—ì„œ ì‹¤ì œ ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ í™•ì¸
date_columns = []
for col in df.columns:
    if 'date' in col.lower() or 'time' in col.lower() or 'datetime' in col.lower():
        date_columns.append(col)
        print(f"âœ“ ë‚ ì§œ ì»¬ëŸ¼ ë°œê²¬: {col}")

# ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¶„ ë‹¨ìœ„ë¡œ ìƒì„±
if date_columns:
    # ì‹¤ì œ ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°
    date_col = date_columns[0]
    df['timestamp'] = pd.to_datetime(df[date_col])
    print(f"âœ“ ë‚ ì§œ ì»¬ëŸ¼ '{date_col}' ì‚¬ìš©")
else:
    # ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë¶„ ë‹¨ìœ„ë¡œ ê°€ì • (ë°ì´í„°ê°€ 1ë¶„ ê°„ê²©ìœ¼ë¡œ ìˆ˜ì§‘ëœë‹¤ê³  ê°€ì •)
    print("âš ï¸ ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ì–´ ì¸ë±ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ì‹œê°„ ìƒì„± (1ë¶„ ê°„ê²© ê°€ì •)")
    # ì²« í–‰ì„ ì„ì˜ì˜ ì‹œì‘ ì‹œê°„ìœ¼ë¡œ ì„¤ì • (ì‹¤ì œ ë°ì´í„°ì— ë§ê²Œ ì¡°ì • í•„ìš”)
    # ë°ì´í„°ê°€ ë¶„ ë‹¨ìœ„ë¼ê³  ê°€ì •
    base_date = datetime(2024, 2, 1, 0, 0)  # ì„ì‹œ ì‹œì‘ì¼
    df['timestamp'] = [base_date + timedelta(minutes=i) for i in range(len(df))]

# ì‹¤ì œ ë‚ ì§œ ë²”ìœ„ í™•ì¸
actual_start = df['timestamp'].iloc[0]
actual_end = df['timestamp'].iloc[-1]
total_duration = actual_end - actual_start

print(f"\nğŸ“… ë°ì´í„° ì‹¤ì œ ê¸°ê°„:")
print(f"  ì‹œì‘: {actual_start}")
print(f"  ì¢…ë£Œ: {actual_end}")
print(f"  ì´ ê¸°ê°„: {total_duration.days}ì¼ ({total_duration.days/30:.1f}ê°œì›”)")
print(f"  ë°ì´í„° í¬ì¸íŠ¸: {len(df):,}ê°œ")

# 2. ì›ë³¸ ì½”ë“œì˜ ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜ ì‚¬ìš©
def assign_level(totalcnt_value):
    if totalcnt_value < 1400:
        return 0  # ì •ìƒ
    elif totalcnt_value < 1700:
        return 1  # í™•ì¸
    else:
        return 2  # ìœ„í—˜

def create_sequences_280to10(data, seq_length=280, pred_horizon=10):
    """ì›ë³¸ ì½”ë“œì™€ ë™ì¼í•œ ì‹œí€€ìŠ¤ ìƒì„±"""
    
    print(f"\nì‹œí€€ìŠ¤ ìƒì„± ì¤‘... (280ë¶„ â†’ 10ë¶„ í›„)")
    
    feature_cols = ['M14AM14B', 'M14AM10A', 'M14AM16', 'M14AM14BSUM', 
                   'M14AM10ASUM', 'M14AM16SUM', 'M14BM14A', 'M10AM14A', 'M16M14A', 'TOTALCNT']
    
    # í•„ìš”í•œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
    missing_cols = [col for col in feature_cols if col not in data.columns]
    if missing_cols:
        print(f"âš ï¸ ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_cols}")
        # ëˆ„ë½ëœ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
        feature_cols = [col for col in feature_cols if col in data.columns]
        print(f"âœ“ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {feature_cols}")
    
    # íŒŒìƒ ë³€ìˆ˜ ìƒì„±
    data = data.copy()
    
    if 'M14AM14B' in data.columns and 'M14AM10A' in data.columns:
        data['ratio_M14B_M14A'] = np.clip(data['M14AM14B'] / (data['M14AM10A'] + 1), 0, 1000)
    if 'M14AM14B' in data.columns and 'M14AM16' in data.columns:
        data['ratio_M14B_M16'] = np.clip(data['M14AM14B'] / (data['M14AM16'] + 1), 0, 1000)
    
    if 'TOTALCNT' in data.columns:
        data['totalcnt_change'] = data['TOTALCNT'].diff().fillna(0)
        data['totalcnt_pct_change'] = data['TOTALCNT'].pct_change().fillna(0)
        data['totalcnt_pct_change'] = np.clip(data['totalcnt_pct_change'], -10, 10)
    
    data = data.replace([np.inf, -np.inf], 0)
    data = data.fillna(0)
    
    X_list = []
    y_list = []
    timestamps = []
    
    n_sequences = len(data) - seq_length - pred_horizon + 1
    print(f"âœ“ ìƒì„± ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤: {n_sequences:,}ê°œ")
    
    for i in range(n_sequences):
        if i % 10000 == 0:
            print(f"  ì§„í–‰ë¥ : {i/n_sequences*100:.1f}%", end='\r')
        
        start_idx = i
        end_idx = i + seq_length
        seq_data = data.iloc[start_idx:end_idx]
        
        features = []
        
        for col in feature_cols:
            values = seq_data[col].values
            
            features.extend([
                np.mean(values),
                np.std(values) if len(values) > 1 else 0,
                np.min(values),
                np.max(values),
                np.percentile(values, 25),
                np.percentile(values, 50),
                np.percentile(values, 75),
                values[-1],
                values[-1] - values[0],
                np.mean(values[-60:]),
                np.max(values[-60:]),
                np.mean(values[-30:]),
                np.max(values[-30:]),
            ])
            
            if col == 'TOTALCNT':
                features.append(np.sum((values >= 1650) & (values < 1700)))
                features.append(np.sum(values >= 1700))
                features.append(np.max(values[-20:]))
                features.append(np.sum(values < 1400))
                features.append(np.sum((values >= 1400) & (values < 1700)))
                features.append(np.sum(values >= 1700))
                features.append(np.sum((values >= 1651) & (values <= 1682)))
                
                anomaly_values = values[(values >= 1651) & (values <= 1682)]
                features.append(np.max(anomaly_values) if len(anomaly_values) > 0 else 0)
                
                normal_vals = values[values < 1400]
                check_vals = values[(values >= 1400) & (values < 1700)]
                danger_vals = values[values >= 1700]
                
                features.append(np.mean(normal_vals) if len(normal_vals) > 0 else 0)
                features.append(np.mean(check_vals) if len(check_vals) > 0 else 0)
                features.append(np.mean(danger_vals) if len(danger_vals) > 0 else 0)
                
                try:
                    slope = np.polyfit(np.arange(len(values)), values, 1)[0]
                    features.append(np.clip(slope, -100, 100))
                except:
                    features.append(0)
                
                try:
                    recent_slope = np.polyfit(np.arange(60), values[-60:], 1)[0]
                    features.append(np.clip(recent_slope, -100, 100))
                except:
                    features.append(0)
        
        # íŒŒìƒ ë³€ìˆ˜ ì¶”ê°€
        last_idx = end_idx - 1
        if 'ratio_M14B_M14A' in data.columns:
            features.append(np.clip(data['ratio_M14B_M14A'].iloc[last_idx], 0, 1000))
        if 'ratio_M14B_M16' in data.columns:
            features.append(np.clip(data['ratio_M14B_M16'].iloc[last_idx], 0, 1000))
        if 'totalcnt_change' in data.columns:
            features.append(np.clip(data['totalcnt_change'].iloc[last_idx], -1000, 1000))
        if 'totalcnt_pct_change' in data.columns:
            features.append(np.clip(data['totalcnt_pct_change'].iloc[last_idx], -10, 10))
        
        # íƒ€ê²Ÿ
        target_idx = end_idx + pred_horizon - 1
        if target_idx < len(data) and 'TOTALCNT' in data.columns:
            X_list.append(features)
            y_list.append(data['TOTALCNT'].iloc[target_idx])
            timestamps.append(data['timestamp'].iloc[target_idx])
    
    X = np.array(X_list)
    y = np.array(y_list)
    X = np.nan_to_num(X, nan=0.0, posinf=1000.0, neginf=-1000.0)
    
    print(f"\nâœ“ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ! Shape: {X.shape}")
    
    return X, y, timestamps

# 3. ì‹œí€€ìŠ¤ ìƒì„±
print("\n[2] ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±...")
X, y, timestamps = create_sequences_280to10(df)

# 4. ì‹œê³„ì—´ ìˆœì„œë¡œ ë¶„í•  (60:20:20)
print("\n[3] ì‹œê³„ì—´ ìˆœì„œ ìœ ì§€ ë°ì´í„° ë¶„í• ...")

n_samples = len(X)
train_size = int(n_samples * 0.6)
val_size = int(n_samples * 0.2)

X_train = X[:train_size]
y_train = y[:train_size]
timestamps_train = timestamps[:train_size]

X_val = X[train_size:train_size+val_size]
y_val = y[train_size:train_size+val_size]
timestamps_val = timestamps[train_size:train_size+val_size]

X_test = X[train_size+val_size:]
y_test = y[train_size+val_size:]
timestamps_test = timestamps[train_size+val_size:]

# 5. ì‹¤ì œ ë°ì´í„° ê¸°ê°„ ì¶œë ¥
print("\n" + "="*80)
print("ğŸ“… ì‹¤ì œ ë°ì´í„°ì…‹ ë‚ ì§œ ë²”ìœ„ (CSV íŒŒì¼ ê¸°ì¤€)")
print("="*80)

print(f"\n[Train Set] - 60%")
print(f"  ì‹œì‘: {timestamps_train[0]}")
print(f"  ì¢…ë£Œ: {timestamps_train[-1]}")
print(f"  ê¸°ê°„: {(timestamps_train[-1] - timestamps_train[0]).days}ì¼")
print(f"  ìƒ˜í”Œ: {len(X_train):,}ê°œ")

print(f"\n[Validation Set] - 20%")
print(f"  ì‹œì‘: {timestamps_val[0]}")
print(f"  ì¢…ë£Œ: {timestamps_val[-1]}")
print(f"  ê¸°ê°„: {(timestamps_val[-1] - timestamps_val[0]).days}ì¼")
print(f"  ìƒ˜í”Œ: {len(X_val):,}ê°œ")

print(f"\n[Test Set] - 20%")
print(f"  ì‹œì‘: {timestamps_test[0]}")
print(f"  ì¢…ë£Œ: {timestamps_test[-1]}")
print(f"  ê¸°ê°„: {(timestamps_test[-1] - timestamps_test[0]).days}ì¼")
print(f"  ìƒ˜í”Œ: {len(X_test):,}ê°œ")

# 6. ì •ê·œí™”
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# 7. ExtraTrees í•™ìŠµ ê³¡ì„ 
print("\n[4] ExtraTrees í•™ìŠµ ê³¡ì„  ìƒì„±...")

n_estimators_list = [50, 100, 150, 200, 250, 300]
train_losses = []
val_losses = []

for n_est in n_estimators_list:
    print(f"  n_estimators={n_est} í•™ìŠµ...", end=' ')
    
    model = ExtraTreesRegressor(
        n_estimators=n_est,
        max_depth=20,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )
    
    model.fit(X_train_scaled, y_train)
    
    train_mae = mean_absolute_error(y_train, model.predict(X_train_scaled))
    val_mae = mean_absolute_error(y_val, model.predict(X_val_scaled))
    
    train_losses.append(train_mae)
    val_losses.append(val_mae)
    
    print(f"Train: {train_mae:.2f}, Val: {val_mae:.2f}, Gap: {val_mae-train_mae:.2f}")

# 8. ìµœì¢… ëª¨ë¸ (n_estimators=300)
print("\n[5] ìµœì¢… ëª¨ë¸ í•™ìŠµ (n_estimators=300)...")

final_model = ExtraTreesRegressor(
    n_estimators=300,
    max_depth=20,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)

final_model.fit(X_train_scaled, y_train)

# 9. ì„±ëŠ¥ í‰ê°€
y_train_pred = final_model.predict(X_train_scaled)
y_val_pred = final_model.predict(X_val_scaled)
y_test_pred = final_model.predict(X_test_scaled)

train_mae = mean_absolute_error(y_train, y_train_pred)
val_mae = mean_absolute_error(y_val, y_val_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

print("\n" + "="*80)
print("ğŸ“Š ìµœì¢… ì„±ëŠ¥")
print("="*80)
print(f"Train MAE: {train_mae:.2f} (RÂ²: {train_r2:.4f})")
print(f"Val MAE: {val_mae:.2f} (RÂ²: {val_r2:.4f})")
print(f"Test MAE: {test_mae:.2f} (RÂ²: {test_r2:.4f})")

print(f"\nğŸ” Overfitting ì²´í¬:")
print(f"Val-Train Gap: {val_mae-train_mae:.2f}")
print(f"Test-Train Gap: {test_mae-train_mae:.2f}")

if val_mae-train_mae < 30:
    print("âœ… Overfitting ìƒíƒœ: ì–‘í˜¸")
elif val_mae-train_mae < 50:
    print("âš ï¸ Overfitting ìƒíƒœ: ì£¼ì˜")
else:
    print("âŒ Overfitting ìƒíƒœ: ì‹¬ê°")

# 10. ê·¸ë˜í”„
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Loss ê³¡ì„ 
ax1 = axes[0, 0]
ax1.plot(n_estimators_list, train_losses, 'b-', marker='o', label='Train MAE')
ax1.plot(n_estimators_list, val_losses, 'r-', marker='s', label='Val MAE')
ax1.set_xlabel('n_estimators')
ax1.set_ylabel('MAE')
ax1.set_title('Learning Curve')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Overfitting Gap
ax2 = axes[0, 1]
gaps = [val_losses[i] - train_losses[i] for i in range(len(n_estimators_list))]
ax2.bar(n_estimators_list, gaps, color=['green' if g < 30 else 'orange' if g < 50 else 'red' for g in gaps])
ax2.axhline(y=30, color='orange', linestyle='--', alpha=0.5)
ax2.axhline(y=50, color='red', linestyle='--', alpha=0.5)
ax2.set_xlabel('n_estimators')
ax2.set_ylabel('Val MAE - Train MAE')
ax2.set_title('Overfitting Gap')
ax2.grid(True, alpha=0.3)

# Test ì˜ˆì¸¡
ax3 = axes[1, 0]
sample_idx = np.random.choice(len(y_test), min(1000, len(y_test)), replace=False)
ax3.scatter(y_test[sample_idx], y_test_pred[sample_idx], alpha=0.5, s=10)
ax3.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
ax3.set_xlabel('Actual')
ax3.set_ylabel('Predicted')
ax3.set_title(f'Test Predictions (RÂ²={test_r2:.3f})')
ax3.grid(True, alpha=0.3)

# ì„±ëŠ¥ ìš”ì•½
ax4 = axes[1, 1]
datasets = ['Train', 'Val', 'Test']
maes = [train_mae, val_mae, test_mae]
r2s = [train_r2, val_r2, test_r2]
x = np.arange(3)
ax4.bar(x - 0.2, maes, 0.4, label='MAE', color='blue', alpha=0.7)
ax4.bar(x + 0.2, [r*100 for r in r2s], 0.4, label='RÂ² (%)', color='green', alpha=0.7)
ax4.set_xticks(x)
ax4.set_xticklabels(datasets)
ax4.set_ylabel('Value')
ax4.set_title('Performance Summary')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('extratrees_analysis.png', dpi=150)
plt.show()

print("\nâœ… ë¶„ì„ ì™„ë£Œ! (extratrees_analysis.png ì €ì¥ë¨)")