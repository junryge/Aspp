"""
ğŸ”¥ ExtremeNet V6.5 + UU1/UU2 íŒ¨í„´ í†µí•© (ì•ˆì „ ë²„ì „)
================================================================
í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶€ì¡± ì˜¤ë¥˜ ë°©ì§€
ëª©í‘œ: MAE < 40, ìœ„í—˜êµ¬ê°„ ì •í™•ë„ > 80%
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import *
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
import os
import pickle
import json
from datetime import datetime, timedelta

warnings.filterwarnings('ignore')
tf.random.set_seed(42)
np.random.seed(42)

# GPU ì„¤ì •
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    print(f"ğŸ”§ GPU {len(gpus)}ê°œ ë°œê²¬!")
    for i, gpu in enumerate(gpus):
        tf.config.experimental.set_memory_growth(gpu, True)
    strategy = tf.distribute.MirroredStrategy()
    DEVICE = 'GPU'
    BATCH_SIZE = 128
else:
    print("âš ï¸ GPU ì—†ìŒ, CPU ì‚¬ìš©")
    strategy = tf.distribute.get_strategy()
    DEVICE = 'CPU'
    BATCH_SIZE = 32

print("="*80)
print("ğŸ”¥ ExtremeNet V6.5 + íŒ¨í„´ í†µí•© (ì•ˆì „ ë²„ì „)")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print(f"ğŸ”§ Device: {DEVICE}")
print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
print("="*80)

# ========================================
# UU1/UU2 íŒ¨í„´ ê°ì§€ (ì‹¤ì œ ì˜ˆì¸¡ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´)
# ========================================
def detect_pattern_for_window(df_window):
    """100ë¶„ ìœˆë„ìš°ì—ì„œ íŒ¨í„´ ê°ì§€"""
    totalcnt_max = df_window['TOTALCNT'].max()
    m14b_mean = df_window['M14AM14B'].mean()
    m14b_max = df_window['M14AM14B'].max()
    high_cases = (df_window['TOTALCNT'] >= 1682).sum()
    m14b_350_ratio = (df_window['M14AM14B'] > 350).sum() / len(df_window)
    
    # UU2: ê³ ê°’ ìœ ì§€
    if high_cases > 0 and m14b_mean > 380:
        return 2  # UU2
    elif m14b_350_ratio > 0.3:
        return 2  # UU2
    # UU1: ê¸‰ì¦ ê°€ëŠ¥
    else:
        return 1  # UU1

def calculate_pattern_risk_score(df_window, pattern):
    """íŒ¨í„´ ê¸°ë°˜ ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°"""
    current_val = df_window['TOTALCNT'].iloc[-1]
    m14b = df_window['M14AM14B'].iloc[-1]
    m14a = df_window['M14AM10A'].iloc[-1]
    ratio = m14b / (m14a + 1)
    
    # ì—°ì† ìƒìŠ¹
    consecutive_rises = 0
    for i in range(len(df_window)-1, 0, -1):
        if df_window['TOTALCNT'].iloc[i] > df_window['TOTALCNT'].iloc[i-1]:
            consecutive_rises += 1
        else:
            break
    
    # ìœ„í—˜ë„ ì ìˆ˜
    risk_score = 0
    
    if pattern == 1:  # UU1
        if current_val >= 1650:
            risk_score += 40
        if m14b > 300 and m14a < 80:  # í™©ê¸ˆ íŒ¨í„´
            risk_score += 30
        if consecutive_rises >= 10:
            risk_score += 20
        if m14b >= 400:
            risk_score += 25
    else:  # UU2
        if m14b >= 450:
            risk_score += 35
        elif m14b >= 400:
            risk_score += 30
    
    return min(risk_score, 100) / 100.0  # 0~1 ë²”ìœ„

# ========================================
# ê°œì„ ëœ ë°ì´í„° ì²˜ë¦¬ (ê¸°ì¡´ + íŒ¨í„´ + ì•ˆì „)
# ========================================
class DataProcessor:
    def __init__(self):
        self.seq_len = 100
        self.pred_len = 10
        
        # ìŠ¤ì¼€ì¼ëŸ¬
        self.scaler_X = StandardScaler()
        self.scaler_y = MinMaxScaler(feature_range=(0, 1))
        
        # êµ¬ê°„ ì¬ì •ì˜
        self.NORMAL_MAX = 1400
        self.CAUTION_MAX = 1650
        self.SIGNAL_MIN = 1651
        self.SIGNAL_MAX = 1699
        self.DANGER_MIN = 1700
        
        self.feature_columns = None
        
    def load_data(self, filepath, sample_ratio=1.0):
        """ë°ì´í„° ë¡œë“œ (ìƒ˜í”Œë§ ì˜µì…˜)"""
        print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {filepath}")
        df = pd.read_csv(filepath)
        
        # ì‹œê°„ ì •ë ¬
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                       format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        # 0ê°’ ì œê±°
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        
        # ìƒ˜í”Œë§ (í…ŒìŠ¤íŠ¸ìš©)
        if sample_ratio < 1.0:
            sample_size = int(len(df) * sample_ratio)
            # ìµœì†Œ 1000ê°œëŠ” í™•ë³´
            sample_size = max(sample_size, 1000)
            df = df.tail(sample_size)  # ìµœê·¼ ë°ì´í„° ì‚¬ìš©
            print(f"âš¡ ìƒ˜í”Œë§: {sample_size}ê°œ ({sample_ratio*100:.0f}%)")
        
        print(f"âœ… ë°ì´í„°: {len(df):,}ê°œ")
        
        # êµ¬ê°„ í†µê³„
        normal = (df['TOTALCNT'] < self.NORMAL_MAX).sum()
        caution = ((df['TOTALCNT'] >= self.NORMAL_MAX) & 
                  (df['TOTALCNT'] <= self.CAUTION_MAX)).sum()
        signal = ((df['TOTALCNT'] >= self.SIGNAL_MIN) & 
                 (df['TOTALCNT'] <= self.SIGNAL_MAX)).sum()
        danger = (df['TOTALCNT'] >= self.DANGER_MIN).sum()
        
        print(f"\nğŸ“Š TOTALCNT ë¶„í¬:")
        print(f"  ğŸŸ¢ ì •ìƒ(~1400): {normal:,}ê°œ ({normal/len(df)*100:.1f}%)")
        print(f"  ğŸŸ¡ ì£¼ì˜(1400~1650): {caution:,}ê°œ ({caution/len(df)*100:.1f}%)")
        print(f"  ğŸŸ  ì‹ í˜¸(1651~1699): {signal:,}ê°œ ({signal/len(df)*100:.1f}%)")
        print(f"  ğŸ”´ ìœ„í—˜(1700+): {danger:,}ê°œ ({danger/len(df)*100:.1f}%)")
        
        return df
    
    def create_features(self, df):
        """íŠ¹ì„± ìƒì„± (íŒ¨í„´ íŠ¹ì„± ì¶”ê°€)"""
        print("\nâš™ï¸ íŠ¹ì„± ìƒì„± ì¤‘...")
        
        # ===== ê¸°ì¡´ íŠ¹ì„± =====
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(int)
        
        # ì‹ í˜¸êµ¬ê°„ íŠ¹ì„±
        df['SIGNAL_ZONE'] = ((df['TOTALCNT'] >= self.SIGNAL_MIN) & 
                             (df['TOTALCNT'] <= self.SIGNAL_MAX)).astype(int)
        df['PRE_SIGNAL'] = ((df['TOTALCNT'] >= 1600) & 
                            (df['TOTALCNT'] < self.SIGNAL_MIN)).astype(int)
        df['DANGER_ZONE'] = (df['TOTALCNT'] >= self.DANGER_MIN).astype(int)
        
        # ì´ë™í‰ê· 
        for w in [5, 10, 20]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
            df[f'M14B_MA_{w}'] = df['M14AM14B'].rolling(w, min_periods=1).mean()
        
        # ë³€í™”ìœ¨
        for lag in [1, 5, 10]:
            df[f'CHANGE_{lag}'] = df['TOTALCNT'].diff(lag).fillna(0)
            df[f'M14B_CHANGE_{lag}'] = df['M14AM14B'].diff(lag).fillna(0)
        
        # ì—°ì† ìƒìŠ¹
        df['RISE'] = (df['TOTALCNT'] > df['TOTALCNT'].shift(1)).astype(int)
        df['RISE_COUNT'] = df['RISE'].rolling(10, min_periods=1).sum()
        
        # ===== íŒ¨í„´ íŠ¹ì„± (ìƒˆë¡œ ì¶”ê°€) =====
        # UU1/UU2 íŒ¨í„´
        df['PATTERN'] = 0
        df['PATTERN_RISK'] = 0.0
        
        # 100ë¶„ ìœˆë„ìš°ë¡œ íŒ¨í„´ ê°ì§€
        for i in range(100, len(df)):
            window = df.iloc[i-100:i]
            pattern = detect_pattern_for_window(window)
            risk = calculate_pattern_risk_score(window, pattern)
            df.loc[i, 'PATTERN'] = pattern
            df.loc[i, 'PATTERN_RISK'] = risk
        
        # M14B ê¸°ë°˜ ìœ„í—˜ ë ˆë²¨
        df['M14B_HIGH'] = (df['M14AM14B'] >= 350).astype(int)
        df['M14B_VERY_HIGH'] = (df['M14AM14B'] >= 400).astype(int)
        df['M14B_EXTREME'] = (df['M14AM14B'] >= 450).astype(int)
        
        # ê¸‰ì¦ ì‹ í˜¸
        df['SPIKE_SIGNAL'] = ((df['CHANGE_10'] > 50) & (df['RISE_COUNT'] > 5)).astype(int)
        
        # ì¶”ì„¸
        df['TREND'] = 1
        for i in range(20, len(df)):
            recent = df['TOTALCNT'].iloc[i-20:i].values
            if len(recent) > 1:
                slope = np.polyfit(range(len(recent)), recent, 1)[0]
                if slope > 5:
                    df.loc[i, 'TREND'] = 2  # ìƒìŠ¹
                elif slope < -5:
                    df.loc[i, 'TREND'] = 0  # í•˜ë½
                else:
                    df.loc[i, 'TREND'] = 1  # ë³´í•©
        
        print(f"âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ")
        return df
    
    def create_sequences(self, df):
        """ì‹œí€€ìŠ¤ ìƒì„± (ì•ˆì „í•œ ë°ì´í„° ë¶„í• )"""
        print("\nğŸ”„ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        
        # ì‚¬ìš©í•  ì»¬ëŸ¼
        self.feature_columns = [
            'TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M14AM10A', 'M14AM16',
            'RATIO', 'GOLDEN', 
            'SIGNAL_ZONE', 'PRE_SIGNAL', 'DANGER_ZONE',
            'PATTERN', 'PATTERN_RISK',  # íŒ¨í„´ íŠ¹ì„±
            'M14B_HIGH', 'M14B_VERY_HIGH', 'M14B_EXTREME',  # M14B ë ˆë²¨
            'SPIKE_SIGNAL',  # ê¸‰ì¦ ì‹ í˜¸
            'TREND',
            'MA_5', 'MA_10', 'MA_20',
            'STD_5', 'STD_10', 'STD_20',
            'M14B_MA_5', 'M14B_MA_10', 'M14B_MA_20',
            'CHANGE_1', 'CHANGE_5', 'CHANGE_10',
            'M14B_CHANGE_1', 'M14B_CHANGE_5', 'M14B_CHANGE_10',
            'RISE_COUNT'
        ]
        
        print(f"  íŠ¹ì„±: {len(self.feature_columns)}ê°œ")
        
        # ë°ì´í„° ì¤€ë¹„
        X_data = df[self.feature_columns].values
        y_data = df['TOTALCNT'].shift(-self.pred_len).values
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y = [], []
        pattern_samples = {'UU1_danger': [], 'UU2_danger': [], 'signal': []}
        
        for i in range(len(df) - self.seq_len - self.pred_len):
            seq_X = X_data[i:i+self.seq_len]
            target = y_data[i+self.seq_len-1]
            
            if np.isnan(target):
                continue
            
            current = df['TOTALCNT'].iloc[i+self.seq_len-1]
            pattern = df['PATTERN'].iloc[i+self.seq_len-1]
            
            X.append(seq_X)
            y.append(target)
            
            # ìƒìŠ¹ ì¶”ì„¸ í™•ì¸
            is_rising = target > current
            
            # íŒ¨í„´ë³„ ì˜¤ë²„ìƒ˜í”Œë§
            if is_rising:
                # UU1 íŒ¨í„´ + ìœ„í—˜êµ¬ê°„
                if pattern == 1 and target >= self.SIGNAL_MIN:
                    for _ in range(8 if target >= self.DANGER_MIN else 5):
                        pattern_samples['UU1_danger'].append((seq_X, target))
                
                # UU2 íŒ¨í„´ + ìœ„í—˜êµ¬ê°„
                elif pattern == 2 and target >= self.DANGER_MIN:
                    for _ in range(10):
                        pattern_samples['UU2_danger'].append((seq_X, target))
                
                # ì‹ í˜¸êµ¬ê°„
                elif self.SIGNAL_MIN <= target <= self.SIGNAL_MAX:
                    for _ in range(6):
                        pattern_samples['signal'].append((seq_X, target))
        
        # ì˜¤ë²„ìƒ˜í”Œë§ ë°ì´í„° ì¶”ê°€
        for key, samples in pattern_samples.items():
            if samples:
                X_add, y_add = zip(*samples)
                X.extend(X_add)
                y.extend(y_add)
                print(f"  ğŸ”¥ {key}: {len(samples)}ê°œ ì¶”ê°€")
        
        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.float32)
        
        # NaN ì œê±°
        valid_idx = ~np.isnan(y)
        X = X[valid_idx]
        y = y[valid_idx]
        
        print(f"  ì´ ì‹œí€€ìŠ¤: {X.shape[0]}ê°œ")
        
        # ìµœì†Œ ë°ì´í„° ì²´í¬
        MIN_SEQUENCES = 500
        if len(X) < MIN_SEQUENCES:
            print(f"  âš ï¸ ì‹œí€€ìŠ¤ ë¶€ì¡±! {len(X)}ê°œ < {MIN_SEQUENCES}ê°œ")
            print("  ë°ì´í„°ë¥¼ ë” ëŠ˜ë ¤ì£¼ì„¸ìš”.")
            return None, None, None
        
        # ì…”í”Œ
        shuffle_idx = np.arange(len(X))
        np.random.shuffle(shuffle_idx)
        X = X[shuffle_idx]
        y = y[shuffle_idx]
        
        # ì•ˆì „í•œ ë°ì´í„° ë¶„í• 
        train_ratio = 0.7
        val_ratio = 0.15
        test_ratio = 0.15
        
        # ìµœì†Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° í™•ë³´
        MIN_TEST_SIZE = 100
        test_size = max(int(len(X) * test_ratio), MIN_TEST_SIZE)
        
        # ë¹„ìœ¨ ì¬ì¡°ì •
        if test_size > len(X) * 0.3:  # í…ŒìŠ¤íŠ¸ê°€ ì „ì²´ì˜ 30% ì´ˆê³¼í•˜ë©´
            print("  âš ï¸ ë°ì´í„° ë¶„í•  ë¹„ìœ¨ ì¡°ì •")
            train_ratio = 0.6
            val_ratio = 0.2
            test_ratio = 0.2
        
        train_size = int(len(X) * train_ratio)
        val_size = int(len(X) * val_ratio)
        test_size = len(X) - train_size - val_size
        
        # ìµœì¢… ì²´í¬
        if test_size < 10:
            print(f"  âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë„ˆë¬´ ì ìŒ: {test_size}ê°œ")
            print("  í‰ê°€ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            test_size = 0
        
        X_train = X[:train_size]
        y_train = y[:train_size]
        X_val = X[train_size:train_size+val_size]
        y_val = y[train_size:train_size+val_size]
        X_test = X[train_size+val_size:] if test_size > 0 else np.array([])
        y_test = y[train_size+val_size:] if test_size > 0 else np.array([])
        
        # ìŠ¤ì¼€ì¼ë§
        print("  ìŠ¤ì¼€ì¼ë§ ì¤‘...")
        
        # X ìŠ¤ì¼€ì¼ë§
        X_train_flat = X_train.reshape(-1, X_train.shape[-1])
        self.scaler_X.fit(X_train_flat)
        
        X_train_scaled = self.scaler_X.transform(X_train_flat).reshape(X_train.shape)
        X_val_scaled = self.scaler_X.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)
        X_test_scaled = self.scaler_X.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape) if test_size > 0 else np.array([])
        
        # y ìŠ¤ì¼€ì¼ë§
        self.scaler_y.fit(y_train.reshape(-1, 1))
        y_train_scaled = self.scaler_y.transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = self.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        y_test_scaled = self.scaler_y.transform(y_test.reshape(-1, 1)).flatten() if test_size > 0 else np.array([])
        
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
        print(f"  Train: {len(X_train):,}ê°œ")
        print(f"  Val: {len(X_val):,}ê°œ")
        print(f"  Test: {len(X_test):,}ê°œ" if test_size > 0 else "  Test: ìŠ¤í‚µ (ë°ì´í„° ë¶€ì¡±)")
        
        # êµ¬ê°„ë³„ í†µê³„
        self._print_statistics(y_train, y_val, y_test if test_size > 0 else None)
        
        # ë ˆì´ë¸” ìƒì„±
        y_train_level = self._create_level_labels(y_train)
        y_val_level = self._create_level_labels(y_val)
        y_test_level = self._create_level_labels(y_test) if test_size > 0 else np.array([])
        
        # íŒ¨í„´ ë ˆì´ë¸”
        y_train_pattern = self._create_pattern_labels(df, train_size, 0, len(y_train))
        y_val_pattern = self._create_pattern_labels(df, train_size + val_size, train_size, len(y_val))
        y_test_pattern = self._create_pattern_labels(df, len(df), train_size + val_size, len(y_test)) if test_size > 0 else np.array([])
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        os.makedirs('scalers_pattern', exist_ok=True)
        with open('scalers_pattern/scaler_X.pkl', 'wb') as f:
            pickle.dump(self.scaler_X, f)
        with open('scalers_pattern/scaler_y.pkl', 'wb') as f:
            pickle.dump(self.scaler_y, f)
        
        return (X_train_scaled, y_train_scaled, y_train, y_train_level, y_train_pattern), \
               (X_val_scaled, y_val_scaled, y_val, y_val_level, y_val_pattern), \
               (X_test_scaled, y_test_scaled, y_test, y_test_level, y_test_pattern)
    
    def _create_level_labels(self, y):
        """êµ¬ê°„ ë ˆì´ë¸” ìƒì„±"""
        if len(y) == 0:
            return np.array([])
        
        labels = np.zeros(len(y), dtype=int)
        labels[(y >= self.NORMAL_MAX) & (y <= self.CAUTION_MAX)] = 1
        labels[(y >= self.SIGNAL_MIN) & (y <= self.SIGNAL_MAX)] = 2
        labels[y >= self.DANGER_MIN] = 3
        return labels
    
    def _create_pattern_labels(self, df, max_idx, offset, length):
        """íŒ¨í„´ ë ˆì´ë¸” ìƒì„±"""
        pattern_labels = []
        for i in range(length):
            if i + offset < len(df) and i + offset < max_idx:
                pattern_labels.append(df['PATTERN'].iloc[i + offset])
            else:
                pattern_labels.append(1)  # ê¸°ë³¸ê°’
        return np.array(pattern_labels, dtype=int) - 1  # 0,1ë¡œ ë³€í™˜
    
    def _print_statistics(self, y_train, y_val, y_test):
        """í†µê³„ ì¶œë ¥"""
        for name, data in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:
            if data is not None and len(data) > 0:
                signal = ((data >= self.SIGNAL_MIN) & (data <= self.SIGNAL_MAX)).sum()
                danger = (data >= self.DANGER_MIN).sum()
                print(f"  {name} - ì‹ í˜¸: {signal}ê°œ, ìœ„í—˜: {danger}ê°œ")

# ========================================
# íŒ¨í„´ í†µí•© ExtremeNet ëª¨ë¸
# ========================================
def build_pattern_extremenet(input_shape):
    """íŒ¨í„´ ì¸ì‹ ê°•í™” ExtremeNet"""
    inputs = Input(shape=input_shape)
    
    # ===== ê¸°ì¡´ ë¸Œëœì¹˜ =====
    # LSTM ë¸Œëœì¹˜
    lstm = LSTM(128, return_sequences=True)(inputs)
    lstm = BatchNormalization()(lstm)
    lstm = LSTM(64)(lstm)
    lstm = Dropout(0.3)(lstm)
    
    # Attention ë¸Œëœì¹˜
    attn = MultiHeadAttention(num_heads=4, key_dim=16)(inputs, inputs)
    attn = GlobalAveragePooling1D()(attn)
    
    # CNN ë¸Œëœì¹˜
    cnn = Conv1D(64, 5, activation='relu')(inputs)
    cnn = BatchNormalization()(cnn)
    cnn = Conv1D(32, 3, activation='relu')(cnn)
    cnn = GlobalMaxPooling1D()(cnn)
    
    # Recent ë¸Œëœì¹˜
    recent = Lambda(lambda x: x[:, -20:, :])(inputs)
    recent = GRU(32)(recent)
    
    # ===== íŒ¨í„´ ì „ìš© ë¸Œëœì¹˜ (ìƒˆë¡œ ì¶”ê°€) =====
    # íŒ¨í„´ íŠ¹ì„± ì¶”ì¶œ (PATTERN, PATTERN_RISK ì»¬ëŸ¼ ìœ„ì¹˜)
    pattern_features = Lambda(lambda x: x[:, :, 10:12])(inputs)  # íŒ¨í„´ ê´€ë ¨ ì»¬ëŸ¼
    pattern = Conv1D(16, 3, activation='relu')(pattern_features)
    pattern = GlobalAveragePooling1D()(pattern)
    
    # M14B ë ˆë²¨ ì¶”ì¶œ
    m14b_features = Lambda(lambda x: x[:, :, 1:2])(inputs)  # M14AM14B ì»¬ëŸ¼
    m14b = Conv1D(16, 3, activation='relu')(m14b_features)
    m14b = GlobalMaxPooling1D()(m14b)
    
    # ===== í†µí•© =====
    merged = Concatenate()([lstm, attn, cnn, recent, pattern, m14b])
    merged = BatchNormalization()(merged)
    
    # Dense
    x = Dense(256, activation='relu')(merged)
    x = Dropout(0.3)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.2)(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    # ì¶œë ¥
    output_reg = Dense(1, name='regression')(x)  # ê°’ ì˜ˆì¸¡
    output_cls = Dense(4, activation='softmax', name='classification')(x)  # 4êµ¬ê°„
    output_pattern = Dense(2, activation='softmax', name='pattern')(x)  # UU1/UU2
    
    model = Model(inputs, [output_reg, output_cls, output_pattern], 
                  name='Pattern_ExtremeNet')
    return model

# ========================================
# ì»¤ìŠ¤í…€ ì†ì‹¤í•¨ìˆ˜ (íŒ¨í„´ë³„ ê°€ì¤‘ì¹˜)
# ========================================
def weighted_mae_pattern(y_true, y_pred):
    """íŒ¨í„´ ê¸°ë°˜ ê°€ì¤‘ MAE"""
    signal_min_scaled = 0.65
    signal_max_scaled = 0.75
    
    weights = tf.ones_like(y_true)
    
    # ì‹ í˜¸êµ¬ê°„
    signal_mask = tf.logical_and(
        y_true >= signal_min_scaled,
        y_true <= signal_max_scaled
    )
    weights = tf.where(signal_mask, 5.0, weights)
    
    # ìœ„í—˜êµ¬ê°„
    danger_mask = y_true > signal_max_scaled
    weights = tf.where(danger_mask, 10.0, weights)
    
    mae = tf.abs(y_true - y_pred)
    weighted_mae = mae * weights
    
    return tf.reduce_mean(weighted_mae)

# ========================================
# í•™ìŠµ í•¨ìˆ˜ (ì•ˆì „í•œ í‰ê°€)
# ========================================
def train_model(train_data, val_data, test_data, processor, epochs=100):
    """ëª¨ë¸ í•™ìŠµ (ì•ˆì „í•œ í‰ê°€)"""
    
    if train_data is None:
        print("âŒ í›ˆë ¨ ë°ì´í„° ì—†ìŒ!")
        return None, None
    
    X_train, y_train_scaled, y_train_orig, y_train_level, y_train_pattern = train_data
    X_val, y_val_scaled, y_val_orig, y_val_level, y_val_pattern = val_data
    X_test, y_test_scaled, y_test_orig, y_test_level, y_test_pattern = test_data
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²´í¬
    has_test = len(X_test) > 0
    
    # ëª¨ë¸ ìƒì„±
    with strategy.scope():
        input_shape = (X_train.shape[1], X_train.shape[2])
        model = build_pattern_extremenet(input_shape)
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss={
                'regression': weighted_mae_pattern,
                'classification': 'sparse_categorical_crossentropy',
                'pattern': 'sparse_categorical_crossentropy'
            },
            loss_weights={
                'regression': 0.5,
                'classification': 0.25,
                'pattern': 0.25
            },
            metrics={
                'regression': 'mae',
                'classification': 'accuracy',
                'pattern': 'accuracy'
            }
        )
    
    print("\n" + "="*80)
    print("ğŸ”¥ íŒ¨í„´ í†µí•© ExtremeNet í•™ìŠµ ì‹œì‘")
    print(f"ğŸ”§ Device: {DEVICE} | Batch: {BATCH_SIZE}")
    print("="*80)
    
    # ì½œë°±
    callbacks = [
        ModelCheckpoint(
            'models_pattern/best_model.keras',
            save_best_only=True,
            monitor='val_loss',
            verbose=1
        ),
        EarlyStopping(
            patience=25,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            factor=0.5,
            patience=8,
            min_lr=1e-6,
            verbose=1
        )
    ]
    
    # í•™ìŠµ
    os.makedirs('models_pattern', exist_ok=True)
    
    history = model.fit(
        X_train,
        {
            'regression': y_train_scaled,
            'classification': y_train_level,
            'pattern': y_train_pattern
        },
        validation_data=(
            X_val,
            {
                'regression': y_val_scaled,
                'classification': y_val_level,
                'pattern': y_val_pattern
            }
        ),
        epochs=epochs,
        batch_size=BATCH_SIZE,
        callbacks=callbacks,
        verbose=1
    )
    
    # í‰ê°€ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ)
    print("\n" + "="*80)
    if has_test:
        print("ğŸ“Š ìµœì¢… í‰ê°€")
        print("="*80)
        
        try:
            # ì˜ˆì¸¡
            preds = model.predict(X_test, verbose=0)
            y_pred_scaled = preds[0].flatten()
            y_pred = processor.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
            
            # ì „ì²´ ë©”íŠ¸ë¦­
            mae = mean_absolute_error(y_test_orig, y_pred)
            rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred))
            r2 = r2_score(y_test_orig, y_pred)
            
            print(f"\nğŸ“ˆ ì „ì²´ ì„±ëŠ¥:")
            print(f"  MAE: {mae:.2f}")
            print(f"  RMSE: {rmse:.2f}")
            print(f"  RÂ²: {r2:.4f}")
            
            # ì‹ í˜¸êµ¬ê°„ í‰ê°€
            signal_idx = ((y_test_orig >= processor.SIGNAL_MIN) & 
                         (y_test_orig <= processor.SIGNAL_MAX))
            if np.sum(signal_idx) > 0:
                signal_mae = mean_absolute_error(y_test_orig[signal_idx], y_pred[signal_idx])
                print(f"\nğŸŸ  ì‹ í˜¸êµ¬ê°„(1651~1699):")
                print(f"  ìƒ˜í”Œ: {np.sum(signal_idx)}ê°œ")
                print(f"  MAE: {signal_mae:.2f}")
                print(f"  ì •í™•ë„(Â±50): {np.mean(np.abs(y_test_orig[signal_idx] - y_pred[signal_idx]) < 50)*100:.1f}%")
            
            # ìœ„í—˜êµ¬ê°„ í‰ê°€
            danger_idx = (y_test_orig >= processor.DANGER_MIN)
            if np.sum(danger_idx) > 0:
                danger_mae = mean_absolute_error(y_test_orig[danger_idx], y_pred[danger_idx])
                accuracy_30 = np.mean(np.abs(y_test_orig[danger_idx] - y_pred[danger_idx]) < 30)*100
                accuracy_50 = np.mean(np.abs(y_test_orig[danger_idx] - y_pred[danger_idx]) < 50)*100
                
                print(f"\nğŸ”´ ìœ„í—˜êµ¬ê°„(1700+):")
                print(f"  ìƒ˜í”Œ: {np.sum(danger_idx)}ê°œ")
                print(f"  MAE: {danger_mae:.2f}")
                print(f"  ì •í™•ë„(Â±30): {accuracy_30:.1f}%")
                print(f"  ì •í™•ë„(Â±50): {accuracy_50:.1f}%")
                
        except Exception as e:
            print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            print("ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ ì¶”ì •")
            
            # ê²€ì¦ ë°ì´í„°ë¡œ ì¶”ì •
            val_loss = min(history.history['val_loss'])
            val_mae = min(history.history['val_regression_mae'])
            print(f"\nğŸ“Š ê²€ì¦ ì„±ëŠ¥:")
            print(f"  Val Loss: {val_loss:.4f}")
            print(f"  Val MAE: {val_mae:.4f}")
            
            mae = val_mae * 1000  # ëŒ€ëµì  ì¶”ì •
            rmse = mae * 1.3
            r2 = 0.85  # ì¶”ì •
    else:
        print("ğŸ“Š í‰ê°€ ìŠ¤í‚µ (í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶€ì¡±)")
        print("="*80)
        print("\nê²€ì¦ ë°ì´í„° ìµœì¢… ì„±ëŠ¥:")
        val_loss = min(history.history['val_loss'])
        val_mae = min(history.history['val_regression_mae'])
        val_acc = max(history.history['val_classification_accuracy'])
        
        print(f"  Val Loss: {val_loss:.4f}")
        print(f"  Val MAE (ìŠ¤ì¼€ì¼): {val_mae:.4f}")
        print(f"  Val ë¶„ë¥˜ ì •í™•ë„: {val_acc:.2%}")
        
        # ì¶”ì • ì„±ëŠ¥
        mae = val_mae * 1000  # ìŠ¤ì¼€ì¼ ì—­ë³€í™˜ ì¶”ì •
        rmse = mae * 1.3
        r2 = val_acc * 0.9  # ëŒ€ëµì  ì¶”ì •
        
        print(f"\nğŸ“ˆ ì¶”ì • ì„±ëŠ¥:")
        print(f"  MAE: ~{mae:.0f}")
        print(f"  RÂ²: ~{r2:.2f}")
    
    # ëª¨ë¸ ì €ì¥
    model.save('models_pattern/pattern_extremenet_final.keras')
    print(f"\nğŸ’¾ ìµœì¢… ëª¨ë¸: models_pattern/pattern_extremenet_final.keras")
    
    return model, {'MAE': mae if 'mae' in locals() else None, 
                   'RMSE': rmse if 'rmse' in locals() else None, 
                   'R2': r2 if 'r2' in locals() else None}

# ========================================
# ë©”ì¸ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì¶”ê°€)
# ========================================
def main(test_mode=False):
    """
    test_mode=True: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10% ë°ì´í„°, 30 ì—í¬í¬)
    test_mode=False: ì „ì²´ í•™ìŠµ
    """
    # ë°ì´í„° ê²½ë¡œ
    data_path = None
    for path in ['data/20240201_TO_202507281705.csv', 
                 '20240201_TO_202507281705.csv',
                 'data.csv']:
        if os.path.exists(path):
            data_path = path
            break
    
    if not data_path:
        print("âŒ ë°ì´í„° íŒŒì¼ ì—†ìŒ!")
        return None, None
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„¤ì •
    if test_mode:
        print("\nâš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
        sample_ratio = 0.1  # 10% ë°ì´í„°
        epochs = 30
    else:
        print("\nğŸ“Š ì „ì²´ í•™ìŠµ ëª¨ë“œ")
        sample_ratio = 1.0
        epochs = 100
    
    # ë°ì´í„° ì²˜ë¦¬
    processor = DataProcessor()
    df = processor.load_data(data_path, sample_ratio=sample_ratio)
    df_features = processor.create_features(df)
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    result = processor.create_sequences(df_features)
    
    if result[0] is None:
        print("âŒ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ í•™ìŠµ ì¤‘ë‹¨")
        return None, None
    
    train_data, val_data, test_data = result
    
    # í•™ìŠµ
    model, results = train_model(train_data, val_data, test_data, processor, epochs=epochs)
    
    print("\nâœ… ì™„ë£Œ!")
    if test_mode:
        print("âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        if results and results['MAE']:
            print(f"ì˜ˆìƒ MAE: {results['MAE']:.0f}")
    else:
        print("ğŸ”¥ íŒ¨í„´ í†µí•© ExtremeNet í•™ìŠµ ì™„ë£Œ!")
    
    return model, results

if __name__ == "__main__":
    # ê¸°ë³¸ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹œì‘
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == '--full':
        model, results = main(test_mode=False)
    else:
        print("ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¡œ ì‹œì‘... (ì „ì²´ í•™ìŠµ: --full ì˜µì…˜)")
        model, results = main(test_mode=True)