# -*- coding: utf-8 -*-
"""
ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - 280ë¶„ â†’ 10ë¶„ í›„ ì˜ˆì¸¡
ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œìœ¼ë¡œ ì¤‘ë‹¨/ì¬ê°œ ê°€ëŠ¥
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor,
                             RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier)
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.linear_model import Ridge, ElasticNet, LogisticRegression
from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score,
                           classification_report, confusion_matrix, accuracy_score,
                           precision_recall_fscore_support)
import joblib
import pickle
import json
import os
import sys
import time
import warnings
import traceback
from datetime import datetime
import platform
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("âš ï¸ psutil ë¯¸ì„¤ì¹˜ - ì‹œìŠ¤í…œ ì •ë³´ ì œí•œì ")
warnings.filterwarnings('ignore')

# ======================== ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸ ========================
def check_system_info():
    """ì‹œìŠ¤í…œ ë° GPU ì •ë³´ í™•ì¸"""
    print("\n" + "="*70)
    print(" ğŸ“Œ ì‹œìŠ¤í…œ ì •ë³´")
    print("="*70)
    
    # CPU ì •ë³´
    print(f"  OS: {platform.system()} {platform.release()}")
    print(f"  Python: {sys.version.split()[0]}")
    
    if PSUTIL_AVAILABLE:
        print(f"  CPU: {psutil.cpu_count()} cores")
        print(f"  RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB")
        print(f"  ì‚¬ìš© ê°€ëŠ¥ RAM: {psutil.virtual_memory().available / (1024**3):.1f} GB")
    else:
        import multiprocessing
        print(f"  CPU: {multiprocessing.cpu_count()} cores")
        print("  RAM: psutil í•„ìš” (pip install psutil)")
    
    # GPU í™•ì¸ (sklearnì€ GPUë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
    print("\n  ğŸ” GPU í™•ì¸:")
    try:
        import tensorflow as tf
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            print(f"    TensorFlow GPU ë°œê²¬: {len(gpus)}ê°œ")
            for i, gpu in enumerate(gpus):
                print(f"      GPU {i}: {gpu}")
        else:
            print("    TensorFlow GPU ì—†ìŒ (CPU ëª¨ë“œ)")
    except ImportError:
        print("    TensorFlow ë¯¸ì„¤ì¹˜ - Sklearnì€ CPUë§Œ ì‚¬ìš©")
    
    try:
        import torch
        if torch.cuda.is_available():
            print(f"    PyTorch GPU ë°œê²¬: {torch.cuda.device_count()}ê°œ")
            print(f"      GPU: {torch.cuda.get_device_name(0)}")
        else:
            print("    PyTorch GPU ì—†ìŒ (CPU ëª¨ë“œ)")
    except ImportError:
        pass
    
    # sklearn ì •ë³´
    import sklearn
    print(f"\n  ğŸ“š Scikit-learn ë²„ì „: {sklearn.__version__}")
    print("  âš ï¸ ì£¼ì˜: Sklearn ëª¨ë¸ì€ CPUë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤")
    print("  ğŸ’¡ GPU ì‚¬ìš©í•˜ë ¤ë©´ TensorFlow/PyTorch ëª¨ë¸ í•„ìš”")
    
    print("="*70)

plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['figure.figsize'] = (15, 10)

# ======================== ì„¤ì • ========================
MODEL_DIR = 'regression_classification_models_280to10'
CHECKPOINT_DIR = 'checkpoint'
BACKUP_DIR = 'checkpoint_backup'
SEQ_LENGTH = 280  # ê³¼ê±° 280ë¶„
PRED_HORIZON = 10  # 10ë¶„ í›„ ì˜ˆì¸¡

# í´ë” ìƒì„±
for dir_path in [MODEL_DIR, CHECKPOINT_DIR, BACKUP_DIR]:
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)

# ======================== ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ ========================
class CheckpointManager:
    def __init__(self):
        self.checkpoint_file = os.path.join(CHECKPOINT_DIR, 'state.json')
        self.start_time = time.time()
        
    def load_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        if os.path.exists(self.checkpoint_file):
            with open(self.checkpoint_file, 'r') as f:
                return json.load(f)
        return None
    
    def save_checkpoint(self, state):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        state['last_saved'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        state['elapsed_time'] = time.time() - self.start_time
        
        with open(self.checkpoint_file, 'w') as f:
            json.dump(state, f, indent=2)
        
        # ë°±ì—… (1ì‹œê°„ë§ˆë‹¤)
        if state['elapsed_time'] > 3600:
            backup_dir = os.path.join(BACKUP_DIR, f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            if not os.path.exists(backup_dir):
                os.makedirs(backup_dir)
            
            import shutil
            shutil.copy(self.checkpoint_file, backup_dir)
    
    def create_new_state(self):
        """ìƒˆë¡œìš´ ìƒíƒœ ìƒì„±"""
        return {
            'version': '1.0',
            'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'stage': 'INIT',
            'completed_steps': {
                'data_loading': False,
                'sequence_creation': False,
                'train_test_split': False,
                'scaling': False
            },
            'completed_models': [],
            'current_model': None,
            'models_metrics': {},
            'data_info': {},
            'elapsed_time': 0
        }

# ======================== ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ ========================
def assign_level(totalcnt_value):
    """3êµ¬ê°„ ë¶„ë¥˜"""
    if totalcnt_value < 1400:
        return 0  # ì •ìƒ
    elif totalcnt_value < 1700:
        return 1  # í™•ì¸
    else:
        return 2  # ìœ„í—˜

def detect_anomaly_signal(totalcnt_value):
    """ì´ìƒì‹ í˜¸ ê°ì§€ (1651-1682)"""
    return 1 if 1651 <= totalcnt_value <= 1682 else 0

def create_sequences_280to10(data, seq_length=280, pred_horizon=10):
    """280ë¶„ ì‹œí€€ìŠ¤ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡ ë°ì´í„° ìƒì„±"""
    
    print(f"\nğŸ“Š ì‹œí€€ìŠ¤ ìƒì„± ì¤‘... (280ë¶„ â†’ 10ë¶„ í›„)")
    
    feature_cols = ['M14AM14B', 'M14AM10A', 'M14AM16', 'M14AM14BSUM', 
                   'M14AM10ASUM', 'M14AM16SUM', 'M14BM14A', 'M10AM14A', 
                   'M16M14A', 'TOTALCNT']
    
    data = data.copy()
    
    # íŒŒìƒ ë³€ìˆ˜ ìƒì„±
    data['ratio_M14B_M14A'] = np.clip(data['M14AM14B'] / (data['M14AM10A'] + 1), 0, 1000)
    data['ratio_M14B_M16'] = np.clip(data['M14AM14B'] / (data['M14AM16'] + 1), 0, 1000)
    data['totalcnt_change'] = data['TOTALCNT'].diff().fillna(0)
    data['totalcnt_pct_change'] = data['TOTALCNT'].pct_change().fillna(0)
    data['totalcnt_pct_change'] = np.clip(data['totalcnt_pct_change'], -10, 10)
    
    data = data.replace([np.inf, -np.inf], 0)
    data = data.fillna(0)
    
    X_list = []
    y_reg_list = []
    y_cls_list = []
    y_anomaly_list = []
    
    n_sequences = len(data) - seq_length - pred_horizon + 1
    print(f"âœ… ìƒì„± ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤: {n_sequences:,}ê°œ")
    
    # ì²´í¬í¬ì¸íŠ¸ í™•ì¸
    seq_checkpoint = os.path.join(CHECKPOINT_DIR, 'sequences_progress.json')
    start_idx = 0
    
    if os.path.exists(seq_checkpoint):
        with open(seq_checkpoint, 'r') as f:
            seq_progress = json.load(f)
            start_idx = seq_progress.get('last_processed', 0)
            
            if start_idx > 0:
                print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {start_idx}/{n_sequences} ì™„ë£Œ")
                # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
                X_list = np.load(os.path.join(CHECKPOINT_DIR, 'X_partial.npy')).tolist()
                y_reg_list = np.load(os.path.join(CHECKPOINT_DIR, 'y_reg_partial.npy')).tolist()
                y_cls_list = np.load(os.path.join(CHECKPOINT_DIR, 'y_cls_partial.npy')).tolist()
                y_anomaly_list = np.load(os.path.join(CHECKPOINT_DIR, 'y_anomaly_partial.npy')).tolist()
    
    # ì‹œí€€ìŠ¤ ìƒì„± (ì´ì–´ì„œ)
    for i in range(start_idx, n_sequences):
        if i % 1000 == 0:
            print(f"  ì§„í–‰ë¥ : {i}/{n_sequences} ({i/n_sequences*100:.1f}%)", end='\r')
            
            # ì¤‘ê°„ ì €ì¥ (5000ê°œë§ˆë‹¤)
            if i > 0 and i % 5000 == 0:
                np.save(os.path.join(CHECKPOINT_DIR, 'X_partial.npy'), X_list)
                np.save(os.path.join(CHECKPOINT_DIR, 'y_reg_partial.npy'), y_reg_list)
                np.save(os.path.join(CHECKPOINT_DIR, 'y_cls_partial.npy'), y_cls_list)
                np.save(os.path.join(CHECKPOINT_DIR, 'y_anomaly_partial.npy'), y_anomaly_list)
                
                with open(seq_checkpoint, 'w') as f:
                    json.dump({'last_processed': i}, f)
                print(f"\nğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {i}/{n_sequences}")
        
        start = i
        end = i + seq_length
        seq_data = data.iloc[start:end]
        
        features = []
        
        # ê° ì»¬ëŸ¼ë³„ íŠ¹ì§• ì¶”ì¶œ
        for col in feature_cols:
            values = seq_data[col].values
            
            # ê¸°ë³¸ í†µê³„
            features.extend([
                np.mean(values),
                np.std(values) if len(values) > 1 else 0,
                np.min(values),
                np.max(values),
                np.percentile(values, 25),
                np.percentile(values, 50),
                np.percentile(values, 75),
                values[-1],  # í˜„ì¬ê°’
                values[-1] - values[0],  # ì „ì²´ ë³€í™”ëŸ‰
                np.mean(values[-60:]),  # ìµœê·¼ 1ì‹œê°„ í‰ê· 
                np.max(values[-60:]),  # ìµœê·¼ 1ì‹œê°„ ìµœëŒ€
                np.mean(values[-30:]),  # ìµœê·¼ 30ë¶„ í‰ê· 
                np.max(values[-30:])   # ìµœê·¼ 30ë¶„ ìµœëŒ€
            ])
            
            # TOTALCNT íŠ¹ë³„ ì²˜ë¦¬
            if col == 'TOTALCNT':
                features.append(np.sum((values >= 1650) & (values < 1700)))
                features.append(np.sum(values >= 1700))
                features.append(np.max(values[-20:]))
                features.append(np.sum(values < 1400))
                features.append(np.sum((values >= 1400) & (values < 1700)))
                features.append(np.sum(values >= 1700))
                features.append(np.sum((values >= 1651) & (values <= 1682)))
                
                anomaly_values = values[(values >= 1651) & (values <= 1682)]
                features.append(np.max(anomaly_values) if len(anomaly_values) > 0 else 0)
                
                normal_vals = values[values < 1400]
                check_vals = values[(values >= 1400) & (values < 1700)]
                danger_vals = values[values >= 1700]
                
                features.append(np.mean(normal_vals) if len(normal_vals) > 0 else 0)
                features.append(np.mean(check_vals) if len(check_vals) > 0 else 0)
                features.append(np.mean(danger_vals) if len(danger_vals) > 0 else 0)
                
                try:
                    x = np.arange(len(values))
                    slope, _ = np.polyfit(x, values, 1)
                    features.append(np.clip(slope, -100, 100))
                except:
                    features.append(0)
                
                try:
                    recent_slope = np.polyfit(np.arange(60), values[-60:], 1)[0]
                    features.append(np.clip(recent_slope, -100, 100))
                except:
                    features.append(0)
        
        # íŒŒìƒ ë³€ìˆ˜
        last_idx = end - 1
        features.extend([
            np.clip(data['ratio_M14B_M14A'].iloc[last_idx], 0, 1000),
            np.clip(data['ratio_M14B_M16'].iloc[last_idx], 0, 1000),
            np.clip(data['totalcnt_change'].iloc[last_idx], -1000, 1000),
            np.clip(data['totalcnt_pct_change'].iloc[last_idx], -10, 10),
        ])
        
        # íƒ€ê²Ÿ: 10ë¶„ í›„ TOTALCNT
        target_idx = end + pred_horizon - 1
        if target_idx < len(data):
            future_totalcnt = data['TOTALCNT'].iloc[target_idx]
            
            X_list.append(features)
            y_reg_list.append(future_totalcnt)
            y_cls_list.append(assign_level(future_totalcnt))
            y_anomaly_list.append(detect_anomaly_signal(future_totalcnt))
    
    X = np.array(X_list)
    y_reg = np.array(y_reg_list)
    y_cls = np.array(y_cls_list)
    y_anomaly = np.array(y_anomaly_list)
    
    X = np.nan_to_num(X, nan=0.0, posinf=1000.0, neginf=-1000.0)
    
    # ì™„ë£Œ í›„ ì„ì‹œ íŒŒì¼ ì‚­ì œ
    for file in ['X_partial.npy', 'y_reg_partial.npy', 'y_cls_partial.npy', 'y_anomaly_partial.npy']:
        filepath = os.path.join(CHECKPOINT_DIR, file)
        if os.path.exists(filepath):
            os.remove(filepath)
    
    if os.path.exists(seq_checkpoint):
        os.remove(seq_checkpoint)
    
    print(f"\nâœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ!")
    print(f"  íŠ¹ì§• ê°œìˆ˜: {X.shape[1]}ê°œ")
    
    return X, y_reg, y_cls, y_anomaly

# ======================== ì§„í–‰ ìƒí™© í‘œì‹œ ========================
def display_progress(state, checkpoint_manager):
    """ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ"""
    
    os.system('cls' if os.name == 'nt' else 'clear')  # í™”ë©´ í´ë¦¬ì–´
    
    print("\n" + "="*70)
    print(" ğŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - 280ë¶„ â†’ 10ë¶„ í›„ ì˜ˆì¸¡")
    print("="*70)
    
    # ë°ì´í„° ì²˜ë¦¬ ìƒíƒœ
    print("\nğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬:")
    steps = state['completed_steps']
    print(f"  {'âœ…' if steps['data_loading'] else 'â¸ï¸'} ë°ì´í„° ë¡œë”©")
    print(f"  {'âœ…' if steps['sequence_creation'] else 'â¸ï¸'} ì‹œí€€ìŠ¤ ìƒì„± (280ë¶„)")
    print(f"  {'âœ…' if steps['train_test_split'] else 'â¸ï¸'} Train/Test ë¶„í• ")
    print(f"  {'âœ…' if steps['scaling'] else 'â¸ï¸'} ë°ì´í„° ìŠ¤ì¼€ì¼ë§")
    
    # ëª¨ë¸ í•™ìŠµ ìƒíƒœ
    regression_models = ['RandomForest', 'ExtraTrees', 'GradientBoosting', 
                        'MLP', 'Ridge', 'ElasticNet']
    classification_models = ['RandomForest_Cls', 'ExtraTrees_Cls', 
                            'GradientBoosting_Cls', 'MLP_Cls', 'LogisticRegression']
    
    print("\nğŸ“ˆ íšŒê·€ ëª¨ë¸ (6ê°œ):")
    for model in regression_models:
        if model in state['completed_models']:
            metrics = state['models_metrics'].get(model, {})
            print(f"  âœ… {model:20} MAE: {metrics.get('mae', 0):.2f}, RÂ²: {metrics.get('r2', 0):.4f}")
        elif model == state['current_model']:
            print(f"  â³ {model:20} [í•™ìŠµ ì¤‘...]")
        else:
            print(f"  â¸ï¸ {model:20}")
    
    print("\nğŸ“Š ë¶„ë¥˜ ëª¨ë¸ (5ê°œ):")
    for model in classification_models:
        if model in state['completed_models']:
            metrics = state['models_metrics'].get(model, {})
            print(f"  âœ… {model:20} ì •í™•ë„: {metrics.get('accuracy', 0):.3f}")
        elif model == state['current_model']:
            print(f"  â³ {model:20} [í•™ìŠµ ì¤‘...]")
        else:
            print(f"  â¸ï¸ {model:20}")
    
    # ì „ì²´ ì§„í–‰ë¥ 
    total_models = 11
    completed = len(state['completed_models'])
    progress = completed / total_models * 100 if total_models > 0 else 0
    
    print(f"\nğŸ“Š ì „ì²´ ì§„í–‰ë¥ : {progress:.1f}% [{completed}/{total_models}]")
    print(f"[{'â–ˆ' * int(progress/2)}{' ' * (50-int(progress/2))}]")
    
    # ì‹œê°„ ì •ë³´
    elapsed = state.get('elapsed_time', 0)
    print(f"\nâ±ï¸ ê²½ê³¼ ì‹œê°„: {elapsed//60:.0f}ë¶„ {elapsed%60:.0f}ì´ˆ")
    
    if 'last_saved' in state:
        print(f"ğŸ’¾ ë§ˆì§€ë§‰ ì €ì¥: {state['last_saved']}")
    
    print("="*70)

# ======================== ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ ========================
def train_with_checkpoint():
    """ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ í•™ìŠµ"""
    
    checkpoint_manager = CheckpointManager()
    
    # ì²´í¬í¬ì¸íŠ¸ í™•ì¸
    state = checkpoint_manager.load_checkpoint()
    
    if state is None:
        print("ğŸ†• ìƒˆë¡œìš´ í•™ìŠµ ì‹œì‘...")
        state = checkpoint_manager.create_new_state()
    else:
        print("âœ… ì²´í¬í¬ì¸íŠ¸ ë°œê²¬! ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤...")
        print(f"  ì™„ë£Œëœ ëª¨ë¸: {len(state['completed_models'])}ê°œ")
    
    try:
        # ============ STEP 1: ë°ì´í„° ë¡œë”© ============
        if not state['completed_steps']['data_loading']:
            print("\n[Step 1] ë°ì´í„° ë¡œë”©...")
            
            df1 = pd.read_csv('uu.csv')
            df2 = pd.read_csv('uu2.csv')
            df = pd.concat([df1, df2], ignore_index=True).reset_index(drop=True)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_cols = ['M14AM14B', 'M14AM10A', 'M14AM16', 'M14AM14BSUM', 
                           'M14AM10ASUM', 'M14AM16SUM', 'M14BM14A', 'M10AM14A', 
                           'M16M14A', 'TOTALCNT']
            
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")
                print(f"   í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}")
                raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_cols}")
            
            print(f"âœ… ì „ì²´ ë°ì´í„°: {len(df):,}í–‰")
            print(f"âœ… ì‹œê°„ ë²”ìœ„: {len(df)} ë¶„ ({len(df)/60:.1f} ì‹œê°„)")
            print(f"âœ… ëª¨ë“  í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ ì™„ë£Œ")
            
            # ë°ì´í„° ì €ì¥
            df.to_pickle(os.path.join(CHECKPOINT_DIR, 'df_combined.pkl'))
            
            state['data_info']['total_rows'] = len(df)
            state['completed_steps']['data_loading'] = True
            checkpoint_manager.save_checkpoint(state)
        else:
            print("\nâœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ (ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µì›)")
            df = pd.read_pickle(os.path.join(CHECKPOINT_DIR, 'df_combined.pkl'))
        
        # ============ STEP 2: ì‹œí€€ìŠ¤ ìƒì„± ============
        if not state['completed_steps']['sequence_creation']:
            print("\n[Step 2] 280ë¶„ â†’ 10ë¶„ í›„ ì‹œí€€ìŠ¤ ìƒì„±...")
            
            X, y_reg, y_cls, y_anomaly = create_sequences_280to10(df, SEQ_LENGTH, PRED_HORIZON)
            
            # ì €ì¥
            np.save(os.path.join(CHECKPOINT_DIR, 'X.npy'), X)
            np.save(os.path.join(CHECKPOINT_DIR, 'y_reg.npy'), y_reg)
            np.save(os.path.join(CHECKPOINT_DIR, 'y_cls.npy'), y_cls)
            np.save(os.path.join(CHECKPOINT_DIR, 'y_anomaly.npy'), y_anomaly)
            
            state['data_info']['num_sequences'] = X.shape[0]
            state['data_info']['num_features'] = X.shape[1]
            state['completed_steps']['sequence_creation'] = True
            checkpoint_manager.save_checkpoint(state)
            
            # ë°ì´í„° ë¶„ì„
            print(f"\nğŸ“Š íƒ€ê²Ÿ(10ë¶„ í›„ TOTALCNT) ë¶„ì„:")
            print(f"  ìµœì†Œê°’: {y_reg.min():.0f}")
            print(f"  ìµœëŒ€ê°’: {y_reg.max():.0f}")
            print(f"  í‰ê· ê°’: {y_reg.mean():.0f} (Â±{y_reg.std():.0f})")
            
            print(f"\nğŸ“Š 3êµ¬ê°„ ë¶„í¬:")
            for i in range(3):
                count = np.sum(y_cls == i)
                names = ["ì •ìƒ(900-1399)", "í™•ì¸(1400-1699)", "ìœ„í—˜(1700+)"]
                print(f"  {names[i]}: {count:,}ê°œ ({count/len(y_cls)*100:.2f}%)")
        else:
            print("\nâœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ (ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µì›)")
            X = np.load(os.path.join(CHECKPOINT_DIR, 'X.npy'))
            y_reg = np.load(os.path.join(CHECKPOINT_DIR, 'y_reg.npy'))
            y_cls = np.load(os.path.join(CHECKPOINT_DIR, 'y_cls.npy'))
            y_anomaly = np.load(os.path.join(CHECKPOINT_DIR, 'y_anomaly.npy'))
        
        # ============ STEP 3: Train/Test ë¶„í•  ============
        if not state['completed_steps']['train_test_split']:
            print("\n[Step 3] Train/Test ë¶„í• ...")
            
            X_train, X_test, y_reg_train, y_reg_test, y_cls_train, y_cls_test, y_anomaly_train, y_anomaly_test = train_test_split(
                X, y_reg, y_cls, y_anomaly, test_size=0.2, random_state=42, shuffle=True
            )
            
            # ì €ì¥
            np.save(os.path.join(CHECKPOINT_DIR, 'X_train.npy'), X_train)
            np.save(os.path.join(CHECKPOINT_DIR, 'X_test.npy'), X_test)
            np.save(os.path.join(CHECKPOINT_DIR, 'y_reg_train.npy'), y_reg_train)
            np.save(os.path.join(CHECKPOINT_DIR, 'y_reg_test.npy'), y_reg_test)
            np.save(os.path.join(CHECKPOINT_DIR, 'y_cls_train.npy'), y_cls_train)
            np.save(os.path.join(CHECKPOINT_DIR, 'y_cls_test.npy'), y_cls_test)
            
            print(f"âœ… í›ˆë ¨ ë°ì´í„°: {X_train.shape[0]:,}ê°œ")
            print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]:,}ê°œ")
            
            state['data_info']['train_samples'] = X_train.shape[0]
            state['data_info']['test_samples'] = X_test.shape[0]
            state['completed_steps']['train_test_split'] = True
            checkpoint_manager.save_checkpoint(state)
        else:
            print("\nâœ… Train/Test ë¶„í•  ì™„ë£Œ (ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µì›)")
            X_train = np.load(os.path.join(CHECKPOINT_DIR, 'X_train.npy'))
            X_test = np.load(os.path.join(CHECKPOINT_DIR, 'X_test.npy'))
            y_reg_train = np.load(os.path.join(CHECKPOINT_DIR, 'y_reg_train.npy'))
            y_reg_test = np.load(os.path.join(CHECKPOINT_DIR, 'y_reg_test.npy'))
            y_cls_train = np.load(os.path.join(CHECKPOINT_DIR, 'y_cls_train.npy'))
            y_cls_test = np.load(os.path.join(CHECKPOINT_DIR, 'y_cls_test.npy'))
        
        # ============ STEP 4: ìŠ¤ì¼€ì¼ë§ ============
        if not state['completed_steps']['scaling']:
            print("\n[Step 4] ë°ì´í„° ìŠ¤ì¼€ì¼ë§...")
            
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # ì €ì¥
            joblib.dump(scaler, os.path.join(CHECKPOINT_DIR, 'scaler.pkl'))
            np.save(os.path.join(CHECKPOINT_DIR, 'X_train_scaled.npy'), X_train_scaled)
            np.save(os.path.join(CHECKPOINT_DIR, 'X_test_scaled.npy'), X_test_scaled)
            
            state['completed_steps']['scaling'] = True
            checkpoint_manager.save_checkpoint(state)
        else:
            print("\nâœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ (ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µì›)")
            scaler = joblib.load(os.path.join(CHECKPOINT_DIR, 'scaler.pkl'))
            X_train_scaled = np.load(os.path.join(CHECKPOINT_DIR, 'X_train_scaled.npy'))
            X_test_scaled = np.load(os.path.join(CHECKPOINT_DIR, 'X_test_scaled.npy'))
        
        # ============ STEP 5: ëª¨ë¸ í•™ìŠµ ============
        print("\n" + "="*70)
        print(" ğŸ“š ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print("="*70)
        
        # XGBoost ëª¨ë¸ ì¶”ê°€ (GPU ìš°ì„ , ì‹¤íŒ¨ì‹œ CPU)
        xgboost_models = {}
        try:
            import xgboost as xgb
            print("\nâœ… XGBoost ë°œê²¬! GPU ê°€ì† í™•ì¸ ì¤‘...")
            
            # GPU í…ŒìŠ¤íŠ¸
            test_X = np.random.rand(100, 10)
            test_y = np.random.rand(100)
            
            try:
                # GPU ë²„ì „ ì‹œë„
                test_model = xgb.XGBRegressor(tree_method='gpu_hist', gpu_id=0, n_estimators=10)
                test_model.fit(test_X, test_y)
                print("  ğŸ® GPU ì‚¬ìš© ê°€ëŠ¥! XGBoost GPU ëª¨ë¸ ì¶”ê°€")
                
                # GPU íšŒê·€ ëª¨ë¸
                xgboost_models['XGBoost_GPU'] = xgb.XGBRegressor(
                    n_estimators=300,
                    max_depth=10, 
                    learning_rate=0.1,
                    tree_method='gpu_hist',  # GPU ì‚¬ìš©
                    gpu_id=0,
                    random_state=42,
                    verbosity=0
                )
                
                # GPU ë¶„ë¥˜ ëª¨ë¸
                xgboost_models['XGBoost_GPU_Cls'] = xgb.XGBClassifier(
                    n_estimators=300,
                    max_depth=10,
                    learning_rate=0.1,
                    tree_method='gpu_hist',  # GPU ì‚¬ìš©
                    gpu_id=0,
                    random_state=42,
                    verbosity=0
                )
                
            except Exception as gpu_error:
                print(f"  âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€: {str(gpu_error)[:50]}")
                print("  ğŸ’» CPU ëª¨ë“œë¡œ XGBoost ì‹¤í–‰")
                
                # CPU ë²„ì „ìœ¼ë¡œ í´ë°± (2ì½”ì–´ ì œí•œ)
                xgboost_models['XGBoost_CPU'] = xgb.XGBRegressor(
                    n_estimators=300,
                    max_depth=10,
                    learning_rate=0.1,
                    tree_method='hist',  # CPU ì‚¬ìš©
                    nthread=2,  # CPU 2ì½”ì–´ë§Œ
                    random_state=42,
                    verbosity=0
                )
                
                xgboost_models['XGBoost_CPU_Cls'] = xgb.XGBClassifier(
                    n_estimators=300,
                    max_depth=10,
                    learning_rate=0.1,
                    tree_method='hist',  # CPU ì‚¬ìš©
                    nthread=2,  # CPU 2ì½”ì–´ë§Œ
                    random_state=42,
                    verbosity=0
                )
                
        except ImportError:
            print("âŒ XGBoost ë¯¸ì„¤ì¹˜! ì„¤ì¹˜ ëª…ë ¹: pip install xgboost")
        
        try:
            import lightgbm as lgb
            print("âœ… LightGBM ë°œê²¬ - GPU ê°€ì† ì‹œë„")
            xgboost_models['LightGBM_GPU'] = lgb.LGBMRegressor(
                n_estimators=300, max_depth=10, learning_rate=0.1,
                device='gpu', gpu_platform_id=0, gpu_device_id=0,
                random_state=42, verbose=-1
            )
            xgboost_models['LightGBM_GPU_Cls'] = lgb.LGBMClassifier(
                n_estimators=300, max_depth=10, learning_rate=0.1,
                device='gpu', gpu_platform_id=0, gpu_device_id=0,
                random_state=42, verbose=-1
            )
        except:
            print("âš ï¸ LightGBM GPU ì‚¬ìš© ë¶ˆê°€ ë˜ëŠ” ë¯¸ì„¤ì¹˜")
        
        try:
            from catboost import CatBoostRegressor, CatBoostClassifier
            print("âœ… CatBoost ë°œê²¬ - GPU ê°€ì† ì‹œë„")
            xgboost_models['CatBoost_GPU'] = CatBoostRegressor(
                iterations=300, depth=10, learning_rate=0.1,
                task_type='GPU', devices='0', random_state=42, verbose=False
            )
            xgboost_models['CatBoost_GPU_Cls'] = CatBoostClassifier(
                iterations=300, depth=10, learning_rate=0.1,
                task_type='GPU', devices='0', random_state=42, verbose=False
            )
        except:
            print("âš ï¸ CatBoost GPU ì‚¬ìš© ë¶ˆê°€ ë˜ëŠ” ë¯¸ì„¤ì¹˜")
        
        # íšŒê·€ ëª¨ë¸ ì •ì˜ (CPU 2ì½”ì–´ ì œí•œ)
        regression_models = {
            'RandomForest': RandomForestRegressor(
                n_estimators=300, max_depth=20, min_samples_split=5,
                min_samples_leaf=2, random_state=42, n_jobs=2  # CPU 2ì½”ì–´ë§Œ
            ),
            'ExtraTrees': ExtraTreesRegressor(
                n_estimators=300, max_depth=20, random_state=42, n_jobs=2  # CPU 2ì½”ì–´ë§Œ
            ),
            'GradientBoosting': GradientBoostingRegressor(
                n_estimators=300, learning_rate=0.05, max_depth=10,
                subsample=0.8, random_state=42
                # GradientBoostingì€ n_jobs íŒŒë¼ë¯¸í„° ì—†ìŒ
            ),
            'MLP': MLPRegressor(
                hidden_layer_sizes=(200, 100, 50), activation='relu',
                max_iter=1000, early_stopping=True,
                validation_fraction=0.1, random_state=42
                # MLPRegressorëŠ” n_jobs íŒŒë¼ë¯¸í„° ì—†ìŒ
            ),
            'Ridge': Ridge(alpha=1.0, random_state=42),
            'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)
        }
        
        # ë¶„ë¥˜ ëª¨ë¸ ì •ì˜ (CPU 2ì½”ì–´ ì œí•œ)
        classification_models = {
            'RandomForest_Cls': RandomForestClassifier(
                n_estimators=300, max_depth=20, min_samples_split=5,
                min_samples_leaf=2, random_state=42, n_jobs=2,  # CPU 2ì½”ì–´ë§Œ
                class_weight='balanced'
            ),
            'ExtraTrees_Cls': ExtraTreesClassifier(
                n_estimators=300, max_depth=20, random_state=42,
                n_jobs=2, class_weight='balanced'  # CPU 2ì½”ì–´ë§Œ
            ),
            'GradientBoosting_Cls': GradientBoostingClassifier(
                n_estimators=300, learning_rate=0.05, max_depth=10,
                subsample=0.8, random_state=42
                # GradientBoostingClassifierëŠ” n_jobs íŒŒë¼ë¯¸í„° ì—†ìŒ
            ),
            'MLP_Cls': MLPClassifier(
                hidden_layer_sizes=(200, 100, 50), activation='relu',
                max_iter=1000, early_stopping=True,
                validation_fraction=0.1, random_state=42
                # MLPClassifierëŠ” n_jobs íŒŒë¼ë¯¸í„° ì—†ìŒ
            ),
            'LogisticRegression': LogisticRegression(
                random_state=42, max_iter=1000, class_weight='balanced',
                n_jobs=2  # CPU 2ì½”ì–´ë§Œ
            )
        }
        
        # ëª¨ë“  ëª¨ë¸ í†µí•©
        all_models = list(regression_models.items()) + list(classification_models.items()) + list(xgboost_models.items())
        
        if xgboost_models:
            print(f"\nğŸ® GPU/ê³ ì„±ëŠ¥ ëª¨ë¸ {len(xgboost_models)}ê°œ ì¶”ê°€ë¨!")
            for model_name in xgboost_models.keys():
                print(f"    - {model_name}")
            print(f"\nğŸ“Š ì´ {len(all_models)}ê°œ ëª¨ë¸ í•™ìŠµ ì˜ˆì •\n")
        
        # ëª¨ë¸ í•™ìŠµ (ì´ì–´ì„œ)
        for model_name, model in all_models:
            if model_name in state['completed_models']:
                continue  # ì´ë¯¸ ì™„ë£Œëœ ëª¨ë¸ ê±´ë„ˆë›°ê¸°
            
            state['current_model'] = model_name
            checkpoint_manager.save_checkpoint(state)
            display_progress(state, checkpoint_manager)
            
            print(f"\nğŸš€ [{model_name}] í•™ìŠµ ì‹œì‘...")
            start_time = time.time()
            
            try:
                if 'Cls' in model_name or model_name == 'LogisticRegression':
                    # ë¶„ë¥˜ ëª¨ë¸
                    model.fit(X_train_scaled, y_cls_train)
                    y_pred = model.predict(X_test_scaled)
                    
                    accuracy = accuracy_score(y_cls_test, y_pred)
                    precision, recall, f1, _ = precision_recall_fscore_support(
                        y_cls_test, y_pred, average='weighted'
                    )
                    
                    metrics = {
                        'accuracy': accuracy,
                        'precision': precision,
                        'recall': recall,
                        'f1': f1
                    }
                    
                    print(f"  âœ… ì •í™•ë„: {accuracy:.3f}")
                    print(f"  âœ… F1-Score: {f1:.3f}")
                else:
                    # íšŒê·€ ëª¨ë¸
                    model.fit(X_train_scaled, y_reg_train)
                    y_pred = model.predict(X_test_scaled)
                    
                    mae = mean_absolute_error(y_reg_test, y_pred)
                    rmse = np.sqrt(mean_squared_error(y_reg_test, y_pred))
                    r2 = r2_score(y_reg_test, y_pred)
                    
                    # 3êµ¬ê°„ ì •í™•ë„
                    pred_cls = np.array([assign_level(val) for val in y_pred])
                    cls_accuracy = accuracy_score(y_cls_test, pred_cls)
                    
                    metrics = {
                        'mae': mae,
                        'rmse': rmse,
                        'r2': r2,
                        'cls_accuracy': cls_accuracy
                    }
                    
                    print(f"  âœ… MAE: {mae:.2f}")
                    print(f"  âœ… RÂ²: {r2:.4f}")
                    print(f"  âœ… 3êµ¬ê°„ ì •í™•ë„: {cls_accuracy:.3f}")
                
                # ëª¨ë¸ ì €ì¥
                model_path = os.path.join(MODEL_DIR, f'{model_name}_model.pkl')
                joblib.dump(model, model_path)
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                state['completed_models'].append(model_name)
                state['models_metrics'][model_name] = metrics
                state['current_model'] = None
                
                elapsed = time.time() - start_time
                print(f"  â±ï¸ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
                
                checkpoint_manager.save_checkpoint(state)
                
            except Exception as e:
                print(f"\nâŒ {model_name} ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                print(f"   ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í›„ ì´ ëª¨ë¸ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
                traceback.print_exc()
                
                # ì˜¤ë¥˜ ë°œìƒí•´ë„ ë‹¤ìŒ ëª¨ë¸ë¡œ ì§„í–‰
                state['completed_models'].append(model_name + "_ERROR")
                checkpoint_manager.save_checkpoint(state)
                continue
        
        # ============ ìµœì¢… ê²°ê³¼ ============
        print("\n" + "="*70)
        print(" ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        print("="*70)
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        reg_metrics = {k: v for k, v in state['models_metrics'].items() 
                      if 'Cls' not in k and 'LogisticRegression' not in k}
        cls_metrics = {k: v for k, v in state['models_metrics'].items() 
                      if 'Cls' in k or k == 'LogisticRegression'}
        
        if reg_metrics:
            best_r2_model = max(reg_metrics.items(), key=lambda x: x[1].get('r2', 0))[0]
            best_mae_model = min(reg_metrics.items(), key=lambda x: x[1].get('mae', float('inf')))[0]
            
            print(f"\nğŸ“ˆ íšŒê·€ ëª¨ë¸ ìµœê³  ì„±ëŠ¥:")
            print(f"  ìµœê³  RÂ²: {best_r2_model} (RÂ²={reg_metrics[best_r2_model]['r2']:.4f})")
            print(f"  ìµœì € MAE: {best_mae_model} (MAE={reg_metrics[best_mae_model]['mae']:.2f})")
        
        if cls_metrics:
            best_cls_model = max(cls_metrics.items(), key=lambda x: x[1].get('accuracy', 0))[0]
            
            print(f"\nğŸ“Š ë¶„ë¥˜ ëª¨ë¸ ìµœê³  ì„±ëŠ¥:")
            print(f"  ìµœê³  ì •í™•ë„: {best_cls_model} ({cls_metrics[best_cls_model]['accuracy']:.3f})")
        
        # ìµœì¢… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        joblib.dump(scaler, os.path.join(MODEL_DIR, 'scaler.pkl'))
        
        # ìµœì¢… ì„¤ì • ì €ì¥
        config = {
            'sequence_length': SEQ_LENGTH,
            'prediction_horizon': PRED_HORIZON,
            'num_features': X.shape[1],
            'best_models': {
                'r2': best_r2_model if reg_metrics else None,
                'mae': best_mae_model if reg_metrics else None,
                'accuracy': best_cls_model if cls_metrics else None
            },
            'all_metrics': state['models_metrics'],
            'training_completed': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        with open(os.path.join(MODEL_DIR, 'config.pkl'), 'wb') as f:
            pickle.dump(config, f)
        
        print(f"\nâœ… ëª¨ë“  ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {MODEL_DIR}/")
        print(f"âœ… ì´ {len(state['completed_models'])}ê°œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        
        # ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
        print("\nğŸ§¹ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì •ë¦¬...")
        for file in os.listdir(CHECKPOINT_DIR):
            if file not in ['state.json']:
                os.remove(os.path.join(CHECKPOINT_DIR, file))
        
        print("\nâœ¨ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! 280ë¶„ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡ ê°€ëŠ¥!")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨ ê°ì§€!")
        print("ğŸ’¾ í˜„ì¬ ìƒíƒœë¥¼ ì €ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        checkpoint_manager.save_checkpoint(state)
        print("âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ. ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.")
        sys.exit(1)
    
    except Exception as e:
        print(f"\n\nâŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print("ğŸ’¾ í˜„ì¬ ìƒíƒœë¥¼ ì €ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        checkpoint_manager.save_checkpoint(state)
        traceback.print_exc()
        print("\nâœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ. ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.")
        sys.exit(1)

# ======================== ì‹¤ì‹œê°„ ì˜ˆì¸¡ í•¨ìˆ˜ ========================
def extract_features_from_sequence(data_280min):
    """280ë¶„ ì‹œí€€ìŠ¤ì—ì„œ íŠ¹ì§• ì¶”ì¶œ (ì‹¤ì œ êµ¬í˜„)"""
    
    feature_cols = ['M14AM14B', 'M14AM10A', 'M14AM16', 'M14AM14BSUM', 
                   'M14AM10ASUM', 'M14AM16SUM', 'M14BM14A', 'M10AM14A', 
                   'M16M14A', 'TOTALCNT']
    
    # DataFrameìœ¼ë¡œ ë³€í™˜
    if isinstance(data_280min, np.ndarray):
        df = pd.DataFrame(data_280min, columns=feature_cols)
    else:
        df = data_280min
    
    # íŒŒìƒ ë³€ìˆ˜
    df['ratio_M14B_M14A'] = np.clip(df['M14AM14B'] / (df['M14AM10A'] + 1), 0, 1000)
    df['ratio_M14B_M16'] = np.clip(df['M14AM14B'] / (df['M14AM16'] + 1), 0, 1000)
    df['totalcnt_change'] = df['TOTALCNT'].diff().fillna(0)
    df['totalcnt_pct_change'] = df['TOTALCNT'].pct_change().fillna(0).clip(-10, 10)
    
    features = []
    
    # ê° ì»¬ëŸ¼ë³„ íŠ¹ì§• ì¶”ì¶œ (create_sequences_280to10ê³¼ ë™ì¼)
    for col in feature_cols:
        values = df[col].values
        
        features.extend([
            np.mean(values),
            np.std(values) if len(values) > 1 else 0,
            np.min(values),
            np.max(values),
            np.percentile(values, 25),
            np.percentile(values, 50),
            np.percentile(values, 75),
            values[-1],
            values[-1] - values[0],
            np.mean(values[-60:]),
            np.max(values[-60:]),
            np.mean(values[-30:]),
            np.max(values[-30:])
        ])
        
        if col == 'TOTALCNT':
            features.append(np.sum((values >= 1650) & (values < 1700)))
            features.append(np.sum(values >= 1700))
            features.append(np.max(values[-20:]))
            features.append(np.sum(values < 1400))
            features.append(np.sum((values >= 1400) & (values < 1700)))
            features.append(np.sum(values >= 1700))
            features.append(np.sum((values >= 1651) & (values <= 1682)))
            
            anomaly_values = values[(values >= 1651) & (values <= 1682)]
            features.append(np.max(anomaly_values) if len(anomaly_values) > 0 else 0)
            
            normal_vals = values[values < 1400]
            check_vals = values[(values >= 1400) & (values < 1700)]
            danger_vals = values[values >= 1700]
            
            features.append(np.mean(normal_vals) if len(normal_vals) > 0 else 0)
            features.append(np.mean(check_vals) if len(check_vals) > 0 else 0)
            features.append(np.mean(danger_vals) if len(danger_vals) > 0 else 0)
            
            try:
                x = np.arange(len(values))
                slope, _ = np.polyfit(x, values, 1)
                features.append(np.clip(slope, -100, 100))
            except:
                features.append(0)
            
            try:
                recent_slope = np.polyfit(np.arange(60), values[-60:], 1)[0]
                features.append(np.clip(recent_slope, -100, 100))
            except:
                features.append(0)
    
    # íŒŒìƒ ë³€ìˆ˜ ì¶”ê°€
    features.extend([
        np.clip(df['ratio_M14B_M14A'].iloc[-1], 0, 1000),
        np.clip(df['ratio_M14B_M16'].iloc[-1], 0, 1000),
        np.clip(df['totalcnt_change'].iloc[-1], -1000, 1000),
        np.clip(df['totalcnt_pct_change'].iloc[-1], -10, 10),
    ])
    
    return np.array(features)

def predict_10min_later_integrated(sequence_280min, model_name=None, model_type='regression'):
    """
    280ë¶„ ë°ì´í„°ë¡œ 10ë¶„ í›„ TOTALCNT ì˜ˆì¸¡ + 3êµ¬ê°„ ë¶„ë¥˜
    
    Parameters:
    -----------
    sequence_280min : DataFrame or array
        280ë¶„ ì‹œí€€ìŠ¤ ë°ì´í„° (280í–‰ x 10ì»¬ëŸ¼)
    model_name : str
        ì‚¬ìš©í•  ëª¨ë¸ (Noneì´ë©´ best ëª¨ë¸ ì‚¬ìš©)
    model_type : str
        'regression' ë˜ëŠ” 'classification'
    
    Returns:
    --------
    dict : ì˜ˆì¸¡ ê²°ê³¼
    """
    try:
        # ì„¤ì • ë¡œë“œ
        config_path = os.path.join(MODEL_DIR, 'config.pkl')
        if not os.path.exists(config_path):
            raise FileNotFoundError("í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        config = pickle.load(open(config_path, 'rb'))
        
        # ëª¨ë¸ ì„ íƒ
        if model_name is None:
            if model_type == 'regression':
                model_name = config['best_models'].get('r2', 'RandomForest')
            else:
                model_name = config['best_models'].get('accuracy', 'RandomForest_Cls')
        
        # ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        model_path = os.path.join(MODEL_DIR, f'{model_name}_model.pkl')
        scaler_path = os.path.join(MODEL_DIR, 'scaler.pkl')
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        
        model = joblib.load(model_path)
        scaler = joblib.load(scaler_path)
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = extract_features_from_sequence(sequence_280min)
        features_scaled = scaler.transform([features])
        
        # ì˜ˆì¸¡
        if 'Cls' in model_name or model_name == 'LogisticRegression':
            # ë¶„ë¥˜ ëª¨ë¸
            predicted_level = model.predict(features_scaled)[0]
            level_values = {0: 1150, 1: 1550, 2: 1850}  # ê° ë ˆë²¨ì˜ ëŒ€í‘œê°’
            predicted_value = level_values[predicted_level]
            
            if hasattr(model, 'predict_proba'):
                confidence = np.max(model.predict_proba(features_scaled)[0])
            else:
                confidence = 0.8
        else:
            # íšŒê·€ ëª¨ë¸
            predicted_value = model.predict(features_scaled)[0]
            predicted_level = assign_level(predicted_value)
            confidence = 0.85  # íšŒê·€ ëª¨ë¸ì˜ ê¸°ë³¸ ì‹ ë¢°ë„
        
        level_names = {0: 'ì •ìƒ (900-1399)', 1: 'í™•ì¸ (1400-1699)', 2: 'ìœ„í—˜ (1700+)'}
        
        return {
            'predicted_value': float(predicted_value),
            'predicted_level': int(predicted_level),
            'level_name': level_names[predicted_level],
            'is_danger': predicted_value >= 1700,
            'is_anomaly': detect_anomaly_signal(predicted_value),
            'confidence': float(confidence),
            'model_used': model_name,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ì˜¤ë¥˜: {str(e)}")
        return None

# ======================== ì‹œê°í™” í•¨ìˆ˜ ========================
def create_visualizations(state):
    """í•™ìŠµ ê²°ê³¼ ì‹œê°í™”"""
    if not state['models_metrics']:
        print("ì‹œê°í™”í•  ë©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    fig, axes = plt.subplots(2, 3, figsize=(20, 12))
    
    # íšŒê·€ ëª¨ë¸ ì„±ëŠ¥
    reg_models = [k for k in state['models_metrics'].keys() 
                  if 'Cls' not in k and 'LogisticRegression' not in k]
    
    if reg_models:
        # MAE ë¹„êµ
        ax = axes[0, 0]
        mae_values = [state['models_metrics'][m].get('mae', 0) for m in reg_models]
        ax.bar(reg_models, mae_values, color='skyblue')
        ax.set_title('íšŒê·€ ëª¨ë¸ - MAE')
        ax.set_ylabel('MAE')
        ax.tick_params(axis='x', rotation=45)
        
        # RÂ² ë¹„êµ
        ax = axes[0, 1]
        r2_values = [state['models_metrics'][m].get('r2', 0) for m in reg_models]
        ax.bar(reg_models, r2_values, color='green', alpha=0.7)
        ax.set_title('íšŒê·€ ëª¨ë¸ - RÂ² Score')
        ax.set_ylabel('RÂ²')
        ax.set_ylim(0, 1)
        ax.tick_params(axis='x', rotation=45)
        
        # 3êµ¬ê°„ ì •í™•ë„
        ax = axes[0, 2]
        cls_acc_values = [state['models_metrics'][m].get('cls_accuracy', 0) for m in reg_models]
        ax.bar(reg_models, cls_acc_values, color='purple', alpha=0.7)
        ax.set_title('íšŒê·€ ëª¨ë¸ - 3êµ¬ê°„ ì •í™•ë„')
        ax.set_ylabel('ì •í™•ë„')
        ax.tick_params(axis='x', rotation=45)
    
    # ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥
    cls_models = [k for k in state['models_metrics'].keys() 
                  if 'Cls' in k or k == 'LogisticRegression']
    
    if cls_models:
        # ì •í™•ë„ ë¹„êµ
        ax = axes[1, 0]
        acc_values = [state['models_metrics'][m].get('accuracy', 0) for m in cls_models]
        ax.bar(cls_models, acc_values, color='orange', alpha=0.7)
        ax.set_title('ë¶„ë¥˜ ëª¨ë¸ - ì •í™•ë„')
        ax.set_ylabel('Accuracy')
        ax.tick_params(axis='x', rotation=45)
        
        # F1 Score ë¹„êµ
        ax = axes[1, 1]
        f1_values = [state['models_metrics'][m].get('f1', 0) for m in cls_models]
        ax.bar(cls_models, f1_values, color='red', alpha=0.7)
        ax.set_title('ë¶„ë¥˜ ëª¨ë¸ - F1 Score')
        ax.set_ylabel('F1')
        ax.tick_params(axis='x', rotation=45)
    
    # ì „ì²´ ì§„í–‰ ìƒí™©
    ax = axes[1, 2]
    completed = len(state['completed_models'])
    remaining = 11 - completed
    ax.pie([completed, remaining], labels=['ì™„ë£Œ', 'ë‚¨ìŒ'], 
           colors=['green', 'gray'], autopct='%1.1f%%')
    ax.set_title('ì „ì²´ ì§„í–‰ ìƒí™©')
    
    plt.tight_layout()
    plt.savefig(os.path.join(MODEL_DIR, 'training_results.png'), dpi=150)
    plt.close()
    print(f"âœ… ì‹œê°í™” ì €ì¥: {MODEL_DIR}/training_results.png")

# ======================== ìë™ ì¬ì‹œì‘ ê¸°ëŠ¥ í†µí•© ========================
def main_with_auto_restart():
    """ìë™ ì¬ì‹œì‘ ê¸°ëŠ¥ì´ í†µí•©ëœ ë©”ì¸ í•¨ìˆ˜"""
    max_retries = 10
    retry_count = 0
    
    print("\n" + "="*70)
    print(" ğŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ v1.0")
    print(" 280ë¶„ ë°ì´í„° â†’ 10ë¶„ í›„ ì˜ˆì¸¡")
    print(" ì²´í¬í¬ì¸íŠ¸ + ìë™ ì¬ì‹œì‘ ì‹œìŠ¤í…œ")
    print("="*70)
    
    # ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
    check_system_info()
    
    # ì‚¬ìš© ëª¨ë¸ ì •ë³´ í‘œì‹œ
    print("\n" + "="*70)
    print(" ğŸ“š ì‚¬ìš© ëª¨ë¸ ì •ë³´")
    print("="*70)
    print("\n  ğŸ“ˆ íšŒê·€ ëª¨ë¸ (6ê°œ) - TOTALCNT ê°’ ì˜ˆì¸¡:")
    print("    1. RandomForest       - ì•™ìƒë¸” íŠ¸ë¦¬ (300ê°œ)")
    print("    2. ExtraTrees        - ê·¹ë‹¨ ëœë¤ íŠ¸ë¦¬ (300ê°œ)")
    print("    3. GradientBoosting  - ë¶€ìŠ¤íŒ… (300 estimators)")
    print("    4. MLP               - ì‹ ê²½ë§ (200-100-50 hidden layers)")
    print("    5. Ridge             - L2 ì •ê·œí™” ì„ í˜• íšŒê·€")
    print("    6. ElasticNet        - L1+L2 ì •ê·œí™”")
    
    print("\n  ğŸ“Š ë¶„ë¥˜ ëª¨ë¸ (5ê°œ) - 3êµ¬ê°„ ë¶„ë¥˜:")
    print("    1. RandomForest_Cls      - ì•™ìƒë¸” ë¶„ë¥˜ê¸°")
    print("    2. ExtraTrees_Cls       - ê·¹ë‹¨ ëœë¤ íŠ¸ë¦¬ ë¶„ë¥˜ê¸°")
    print("    3. GradientBoosting_Cls - ë¶€ìŠ¤íŒ… ë¶„ë¥˜ê¸°")
    print("    4. MLP_Cls              - ì‹ ê²½ë§ ë¶„ë¥˜ê¸°")
    print("    5. LogisticRegression   - ë¡œì§€ìŠ¤í‹± íšŒê·€")
    
    print("\n  ğŸ’¡ ëª¨ë“  Sklearn ëª¨ë¸ì€ CPU 2ì½”ì–´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ (n_jobs=2)")
    print("  ğŸ’¡ XGBoostëŠ” GPU ìˆìœ¼ë©´ GPU, ì—†ìœ¼ë©´ CPU 2ì½”ì–´ ì‚¬ìš©")
    print("="*70)
    
    print("\nğŸ’¡ Ctrl+Cë¡œ ì–¸ì œë“  ì¤‘ë‹¨ ê°€ëŠ¥ (ìë™ ì €ì¥ë©ë‹ˆë‹¤)")
    print("ğŸ’¡ ì˜¤ë¥˜ ë°œìƒ ì‹œ ìë™ìœ¼ë¡œ ì¬ì‹œì‘í•©ë‹ˆë‹¤ (ìµœëŒ€ 10íšŒ)")
    print("-"*70)
    
    while retry_count < max_retries:
        try:
            print(f"\nğŸ“Š í•™ìŠµ ì‹œì‘ (ì‹œë„ {retry_count + 1}/{max_retries})")
            print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            # í•™ìŠµ ì‹¤í–‰
            train_with_checkpoint()
            
            # ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ
            print("\nâœ… í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            # ì‹œê°í™” ìƒì„±
            checkpoint_manager = CheckpointManager()
            state = checkpoint_manager.load_checkpoint()
            if state:
                create_visualizations(state)
            
            break
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
            print("ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ë˜ì—ˆìœ¼ë¯€ë¡œ ë‚˜ì¤‘ì— ì´ì–´ì„œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            break
            
        except Exception as e:
            retry_count += 1
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            
            if retry_count < max_retries:
                print(f"â³ 5ì´ˆ í›„ ìë™ìœ¼ë¡œ ì¬ì‹œì‘í•©ë‹ˆë‹¤... (ë‚¨ì€ ì‹œë„: {max_retries - retry_count})")
                time.sleep(5)
            else:
                print(f"\nâŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                print("ìˆ˜ë™ìœ¼ë¡œ ë¬¸ì œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                traceback.print_exc()
                break
    
    print("\n" + "="*70)
    print(" í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("="*70)

# ======================== ì‹¤í–‰ ========================
if __name__ == "__main__":
    # ë°ì´í„° íŒŒì¼ í™•ì¸
    required_files = ['uu.csv', 'uu2.csv']
    missing_files = [f for f in required_files if not os.path.exists(f)]
    
    if missing_files:
        print(f"\nâŒ í•„ìš”í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_files}")
        print("uu.csvì™€ uu2.csv íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        sys.exit(1)
    
    # ìë™ ì¬ì‹œì‘ ëª¨ë“œë¡œ ì‹¤í–‰
    main_with_auto_restart()