"""
sequence_generator_v6_parallel.py - ë³‘ë ¬ì²˜ë¦¬ ì‹œí€€ìŠ¤ ìƒì„±
ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ì‹œí€€ìŠ¤ ìƒì„± ì†ë„ ëŒ€í­ í–¥ìƒ
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import RobustScaler
import pickle
import warnings
from datetime import datetime
import os
from multiprocessing import Pool, cpu_count, Manager
from functools import partial
import gc
warnings.filterwarnings('ignore')

print("="*60)
print("ğŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ ì‹œí€€ìŠ¤ ìƒì„±ê¸° V6 (ë³‘ë ¬ì²˜ë¦¬)")
print(f"ğŸ’» ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´: {cpu_count()}ê°œ")
print("="*60)

# ============================================
# ì„¤ì •
# ============================================
# ë°ì´í„° íŒŒì¼
DATA_FILE = '20240201_TO_202507281705.CSV'

# ë°ì´í„° ì„¤ì •
LOOKBACK = 100  # ê³¼ê±° 100ë¶„
FORECAST = 10   # 10ë¶„ í›„ ì˜ˆì¸¡

# M14 ì„ê³„ê°’
M14B_THRESHOLDS = {
    1400: 320,
    1500: 400,
    1600: 450,
    1700: 500
}

RATIO_THRESHOLDS = {
    1400: 4,
    1500: 5,
    1600: 6,
    1700: 7
}

# ì €ì¥ ê²½ë¡œ
SAVE_PATH = './sequences_v6.npz'

# ë³‘ë ¬ì²˜ë¦¬ ì„¤ì •
N_WORKERS = min(cpu_count() - 1, 8)  # ìµœëŒ€ 8ê°œ ì½”ì–´ ì‚¬ìš©
CHUNK_SIZE = 5000  # ê° í”„ë¡œì„¸ìŠ¤ê°€ ì²˜ë¦¬í•  ì‹œí€€ìŠ¤ ìˆ˜

# ============================================
# ë³‘ë ¬ì²˜ë¦¬ìš© í•¨ìˆ˜
# ============================================
def process_chunk(args):
    """ì²­í¬ ë‹¨ìœ„ë¡œ ì‹œí€€ìŠ¤ ìƒì„± (ë³‘ë ¬ì²˜ë¦¬ìš©)"""
    start_idx, end_idx, df_values, feature_cols, lookback, forecast = args
    
    X_chunk = []
    y_chunk = []
    m14_chunk = []
    
    # DataFrame ì¬êµ¬ì„± (valuesì—ì„œ)
    df = pd.DataFrame(df_values, columns=feature_cols)
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    for i in range(start_idx, min(end_idx, len(df) - forecast)):
        if i >= lookback:
            # ì‹œê³„ì—´ ë°ì´í„°
            X_chunk.append(df.iloc[i-lookback:i].values)
            
            # íƒ€ê²Ÿ
            y_chunk.append(df['target'].iloc[i+forecast-1])
            
            # M14 íŠ¹ì§•
            m14_chunk.append([
                df['M14AM14B'].iloc[i],
                df['M14AM10A'].iloc[i],
                df['M14AM16'].iloc[i],
                df['ratio_14B_10A'].iloc[i] if 'ratio_14B_10A' in df else 0
            ])
    
    return np.array(X_chunk, dtype=np.float32), \
           np.array(y_chunk, dtype=np.float32), \
           np.array(m14_chunk, dtype=np.float32)

# ============================================
# ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì§• ìƒì„±
# ============================================
print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {DATA_FILE}")

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv(DATA_FILE)
print(f"  ë°ì´í„° í¬ê¸°: {len(df)}í–‰")

# í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
if 'TOTALCNT' in df.columns:
    df['current_value'] = df['TOTALCNT']
else:
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    df['current_value'] = df[numeric_cols[0]]

# M14 ì»¬ëŸ¼ í™•ì¸
for col in ['M14AM10A', 'M14AM14B', 'M14AM16']:
    if col not in df.columns:
        print(f"  âš ï¸ {col} ì—†ìŒ â†’ 0ìœ¼ë¡œ ì´ˆê¸°í™”")
        df[col] = 0

# íƒ€ê²Ÿ ìƒì„± (10ë¶„ í›„)
df['target'] = df['current_value'].shift(-10)

print("\nğŸ”§ íŠ¹ì§• ìƒì„± ì¤‘...")

# 1. ê¸°ë³¸ M14 íŠ¹ì§•
df['M14B_norm'] = df['M14AM14B'] / 600
df['M10A_inverse'] = (100 - df['M14AM10A']) / 100
df['M16_norm'] = df['M14AM16'] / 100

# 2. ë¹„ìœ¨ íŠ¹ì§•
df['ratio_14B_10A'] = df['M14AM14B'] / df['M14AM10A'].clip(lower=1)
df['ratio_14B_16'] = df['M14AM14B'] / df['M14AM16'].clip(lower=1)
df['ratio_10A_16'] = df['M14AM10A'] / df['M14AM16'].clip(lower=1)

# 3. ë³€í™”ëŸ‰ íŠ¹ì§• (ë²¡í„°í™”ë¡œ ë¹ ë¥´ê²Œ)
print("  ë³€í™”ëŸ‰ íŠ¹ì§• ìƒì„± ì¤‘...")
for col in ['current_value', 'M14AM14B', 'M14AM10A', 'M14AM16']:
    if col in df.columns:
        df[f'{col}_change_5'] = df[col].diff(5)
        df[f'{col}_change_10'] = df[col].diff(10)
        df[f'{col}_ma_10'] = df[col].rolling(10, min_periods=1).mean()
        df[f'{col}_std_10'] = df[col].rolling(10, min_periods=1).std()

# 4. M14AM10A ì—­íŒ¨í„´
df['M10A_drop_5'] = -df['M14AM10A'].diff(5)
df['M10A_drop_10'] = -df['M14AM10A'].diff(10)

# 5. ê¸‰ë³€ ì‹ í˜¸ (ë²¡í„°í™”)
print("  ì‹ í˜¸ íŠ¹ì§• ìƒì„± ì¤‘...")
df['signal_1400'] = (df['M14AM14B'] >= M14B_THRESHOLDS[1400]).astype(float)
df['signal_1500'] = (df['M14AM14B'] >= M14B_THRESHOLDS[1500]).astype(float)
df['signal_1600'] = (df['M14AM14B'] >= M14B_THRESHOLDS[1600]).astype(float)
df['signal_1700'] = (df['M14AM14B'] >= M14B_THRESHOLDS[1700]).astype(float)

# 6. ë¹„ìœ¨ ì‹ í˜¸
df['ratio_signal_1400'] = (df['ratio_14B_10A'] >= RATIO_THRESHOLDS[1400]).astype(float)
df['ratio_signal_1500'] = (df['ratio_14B_10A'] >= RATIO_THRESHOLDS[1500]).astype(float)
df['ratio_signal_1600'] = (df['ratio_14B_10A'] >= RATIO_THRESHOLDS[1600]).astype(float)
df['ratio_signal_1700'] = (df['ratio_14B_10A'] >= RATIO_THRESHOLDS[1700]).astype(float)

# 7. ì¡°í•© íŠ¹ì§•
df['m14b_high_m10a_low'] = ((df['M14AM14B'] >= 350) & (df['M14AM10A'] < 70)).astype(float)
df['spike_imminent'] = ((df['M14AM14B'] >= 400) | (df['ratio_14B_10A'] >= 5)).astype(float)

# 8. í†µê³„ íŠ¹ì§•
df['current_vs_ma'] = df['current_value'] / df['current_value_ma_10'].clip(lower=1)
df['m14b_vs_ma'] = df['M14AM14B'] / df['M14AM14B_ma_10'].clip(lower=1)

# íŠ¹ì§• ì»¬ëŸ¼ ì„ íƒ
exclude_cols = ['TIME', 'CURRTIME', 'TOTALCNT']
feature_cols = [col for col in df.columns if col not in exclude_cols]

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
df = df.fillna(0)
df = df.dropna(subset=['target'])

print(f"  ìƒì„±ëœ íŠ¹ì§• ìˆ˜: {len(feature_cols)}ê°œ")

# ============================================
# ë³‘ë ¬ ì‹œí€€ìŠ¤ ìƒì„±
# ============================================
print(f"\nğŸ“Š ë³‘ë ¬ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘... (ì›Œì»¤: {N_WORKERS}ê°œ)")

total_sequences = len(df) - LOOKBACK - FORECAST
print(f"  ì´ {total_sequences:,}ê°œ ì‹œí€€ìŠ¤ ìƒì„± ì˜ˆì •")

# ì²­í¬ ì¸ë±ìŠ¤ ìƒì„±
chunk_indices = []
for i in range(LOOKBACK, len(df) - FORECAST, CHUNK_SIZE):
    chunk_indices.append((
        i, 
        min(i + CHUNK_SIZE, len(df) - FORECAST)
    ))

print(f"  ì²­í¬ ìˆ˜: {len(chunk_indices)}ê°œ")

# DataFrameì„ numpy arrayë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨)
df_values = df[feature_cols].values

# ë³‘ë ¬ì²˜ë¦¬ ì¸ì ì¤€ë¹„
process_args = [
    (start, end, df_values, feature_cols, LOOKBACK, FORECAST)
    for start, end in chunk_indices
]

# ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
print("\nâš¡ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")
start_time = datetime.now()

with Pool(processes=N_WORKERS) as pool:
    results = pool.map(process_chunk, process_args)

# ê²°ê³¼ í•©ì¹˜ê¸°
print("\nğŸ“¦ ê²°ê³¼ ë³‘í•© ì¤‘...")
X_list = [r[0] for r in results if len(r[0]) > 0]
y_list = [r[1] for r in results if len(r[1]) > 0]
m14_list = [r[2] for r in results if len(r[2]) > 0]

X = np.concatenate(X_list, axis=0)
y = np.concatenate(y_list, axis=0)
m14_features = np.concatenate(m14_list, axis=0)

# ë©”ëª¨ë¦¬ ì •ë¦¬
del X_list, y_list, m14_list, results
gc.collect()

elapsed_time = datetime.now() - start_time
print(f"  ë³‘ë ¬ ì²˜ë¦¬ ì‹œê°„: {elapsed_time}")

print(f"\nâœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ!")
print(f"  X shape: {X.shape}")
print(f"  y shape: {y.shape}")
print(f"  m14_features shape: {m14_features.shape}")
print(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©: {(X.nbytes + y.nbytes + m14_features.nbytes) / 1024**3:.2f}GB")

# ============================================
# ìŠ¤ì¼€ì¼ë§ (ë³‘ë ¬ì²˜ë¦¬)
# ============================================
print("\nğŸ“ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (ë³‘ë ¬)...")

def scale_feature(args):
    """íŠ¹ì§• í•˜ë‚˜ë¥¼ ìŠ¤ì¼€ì¼ë§ (ë³‘ë ¬ì²˜ë¦¬ìš©)"""
    feature_idx, feature_data = args
    scaler = RobustScaler()
    feature_flat = feature_data.reshape(-1, 1)
    scaler.fit(feature_flat)
    scaled = scaler.transform(feature_flat).reshape(feature_data.shape)
    return feature_idx, scaled, scaler

# ë³‘ë ¬ ìŠ¤ì¼€ì¼ë§
X_scaled = X.copy()
scalers = {}

# íŠ¹ì§•ë³„ë¡œ ë¶„ë¦¬
feature_data_list = [(i, X[:, :, i]) for i in range(X.shape[2])]

# ë³‘ë ¬ ì²˜ë¦¬
with Pool(processes=N_WORKERS) as pool:
    scaling_results = pool.map(scale_feature, feature_data_list)

# ê²°ê³¼ ì ìš©
for feature_idx, scaled_data, scaler in scaling_results:
    X_scaled[:, :, feature_idx] = scaled_data
    scalers[f'feature_{feature_idx}'] = scaler
    
    if (feature_idx + 1) % 10 == 0:
        print(f"  {feature_idx+1}/{X.shape[2]} íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")

print("  âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")

# ============================================
# ì €ì¥
# ============================================
print(f"\nğŸ’¾ ì‹œí€€ìŠ¤ ì €ì¥ ì¤‘...")

# ì••ì¶• ì €ì¥
np.savez_compressed(
    SAVE_PATH,
    X=X_scaled,
    y=y,
    m14_features=m14_features,
    feature_names=feature_cols
)

# ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
with open('./scalers_v6.pkl', 'wb') as f:
    pickle.dump(scalers, f)

print(f"  âœ… ì €ì¥ ì™„ë£Œ: {SAVE_PATH}")
print(f"  âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: ./scalers_v6.pkl")

# ============================================
# ë°ì´í„° í†µê³„
# ============================================
print("\nğŸ“Š ë°ì´í„° í†µê³„:")
print(f"  íƒ€ê²Ÿê°’ ë²”ìœ„: {y.min():.0f} ~ {y.max():.0f}")
print(f"  1400+ ë¹„ìœ¨: {(y >= 1400).mean():.1%} ({(y >= 1400).sum():,}ê°œ)")
print(f"  1500+ ë¹„ìœ¨: {(y >= 1500).mean():.1%} ({(y >= 1500).sum():,}ê°œ)")
print(f"  1600+ ë¹„ìœ¨: {(y >= 1600).mean():.1%} ({(y >= 1600).sum():,}ê°œ)")
print(f"  1700+ ë¹„ìœ¨: {(y >= 1700).mean():.1%} ({(y >= 1700).sum():,}ê°œ)")

total_time = datetime.now() - start_time
print("\n" + "="*60)
print("âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ!")
print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time}")
print(f"ğŸš€ ì†ë„ í–¥ìƒ: ì•½ {N_WORKERS}ë°° ë¹ ë¦„")
print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: train_v6.py ì‹¤í–‰")
print("="*60)