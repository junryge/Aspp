"""
í•™ìŠµV6.py - ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (ì „ì²´ êµ¬ê°„ + 1400-1700+ íŠ¹í™”)
M14 íŒ¨í„´ í†µí•© í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
TensorFlow 2.15.0
í•™ìŠµ ë°ì´í„°: 20240201_TO_202507281705.CSV
"""

import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
import json
import os
warnings.filterwarnings('ignore')

print("="*60)
print("ğŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ V6")
print(f"ğŸ“¦ TensorFlow ë²„ì „: {tf.__version__}")
print("="*60)

# ============================================
# 1. ì„¤ì • í´ë˜ìŠ¤
# ============================================
class Config:
    # ë°ì´í„° íŒŒì¼
    DATA_FILE = '20240201_TO_202507281705.CSV'  # í†µí•© í•™ìŠµ ë°ì´í„°
    
    # ë°ì´í„° ì„¤ì •
    LOOKBACK = 100  # ê³¼ê±° 100ë¶„
    FORECAST = 1    # 10ë¶„ í›„ ì˜ˆì¸¡
    
    # M14 ì„ê³„ê°’ (ë°œê²¬ëœ íŒ¨í„´)
    M14B_THRESHOLDS = {
        1400: 320,
        1500: 400,
        1600: 450,
        1700: 500
    }
    
    RATIO_THRESHOLDS = {
        1400: 4,
        1500: 5,
        1600: 6,
        1700: 7
    }
    
    # í•™ìŠµ ì„¤ì •
    BATCH_SIZE = 32
    EPOCHS = 100
    LEARNING_RATE = 0.001
    PATIENCE = 15
    
    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    MODEL_DIR = './models_v6/'
    
    # ê°€ì¤‘ì¹˜ ì„¤ì •
    SPIKE_WEIGHTS = {
        'normal': 1.0,      # < 1400
        'level_1400': 3.0,  # 1400-1499
        'level_1500': 5.0,  # 1500-1599
        'level_1600': 8.0,  # 1600-1699
        'level_1700': 10.0  # >= 1700
    }

# ============================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬
# ============================================
class DataPreprocessorV6:
    def __init__(self):
        self.value_scaler = RobustScaler()
        self.m14_scaler = StandardScaler()
        self.feature_names = []
        
    def prepare_data(self, file_path=None):
        """í†µí•© ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        if file_path is None:
            file_path = Config.DATA_FILE
            
        print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {file_path}")
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(file_path)
        print(f"  ë°ì´í„° í¬ê¸°: {len(df)}í–‰ Ã— {len(df.columns)}ì—´")
        
        # ì»¬ëŸ¼ëª… í™•ì¸ ë° ì •ë¦¬
        print(f"  ì»¬ëŸ¼ í™•ì¸ ì¤‘...")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_cols = ['TOTALCNT', 'M14AM10A', 'M14AM14B', 'M14AM16']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"  âš ï¸ ê²½ê³ : ëˆ„ë½ëœ ì»¬ëŸ¼ {missing_cols}")
            print(f"  ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {df.columns.tolist()[:10]}...")
        
        # ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬ (CURRTIME ë˜ëŠ” TIME)
        if 'CURRTIME' in df.columns:
            df['TIME'] = df['CURRTIME']
        elif 'TIME' not in df.columns:
            # ì‹œê°„ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ë¡œ ìƒì„±
            df['TIME'] = range(len(df))
        
        # TOTALCNTë¥¼ í˜„ì¬ê°’ìœ¼ë¡œ ì‚¬ìš©
        if 'TOTALCNT' in df.columns:
            df['current_value'] = df['TOTALCNT']
        else:
            print("  âš ï¸ TOTALCNT ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ ìˆ˜ì¹˜ ì»¬ëŸ¼ ì‚¬ìš©")
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            df['current_value'] = df[numeric_cols[0]]
        
        # íƒ€ê²Ÿ ìƒì„± (10ë¶„ í›„ ê°’)
        df['target'] = df['current_value'].shift(-10)  # 10ë¶„ í›„ ê°’
        
        # M14 ì»¬ëŸ¼ í™•ì¸
        for col in ['M14AM10A', 'M14AM14B', 'M14AM16']:
            if col not in df.columns:
                print(f"  âš ï¸ {col} ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. 0ìœ¼ë¡œ ì´ˆê¸°í™”")
                df[col] = 0
        
        # ê²°ì¸¡ì¹˜ ì œê±°
        df = df.dropna(subset=['target'])
        
        print(f"  ì „ì²˜ë¦¬ í›„: {len(df)}í–‰")
        
        # íŠ¹ì§• ìƒì„±
        print("\nğŸ”§ íŠ¹ì§• ìƒì„± ì¤‘...")
        df = self._create_features(df)
        
        # íƒ€ê²Ÿ ë ˆë²¨ ìƒì„± (ë‹¤ë‹¨ê³„)
        df['target_level'] = pd.cut(
            df['target'],
            bins=[-np.inf, 1400, 1500, 1600, 1700, np.inf],
            labels=[0, 1, 2, 3, 4]
        )
        
        # ë°ì´í„° í†µê³„
        print(f"\nğŸ“Š ë°ì´í„° í†µê³„:")
        print(f"  í˜„ì¬ê°’ ë²”ìœ„: {df['current_value'].min():.0f} ~ {df['current_value'].max():.0f}")
        print(f"  íƒ€ê²Ÿê°’ ë²”ìœ„: {df['target'].min():.0f} ~ {df['target'].max():.0f}")
        print(f"  1400+ ë¹„ìœ¨: {(df['target'] >= 1400).mean():.1%}")
        print(f"  1500+ ë¹„ìœ¨: {(df['target'] >= 1500).mean():.1%}")
        print(f"  1600+ ë¹„ìœ¨: {(df['target'] >= 1600).mean():.1%}")
        print(f"  1700+ ë¹„ìœ¨: {(df['target'] >= 1700).mean():.1%}")
        
        return df
    
    def _create_features(self, df):
        """M14 ê¸°ë°˜ íŠ¹ì§• ìƒì„±"""
        
        # 1. ê¸°ë³¸ M14 íŠ¹ì§•
        df['M14B_norm'] = df['M14AM14B'] / 600  # ì •ê·œí™”
        df['M10A_inverse'] = (100 - df['M14AM10A']) / 100  # ì—­íŒ¨í„´
        df['M16_norm'] = df['M14AM16'] / 100
        
        # 2. ë¹„ìœ¨ íŠ¹ì§• (í•µì‹¬!)
        df['ratio_14B_10A'] = df['M14AM14B'] / df['M14AM10A'].clip(lower=1)
        df['ratio_14B_16'] = df['M14AM14B'] / df['M14AM16'].clip(lower=1)
        df['ratio_10A_16'] = df['M14AM10A'] / df['M14AM16'].clip(lower=1)
        
        # 3. ë³€í™”ëŸ‰ íŠ¹ì§•
        for col in ['current_value', 'M14AM14B', 'M14AM10A', 'M14AM16']:
            if col in df.columns:
                df[f'{col}_change_5'] = df[col].diff(5)
                df[f'{col}_change_10'] = df[col].diff(10)
                df[f'{col}_ma_10'] = df[col].rolling(10, min_periods=1).mean()
                df[f'{col}_std_10'] = df[col].rolling(10, min_periods=1).std()
        
        # 4. M14AM10A ì—­íŒ¨í„´ íŠ¹ì§•
        df['M10A_drop_5'] = -df['M14AM10A'].diff(5)  # ê°ì†Œê°€ ì–‘ìˆ˜
        df['M10A_drop_10'] = -df['M14AM10A'].diff(10)
        
        # 5. ê¸‰ë³€ ì‹ í˜¸ íŠ¹ì§•
        df['signal_1400'] = (df['M14AM14B'] >= Config.M14B_THRESHOLDS[1400]).astype(float)
        df['signal_1500'] = (df['M14AM14B'] >= Config.M14B_THRESHOLDS[1500]).astype(float)
        df['signal_1600'] = (df['M14AM14B'] >= Config.M14B_THRESHOLDS[1600]).astype(float)
        df['signal_1700'] = (df['M14AM14B'] >= Config.M14B_THRESHOLDS[1700]).astype(float)
        
        # 6. ë¹„ìœ¨ ì„ê³„ê°’ ì‹ í˜¸
        df['ratio_signal_1400'] = (df['ratio_14B_10A'] >= Config.RATIO_THRESHOLDS[1400]).astype(float)
        df['ratio_signal_1500'] = (df['ratio_14B_10A'] >= Config.RATIO_THRESHOLDS[1500]).astype(float)
        df['ratio_signal_1600'] = (df['ratio_14B_10A'] >= Config.RATIO_THRESHOLDS[1600]).astype(float)
        df['ratio_signal_1700'] = (df['ratio_14B_10A'] >= Config.RATIO_THRESHOLDS[1700]).astype(float)
        
        # 7. ì¡°í•© íŠ¹ì§•
        df['m14b_high_m10a_low'] = ((df['M14AM14B'] >= 350) & (df['M14AM10A'] < 70)).astype(float)
        df['spike_imminent'] = ((df['M14AM14B'] >= 400) | (df['ratio_14B_10A'] >= 5)).astype(float)
        
        # 8. í†µê³„ íŠ¹ì§•
        df['current_vs_ma'] = df['current_value'] / df['current_value_ma_10'].clip(lower=1)
        df['m14b_vs_ma'] = df['M14AM14B'] / df['M14AM14B_ma_10'].clip(lower=1)
        
        # íŠ¹ì§• ì´ë¦„ ì €ì¥
        exclude_cols = ['TIME', 'CURRTIME', 'target', 'target_level', 'TOTALCNT']
        self.feature_names = [col for col in df.columns if col not in exclude_cols]
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df[self.feature_names] = df[self.feature_names].fillna(0)
        
        print(f"  ìƒì„±ëœ íŠ¹ì§• ìˆ˜: {len(self.feature_names)}ê°œ")
        
        return df
    
    def create_sequences(self, df):
        """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±"""
        print("\nğŸ“Š ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        
        # ì‚¬ìš©í•  íŠ¹ì§• ì„ íƒ
        feature_cols = [col for col in self.feature_names if col in df.columns]
        
        X, y, m14_features = [], [], []
        
        for i in range(Config.LOOKBACK, len(df) - Config.FORECAST):
            # ì‹œê³„ì—´ ë°ì´í„° (ê³¼ê±° 100ë¶„)
            X.append(df[feature_cols].iloc[i-Config.LOOKBACK:i].values)
            
            # íƒ€ê²Ÿ (10ë¶„ í›„)
            y.append(df['target'].iloc[i])
            
            # í˜„ì¬ M14 íŠ¹ì§• (ì•™ìƒë¸” ê°€ì¤‘ì¹˜ìš©)
            m14_features.append([
                df['M14AM14B'].iloc[i],
                df['M14AM10A'].iloc[i],
                df['M14AM16'].iloc[i],
                df['ratio_14B_10A'].iloc[i] if 'ratio_14B_10A' in df.columns else 0
            ])
        
        X = np.array(X)
        y = np.array(y)
        m14_features = np.array(m14_features)
        
        print(f"  ì‹œí€€ìŠ¤ shape: X={X.shape}, y={y.shape}")
        print(f"  íŠ¹ì§• ìˆ˜: {X.shape[2]}ê°œ")
        
        return X, y, m14_features

# ============================================
# 3. ëª¨ë¸ ì •ì˜
# ============================================
class ModelsV6:
    
    @staticmethod
    def build_enhanced_gru(input_shape):
        """ê°œì„ ëœ GRU ëª¨ë¸ (ì „ì²´ êµ¬ê°„ ì•ˆì •ì  ì˜ˆì¸¡)"""
        inputs = tf.keras.Input(shape=input_shape, name='time_series_input')
        
        # Layer Normalization
        x = tf.keras.layers.LayerNormalization()(inputs)
        
        # Stacked GRU with residual
        gru1 = tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2)(x)
        gru2 = tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2)(gru1)
        
        # Residual connection
        residual = tf.keras.layers.Add()([gru1, gru2])
        
        # Final GRU
        gru3 = tf.keras.layers.GRU(64, dropout=0.2)(residual)
        
        # Dense layers
        dense1 = tf.keras.layers.Dense(128, activation='relu')(gru3)
        dropout = tf.keras.layers.Dropout(0.3)(dense1)
        dense2 = tf.keras.layers.Dense(64, activation='relu')(dropout)
        
        # Output
        output = tf.keras.layers.Dense(1, name='prediction')(dense2)
        
        model = tf.keras.Model(inputs=inputs, outputs=output, name='Enhanced_GRU')
        return model
    
    @staticmethod
    def build_spike_detector(input_shape):
        """M14 ê¸°ë°˜ ê¸‰ë³€ ê°ì§€ CNN-LSTM ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape, name='time_series_input')
        
        # Multi-scale CNN for pattern detection
        conv1 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)
        conv2 = tf.keras.layers.Conv1D(64, 5, activation='relu', padding='same')(inputs)
        conv3 = tf.keras.layers.Conv1D(64, 7, activation='relu', padding='same')(inputs)
        
        # Concatenate multi-scale features
        concat = tf.keras.layers.Concatenate()([conv1, conv2, conv3])
        
        # Batch normalization
        norm = tf.keras.layers.BatchNormalization()(concat)
        
        # Attention mechanism
        attention = tf.keras.layers.MultiHeadAttention(
            num_heads=4, 
            key_dim=48,
            dropout=0.2
        )(norm, norm)
        
        # BiLSTM
        lstm = tf.keras.layers.Bidirectional(
            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)
        )(attention)
        
        # Global pooling
        pooled = tf.keras.layers.GlobalAveragePooling1D()(lstm)
        
        # Dense layers
        dense1 = tf.keras.layers.Dense(256, activation='relu')(pooled)
        dropout1 = tf.keras.layers.Dropout(0.3)(dense1)
        dense2 = tf.keras.layers.Dense(128, activation='relu')(dropout1)
        dropout2 = tf.keras.layers.Dropout(0.2)(dense2)
        
        # Dual output
        regression_output = tf.keras.layers.Dense(1, name='spike_value')(dropout2)
        classification_output = tf.keras.layers.Dense(1, activation='sigmoid', name='spike_prob')(dropout2)
        
        model = tf.keras.Model(
            inputs=inputs,
            outputs=[regression_output, classification_output],
            name='Spike_Detector'
        )
        return model
    
    @staticmethod
    def build_ensemble_model(gru_model, spike_model, m14_input_shape):
        """ë™ì  ì•™ìƒë¸” ëª¨ë¸"""
        # ì…ë ¥
        time_series_input = tf.keras.Input(shape=gru_model.input_shape[1:], name='time_series')
        m14_input = tf.keras.Input(shape=m14_input_shape, name='m14_features')
        
        # ê° ëª¨ë¸ ì˜ˆì¸¡
        gru_pred = gru_model(time_series_input)
        spike_pred, spike_prob = spike_model(time_series_input)
        
        # M14 ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        weight_dense = tf.keras.layers.Dense(16, activation='relu')(m14_input)
        weight_dense = tf.keras.layers.Dense(8, activation='relu')(weight_dense)
        weights = tf.keras.layers.Dense(2, activation='softmax', name='weights')(weight_dense)
        
        # ê°€ì¤‘ í‰ê· 
        gru_weight = tf.keras.layers.Lambda(lambda x: x[:, 0:1])(weights)
        spike_weight = tf.keras.layers.Lambda(lambda x: x[:, 1:2])(weights)
        
        weighted_gru = tf.keras.layers.Multiply()([gru_pred, gru_weight])
        weighted_spike = tf.keras.layers.Multiply()([spike_pred, spike_weight])
        
        # ìµœì¢… ì˜ˆì¸¡
        final_pred = tf.keras.layers.Add()([weighted_gru, weighted_spike])
        
        # M14 ê·œì¹™ ê¸°ë°˜ ë³´ì •
        final_pred = M14RuleCorrection()([final_pred, m14_input])
        
        model = tf.keras.Model(
            inputs=[time_series_input, m14_input],
            outputs=[final_pred, spike_prob],
            name='Ensemble_Model'
        )
        return model

# ============================================
# 4. ì»¤ìŠ¤í…€ ë ˆì´ì–´ ë° ì†ì‹¤ í•¨ìˆ˜
# ============================================
class M14RuleCorrection(tf.keras.layers.Layer):
    """M14 ê·œì¹™ ê¸°ë°˜ ë³´ì • ë ˆì´ì–´"""
    def __init__(self):
        super().__init__()
        
    def call(self, inputs):
        pred, m14_features = inputs
        
        # M14 íŠ¹ì§• ë¶„í•´
        m14b = m14_features[:, 0:1]
        m10a = m14_features[:, 1:2]
        ratio = m14_features[:, 3:4]
        
        # ê·œì¹™ ê¸°ë°˜ ë³´ì •
        # 1700+ ì‹ í˜¸
        condition_1700 = tf.logical_and(
            tf.greater_equal(m14b, 500),
            tf.greater_equal(ratio, 7)
        )
        pred = tf.where(condition_1700, tf.maximum(pred, 1700), pred)
        
        # 1600+ ì‹ í˜¸
        condition_1600 = tf.logical_and(
            tf.greater_equal(m14b, 450),
            tf.greater_equal(ratio, 6)
        )
        pred = tf.where(condition_1600, tf.maximum(pred, 1600), pred)
        
        # 1500+ ì‹ í˜¸
        condition_1500 = tf.logical_and(
            tf.greater_equal(m14b, 400),
            tf.greater_equal(ratio, 5)
        )
        pred = tf.where(condition_1500, tf.maximum(pred, 1500), pred)
        
        # 1400+ ì‹ í˜¸
        condition_1400 = tf.greater_equal(m14b, 320)
        pred = tf.where(condition_1400, tf.maximum(pred, 1400), pred)
        
        # M10A ì—­íŒ¨í„´ ë³´ì •
        condition_inverse = tf.logical_and(
            tf.less(m10a, 70),
            tf.greater_equal(m14b, 250)
        )
        pred = tf.where(condition_inverse, pred * 1.08, pred)
        
        return pred

class WeightedLoss(tf.keras.losses.Loss):
    """ë ˆë²¨ë³„ ê°€ì¤‘ ì†ì‹¤ í•¨ìˆ˜"""
    def __init__(self):
        super().__init__()
        self.mae = tf.keras.losses.MeanAbsoluteError()
        
    def call(self, y_true, y_pred):
        # ë ˆë²¨ë³„ ê°€ì¤‘ì¹˜ ì ìš©
        weights = tf.where(y_true < 1400, 1.0,
                 tf.where(y_true < 1500, 3.0,
                 tf.where(y_true < 1600, 5.0,
                 tf.where(y_true < 1700, 8.0, 10.0))))
        
        # ê°€ì¤‘ MAE
        mae = tf.abs(y_true - y_pred)
        weighted_mae = mae * weights
        
        return tf.reduce_mean(weighted_mae)

# ============================================
# 5. ì½œë°± ë° í‰ê°€
# ============================================
class SpikePerformanceCallback(tf.keras.callbacks.Callback):
    """1400+ êµ¬ê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    def __init__(self, X_val, y_val):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val
        self.best_spike_mae = float('inf')
        
    def on_epoch_end(self, epoch, logs=None):
        # ì˜ˆì¸¡
        y_pred = self.model.predict(self.X_val, verbose=0)
        if isinstance(y_pred, list):
            y_pred = y_pred[0]
        y_pred = y_pred.flatten()
        
        # 1400+ êµ¬ê°„ ì„±ëŠ¥
        spike_mask = self.y_val >= 1400
        if np.any(spike_mask):
            spike_mae = np.mean(np.abs(self.y_val[spike_mask] - y_pred[spike_mask]))
            
            # ë ˆë²¨ë³„ ì„±ëŠ¥
            level_performance = {}
            for level in [1400, 1500, 1600, 1700]:
                level_mask = self.y_val >= level
                if np.any(level_mask):
                    recall = np.sum((y_pred >= level) & level_mask) / np.sum(level_mask)
                    level_performance[f'{level}+'] = recall
            
            # ì¶œë ¥
            if spike_mae < self.best_spike_mae:
                self.best_spike_mae = spike_mae
                print(f"\nğŸ¯ Epoch {epoch+1} - 1400+ MAE: {spike_mae:.2f} (Best!)")
                for level, recall in level_performance.items():
                    print(f"   {level} Recall: {recall:.2%}")

# ============================================
# 6. ë©”ì¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸
# ============================================
class TrainingPipelineV6:
    def __init__(self):
        self.preprocessor = DataPreprocessorV6()
        self.models = {}
        self.history = {}
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(Config.MODEL_DIR, exist_ok=True)
        
    def run(self):
        """ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("\n" + "="*60)
        print("ğŸš€ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("="*60)
        
        # 1. ë°ì´í„° ì¤€ë¹„
        df = self.preprocessor.prepare_data()
        X, y, m14_features = self.preprocessor.create_sequences(df)
        
        # 2. í•™ìŠµ/ê²€ì¦ ë¶„í• 
        X_train, X_val, y_train, y_val, m14_train, m14_val = train_test_split(
            X, y, m14_features, test_size=0.2, random_state=42
        )
        
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
        print(f"  í•™ìŠµ: {X_train.shape[0]}ê°œ")
        print(f"  ê²€ì¦: {X_val.shape[0]}ê°œ")
        
        # 3. ìŠ¤ì¼€ì¼ë§
        print("\nğŸ“ ë°ì´í„° ìŠ¤ì¼€ì¼ë§...")
        
        # ê° íŠ¹ì§•ë³„ ìŠ¤ì¼€ì¼ë§
        n_features = X_train.shape[2]
        for i in range(n_features):
            scaler = RobustScaler()
            X_train[:, :, i] = scaler.fit_transform(X_train[:, :, i].reshape(-1, 1)).reshape(X_train[:, :, i].shape)
            X_val[:, :, i] = scaler.transform(X_val[:, :, i].reshape(-1, 1)).reshape(X_val[:, :, i].shape)
        
        # 4. ëª¨ë¸ í•™ìŠµ
        self._train_models(X_train, y_train, X_val, y_val, m14_train, m14_val)
        
        # 5. í‰ê°€
        self._evaluate_models(X_val, y_val, m14_val)
        
        # 6. ê²°ê³¼ ì €ì¥
        self._save_results()
        
        print("\n" + "="*60)
        print("âœ… í•™ìŠµ ì™„ë£Œ!")
        print("="*60)
    
    def _train_models(self, X_train, y_train, X_val, y_val, m14_train, m14_val):
        """ëª¨ë¸ í•™ìŠµ"""
        print("\nğŸ‹ï¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        
        # 1. GRU ëª¨ë¸
        print("\n1ï¸âƒ£ Enhanced GRU í•™ìŠµ")
        gru_model = ModelsV6.build_enhanced_gru(X_train.shape[1:])
        gru_model.compile(
            optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
            loss=WeightedLoss(),
            metrics=['mae']
        )
        
        gru_history = gru_model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=Config.EPOCHS,
            batch_size=Config.BATCH_SIZE,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(patience=Config.PATIENCE, restore_best_weights=True),
                tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5),
                SpikePerformanceCallback(X_val, y_val)
            ],
            verbose=1
        )
        
        self.models['gru'] = gru_model
        self.history['gru'] = gru_history
        
        # 2. Spike Detector ëª¨ë¸
        print("\n2ï¸âƒ£ Spike Detector í•™ìŠµ")
        spike_model = ModelsV6.build_spike_detector(X_train.shape[1:])
        
        # 1400+ ì—¬ë¶€ ë ˆì´ë¸” ìƒì„±
        y_spike_class = (y_train >= 1400).astype(float)
        y_val_spike_class = (y_val >= 1400).astype(float)
        
        spike_model.compile(
            optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
            loss={
                'spike_value': WeightedLoss(),
                'spike_prob': 'binary_crossentropy'
            },
            loss_weights={
                'spike_value': 1.0,
                'spike_prob': 0.5
            },
            metrics={
                'spike_value': 'mae',
                'spike_prob': 'accuracy'
            }
        )
        
        spike_history = spike_model.fit(
            X_train, 
            {'spike_value': y_train, 'spike_prob': y_spike_class},
            validation_data=(
                X_val, 
                {'spike_value': y_val, 'spike_prob': y_val_spike_class}
            ),
            epochs=Config.EPOCHS,
            batch_size=Config.BATCH_SIZE,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(patience=Config.PATIENCE, restore_best_weights=True),
                tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5)
            ],
            verbose=1
        )
        
        self.models['spike'] = spike_model
        self.history['spike'] = spike_history
        
        # 3. ì•™ìƒë¸” ëª¨ë¸
        print("\n3ï¸âƒ£ ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±")
        ensemble_model = ModelsV6.build_ensemble_model(
            gru_model, 
            spike_model,
            m14_train.shape[1]
        )
        
        ensemble_model.compile(
            optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * 0.5),
            loss={
                'tf.math.add': WeightedLoss(),
                'spike_prob': 'binary_crossentropy'
            },
            metrics=['mae']
        )
        
        # ì•™ìƒë¸” íŒŒì¸íŠœë‹
        ensemble_history = ensemble_model.fit(
            [X_train, m14_train],
            [y_train, y_spike_class],
            validation_data=(
                [X_val, m14_val],
                [y_val, y_val_spike_class]
            ),
            epochs=20,
            batch_size=Config.BATCH_SIZE,
            verbose=1
        )
        
        self.models['ensemble'] = ensemble_model
        self.history['ensemble'] = ensemble_history
    
    def _evaluate_models(self, X_val, y_val, m14_val):
        """ëª¨ë¸ í‰ê°€"""
        print("\nğŸ“Š ëª¨ë¸ í‰ê°€")
        print("="*60)
        
        results = {}
        
        for name, model in self.models.items():
            if name == 'ensemble':
                pred = model.predict([X_val, m14_val], verbose=0)[0].flatten()
            else:
                pred = model.predict(X_val, verbose=0)
                if isinstance(pred, list):
                    pred = pred[0]
                pred = pred.flatten()
            
            # ì „ì²´ ì„±ëŠ¥
            mae = np.mean(np.abs(y_val - pred))
            
            # êµ¬ê°„ë³„ ì„±ëŠ¥
            level_performance = {}
            for level in [1400, 1500, 1600, 1700]:
                mask = y_val >= level
                if np.any(mask):
                    recall = np.sum((pred >= level) & mask) / np.sum(mask)
                    level_mae = np.mean(np.abs(y_val[mask] - pred[mask]))
                    level_performance[level] = {
                        'recall': recall,
                        'mae': level_mae,
                        'count': np.sum(mask)
                    }
            
            results[name] = {
                'overall_mae': mae,
                'levels': level_performance
            }
            
            # ì¶œë ¥
            print(f"\nğŸ¯ {name.upper()} ëª¨ë¸:")
            print(f"  ì „ì²´ MAE: {mae:.2f}")
            for level, perf in level_performance.items():
                print(f"  {level}+: Recall={perf['recall']:.2%}, MAE={perf['mae']:.1f} (n={perf['count']})")
        
        self.evaluation_results = results
        
        # ìµœì¢… ì„ íƒ
        best_model = min(results.keys(), key=lambda x: results[x]['overall_mae'])
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_model.upper()} ëª¨ë¸")
    
    def _save_results(self):
        """ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥"""
        print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        
        # ëª¨ë¸ ì €ì¥
        for name, model in self.models.items():
            model.save(f"{Config.MODEL_DIR}{name}_model.h5")
            print(f"  {name}_model.h5 ì €ì¥ ì™„ë£Œ")
        
        # í‰ê°€ ê²°ê³¼ ì €ì¥
        with open(f"{Config.MODEL_DIR}evaluation_results.json", 'w') as f:
            json.dump(self.evaluation_results, f, indent=2)
        
        # ì„¤ì • ì €ì¥
        config_dict = {k: v for k, v in Config.__dict__.items() if not k.startswith('_')}
        with open(f"{Config.MODEL_DIR}config.json", 'w') as f:
            json.dump(config_dict, f, indent=2)
        
        print("  ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ")
    
    def visualize_results(self):
        """ê²°ê³¼ ì‹œê°í™”"""
        print("\nğŸ“ˆ ê²°ê³¼ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. í•™ìŠµ ê³¡ì„ 
        for idx, (name, history) in enumerate(self.history.items()):
            if idx < 3:
                ax = axes[idx//2, idx%2]
                ax.plot(history.history['loss'], label='Train Loss')
                ax.plot(history.history['val_loss'], label='Val Loss')
                ax.set_title(f'{name.upper()} Learning Curve')
                ax.set_xlabel('Epoch')
                ax.set_ylabel('Loss')
                ax.legend()
                ax.grid(True)
        
        # 4. ì„±ëŠ¥ ë¹„êµ
        ax = axes[1, 1]
        models = list(self.evaluation_results.keys())
        maes = [self.evaluation_results[m]['overall_mae'] for m in models]
        
        bars = ax.bar(models, maes, color=['blue', 'green', 'red'])
        ax.set_title('Model Performance Comparison')
        ax.set_ylabel('MAE')
        ax.set_ylim(0, max(maes) * 1.2)
        
        # ê°’ í‘œì‹œ
        for bar, mae in zip(bars, maes):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{mae:.1f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(f"{Config.MODEL_DIR}training_results.png", dpi=100)
        print("  training_results.png ì €ì¥ ì™„ë£Œ")
        plt.show()

# ============================================
# 7. ì‹¤í–‰
# ============================================
if __name__ == "__main__":
    # GPU ì„¤ì •
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"ğŸ® GPU ì‚¬ìš©: {len(gpus)}ê°œ")
        except RuntimeError as e:
            print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
    else:
        print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = TrainingPipelineV6()
    pipeline.run()
    pipeline.visualize_results()
    
    print("\n" + "="*60)
    print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {Config.MODEL_DIR}")
    print(f"ğŸ“‚ í•™ìŠµ ë°ì´í„°: {Config.DATA_FILE}")
    print("="*60)