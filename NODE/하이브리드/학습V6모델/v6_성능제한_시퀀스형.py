"""
ensemble_only_v6.py - ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„± ì „ìš©
ì´ë¯¸ í•™ìŠµëœ ê°œë³„ ëª¨ë¸ë“¤ì„ ë¡œë“œí•˜ì—¬ ì•™ìƒë¸” êµ¬ì„±
TensorFlow 2.15.0
"""

import tensorflow as tf
import numpy as np
import json
import os
import gc
import warnings
warnings.filterwarnings('ignore')

print("="*60)
print("ğŸ¯ ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„± - ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ í™œìš©")
print(f"ğŸ“¦ TensorFlow ë²„ì „: {tf.__version__}")
print("="*60)

# GPU ë©”ëª¨ë¦¬ ì„¤ì •
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# ============================================
# 1. ì„¤ì •
# ============================================
class Config:
    # ë°ì´í„° íŒŒì¼
    SEQUENCE_FILE = './sequences_v6.npz'
    
    # ëª¨ë¸ ê²½ë¡œ
    MODEL_DIR = './models_v6/'
    
    # M14 ì„ê³„ê°’
    M14B_THRESHOLDS = {
        1400: 320,
        1500: 400,
        1600: 450,
        1700: 500
    }
    
    RATIO_THRESHOLDS = {
        1400: 4,
        1500: 5,
        1600: 6,
        1700: 7
    }
    
    # ì•™ìƒë¸” í•™ìŠµ ì„¤ì •
    BATCH_SIZE = 16
    ENSEMBLE_EPOCHS = 30
    LEARNING_RATE = 0.0005

# ============================================
# 2. ì»¤ìŠ¤í…€ ë ˆì´ì–´ ì •ì˜ (ì•™ìƒë¸”ìš©)
# ============================================
class M14RuleCorrection(tf.keras.layers.Layer):
    """M14 ê·œì¹™ ê¸°ë°˜ ë³´ì • ë ˆì´ì–´"""
    def __init__(self):
        super().__init__()
        
    def call(self, inputs):
        pred, m14_features = inputs
        
        # M14 íŠ¹ì§• ë¶„í•´
        m14b = m14_features[:, 0:1]
        m10a = m14_features[:, 1:2]
        ratio = m14_features[:, 3:4] if m14_features.shape[1] > 3 else tf.ones_like(m14b)
        
        # ê·œì¹™ ê¸°ë°˜ ë³´ì •
        # 1700+ ì‹ í˜¸
        condition_1700 = tf.logical_and(
            tf.greater_equal(m14b, Config.M14B_THRESHOLDS[1700]),
            tf.greater_equal(ratio, Config.RATIO_THRESHOLDS[1700])
        )
        pred = tf.where(condition_1700, tf.maximum(pred, 1700), pred)
        
        # 1600+ ì‹ í˜¸
        condition_1600 = tf.logical_and(
            tf.greater_equal(m14b, Config.M14B_THRESHOLDS[1600]),
            tf.greater_equal(ratio, Config.RATIO_THRESHOLDS[1600])
        )
        pred = tf.where(condition_1600, tf.maximum(pred, 1600), pred)
        
        # 1500+ ì‹ í˜¸
        condition_1500 = tf.logical_and(
            tf.greater_equal(m14b, Config.M14B_THRESHOLDS[1500]),
            tf.greater_equal(ratio, Config.RATIO_THRESHOLDS[1500])
        )
        pred = tf.where(condition_1500, tf.maximum(pred, 1500), pred)
        
        # 1400+ ì‹ í˜¸
        condition_1400 = tf.greater_equal(m14b, Config.M14B_THRESHOLDS[1400])
        pred = tf.where(condition_1400, tf.maximum(pred, 1400), pred)
        
        # M10A ì—­íŒ¨í„´ ë³´ì •
        condition_inverse = tf.logical_and(
            tf.less(m10a, 70),
            tf.greater_equal(m14b, 250)
        )
        pred = tf.where(condition_inverse, pred * 1.08, pred)
        
        return pred

class WeightedLoss(tf.keras.losses.Loss):
    """ë ˆë²¨ë³„ ê°€ì¤‘ ì†ì‹¤ í•¨ìˆ˜"""
    def __init__(self):
        super().__init__()
        
    def call(self, y_true, y_pred):
        # ë ˆë²¨ë³„ ê°€ì¤‘ì¹˜
        weights = tf.where(y_true < 1400, 1.0,
                 tf.where(y_true < 1500, 3.0,
                 tf.where(y_true < 1600, 5.0,
                 tf.where(y_true < 1700, 8.0, 10.0))))
        
        # ê°€ì¤‘ MAE
        mae = tf.abs(y_true - y_pred)
        weighted_mae = mae * weights
        
        return tf.reduce_mean(weighted_mae)

# ============================================
# 3. ë°ì´í„° ë¡œë“œ (í‰ê°€ìš© ì†ŒëŸ‰ë§Œ)
# ============================================
print("\nğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")

# ì „ì²´ ë°ì´í„°ê°€ ì•„ë‹Œ ê²€ì¦ìš© ì¼ë¶€ë§Œ ë¡œë“œ
data = np.load(Config.SEQUENCE_FILE)
X_val = data['X'][-10000:].astype(np.float32)  # ë§ˆì§€ë§‰ 10,000ê°œë§Œ
y_val = data['y'][-10000:].astype(np.float32)
m14_val = data['m14_features'][-10000:].astype(np.float32)

print(f"  ê²€ì¦ ë°ì´í„° shape: {X_val.shape}")
print(f"  1400+ ë¹„ìœ¨: {(y_val >= 1400).mean():.1%}")

# ============================================
# 4. ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
# ============================================
print("\nğŸ“¥ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")

models = {}

# LSTM ëª¨ë¸ ë¡œë“œ
try:
    lstm_path = f"{Config.MODEL_DIR}lstm_model.h5"
    if os.path.exists(lstm_path):
        models['lstm'] = tf.keras.models.load_model(lstm_path, custom_objects={'WeightedLoss': WeightedLoss})
        print("  âœ… LSTM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    else:
        print(f"  âš ï¸ LSTM ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {lstm_path}")
except Exception as e:
    print(f"  âŒ LSTM ë¡œë“œ ì‹¤íŒ¨: {e}")

# GRU ëª¨ë¸ ë¡œë“œ
try:
    gru_path = f"{Config.MODEL_DIR}gru_model.h5"
    if os.path.exists(gru_path):
        models['gru'] = tf.keras.models.load_model(gru_path, custom_objects={'WeightedLoss': WeightedLoss})
        print("  âœ… GRU ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    else:
        print(f"  âš ï¸ GRU ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {gru_path}")
except Exception as e:
    print(f"  âŒ GRU ë¡œë“œ ì‹¤íŒ¨: {e}")

# CNN-LSTM ëª¨ë¸ ë¡œë“œ
try:
    cnn_lstm_path = f"{Config.MODEL_DIR}cnn_lstm_model.h5"
    if os.path.exists(cnn_lstm_path):
        models['cnn_lstm'] = tf.keras.models.load_model(cnn_lstm_path, custom_objects={'WeightedLoss': WeightedLoss})
        print("  âœ… CNN-LSTM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    else:
        print(f"  âš ï¸ CNN-LSTM ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {cnn_lstm_path}")
except Exception as e:
    print(f"  âŒ CNN-LSTM ë¡œë“œ ì‹¤íŒ¨: {e}")

# Spike Detector ëª¨ë¸ ë¡œë“œ
try:
    spike_path = f"{Config.MODEL_DIR}spike_model.h5"
    if os.path.exists(spike_path):
        models['spike'] = tf.keras.models.load_model(spike_path, custom_objects={'WeightedLoss': WeightedLoss})
        print("  âœ… Spike Detector ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    else:
        print(f"  âš ï¸ Spike Detector ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {spike_path}")
except Exception as e:
    print(f"  âŒ Spike Detector ë¡œë“œ ì‹¤íŒ¨: {e}")

print(f"\nğŸ’¡ ë¡œë“œëœ ëª¨ë¸ ìˆ˜: {len(models)}ê°œ")

if len(models) == 0:
    print("âŒ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ê°œë³„ ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµí•´ì£¼ì„¸ìš”.")
    exit(1)

# ============================================
# 5. ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±
# ============================================
print("\nğŸ”§ ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„± ì¤‘...")

# ì…ë ¥ ì •ì˜
time_series_input = tf.keras.Input(shape=(100, 5), name='ensemble_input')
m14_input = tf.keras.Input(shape=(4,), name='m14_features')

# ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ ìˆ˜ì§‘
predictions = []
model_names = []

# LSTM ì˜ˆì¸¡
if 'lstm' in models:
    lstm_pred = models['lstm'](time_series_input)
    predictions.append(lstm_pred)
    model_names.append('lstm')

# GRU ì˜ˆì¸¡
if 'gru' in models:
    gru_pred = models['gru'](time_series_input)
    predictions.append(gru_pred)
    model_names.append('gru')

# CNN-LSTM ì˜ˆì¸¡
if 'cnn_lstm' in models:
    cnn_lstm_pred = models['cnn_lstm'](time_series_input)
    predictions.append(cnn_lstm_pred)
    model_names.append('cnn_lstm')

# Spike Detector ì˜ˆì¸¡
spike_prob = None
if 'spike' in models:
    spike_outputs = models['spike'](time_series_input)
    if isinstance(spike_outputs, list):
        spike_pred = spike_outputs[0]
        spike_prob = spike_outputs[1]
    else:
        spike_pred = spike_outputs
    predictions.append(spike_pred)
    model_names.append('spike')

print(f"  ì•™ìƒë¸”ì— í¬í•¨ëœ ëª¨ë¸: {model_names}")

# ì•™ìƒë¸” ì˜ˆì¸¡ê°’ ê³„ì‚°
if len(predictions) > 1:
    # ê°€ì¤‘ í‰ê·  ê³„ì‚°
    # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì„¤ì •
    weights = {
        'lstm': 0.25,
        'gru': 0.20,
        'cnn_lstm': 0.25,
        'spike': 0.30
    }
    
    # ì‹¤ì œ ì‚¬ìš©í•  ê°€ì¤‘ì¹˜ ê³„ì‚°
    used_weights = [weights.get(name, 0.25) for name in model_names]
    total_weight = sum(used_weights)
    normalized_weights = [w/total_weight for w in used_weights]
    
    print(f"  ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜: {dict(zip(model_names, normalized_weights))}")
    
    # ê°€ì¤‘ í‰ê·  ì ìš©
    weighted_preds = []
    for pred, weight in zip(predictions, normalized_weights):
        weighted_pred = tf.keras.layers.Lambda(lambda x: x * weight)(pred)
        weighted_preds.append(weighted_pred)
    
    ensemble_pred = tf.keras.layers.Add()(weighted_preds)
else:
    # ëª¨ë¸ì´ í•˜ë‚˜ë§Œ ìˆëŠ” ê²½ìš°
    ensemble_pred = predictions[0]

# M14 ê·œì¹™ ê¸°ë°˜ ë³´ì •
final_pred = M14RuleCorrection()([ensemble_pred, m14_input])

# ì•™ìƒë¸” ëª¨ë¸ ìƒì„±
if spike_prob is not None:
    # Spike í™•ë¥  ì¶œë ¥ë„ í¬í•¨
    ensemble_model = tf.keras.Model(
        inputs=[time_series_input, m14_input],
        outputs=[final_pred, spike_prob],
        name='Ensemble_Model'
    )
else:
    # ì˜ˆì¸¡ê°’ë§Œ ì¶œë ¥
    ensemble_model = tf.keras.Model(
        inputs=[time_series_input, m14_input],
        outputs=final_pred,
        name='Ensemble_Model'
    )

print("  âœ… ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„± ì™„ë£Œ")

# ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
ensemble_model.summary()

# ============================================
# 6. ì•™ìƒë¸” ëª¨ë¸ ì»´íŒŒì¼ ë° íŒŒì¸íŠœë‹
# ============================================
print("\nğŸ¯ ì•™ìƒë¸” ëª¨ë¸ íŒŒì¸íŠœë‹...")

# ì»´íŒŒì¼
if spike_prob is not None:
    ensemble_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
        loss=[WeightedLoss(), 'binary_crossentropy'],
        loss_weights=[1.0, 0.3],
        metrics=['mae']
    )
else:
    ensemble_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
        loss=WeightedLoss(),
        metrics=['mae']
    )

# í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (ê²€ì¦ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ í•™ìŠµìš©ìœ¼ë¡œ ì‚¬ìš©)
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test, m14_train, m14_test = train_test_split(
    X_val, y_val, m14_val, test_size=0.3, random_state=42
)

print(f"\n  í•™ìŠµ ë°ì´í„°: {X_train.shape[0]:,}ê°œ")
print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]:,}ê°œ")

# íŒŒì¸íŠœë‹
if spike_prob is not None:
    # Spike ë¶„ë¥˜ ë ˆì´ë¸” ìƒì„±
    y_spike_train = (y_train >= 1400).astype(np.float32)
    y_spike_test = (y_test >= 1400).astype(np.float32)
    
    history = ensemble_model.fit(
        [X_train, m14_train],
        [y_train, y_spike_train],
        validation_data=(
            [X_test, m14_test],
            [y_test, y_spike_test]
        ),
        epochs=Config.ENSEMBLE_EPOCHS,
        batch_size=Config.BATCH_SIZE,
        callbacks=[
            tf.keras.callbacks.EarlyStopping(
                patience=10,
                restore_best_weights=True,
                verbose=1
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                patience=5,
                factor=0.5,
                min_lr=1e-6,
                verbose=1
            )
        ],
        verbose=1
    )
else:
    history = ensemble_model.fit(
        [X_train, m14_train],
        y_train,
        validation_data=(
            [X_test, m14_test],
            y_test
        ),
        epochs=Config.ENSEMBLE_EPOCHS,
        batch_size=Config.BATCH_SIZE,
        callbacks=[
            tf.keras.callbacks.EarlyStopping(
                patience=10,
                restore_best_weights=True,
                verbose=1
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                patience=5,
                factor=0.5,
                min_lr=1e-6,
                verbose=1
            )
        ],
        verbose=1
    )

print("\nâœ… ì•™ìƒë¸” íŒŒì¸íŠœë‹ ì™„ë£Œ")

# ============================================
# 7. ëª¨ë¸ í‰ê°€
# ============================================
print("\nğŸ“Š ì•™ìƒë¸” ëª¨ë¸ í‰ê°€...")

# ì˜ˆì¸¡
if spike_prob is not None:
    y_pred, spike_pred = ensemble_model.predict([X_test, m14_test], batch_size=Config.BATCH_SIZE)
    y_pred = y_pred.flatten()
else:
    y_pred = ensemble_model.predict([X_test, m14_test], batch_size=Config.BATCH_SIZE)
    y_pred = y_pred.flatten()

# ì „ì²´ ì„±ëŠ¥
mae = np.mean(np.abs(y_test - y_pred))
print(f"\n  ì „ì²´ MAE: {mae:.2f}")

# êµ¬ê°„ë³„ ì„±ëŠ¥
for level in [1400, 1500, 1600, 1700]:
    mask = y_test >= level
    if np.any(mask):
        recall = np.sum((y_pred >= level) & mask) / np.sum(mask)
        level_mae = np.mean(np.abs(y_test[mask] - y_pred[mask]))
        print(f"  {level}+: Recall={recall:.2%}, MAE={level_mae:.1f} (n={np.sum(mask)})")

# ============================================
# 8. ëª¨ë¸ ì €ì¥
# ============================================
print("\nğŸ’¾ ì•™ìƒë¸” ëª¨ë¸ ì €ì¥ ì¤‘...")

# ëª¨ë¸ ì €ì¥
ensemble_path = f"{Config.MODEL_DIR}ensemble_model.h5"
try:
    # ì»¤ìŠ¤í…€ ê°ì²´ì™€ í•¨ê»˜ ì €ì¥
    ensemble_model.save(ensemble_path)
    print(f"  âœ… ì•™ìƒë¸” ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {ensemble_path}")
except Exception as e:
    print(f"  âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    # ëŒ€ì•ˆ: weightsë§Œ ì €ì¥
    try:
        ensemble_model.save_weights(f"{Config.MODEL_DIR}ensemble_weights.h5")
        print(f"  âœ… ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì €ì¥ ì™„ë£Œ")
    except Exception as e2:
        print(f"  âŒ ê°€ì¤‘ì¹˜ ì €ì¥ë„ ì‹¤íŒ¨: {e2}")

# í‰ê°€ ê²°ê³¼ ì €ì¥ (JSON ì§ë ¬í™” ì˜¤ë¥˜ ë°©ì§€)
evaluation_results = {
    'ensemble': {
        'overall_mae': float(mae),
        'model_count': len(models),
        'included_models': model_names,
        'weights': dict(zip(model_names, normalized_weights))
    }
}

# êµ¬ê°„ë³„ ì„±ëŠ¥ ì¶”ê°€
level_results = {}
for level in [1400, 1500, 1600, 1700]:
    mask = y_test >= level
    if np.any(mask):
        recall = np.sum((y_pred >= level) & mask) / np.sum(mask)
        level_mae = np.mean(np.abs(y_test[mask] - y_pred[mask]))
        level_results[f'level_{level}'] = {
            'recall': float(recall),
            'mae': float(level_mae),
            'count': int(np.sum(mask))
        }

evaluation_results['ensemble']['levels'] = level_results

# JSON ì €ì¥
try:
    with open(f"{Config.MODEL_DIR}ensemble_evaluation.json", 'w') as f:
        json.dump(evaluation_results, f, indent=2)
    print("  âœ… í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
except Exception as e:
    print(f"  âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}")

# ============================================
# 9. ì‹œê°í™”
# ============================================
print("\nğŸ“ˆ ê²°ê³¼ ì‹œê°í™”...")

try:
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # í•™ìŠµ ê³¡ì„ 
    if hasattr(history, 'history'):
        loss_key = 'loss' if 'loss' in history.history else 'm14_rule_correction_loss'
        val_loss_key = 'val_loss' if 'val_loss' in history.history else 'val_m14_rule_correction_loss'
        
        ax1.plot(history.history[loss_key], label='Train Loss')
        ax1.plot(history.history[val_loss_key], label='Val Loss')
        ax1.set_title('Ensemble Model Training')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
    
    # ì˜ˆì¸¡ vs ì‹¤ì œ (ìƒ˜í”Œë§)
    sample_size = min(500, len(y_test))
    indices = np.random.choice(len(y_test), sample_size, replace=False)
    
    ax2.scatter(y_test[indices], y_pred[indices], alpha=0.5, s=10)
    ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    ax2.set_xlabel('Actual')
    ax2.set_ylabel('Predicted')
    ax2.set_title(f'Ensemble Predictions (MAE: {mae:.2f})')
    ax2.grid(True, alpha=0.3)
    
    # 1400 ë¼ì¸ í‘œì‹œ
    ax2.axhline(y=1400, color='orange', linestyle='--', alpha=0.7, label='1400 threshold')
    ax2.axvline(x=1400, color='orange', linestyle='--', alpha=0.7)
    
    plt.tight_layout()
    plt.savefig(f"{Config.MODEL_DIR}ensemble_results.png", dpi=150, bbox_inches='tight')
    print("  âœ… ì‹œê°í™” ì €ì¥ ì™„ë£Œ")
    plt.show()
    
except Exception as e:
    print(f"  âŒ ì‹œê°í™” ì‹¤íŒ¨: {e}")

# ============================================
# 10. ìµœì¢… ìš”ì•½
# ============================================
print("\n" + "="*60)
print("ğŸ‰ ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„± ì™„ë£Œ!")
print("="*60)
print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {Config.MODEL_DIR}")
print(f"ğŸ”§ í¬í•¨ëœ ëª¨ë¸: {', '.join(model_names)}")
print(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥: MAE = {mae:.2f}")
print(f"ğŸ’¾ ì €ì¥ëœ íŒŒì¼:")
print(f"  - ensemble_model.h5 (ë˜ëŠ” ensemble_weights.h5)")
print(f"  - ensemble_evaluation.json")
print(f"  - ensemble_results.png")
print("="*60)

# ë©”ëª¨ë¦¬ ì •ë¦¬
del X_val, y_val, m14_val
gc.collect()

print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
print("  1. predict_v6.pyë¡œ ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸")
print("  2. í•„ìš”ì‹œ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì¡°ì •")
print("  3. ì¶”ê°€ ëª¨ë¸ í•™ìŠµ í›„ ì¬êµ¬ì„±")