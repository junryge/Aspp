"""
train_v6_memory_efficient.py - ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµ ì½”ë“œ
BATCH_SIZE=32 ì‚¬ìš© ê°€ëŠ¥ ë²„ì „
ë©”ëª¨ë¦¬ ë§¤í•‘ê³¼ ì œë„ˆë ˆì´í„° ì‚¬ìš©ìœ¼ë¡œ RAM ì‚¬ìš© ìµœì†Œí™”
"""

import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import json
import os
import gc
import warnings
warnings.filterwarnings('ignore')

print("="*60)
print("ğŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ í•™ìŠµ V6 (ë©”ëª¨ë¦¬ íš¨ìœ¨ ë²„ì „)")
print(f"ğŸ“¦ TensorFlow ë²„ì „: {tf.__version__}")
print("ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ ëª¨ë“œ: BATCH_SIZE=32 ì‚¬ìš©")
print("="*60)

# ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
gc.collect()
tf.keras.backend.clear_session()

# ============================================
# 1. ì„¤ì •
# ============================================
class Config:
    # ì‹œí€€ìŠ¤ íŒŒì¼
    SEQUENCE_FILE = './sequences_v6.npz'
    
    # M14 ì„ê³„ê°’
    M14B_THRESHOLDS = {
        1400: 320,
        1500: 400,
        1600: 450,
        1700: 500
    }
    
    RATIO_THRESHOLDS = {
        1400: 4,
        1500: 5,
        1600: 6,
        1700: 7
    }
    
    # í•™ìŠµ ì„¤ì •
    BATCH_SIZE = 32  # 32 ìœ ì§€!
    EPOCHS = 100
    LEARNING_RATE = 0.001
    PATIENCE = 15
    
    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    MODEL_DIR = './models_v6/'
    
    # ê°€ì¤‘ì¹˜ ì„¤ì •
    SPIKE_WEIGHTS = {
        'normal': 1.0,
        'level_1400': 3.0,
        'level_1500': 5.0,
        'level_1600': 8.0,
        'level_1700': 10.0
    }

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(Config.MODEL_DIR, exist_ok=True)

# ============================================
# 2. ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° ì œë„ˆë ˆì´í„°
# ============================================
class MemoryEfficientGenerator(tf.keras.utils.Sequence):
    """ë©”ëª¨ë¦¬ ë§¤í•‘ì„ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° ì œë„ˆë ˆì´í„°"""
    
    def __init__(self, npz_path, indices, batch_size=32, for_ensemble=False):
        self.npz_path = npz_path
        self.indices = indices
        self.batch_size = batch_size
        self.for_ensemble = for_ensemble
        
        # ë©”íƒ€ë°ì´í„°ë§Œ ë©”ëª¨ë¦¬ì— ë¡œë“œ
        with np.load(npz_path) as data:
            self.data_shape = data['X'].shape
            self.n_features = data['X'].shape[2]
            
            # yì™€ m14ëŠ” ì‘ìœ¼ë‹ˆê¹Œ ë©”ëª¨ë¦¬ì— ë¡œë“œ
            self.y_all = data['y'][:]
            self.m14_all = data['m14_features'][:]
    
    def __len__(self):
        return len(self.indices) // self.batch_size
    
    def __getitem__(self, idx):
        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]
        
        # ë°°ì¹˜ë§Œí¼ë§Œ ë©”ëª¨ë¦¬ì— ë¡œë“œ
        with np.load(self.npz_path, mmap_mode='r') as data:
            X_batch = np.array(data['X'][batch_indices], dtype=np.float32)
        
        y_batch = self.y_all[batch_indices]
        
        if self.for_ensemble:
            m14_batch = self.m14_all[batch_indices]
            return [X_batch, m14_batch], y_batch
        else:
            return X_batch, y_batch
    
    def on_epoch_end(self):
        np.random.shuffle(self.indices)

class SpikeGenerator(tf.keras.utils.Sequence):
    """Spike Detectorìš© ì œë„ˆë ˆì´í„° (ì´ì¤‘ ì¶œë ¥)"""
    
    def __init__(self, npz_path, indices, batch_size=32):
        self.npz_path = npz_path
        self.indices = indices
        self.batch_size = batch_size
        
        with np.load(npz_path) as data:
            self.y_all = data['y'][:]
            self.y_spike_class = (self.y_all >= 1400).astype(np.float32)
    
    def __len__(self):
        return len(self.indices) // self.batch_size
    
    def __getitem__(self, idx):
        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]
        
        with np.load(self.npz_path, mmap_mode='r') as data:
            X_batch = np.array(data['X'][batch_indices], dtype=np.float32)
        
        y_batch = self.y_all[batch_indices]
        y_class_batch = self.y_spike_class[batch_indices]
        
        return X_batch, {'spike_value': y_batch, 'spike_prob': y_class_batch}

class EnsembleGenerator(tf.keras.utils.Sequence):
    """ì•™ìƒë¸” ëª¨ë¸ìš© ì œë„ˆë ˆì´í„°"""
    
    def __init__(self, npz_path, indices, batch_size=32):
        self.npz_path = npz_path
        self.indices = indices
        self.batch_size = batch_size
        
        with np.load(npz_path) as data:
            self.y_all = data['y'][:]
            self.m14_all = data['m14_features'][:]
            self.y_spike_class = (self.y_all >= 1400).astype(np.float32)
    
    def __len__(self):
        return len(self.indices) // self.batch_size
    
    def __getitem__(self, idx):
        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]
        
        with np.load(self.npz_path, mmap_mode='r') as data:
            X_batch = np.array(data['X'][batch_indices], dtype=np.float32)
        
        y_batch = self.y_all[batch_indices]
        m14_batch = self.m14_all[batch_indices]
        y_class_batch = self.y_spike_class[batch_indices]
        
        return [X_batch, m14_batch], [y_batch, y_class_batch]

# ============================================
# 3. ë°ì´í„° ì¸ë±ìŠ¤ ì¤€ë¹„
# ============================================
print("\nğŸ“‚ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

# ë©”íƒ€ë°ì´í„°ë§Œ ë¡œë“œ
with np.load(Config.SEQUENCE_FILE) as data:
    total_samples = len(data['y'])
    input_shape = data['X'].shape[1:]  # (100, 47)
    m14_shape = data['m14_features'].shape[1]  # 4
    
    # í†µê³„ ì¶œë ¥ìš©
    y_temp = data['y'][:]
    print(f"  ì „ì²´ ìƒ˜í”Œ: {total_samples:,}ê°œ")
    print(f"  ì…ë ¥ shape: {input_shape}")
    print(f"  1400+ ë¹„ìœ¨: {(y_temp >= 1400).mean():.1%}")
    del y_temp

# í•™ìŠµ/ê²€ì¦ ì¸ë±ìŠ¤ ë¶„í• 
np.random.seed(42)
all_indices = np.arange(total_samples)
np.random.shuffle(all_indices)
split_point = int(total_samples * 0.8)
train_indices = all_indices[:split_point]
val_indices = all_indices[split_point:]

print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
print(f"  í•™ìŠµ: {len(train_indices):,}ê°œ")
print(f"  ê²€ì¦: {len(val_indices):,}ê°œ")

# ============================================
# 4. ëª¨ë¸ ì •ì˜
# ============================================
class ModelsV6:
    
    @staticmethod
    def build_lstm_model(input_shape):
        """LSTM ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape, name='lstm_input')
        
        lstm1 = tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)(inputs)
        lstm2 = tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)(lstm1)
        lstm3 = tf.keras.layers.LSTM(64, dropout=0.2)(lstm2)
        
        dense1 = tf.keras.layers.Dense(128, activation='relu')(lstm3)
        dropout = tf.keras.layers.Dropout(0.3)(dense1)
        dense2 = tf.keras.layers.Dense(64, activation='relu')(dropout)
        
        output = tf.keras.layers.Dense(1, name='lstm_output')(dense2)
        
        model = tf.keras.Model(inputs=inputs, outputs=output, name='LSTM_Model')
        return model
    
    @staticmethod
    def build_enhanced_gru(input_shape):
        """GRU ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape, name='gru_input')
        
        x = tf.keras.layers.LayerNormalization()(inputs)
        
        gru1 = tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2)(x)
        gru2 = tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2)(gru1)
        residual = tf.keras.layers.Add()([gru1, gru2])
        gru3 = tf.keras.layers.GRU(64, dropout=0.2)(residual)
        
        dense1 = tf.keras.layers.Dense(128, activation='relu')(gru3)
        dropout = tf.keras.layers.Dropout(0.3)(dense1)
        dense2 = tf.keras.layers.Dense(64, activation='relu')(dropout)
        
        output = tf.keras.layers.Dense(1, name='gru_output')(dense2)
        
        model = tf.keras.Model(inputs=inputs, outputs=output, name='GRU_Model')
        return model
    
    @staticmethod
    def build_cnn_lstm(input_shape):
        """CNN-LSTM ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape, name='cnn_lstm_input')
        
        conv1 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)
        conv2 = tf.keras.layers.Conv1D(64, 5, activation='relu', padding='same')(inputs)
        conv3 = tf.keras.layers.Conv1D(64, 7, activation='relu', padding='same')(inputs)
        
        concat = tf.keras.layers.Concatenate()([conv1, conv2, conv3])
        pool = tf.keras.layers.MaxPooling1D(pool_size=2)(concat)
        
        lstm1 = tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)(pool)
        lstm2 = tf.keras.layers.LSTM(64, dropout=0.2)(lstm1)
        
        dense1 = tf.keras.layers.Dense(128, activation='relu')(lstm2)
        dropout = tf.keras.layers.Dropout(0.3)(dense1)
        dense2 = tf.keras.layers.Dense(64, activation='relu')(dropout)
        
        output = tf.keras.layers.Dense(1, name='cnn_lstm_output')(dense2)
        
        model = tf.keras.Model(inputs=inputs, outputs=output, name='CNN_LSTM_Model')
        return model
    
    @staticmethod
    def build_spike_detector(input_shape):
        """Spike Detector ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape, name='time_series_input')
        
        conv1 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)
        conv2 = tf.keras.layers.Conv1D(64, 5, activation='relu', padding='same')(inputs)
        conv3 = tf.keras.layers.Conv1D(64, 7, activation='relu', padding='same')(inputs)
        
        concat = tf.keras.layers.Concatenate()([conv1, conv2, conv3])
        norm = tf.keras.layers.BatchNormalization()(concat)
        
        attention = tf.keras.layers.MultiHeadAttention(
            num_heads=4, key_dim=48, dropout=0.2
        )(norm, norm)
        
        lstm = tf.keras.layers.Bidirectional(
            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)
        )(attention)
        
        pooled = tf.keras.layers.GlobalAveragePooling1D()(lstm)
        
        dense1 = tf.keras.layers.Dense(256, activation='relu')(pooled)
        dropout1 = tf.keras.layers.Dropout(0.3)(dense1)
        dense2 = tf.keras.layers.Dense(128, activation='relu')(dropout1)
        dropout2 = tf.keras.layers.Dropout(0.2)(dense2)
        
        regression_output = tf.keras.layers.Dense(1, name='spike_value')(dropout2)
        classification_output = tf.keras.layers.Dense(1, activation='sigmoid', name='spike_prob')(dropout2)
        
        model = tf.keras.Model(
            inputs=inputs,
            outputs=[regression_output, classification_output],
            name='Spike_Detector'
        )
        return model

# ============================================
# 5. ì»¤ìŠ¤í…€ ë ˆì´ì–´ ë° ì†ì‹¤ í•¨ìˆ˜
# ============================================
class M14RuleCorrection(tf.keras.layers.Layer):
    """M14 ê·œì¹™ ê¸°ë°˜ ë³´ì • ë ˆì´ì–´"""
    def __init__(self):
        super().__init__()
        
    def call(self, inputs):
        pred, m14_features = inputs
        
        m14b = m14_features[:, 0:1]
        m10a = m14_features[:, 1:2]
        ratio = m14_features[:, 3:4]
        
        # ê·œì¹™ ê¸°ë°˜ ë³´ì •
        condition_1700 = tf.logical_and(
            tf.greater_equal(m14b, 500),
            tf.greater_equal(ratio, 7)
        )
        pred = tf.where(condition_1700, tf.maximum(pred, 1700), pred)
        
        condition_1600 = tf.logical_and(
            tf.greater_equal(m14b, 450),
            tf.greater_equal(ratio, 6)
        )
        pred = tf.where(condition_1600, tf.maximum(pred, 1600), pred)
        
        condition_1500 = tf.logical_and(
            tf.greater_equal(m14b, 400),
            tf.greater_equal(ratio, 5)
        )
        pred = tf.where(condition_1500, tf.maximum(pred, 1500), pred)
        
        condition_1400 = tf.greater_equal(m14b, 320)
        pred = tf.where(condition_1400, tf.maximum(pred, 1400), pred)
        
        condition_inverse = tf.logical_and(
            tf.less(m10a, 70),
            tf.greater_equal(m14b, 250)
        )
        pred = tf.where(condition_inverse, pred * 1.08, pred)
        
        return pred

class WeightedLoss(tf.keras.losses.Loss):
    """ë ˆë²¨ë³„ ê°€ì¤‘ ì†ì‹¤ í•¨ìˆ˜"""
    def __init__(self):
        super().__init__()
        
    def call(self, y_true, y_pred):
        weights = tf.where(y_true < 1400, 1.0,
                 tf.where(y_true < 1500, 3.0,
                 tf.where(y_true < 1600, 5.0,
                 tf.where(y_true < 1700, 8.0, 10.0))))
        
        mae = tf.abs(y_true - y_pred)
        weighted_mae = mae * weights
        
        return tf.reduce_mean(weighted_mae)

# ============================================
# 6. í•™ìŠµ íŒŒì´í”„ë¼ì¸
# ============================================
print("\n" + "="*60)
print("ğŸ‹ï¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ë©”ëª¨ë¦¬ íš¨ìœ¨ ëª¨ë“œ)")
print("="*60)

models = {}
history = {}
evaluation_results = {}

# ============================================
# 1. LSTM ëª¨ë¸
# ============================================
print("\n1ï¸âƒ£ LSTM ëª¨ë¸ í•™ìŠµ")
gc.collect()

# ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
lstm_checkpoint_path = f"{Config.MODEL_DIR}lstm_checkpoint.h5"

# ì´ì „ ì²´í¬í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ ë¡œë“œ
if os.path.exists(lstm_checkpoint_path):
    print(f"  ğŸ“‚ ì´ì „ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {lstm_checkpoint_path}")
    lstm_model = tf.keras.models.load_model(
        lstm_checkpoint_path,
        custom_objects={'WeightedLoss': WeightedLoss}
    )
else:
    lstm_model = ModelsV6.build_lstm_model(input_shape)
    lstm_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
        loss=WeightedLoss(),
        metrics=['mae']
    )

# ì œë„ˆë ˆì´í„° ìƒì„±
train_gen = MemoryEfficientGenerator(Config.SEQUENCE_FILE, train_indices, Config.BATCH_SIZE)
val_gen = MemoryEfficientGenerator(Config.SEQUENCE_FILE, val_indices, Config.BATCH_SIZE)

# í•™ìŠµ
lstm_history = lstm_model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=Config.EPOCHS,
    callbacks=[
        tf.keras.callbacks.ModelCheckpoint(
            lstm_checkpoint_path,
            save_best_only=True,
            monitor='val_loss',
            verbose=1
        ),
        tf.keras.callbacks.EarlyStopping(patience=Config.PATIENCE, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5)
    ],
    verbose=1
)

models['lstm'] = lstm_model
history['lstm'] = lstm_history
gc.collect()

# ============================================
# 2. GRU ëª¨ë¸
# ============================================
print("\n2ï¸âƒ£ Enhanced GRU ëª¨ë¸ í•™ìŠµ")
gc.collect()

gru_checkpoint_path = f"{Config.MODEL_DIR}gru_checkpoint.h5"

if os.path.exists(gru_checkpoint_path):
    print(f"  ğŸ“‚ ì´ì „ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {gru_checkpoint_path}")
    gru_model = tf.keras.models.load_model(
        gru_checkpoint_path,
        custom_objects={'WeightedLoss': WeightedLoss}
    )
else:
    gru_model = ModelsV6.build_enhanced_gru(input_shape)
    gru_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
        loss=WeightedLoss(),
        metrics=['mae']
    )

# ìƒˆ ì œë„ˆë ˆì´í„° (ì¸ë±ìŠ¤ ì¬ì…”í”Œ)
train_gen = MemoryEfficientGenerator(Config.SEQUENCE_FILE, train_indices, Config.BATCH_SIZE)
val_gen = MemoryEfficientGenerator(Config.SEQUENCE_FILE, val_indices, Config.BATCH_SIZE)

gru_history = gru_model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=Config.EPOCHS,
    callbacks=[
        tf.keras.callbacks.ModelCheckpoint(
            gru_checkpoint_path,
            save_best_only=True,
            monitor='val_loss',
            verbose=1
        ),
        tf.keras.callbacks.EarlyStopping(patience=Config.PATIENCE, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5)
    ],
    verbose=1
)

models['gru'] = gru_model
history['gru'] = gru_history
gc.collect()

# ============================================
# 3. CNN-LSTM ëª¨ë¸
# ============================================
print("\n3ï¸âƒ£ CNN-LSTM ëª¨ë¸ í•™ìŠµ")
gc.collect()

cnn_lstm_checkpoint_path = f"{Config.MODEL_DIR}cnn_lstm_checkpoint.h5"

if os.path.exists(cnn_lstm_checkpoint_path):
    print(f"  ğŸ“‚ ì´ì „ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {cnn_lstm_checkpoint_path}")
    cnn_lstm_model = tf.keras.models.load_model(
        cnn_lstm_checkpoint_path,
        custom_objects={'WeightedLoss': WeightedLoss}
    )
else:
    cnn_lstm_model = ModelsV6.build_cnn_lstm(input_shape)
    cnn_lstm_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
        loss=WeightedLoss(),
        metrics=['mae']
    )

train_gen = MemoryEfficientGenerator(Config.SEQUENCE_FILE, train_indices, Config.BATCH_SIZE)
val_gen = MemoryEfficientGenerator(Config.SEQUENCE_FILE, val_indices, Config.BATCH_SIZE)

cnn_lstm_history = cnn_lstm_model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=Config.EPOCHS,
    callbacks=[
        tf.keras.callbacks.ModelCheckpoint(
            cnn_lstm_checkpoint_path,
            save_best_only=True,
            monitor='val_loss',
            verbose=1
        ),
        tf.keras.callbacks.EarlyStopping(patience=Config.PATIENCE, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5)
    ],
    verbose=1
)

models['cnn_lstm'] = cnn_lstm_model
history['cnn_lstm'] = cnn_lstm_history
gc.collect()

# ============================================
# 4. Spike Detector ëª¨ë¸
# ============================================
print("\n4ï¸âƒ£ Spike Detector ëª¨ë¸ í•™ìŠµ (1400+ íŠ¹í™”)")
gc.collect()

spike_checkpoint_path = f"{Config.MODEL_DIR}spike_checkpoint.h5"

if os.path.exists(spike_checkpoint_path):
    print(f"  ğŸ“‚ ì´ì „ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {spike_checkpoint_path}")
    spike_model = tf.keras.models.load_model(
        spike_checkpoint_path,
        custom_objects={'WeightedLoss': WeightedLoss}
    )
else:
    spike_model = ModelsV6.build_spike_detector(input_shape)
    spike_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
        loss={
            'spike_value': WeightedLoss(),
            'spike_prob': 'binary_crossentropy'
        },
        loss_weights={
            'spike_value': 1.0,
            'spike_prob': 0.5
        },
        metrics={
            'spike_value': 'mae',
            'spike_prob': 'accuracy'
        }
    )

# Spikeìš© ì œë„ˆë ˆì´í„°
train_gen_spike = SpikeGenerator(Config.SEQUENCE_FILE, train_indices, Config.BATCH_SIZE)
val_gen_spike = SpikeGenerator(Config.SEQUENCE_FILE, val_indices, Config.BATCH_SIZE)

spike_history = spike_model.fit(
    train_gen_spike,
    validation_data=val_gen_spike,
    epochs=Config.EPOCHS,
    callbacks=[
        tf.keras.callbacks.ModelCheckpoint(
            spike_checkpoint_path,
            save_best_only=True,
            monitor='val_spike_value_loss',
            verbose=1
        ),
        tf.keras.callbacks.EarlyStopping(patience=Config.PATIENCE, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5)
    ],
    verbose=1
)

models['spike'] = spike_model
history['spike'] = spike_history
gc.collect()

# ============================================
# 5. ìµœì¢… ì•™ìƒë¸” ëª¨ë¸
# ============================================
print("\n5ï¸âƒ£ ìµœì¢… ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±")
gc.collect()

# ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±
time_series_input = tf.keras.Input(shape=input_shape, name='ensemble_input')
m14_input = tf.keras.Input(shape=(m14_shape,), name='m14_features')

# ê° ëª¨ë¸ ì˜ˆì¸¡
lstm_pred = lstm_model(time_series_input)
gru_pred = gru_model(time_series_input)
cnn_lstm_pred = cnn_lstm_model(time_series_input)
spike_pred, spike_prob = spike_model(time_series_input)

# M14 ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜
weight_dense = tf.keras.layers.Dense(32, activation='relu')(m14_input)
weight_dense = tf.keras.layers.Dense(16, activation='relu')(weight_dense)
weights = tf.keras.layers.Dense(4, activation='softmax', name='ensemble_weights')(weight_dense)

# ê°€ì¤‘ í‰ê· 
w_lstm = tf.keras.layers.Lambda(lambda x: x[:, 0:1])(weights)
w_gru = tf.keras.layers.Lambda(lambda x: x[:, 1:2])(weights)
w_cnn = tf.keras.layers.Lambda(lambda x: x[:, 2:3])(weights)
w_spike = tf.keras.layers.Lambda(lambda x: x[:, 3:4])(weights)

weighted_lstm = tf.keras.layers.Multiply()([lstm_pred, w_lstm])
weighted_gru = tf.keras.layers.Multiply()([gru_pred, w_gru])
weighted_cnn = tf.keras.layers.Multiply()([cnn_lstm_pred, w_cnn])
weighted_spike = tf.keras.layers.Multiply()([spike_pred, w_spike])

# ìµœì¢… ì˜ˆì¸¡
ensemble_pred = tf.keras.layers.Add()([weighted_lstm, weighted_gru, weighted_cnn, weighted_spike])

# M14 ê·œì¹™ ë³´ì •
final_pred = M14RuleCorrection()([ensemble_pred, m14_input])

ensemble_model = tf.keras.Model(
    inputs=[time_series_input, m14_input],
    outputs=[final_pred, spike_prob],
    name='Final_Ensemble'
)

ensemble_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * 0.5),
    loss={
        'm14_rule_correction': WeightedLoss(),
        'spike_prob': 'binary_crossentropy'
    },
    loss_weights={
        'm14_rule_correction': 1.0,
        'spike_prob': 0.3
    },
    metrics=['mae']
)

# ì•™ìƒë¸”ìš© ì œë„ˆë ˆì´í„°
train_gen_ensemble = EnsembleGenerator(Config.SEQUENCE_FILE, train_indices, Config.BATCH_SIZE)
val_gen_ensemble = EnsembleGenerator(Config.SEQUENCE_FILE, val_indices, Config.BATCH_SIZE)

print("\nğŸ“Š ì•™ìƒë¸” íŒŒì¸íŠœë‹...")
ensemble_history = ensemble_model.fit(
    train_gen_ensemble,
    validation_data=val_gen_ensemble,
    epochs=20,
    verbose=1
)

models['ensemble'] = ensemble_model
history['ensemble'] = ensemble_history

print("\nâœ… ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

# ============================================
# 7. í‰ê°€ (ê°„ë‹¨ ë²„ì „ - ë©”ëª¨ë¦¬ íš¨ìœ¨)
# ============================================
print("\n" + "="*60)
print("ğŸ“Š ëª¨ë¸ í‰ê°€")
print("="*60)

# í‰ê°€ìš© ì‘ì€ ë°°ì¹˜ë§Œ ë¡œë“œ
with np.load(Config.SEQUENCE_FILE) as data:
    eval_size = min(5000, len(val_indices))
    eval_indices = val_indices[:eval_size]
    
    X_eval = data['X'][eval_indices].astype(np.float32)
    y_eval = data['y'][eval_indices]
    m14_eval = data['m14_features'][eval_indices]

for name, model in models.items():
    if name == 'ensemble':
        pred = model.predict([X_eval, m14_eval], verbose=0)[0].flatten()
    else:
        pred = model.predict(X_eval, verbose=0)
        if isinstance(pred, list):
            pred = pred[0]
        pred = pred.flatten()
    
    # ì „ì²´ ì„±ëŠ¥
    mae = np.mean(np.abs(y_eval - pred))
    
    # êµ¬ê°„ë³„ ì„±ëŠ¥
    level_performance = {}
    for level in [1400, 1500, 1600, 1700]:
        mask = y_eval >= level
        if np.any(mask):
            recall = np.sum((pred >= level) & mask) / np.sum(mask)
            level_mae = np.mean(np.abs(y_eval[mask] - pred[mask]))
            level_performance[level] = {
                'recall': recall,
                'mae': level_mae,
                'count': np.sum(mask)
            }
    
    evaluation_results[name] = {
        'overall_mae': mae,
        'levels': level_performance
    }
    
    # ì¶œë ¥
    print(f"\nğŸ¯ {name.upper()} ëª¨ë¸:")
    print(f"  ì „ì²´ MAE: {mae:.2f}")
    for level, perf in level_performance.items():
        print(f"  {level}+: Recall={perf['recall']:.2%}, MAE={perf['mae']:.1f} (n={perf['count']})")

# ìµœì¢… ì„ íƒ
best_model = min(evaluation_results.keys(), key=lambda x: evaluation_results[x]['overall_mae'])
print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_model.upper()} ëª¨ë¸")

# ============================================
# 8. ëª¨ë¸ ì €ì¥
# ============================================
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")

for name, model in models.items():
    model.save(f"{Config.MODEL_DIR}{name}_model.h5")
    print(f"  {name}_model.h5 ì €ì¥ ì™„ë£Œ")

# í‰ê°€ ê²°ê³¼ ì €ì¥
with open(f"{Config.MODEL_DIR}evaluation_results.json", 'w') as f:
    json.dump(evaluation_results, f, indent=2)

# ì„¤ì • ì €ì¥
config_dict = {k: v for k, v in Config.__dict__.items() if not k.startswith('_')}
with open(f"{Config.MODEL_DIR}config.json", 'w') as f:
    json.dump(config_dict, f, indent=2)

print("  ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ")

# ============================================
# 9. ê°„ë‹¨ ì‹œê°í™”
# ============================================
print("\nğŸ“ˆ ê²°ê³¼ ì‹œê°í™” ìƒì„± ì¤‘...")

fig, axes = plt.subplots(2, 3, figsize=(15, 8))
axes = axes.flatten()

# í•™ìŠµ ê³¡ì„ 
for idx, (name, hist) in enumerate(history.items()):
    if idx < 5:
        ax = axes[idx]
        if hasattr(hist, 'history'):
            loss = hist.history.get('loss', hist.history.get('spike_value_loss', []))
            val_loss = hist.history.get('val_loss', hist.history.get('val_spike_value_loss', []))
            
            if loss and val_loss:
                ax.plot(loss, label='Train')
                ax.plot(val_loss, label='Val')
                ax.set_title(f'{name.upper()}')
                ax.set_xlabel('Epoch')
                ax.set_ylabel('Loss')
                ax.legend()
                ax.grid(True, alpha=0.3)

# MAE ë¹„êµ
ax = axes[5]
model_names = list(evaluation_results.keys())
maes = [evaluation_results[m]['overall_mae'] for m in model_names]
ax.bar(model_names, maes)
ax.set_title('Model MAE Comparison')
ax.set_ylabel('MAE')

plt.suptitle('í•™ìŠµ V6 ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ (ë©”ëª¨ë¦¬ íš¨ìœ¨ ëª¨ë“œ)', fontsize=14)
plt.tight_layout()
plt.savefig(f"{Config.MODEL_DIR}training_results.png", dpi=100)
print("  training_results.png ì €ì¥ ì™„ë£Œ")
plt.show()

print("\n" + "="*60)
print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {Config.MODEL_DIR}")
print(f"ğŸ“‚ ì‹œí€€ìŠ¤ íŒŒì¼: {Config.SEQUENCE_FILE}")
print(f"ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ ëª¨ë“œë¡œ BATCH_SIZE=32 í•™ìŠµ ì„±ê³µ!")
print("="*60)

# ë©”ëª¨ë¦¬ ì •ë¦¬
gc.collect()
tf.keras.backend.clear_session()