"""
train_v6.py - ë©”ëª¨ë¦¬ ìµœì í™” í•™ìŠµ ì½”ë“œ
ë°°ì¹˜ ì²˜ë¦¬ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°•í™”
TensorFlow 2.15.0
"""

import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import json
import os
import gc
import warnings
warnings.filterwarnings('ignore')

print("="*60)
print("ğŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ í•™ìŠµ V6 - ë©”ëª¨ë¦¬ ìµœì í™”")
print(f"ğŸ“¦ TensorFlow ë²„ì „: {tf.__version__}")
print("="*60)

# GPU ë©”ëª¨ë¦¬ ì„¤ì •
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# ============================================
# 1. ì„¤ì •
# ============================================
class Config:
    # ì‹œí€€ìŠ¤ íŒŒì¼
    SEQUENCE_FILE = './sequences_v6.npz'
    
    # M14 ì„ê³„ê°’
    M14B_THRESHOLDS = {
        1400: 320,
        1500: 400,
        1600: 450,
        1700: 500
    }
    
    RATIO_THRESHOLDS = {
        1400: 4,
        1500: 5,
        1600: 6,
        1700: 7
    }
    
    # í•™ìŠµ ì„¤ì • - ë©”ëª¨ë¦¬ ìµœì í™”
    BATCH_SIZE = 16  # 32ì—ì„œ 16ìœ¼ë¡œ ê°ì†Œ
    EPOCHS = 100
    LEARNING_RATE = 0.001
    PATIENCE = 15
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬
    MAX_SAMPLES = 100000  # ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ì œí•œ
    SUBSET_RATIO = 0.3    # ë°ì´í„°ì˜ 30%ë§Œ ì‚¬ìš© (í•„ìš”ì‹œ)
    
    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    MODEL_DIR = './models_v6/'
    
    # ê°€ì¤‘ì¹˜ ì„¤ì •
    SPIKE_WEIGHTS = {
        'normal': 1.0,
        'level_1400': 3.0,
        'level_1500': 5.0,
        'level_1600': 8.0,
        'level_1700': 10.0
    }

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(Config.MODEL_DIR, exist_ok=True)

# ============================================
# 2. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„° ë¡œë“œ
# ============================================
print("\nğŸ“‚ ì‹œí€€ìŠ¤ ë¡œë”© ì¤‘ (ë©”ëª¨ë¦¬ ìµœì í™”)...")

def load_data_efficiently():
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ë°ì´í„° ë¡œë“œ"""
    # ë©”ëª¨ë¦¬ ë§µ ëª¨ë“œë¡œ ë¡œë“œ
    data = np.load(Config.SEQUENCE_FILE, mmap_mode='r')
    
    # ë°ì´í„° í¬ê¸° í™•ì¸
    total_samples = data['X'].shape[0]
    print(f"  ì „ì²´ ìƒ˜í”Œ ìˆ˜: {total_samples:,}")
    
    # ë©”ëª¨ë¦¬ ì œí•œì´ í•„ìš”í•œ ê²½ìš°
    if total_samples > Config.MAX_SAMPLES:
        print(f"  âš ï¸ ë©”ëª¨ë¦¬ ì œí•œìœ¼ë¡œ {Config.MAX_SAMPLES:,}ê°œ ìƒ˜í”Œë§Œ ì‚¬ìš©")
        
        # ëœë¤ ì¸ë±ìŠ¤ ì„ íƒ
        np.random.seed(42)
        indices = np.random.choice(total_samples, Config.MAX_SAMPLES, replace=False)
        indices = np.sort(indices)
        
        # float32ë¡œ ë³€í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
        X = np.array(data['X'][indices], dtype=np.float32)
        y = np.array(data['y'][indices], dtype=np.float32)
        m14_features = np.array(data['m14_features'][indices], dtype=np.float32)
    else:
        # float32ë¡œ ë³€í™˜
        X = np.array(data['X'], dtype=np.float32)
        y = np.array(data['y'], dtype=np.float32)
        m14_features = np.array(data['m14_features'], dtype=np.float32)
    
    print(f"  âœ… ë¡œë“œ ì™„ë£Œ!")
    print(f"  X shape: {X.shape} (dtype: {X.dtype})")
    print(f"  y shape: {y.shape} (dtype: {y.dtype})")
    print(f"  m14_features shape: {m14_features.shape} (dtype: {m14_features.dtype})")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
    memory_usage = (X.nbytes + y.nbytes + m14_features.nbytes) / (1024**3)
    print(f"  ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©: {memory_usage:.2f} GB")
    
    return X, y, m14_features

# ë°ì´í„° ë¡œë“œ
X, y, m14_features = load_data_efficiently()

# ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
gc.collect()

# í•™ìŠµ/ê²€ì¦ ë¶„í•  - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
print("\nğŸ“Š ë°ì´í„° ë¶„í•  ì¤‘...")
try:
    X_train, X_val, y_train, y_val, m14_train, m14_val = train_test_split(
        X, y, m14_features, test_size=0.2, random_state=42
    )
    print("  âœ… ë¶„í•  ì™„ë£Œ!")
except MemoryError:
    print("  âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡± - ë” ì‘ì€ ë°ì´í„°ì…‹ ì‚¬ìš©")
    
    # ë°ì´í„° í¬ê¸° ì¶”ê°€ ê°ì†Œ
    subset_size = int(len(X) * Config.SUBSET_RATIO)
    indices = np.random.choice(len(X), subset_size, replace=False)
    
    X_subset = X[indices]
    y_subset = y[indices]
    m14_subset = m14_features[indices]
    
    # ì›ë³¸ ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ
    del X, y, m14_features
    gc.collect()
    
    X_train, X_val, y_train, y_val, m14_train, m14_val = train_test_split(
        X_subset, y_subset, m14_subset, test_size=0.2, random_state=42
    )
    
    del X_subset, y_subset, m14_subset
    gc.collect()
    print(f"  âœ… ì¶•ì†Œëœ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„í•  ì™„ë£Œ ({subset_size:,}ê°œ ìƒ˜í”Œ)")

# 1400+ ì—¬ë¶€ ë ˆì´ë¸” ìƒì„±
y_spike_class = (y_train >= 1400).astype(np.float32)
y_val_spike_class = (y_val >= 1400).astype(np.float32)

print(f"\nğŸ“Š ìµœì¢… ë°ì´í„°:")
print(f"  í•™ìŠµ: {X_train.shape[0]:,}ê°œ")
print(f"  ê²€ì¦: {X_val.shape[0]:,}ê°œ")
print(f"  1400+ í•™ìŠµ ë¹„ìœ¨: {y_spike_class.mean():.1%}")
print(f"  1400+ ê²€ì¦ ë¹„ìœ¨: {y_val_spike_class.mean():.1%}")

# ============================================
# 3. ê²½ëŸ‰í™”ëœ ëª¨ë¸ ì •ì˜
# ============================================
class LightweightModels:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ê²½ëŸ‰ ëª¨ë¸"""
    
    @staticmethod
    def build_lstm_model(input_shape):
        """ê²½ëŸ‰ LSTM ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape, name='lstm_input')
        
        # ë ˆì´ì–´ í¬ê¸° ê°ì†Œ
        lstm1 = tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.2)(inputs)
        lstm2 = tf.keras.layers.LSTM(32, dropout=0.2)(lstm1)
        
        # Dense layers
        dense1 = tf.keras.layers.Dense(64, activation='relu')(lstm2)
        dropout = tf.keras.layers.Dropout(0.3)(dense1)
        
        # Output
        output = tf.keras.layers.Dense(1, name='lstm_output')(dropout)
        
        model = tf.keras.Model(inputs=inputs, outputs=output, name='LSTM_Model')
        return model
    
    @staticmethod
    def build_gru_model(input_shape):
        """ê²½ëŸ‰ GRU ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape, name='gru_input')
        
        # Layer Normalization
        x = tf.keras.layers.LayerNormalization()(inputs)
        
        # GRU layers
        gru1 = tf.keras.layers.GRU(64, return_sequences=True, dropout=0.2)(x)
        gru2 = tf.keras.layers.GRU(32, dropout=0.2)(gru1)
        
        # Dense layers
        dense1 = tf.keras.layers.Dense(64, activation='relu')(gru2)
        dropout = tf.keras.layers.Dropout(0.3)(dense1)
        
        # Output
        output = tf.keras.layers.Dense(1, name='gru_output')(dropout)
        
        model = tf.keras.Model(inputs=inputs, outputs=output, name='GRU_Model')
        return model
    
    @staticmethod
    def build_cnn_lstm(input_shape):
        """ê²½ëŸ‰ CNN-LSTM ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape, name='cnn_lstm_input')
        
        # CNN layers
        conv1 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(inputs)
        conv2 = tf.keras.layers.Conv1D(32, 5, activation='relu', padding='same')(inputs)
        
        # Concatenate
        concat = tf.keras.layers.Concatenate()([conv1, conv2])
        pool = tf.keras.layers.MaxPooling1D(pool_size=2)(concat)
        
        # LSTM layer
        lstm = tf.keras.layers.LSTM(32, dropout=0.2)(pool)
        
        # Dense layers
        dense1 = tf.keras.layers.Dense(32, activation='relu')(lstm)
        dropout = tf.keras.layers.Dropout(0.3)(dense1)
        
        # Output
        output = tf.keras.layers.Dense(1, name='cnn_lstm_output')(dropout)
        
        model = tf.keras.Model(inputs=inputs, outputs=output, name='CNN_LSTM_Model')
        return model
    
    @staticmethod
    def build_spike_detector(input_shape):
        """ê²½ëŸ‰ ê¸‰ë³€ ê°ì§€ ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape, name='spike_input')
        
        # CNN for pattern detection
        conv = tf.keras.layers.Conv1D(32, 5, activation='relu', padding='same')(inputs)
        
        # Batch normalization
        norm = tf.keras.layers.BatchNormalization()(conv)
        
        # BiLSTM
        lstm = tf.keras.layers.Bidirectional(
            tf.keras.layers.LSTM(32, dropout=0.2)
        )(norm)
        
        # Dense layers
        dense1 = tf.keras.layers.Dense(64, activation='relu')(lstm)
        dropout = tf.keras.layers.Dropout(0.3)(dense1)
        
        # Dual output
        regression_output = tf.keras.layers.Dense(1, name='spike_value')(dropout)
        classification_output = tf.keras.layers.Dense(1, activation='sigmoid', name='spike_prob')(dropout)
        
        model = tf.keras.Model(
            inputs=inputs,
            outputs=[regression_output, classification_output],
            name='Spike_Detector'
        )
        return model

# ============================================
# 4. ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜
# ============================================
class WeightedLoss(tf.keras.losses.Loss):
    """ë ˆë²¨ë³„ ê°€ì¤‘ ì†ì‹¤ í•¨ìˆ˜"""
    def __init__(self):
        super().__init__()
        
    def call(self, y_true, y_pred):
        # ë ˆë²¨ë³„ ê°€ì¤‘ì¹˜
        weights = tf.where(y_true < 1400, 1.0,
                 tf.where(y_true < 1500, 3.0,
                 tf.where(y_true < 1600, 5.0,
                 tf.where(y_true < 1700, 8.0, 10.0))))
        
        # ê°€ì¤‘ MAE
        mae = tf.abs(y_true - y_pred)
        weighted_mae = mae * weights
        
        return tf.reduce_mean(weighted_mae)

# ============================================
# 5. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì½œë°±
# ============================================
class MemoryCallback(tf.keras.callbacks.Callback):
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ ì½œë°±"""
    def on_epoch_end(self, epoch, logs=None):
        # ì£¼ê¸°ì ìœ¼ë¡œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        if epoch % 5 == 0:
            gc.collect()
            if gpus:
                tf.keras.backend.clear_session()

class SimpleSpikeCallback(tf.keras.callbacks.Callback):
    """ê°„ë‹¨í•œ 1400+ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    def __init__(self, X_val, y_val, check_interval=10):
        super().__init__()
        self.X_val = X_val[:1000]  # ê²€ì¦ìš© ìƒ˜í”Œ ì œí•œ
        self.y_val = y_val[:1000]
        self.check_interval = check_interval
        
    def on_epoch_end(self, epoch, logs=None):
        if epoch % self.check_interval == 0:
            # ì˜ˆì¸¡
            y_pred = self.model.predict(self.X_val, verbose=0, batch_size=Config.BATCH_SIZE)
            if isinstance(y_pred, list):
                y_pred = y_pred[0]
            y_pred = y_pred.flatten()
            
            # 1400+ êµ¬ê°„ ì„±ëŠ¥
            spike_mask = self.y_val >= 1400
            if np.any(spike_mask):
                spike_mae = np.mean(np.abs(self.y_val[spike_mask] - y_pred[spike_mask]))
                recall = np.sum((y_pred >= 1400) & spike_mask) / np.sum(spike_mask)
                print(f"  [Epoch {epoch+1}] 1400+ MAE: {spike_mae:.1f}, Recall: {recall:.1%}")

# ============================================
# 6. ë°ì´í„° ìƒì„±ê¸° (ë©”ëª¨ë¦¬ íš¨ìœ¨)
# ============================================
class DataGenerator(tf.keras.utils.Sequence):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° ìƒì„±ê¸°"""
    def __init__(self, X, y, m14=None, batch_size=32, shuffle=True):
        self.X = X
        self.y = y
        self.m14 = m14
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indices = np.arange(len(X))
        self.on_epoch_end()
    
    def __len__(self):
        return int(np.ceil(len(self.X) / self.batch_size))
    
    def __getitem__(self, index):
        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]
        
        X_batch = self.X[indices]
        y_batch = self.y[indices]
        
        if self.m14 is not None:
            m14_batch = self.m14[indices]
            return [X_batch, m14_batch], y_batch
        
        return X_batch, y_batch
    
    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indices)

# ============================================
# 7. í•™ìŠµ íŒŒì´í”„ë¼ì¸
# ============================================
print("\n" + "="*60)
print("ğŸ‹ï¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ë©”ëª¨ë¦¬ ìµœì í™”)")
print("="*60)

models = {}
history = {}
evaluation_results = {}

# ë°ì´í„° ìƒì„±ê¸° ìƒì„±
train_gen = DataGenerator(X_train, y_train, batch_size=Config.BATCH_SIZE)
val_gen = DataGenerator(X_val, y_val, batch_size=Config.BATCH_SIZE, shuffle=False)

# ============================================
# 1. LSTM ëª¨ë¸
# ============================================
print("\n1ï¸âƒ£ LSTM ëª¨ë¸ í•™ìŠµ")

try:
    lstm_model = LightweightModels.build_lstm_model(X_train.shape[1:])
    lstm_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
        loss=WeightedLoss(),
        metrics=['mae']
    )
    
    lstm_history = lstm_model.fit(
        train_gen,
        validation_data=val_gen,
        epochs=Config.EPOCHS,
        callbacks=[
            tf.keras.callbacks.ModelCheckpoint(
                f"{Config.MODEL_DIR}lstm_checkpoint.h5",
                save_best_only=True,
                monitor='val_loss',
                verbose=0
            ),
            tf.keras.callbacks.EarlyStopping(
                patience=Config.PATIENCE, 
                restore_best_weights=True,
                verbose=1
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                patience=5, 
                factor=0.5,
                verbose=0
            ),
            MemoryCallback(),
            SimpleSpikeCallback(X_val, y_val)
        ],
        verbose=1
    )
    
    models['lstm'] = lstm_model
    history['lstm'] = lstm_history
    print("  âœ… LSTM í•™ìŠµ ì™„ë£Œ")
    
except Exception as e:
    print(f"  âŒ LSTM í•™ìŠµ ì‹¤íŒ¨: {e}")
    
# ë©”ëª¨ë¦¬ ì •ë¦¬
gc.collect()

# ============================================
# 2. GRU ëª¨ë¸
# ============================================
print("\n2ï¸âƒ£ GRU ëª¨ë¸ í•™ìŠµ")

try:
    gru_model = LightweightModels.build_gru_model(X_train.shape[1:])
    gru_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
        loss=WeightedLoss(),
        metrics=['mae']
    )
    
    gru_history = gru_model.fit(
        train_gen,
        validation_data=val_gen,
        epochs=Config.EPOCHS,
        callbacks=[
            tf.keras.callbacks.ModelCheckpoint(
                f"{Config.MODEL_DIR}gru_checkpoint.h5",
                save_best_only=True,
                monitor='val_loss',
                verbose=0
            ),
            tf.keras.callbacks.EarlyStopping(
                patience=Config.PATIENCE,
                restore_best_weights=True,
                verbose=1
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                patience=5,
                factor=0.5,
                verbose=0
            ),
            MemoryCallback(),
            SimpleSpikeCallback(X_val, y_val)
        ],
        verbose=1
    )
    
    models['gru'] = gru_model
    history['gru'] = gru_history
    print("  âœ… GRU í•™ìŠµ ì™„ë£Œ")
    
except Exception as e:
    print(f"  âŒ GRU í•™ìŠµ ì‹¤íŒ¨: {e}")

# ë©”ëª¨ë¦¬ ì •ë¦¬
gc.collect()

# ============================================
# 3. CNN-LSTM ëª¨ë¸
# ============================================
print("\n3ï¸âƒ£ CNN-LSTM ëª¨ë¸ í•™ìŠµ")

try:
    cnn_lstm_model = LightweightModels.build_cnn_lstm(X_train.shape[1:])
    cnn_lstm_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
        loss=WeightedLoss(),
        metrics=['mae']
    )
    
    cnn_lstm_history = cnn_lstm_model.fit(
        train_gen,
        validation_data=val_gen,
        epochs=Config.EPOCHS,
        callbacks=[
            tf.keras.callbacks.ModelCheckpoint(
                f"{Config.MODEL_DIR}cnn_lstm_checkpoint.h5",
                save_best_only=True,
                monitor='val_loss',
                verbose=0
            ),
            tf.keras.callbacks.EarlyStopping(
                patience=Config.PATIENCE,
                restore_best_weights=True,
                verbose=1
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                patience=5,
                factor=0.5,
                verbose=0
            ),
            MemoryCallback()
        ],
        verbose=1
    )
    
    models['cnn_lstm'] = cnn_lstm_model
    history['cnn_lstm'] = cnn_lstm_history
    print("  âœ… CNN-LSTM í•™ìŠµ ì™„ë£Œ")
    
except Exception as e:
    print(f"  âŒ CNN-LSTM í•™ìŠµ ì‹¤íŒ¨: {e}")

# ë©”ëª¨ë¦¬ ì •ë¦¬
gc.collect()

# ============================================
# 4. Spike Detector ëª¨ë¸
# ============================================
print("\n4ï¸âƒ£ Spike Detector ëª¨ë¸ í•™ìŠµ")

try:
    # Spike ë°ì´í„° ìƒì„±ê¸°
    spike_train_gen = DataGenerator(
        X_train, 
        {'spike_value': y_train, 'spike_prob': y_spike_class},
        batch_size=Config.BATCH_SIZE
    )
    spike_val_gen = DataGenerator(
        X_val,
        {'spike_value': y_val, 'spike_prob': y_val_spike_class},
        batch_size=Config.BATCH_SIZE,
        shuffle=False
    )
    
    spike_model = LightweightModels.build_spike_detector(X_train.shape[1:])
    spike_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
        loss={
            'spike_value': WeightedLoss(),
            'spike_prob': 'binary_crossentropy'
        },
        loss_weights={
            'spike_value': 1.0,
            'spike_prob': 0.5
        },
        metrics={
            'spike_value': 'mae',
            'spike_prob': 'accuracy'
        }
    )
    
    # ì»¤ìŠ¤í…€ fit êµ¬í˜„ (ë©”ëª¨ë¦¬ íš¨ìœ¨)
    spike_history = {'loss': [], 'val_loss': []}
    
    for epoch in range(min(Config.EPOCHS, 50)):  # ìµœëŒ€ 50 ì—í­
        print(f"  Epoch {epoch+1}/{min(Config.EPOCHS, 50)}")
        
        # í•™ìŠµ
        train_loss = []
        for batch_idx in range(len(train_gen)):
            X_batch, y_batch = train_gen[batch_idx]
            
            # y_batchê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
            if not isinstance(y_batch, dict):
                y_batch = {'spike_value': y_batch, 'spike_prob': y_spike_class[batch_idx*Config.BATCH_SIZE:(batch_idx+1)*Config.BATCH_SIZE]}
            
            loss = spike_model.train_on_batch(X_batch, y_batch)
            train_loss.append(loss[0] if isinstance(loss, list) else loss)
        
        # ê²€ì¦
        val_loss = []
        for batch_idx in range(len(val_gen)):
            X_batch, y_batch = val_gen[batch_idx]
            
            # y_batchê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
            if not isinstance(y_batch, dict):
                y_batch = {'spike_value': y_batch, 'spike_prob': y_val_spike_class[batch_idx*Config.BATCH_SIZE:(batch_idx+1)*Config.BATCH_SIZE]}
            
            loss = spike_model.test_on_batch(X_batch, y_batch)
            val_loss.append(loss[0] if isinstance(loss, list) else loss)
        
        avg_train_loss = np.mean(train_loss)
        avg_val_loss = np.mean(val_loss)
        
        spike_history['loss'].append(avg_train_loss)
        spike_history['val_loss'].append(avg_val_loss)
        
        print(f"    Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}")
        
        # Early stopping
        if epoch > 10 and avg_val_loss > min(spike_history['val_loss'][:-1]):
            print("  Early stopping")
            break
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if epoch % 5 == 0:
            gc.collect()
    
    models['spike'] = spike_model
    history['spike'] = type('', (), {'history': spike_history})()
    print("  âœ… Spike Detector í•™ìŠµ ì™„ë£Œ")
    
except Exception as e:
    print(f"  âŒ Spike Detector í•™ìŠµ ì‹¤íŒ¨: {e}")

# ë©”ëª¨ë¦¬ ì •ë¦¬
gc.collect()

print("\nâœ… ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

# ============================================
# 8. í‰ê°€ (ë°°ì¹˜ ì²˜ë¦¬)
# ============================================
print("\n" + "="*60)
print("ğŸ“Š ëª¨ë¸ í‰ê°€")
print("="*60)

# í‰ê°€ìš© ìƒ˜í”Œ ì œí•œ
eval_size = min(5000, len(X_val))
X_eval = X_val[:eval_size]
y_eval = y_val[:eval_size]

for name, model in models.items():
    if model is None:
        continue
        
    # ë°°ì¹˜ ì˜ˆì¸¡
    predictions = []
    for i in range(0, len(X_eval), Config.BATCH_SIZE):
        batch = X_eval[i:i+Config.BATCH_SIZE]
        pred = model.predict(batch, verbose=0)
        if isinstance(pred, list):
            pred = pred[0]
        predictions.append(pred)
    
    pred = np.concatenate(predictions, axis=0).flatten()
    
    # ì „ì²´ ì„±ëŠ¥
    mae = np.mean(np.abs(y_eval - pred))
    
    # êµ¬ê°„ë³„ ì„±ëŠ¥
    level_performance = {}
    for level in [1400, 1500, 1600, 1700]:
        mask = y_eval >= level
        if np.any(mask):
            recall = np.sum((pred >= level) & mask) / np.sum(mask)
            level_mae = np.mean(np.abs(y_eval[mask] - pred[mask]))
            level_performance[level] = {
                'recall': recall,
                'mae': level_mae,
                'count': np.sum(mask)
            }
    
    evaluation_results[name] = {
        'overall_mae': mae,
        'levels': level_performance
    }
    
    # ì¶œë ¥
    print(f"\nğŸ¯ {name.upper()} ëª¨ë¸:")
    print(f"  ì „ì²´ MAE: {mae:.2f}")
    for level, perf in level_performance.items():
        print(f"  {level}+: Recall={perf['recall']:.2%}, MAE={perf['mae']:.1f} (n={perf['count']})")

# ìµœì¢… ì„ íƒ
if evaluation_results:
    best_model = min(evaluation_results.keys(), key=lambda x: evaluation_results[x]['overall_mae'])
    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_model.upper()} ëª¨ë¸")

# ============================================
# 9. ëª¨ë¸ ì €ì¥
# ============================================
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")

for name, model in models.items():
    if model is not None:
        try:
            model.save(f"{Config.MODEL_DIR}{name}_model.h5")
            print(f"  {name}_model.h5 ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"  {name} ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

# í‰ê°€ ê²°ê³¼ ì €ì¥
with open(f"{Config.MODEL_DIR}evaluation_results.json", 'w') as f:
    json.dump(evaluation_results, f, indent=2)

# ì„¤ì • ì €ì¥
config_dict = {k: v for k, v in Config.__dict__.items() if not k.startswith('_')}
with open(f"{Config.MODEL_DIR}config.json", 'w') as f:
    json.dump(config_dict, f, indent=2)

print("  ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ")

# ============================================
# 10. ê°„ë‹¨í•œ ì‹œê°í™”
# ============================================
print("\nğŸ“ˆ ê²°ê³¼ ì‹œê°í™” ìƒì„± ì¤‘...")

try:
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # í•™ìŠµ ê³¡ì„ 
    for idx, (name, hist) in enumerate(history.items()):
        if idx >= 4:
            break
        ax = axes[idx // 2, idx % 2]
        
        if hasattr(hist, 'history'):
            loss = hist.history.get('loss', [])
            val_loss = hist.history.get('val_loss', [])
            
            if loss and val_loss:
                ax.plot(loss, label='Train Loss')
                ax.plot(val_loss, label='Val Loss')
                ax.set_title(f'{name.upper()} Learning Curve')
                ax.set_xlabel('Epoch')
                ax.set_ylabel('Loss')
                ax.legend()
                ax.grid(True, alpha=0.3)
    
    plt.suptitle('ë©”ëª¨ë¦¬ ìµœì í™” í•™ìŠµ ê²°ê³¼', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f"{Config.MODEL_DIR}training_results.png", dpi=100, bbox_inches='tight')
    print("  training_results.png ì €ì¥ ì™„ë£Œ")
    plt.show()
    
except Exception as e:
    print(f"  ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")

print("\n" + "="*60)
print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {Config.MODEL_DIR}")
print(f"ğŸ“‚ ì‹œí€€ìŠ¤ íŒŒì¼: {Config.SEQUENCE_FILE}")
print("="*60)

# ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
del X_train, X_val, y_train, y_val, m14_train, m14_val
gc.collect()

# GPU ì •ë³´ ì¶œë ¥
if gpus:
    print(f"\nğŸ® GPU ì‚¬ìš©: {len(gpus)}ê°œ")
    for gpu in gpus:
        print(f"  {gpu}")
else:
    print("\nğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰ë¨")

print("\nğŸ’¡ ë©”ëª¨ë¦¬ ìµœì í™” íŒ:")
print("  - BATCH_SIZEë¥¼ ë” ì¤„ì´ê¸° (8 ë˜ëŠ” 4)")
print("  - MAX_SAMPLES ê°’ ì¡°ì •")
print("  - ëª¨ë¸ ë ˆì´ì–´ í¬ê¸° ì¶”ê°€ ì¶•ì†Œ")
print("  - ì‹œí€€ìŠ¤ íŒŒì¼ì„ ë” ì‘ê²Œ ì¬ìƒì„±")