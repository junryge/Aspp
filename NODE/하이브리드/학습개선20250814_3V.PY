"""

def load_and_preprocess_data(data_path, checkpoint_manager, force_reload=False):
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (ì§„í–‰ìƒí™© í‘œì‹œ + ìµœì í™”)"""
    
    # ìºì‹œ í™•ì¸
    if not force_reload:
        cached_data = checkpoint_manager.load_data()
        if cached_data:
            logger.info("âœ… ìºì‹œëœ ë°ì´í„° ì‚¬ìš©")
            return cached_data
    
    logger.info(f"ğŸ“‚ ë°ì´í„° ìƒˆë¡œ ë¡œë”©: {data_path}")
    
    try:
        # ========== 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ==========
        logger.info("ğŸ“Š [1/8] CSV íŒŒì¼ ë¡œë”© ì¤‘...")
        data = pd.read_csv(data_path)
        logger.info(f"   âœ“ ì›ë³¸ ë°ì´í„° í¬ê¸°: {data.shape}")
        
        # ========== 2ë‹¨ê³„: ì‹œê°„ ë³€í™˜ ==========
        logger.info("ğŸ•’ [2/8] ì‹œê°„ ë°ì´í„° ë³€í™˜ ì¤‘...")
        data['CURRTIME'] = pd.to_datetime(data['CURRTIME'], format='%Y%m%d%H%M')
        data['TIME'] = pd.to_datetime(data['TIME'], format='%Y%m%d%H%M')
        
        # í•„ìš” ì»¬ëŸ¼ë§Œ ì„ íƒ
        data = data[['CURRTIME', 'TOTALCNT', 'TIME']]
        data.set_index('CURRTIME', inplace=True)
        logger.info(f"   âœ“ ì»¬ëŸ¼ ì„ íƒ ì™„ë£Œ")
        
        # ========== 3ë‹¨ê³„: ë‚ ì§œ í•„í„°ë§ ==========
        logger.info("ğŸ“… [3/8] ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ ì¤‘...")
        start_date = pd.to_datetime('2024-02-01 00:00:00')
        end_date = pd.to_datetime('2024-07-27 23:59:59')
        data = data[(data['TIME'] >= start_date) & (data['TIME'] <= end_date)]
        logger.info(f"   âœ“ í•„í„°ë§ í›„ ë°ì´í„°: {data.shape}")
        
        # ========== 4ë‹¨ê³„: ì´ìƒì¹˜ ì œê±° ==========
        logger.info("ğŸ” [4/8] ì´ìƒì¹˜ ì œê±° ì¤‘...")
        before_outlier = len(data)
        data = data[(data['TOTALCNT'] >= 800) & (data['TOTALCNT'] <= 2500)]
        logger.info(f"   âœ“ ì œê±°ëœ ì´ìƒì¹˜: {before_outlier - len(data)}ê°œ")
        
        # ========== 5ë‹¨ê³„: FUTURE ë° ë¼ë²¨ ìƒì„± ==========
        logger.info("ğŸ¯ [5/8] íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")
        data['FUTURE'] = data['TOTALCNT'].shift(-10)
        data['spike_label'] = (data['FUTURE'] >= 1400).astype(int)
        data.dropna(inplace=True)
        
        logger.info(f"   âœ“ ì „ì²˜ë¦¬ ì™„ë£Œ: {data.shape}")
        logger.info(f"   âœ“ 1400+ ë¹„ìœ¨: {data['spike_label'].mean():.2%}")
        logger.info(f"   âœ“ 1400+ ê°œìˆ˜: {data['spike_label'].sum()}ê°œ")
        
        # ========== 6ë‹¨ê³„: íŠ¹ì§• ìƒì„± (ìµœì í™”) ==========
        logger.info("âš™ï¸ [6/8] íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì¤‘...")
        total_features = 12
        current_feature = 0
        
        # ì´ë™í‰ê·  ê³„ì‚°
        current_feature += 1
        logger.info(f"   [{current_feature}/{total_features}] MA_10 ê³„ì‚°...")
        data['MA_10'] = data['TOTALCNT'].rolling(10, min_periods=1).mean()
        
        current_feature += 1
        logger.info(f"   [{current_feature}/{total_features}] MA_30 ê³„ì‚°...")
        data['MA_30'] = data['TOTALCNT'].rolling(30, min_periods=1).mean()
        
        current_feature += 1
        logger.info(f"   [{current_feature}/{total_features}] MA_60 ê³„ì‚°...")
        data['MA_60'] = data['TOTALCNT'].rolling(60, min_periods=1).mean()
        
        # í‘œì¤€í¸ì°¨ ê³„ì‚°
        current_feature += 1
        logger.info(f"   [{current_feature}/{total_features}] STD_10 ê³„ì‚°...")
        data['STD_10'] = data['TOTALCNT'].rolling(10, min_periods=1).std()
        
        current_feature += 1
        logger.info(f"   [{current_feature}/{total_features}] STD_30 ê³„ì‚°...")
        data['STD_30'] = data['TOTALCNT'].rolling(30, min_periods=1).std()
        
        # ë³€í™”ìœ¨ ê³„ì‚°
        current_feature += 1
        logger.info(f"   [{current_feature}/{total_features}] ë³€í™”ìœ¨ ê³„ì‚°...")
        data['change_rate'] = data['TOTALCNT'].pct_change()
        
        current_feature += 1
        logger.info(f"   [{current_feature}/{total_features}] 10ê¸°ê°„ ë³€í™”ìœ¨ ê³„ì‚°...")
        data['change_rate_10'] = data['TOTALCNT'].pct_change(10)
        
        # ì‹œê°„ íŠ¹ì§•
        current_feature += 1
        logger.info(f"   [{current_feature}/{total_features}] ì‹œê°„ íŠ¹ì§• ì¶”ì¶œ...")
        data['hour'] = data.index.hour
        
        current_feature += 1
        logger.info(f"   [{current_feature}/{total_features}] ìš”ì¼ íŠ¹ì§• ì¶”ì¶œ...")
        data['dayofweek'] = data.index.dayofweek
        
        current_feature += 1
        logger.info(f"   [{current_feature}/{total_features}] ì£¼ë§ í”Œë˜ê·¸ ìƒì„±...")
        data['is_weekend'] = (data.index.dayofweek >= 5).astype(int)
        
        # ì¶”ì„¸ ê³„ì‚°
        current_feature += 1
        logger.info(f"   [{current_feature}/{total_features}] ì¶”ì„¸ ê³„ì‚°...")
        data['trend'] = data['MA_10'] - data['MA_30']
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        current_feature += 1
        logger.info(f"   [{current_feature}/{total_features}] ê²°ì¸¡ì¹˜ ì²˜ë¦¬...")
        data.fillna(method='ffill', inplace=True)
        data.fillna(0, inplace=True)
        
        logger.info("   âœ“ íŠ¹ì§• ìƒì„± ì™„ë£Œ!")
        
        # ========== 7ë‹¨ê³„: ìŠ¤ì¼€ì¼ë§ ==========
        logger.info("ğŸ“ [7/8] ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì¤‘...")
        scaler = RobustScaler()
        feature_cols = ['TOTALCNT', 'MA_10', 'MA_30', 'MA_60', 'STD_10', 'STD_30',
                       'change_rate', 'change_rate_10', 'hour', 'dayofweek', 
                       'is_weekend', 'trend']
        
        data[feature_cols + ['FUTURE']] = scaler.fit_transform(data[feature_cols + ['FUTURE']])
        logger.info(f"   âœ“ {len(feature_cols)} ê°œ íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
        
        # ========== 8ë‹¨ê³„: ì‹œí€€ìŠ¤ ìƒì„± ==========
        logger.info("ğŸ”„ [8/8] ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì¤‘...")
        SEQ_LENGTH = 50
        X, y, spike_labels = [], [], []
        
        # ì§„í–‰ ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ ì´ ê°œìˆ˜
        total_sequences = len(data) - SEQ_LENGTH
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
        logger.info(f"   ì´ {total_sequences}ê°œ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        
        for i in range(total_sequences):
            # 1000ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© í‘œì‹œ
            if i % 1000 == 0:
                progress = (i / total_sequences) * 100
                logger.info(f"   ì§„í–‰ë¥ : {progress:.1f}% ({i}/{total_sequences})")
            
            X.append(data[feature_cols].iloc[i:i+SEQ_LENGTH].values)
            y.append(data['FUTURE'].iloc[i+SEQ_LENGTH])
            spike_labels.append(data['spike_label'].iloc[i+SEQ_LENGTH])
        
        logger.info("   âœ“ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ! NumPy ë°°ì—´ë¡œ ë³€í™˜ ì¤‘...")
        X, y, spike_labels = np.array(X), np.array(y), np.array(spike_labels)
        
        logger.info(f"   âœ“ ìµœì¢… ì‹œí€€ìŠ¤ shape: X={X.shape}, y={y.shape}")
        
        # ========== ë°ì´í„° ë¶„í•  ==========
        logger.info("ğŸ“Š ë°ì´í„°ì…‹ ë¶„í•  ì¤‘...")
        train_size = int(0.7 * len(X))
        val_size = int(0.15 * len(X))
        
        logger.info(f"   âœ“ Train: {train_size}ê°œ")
        logger.info(f"   âœ“ Val: {val_size}ê°œ")
        logger.info(f"   âœ“ Test: {len(X) - train_size - val_size}ê°œ")
        
        # ë”•ì…”ë„ˆë¦¬ ìƒì„±
        data_dict = {
            'X_train': X[:train_size],
            'y_train': y[:train_size],
            'spike_train': spike_labels[:train_size],
            'X_val': X[train_size:train_size+val_size],
            'y_val': y[train_size:train_size+val_size],
            'spike_val': spike_labels[train_size:train_size+val_size],
            'X_test': X[train_size+val_size:],
            'y_test': y[train_size+val_size:],
            'spike_test': spike_labels[train_size+val_size:],
            'scaler': scaler,
            'feature_cols': feature_cols,
            'input_shape': (SEQ_LENGTH, len(feature_cols))
        }
        
        # ========== ìºì‹œ ì €ì¥ ==========
        logger.info("ğŸ’¾ ì „ì²˜ë¦¬ëœ ë°ì´í„° ìºì‹± ì¤‘...")
        checkpoint_manager.save_data(data_dict)
        
        # ========== ì™„ë£Œ ==========
        logger.info("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
        logger.info(f"   - ì…ë ¥ shape: {data_dict['input_shape']}")
        logger.info(f"   - Train 1400+ ë¹„ìœ¨: {spike_labels[:train_size].mean():.2%}")
        logger.info(f"   - Val 1400+ ë¹„ìœ¨: {spike_labels[train_size:train_size+val_size].mean():.2%}")
        logger.info(f"   - Test 1400+ ë¹„ìœ¨: {spike_labels[train_size+val_size:].mean():.2%}")
        
        return data_dict
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(traceback.format_exc())
        raise

ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ULTIMATE v4.0 - ì™„ì „ì²´
========================================
ëª¨ë“  ê¸°ëŠ¥ í¬í•¨ + ì¬ì‹œì‘ ê°€ëŠ¥ + ì˜¤ë²„í”¼íŒ… í•´ê²°

í•µì‹¬ ê¸°ëŠ¥:
1. ì¤‘ë‹¨ í›„ ì¬ì‹œì‘ ê°€ëŠ¥ (--resume)
2. ì—í­ë³„ ì²´í¬í¬ì¸íŠ¸ ìë™ ì €ì¥
3. ì˜¤ë²„í”¼íŒ… ì™„ì „ í•´ê²°
4. ê¸‰ë³€ ê°ì§€ ë¶„ë¥˜ê¸°
5. ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© ì €ì¥

ì‚¬ìš©ë²•:
python model_v4_ultimate.py          # ì²˜ìŒ ì‹œì‘
python model_v4_ultimate.py --resume # ì´ì–´ì„œ ì‹œì‘
python model_v4_ultimate.py --reset  # ì´ˆê¸°í™” í›„ ì‹œì‘
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, RobustScaler
from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.keras.layers import (Input, LSTM, Dense, Dropout, BatchNormalization,
                                     GRU, Conv1D, MaxPooling1D, GlobalAveragePooling1D,
                                     Bidirectional)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l1_l2
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score
import matplotlib.pyplot as plt
import os
import sys
from datetime import datetime
import joblib
import logging
import warnings
import json
import pickle
import traceback
import argparse
import shutil

# ê²½ê³  ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')

# ===================================
# 1. í™˜ê²½ ì„¤ì •
# ===================================

# CPU ëª¨ë“œ ì„¤ì •
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
tf.config.set_visible_devices([], 'GPU')

# ëœë¤ ì‹œë“œ (ì›ë˜ ê°’!)
RANDOM_SEED = 2079936
tf.random.set_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training_ultimate.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ===================================
# 2. ê°•í™”ëœ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
# ===================================

class UltimateCheckpointManager:
    """ì™„ë²½í•œ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬"""
    
    def __init__(self, checkpoint_dir='checkpoints_ultimate'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # íŒŒì¼ ê²½ë¡œë“¤
        self.state_file = os.path.join(checkpoint_dir, 'training_state.json')
        self.data_file = os.path.join(checkpoint_dir, 'preprocessed_data.pkl')
        self.models_dir = os.path.join(checkpoint_dir, 'models')
        os.makedirs(self.models_dir, exist_ok=True)
        
    def save_state(self, state_dict):
        """í•™ìŠµ ìƒíƒœ ì €ì¥ (ë§¤ ì—í­)"""
        # ê¸°ì¡´ ìƒíƒœ ë¡œë“œ
        if os.path.exists(self.state_file):
            with open(self.state_file, 'r') as f:
                existing = json.load(f)
        else:
            existing = {}
        
        # ì—…ë°ì´íŠ¸
        existing.update(state_dict)
        
        # ì €ì¥
        with open(self.state_file, 'w') as f:
            json.dump(existing, f, indent=4, default=str)
            
    def load_state(self):
        """í•™ìŠµ ìƒíƒœ ë¡œë“œ"""
        if os.path.exists(self.state_file):
            with open(self.state_file, 'r') as f:
                return json.load(f)
        return None
    
    def save_data(self, data_dict):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        with open(self.data_file, 'wb') as f:
            pickle.dump(data_dict, f)
        logger.info(f"ë°ì´í„° ì €ì¥: {self.data_file}")
        
    def load_data(self):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
        if os.path.exists(self.data_file):
            with open(self.data_file, 'rb') as f:
                data = pickle.load(f)
            logger.info(f"ë°ì´í„° ë¡œë“œ: {self.data_file}")
            return data
        return None
    
    def save_model_weights(self, model, model_name, epoch):
        """ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥"""
        path = os.path.join(self.models_dir, f'{model_name}_epoch_{epoch}.h5')
        model.save_weights(path)
        return path
    
    def load_model_weights(self, model, model_name, epoch):
        """ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ"""
        path = os.path.join(self.models_dir, f'{model_name}_epoch_{epoch}.h5')
        if os.path.exists(path):
            model.load_weights(path)
            logger.info(f"ê°€ì¤‘ì¹˜ ë¡œë“œ: {path}")
            return True
        return False
    
    def get_latest_epoch(self, model_name):
        """ìµœì‹  ì—í­ ì°¾ê¸°"""
        state = self.load_state()
        if state and 'model_progress' in state:
            return state['model_progress'].get(model_name, {}).get('last_epoch', 0)
        return 0

# ===================================
# 3. ì»¤ìŠ¤í…€ ì½œë°± (ì§„í–‰ìƒí™© ì €ì¥)
# ===================================

class CheckpointCallback(Callback):
    """ë§¤ ì—í­ë§ˆë‹¤ ìƒíƒœ ì €ì¥"""
    
    def __init__(self, checkpoint_manager, model_name):
        super().__init__()
        self.checkpoint_manager = checkpoint_manager
        self.model_name = model_name
        self.history = {'loss': [], 'val_loss': [], 'mae': [], 'val_mae': []}
        
    def on_epoch_end(self, epoch, logs=None):
        """ì—í­ ì¢…ë£Œ ì‹œ ì €ì¥"""
        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        for key in self.history.keys():
            if key in logs:
                self.history[key].append(logs[key])
        
        # 5 ì—í­ë§ˆë‹¤ ê°€ì¤‘ì¹˜ ì €ì¥
        if (epoch + 1) % 5 == 0:
            self.checkpoint_manager.save_model_weights(self.model, self.model_name, epoch + 1)
            
        # ìƒíƒœ ì €ì¥
        state = {
            'current_model': self.model_name,
            'model_progress': {
                self.model_name: {
                    'last_epoch': epoch + 1,
                    'history': self.history,
                    'best_val_loss': min(self.history['val_loss']) if self.history['val_loss'] else 999
                }
            },
            'last_update': datetime.now().isoformat()
        }
        self.checkpoint_manager.save_state(state)
        
        # ë¡œê·¸ ì¶œë ¥
        logger.info(f"[{self.model_name}] Epoch {epoch+1} - "
                   f"Loss: {logs.get('loss', 0):.4f}, "
                   f"Val Loss: {logs.get('val_loss', 0):.4f}, "
                   f"Val MAE: {logs.get('val_mae', 0):.4f}")

# ===================================
# 4. ë°ì´í„° ì „ì²˜ë¦¬ (ìºì‹± í¬í•¨)
# ===================================

def load_and_preprocess_data(data_path, checkpoint_manager, force_reload=False):
    """ë°ì´í„° ë¡œë“œ (ìºì‹±)"""
    
    # ìºì‹œ í™•ì¸
    if not force_reload:
        cached_data = checkpoint_manager.load_data()
        if cached_data:
            logger.info("ìºì‹œëœ ë°ì´í„° ì‚¬ìš©")
            return cached_data
    
    logger.info(f"ë°ì´í„° ìƒˆë¡œ ë¡œë”©: {data_path}")
    
    # ë°ì´í„° ë¡œë“œ
    data = pd.read_csv(data_path)
    
    # ì‹œê°„ ë³€í™˜
    data['CURRTIME'] = pd.to_datetime(data['CURRTIME'], format='%Y%m%d%H%M')
    data['TIME'] = pd.to_datetime(data['TIME'], format='%Y%m%d%H%M')
    
    # í•„ìš” ì»¬ëŸ¼
    data = data[['CURRTIME', 'TOTALCNT', 'TIME']]
    data.set_index('CURRTIME', inplace=True)
    
    # ë‚ ì§œ í•„í„°ë§
    start_date = pd.to_datetime('2024-02-01 00:00:00')
    end_date = pd.to_datetime('2024-07-27 23:59:59')
    data = data[(data['TIME'] >= start_date) & (data['TIME'] <= end_date)]
    
    # ì´ìƒì¹˜ ì œê±°
    data = data[(data['TOTALCNT'] >= 800) & (data['TOTALCNT'] <= 2500)]
    
    # FUTURE ìƒì„±
    data['FUTURE'] = data['TOTALCNT'].shift(-10)
    data['spike_label'] = (data['FUTURE'] >= 1400).astype(int)
    data.dropna(inplace=True)
    
    logger.info(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {data.shape}")
    logger.info(f"1400+ ë¹„ìœ¨: {data['spike_label'].mean():.2%}")
    
    # íŠ¹ì§• ìƒì„±
    data['MA_10'] = data['TOTALCNT'].rolling(10, min_periods=1).mean()
    data['MA_30'] = data['TOTALCNT'].rolling(30, min_periods=1).mean()
    data['MA_60'] = data['TOTALCNT'].rolling(60, min_periods=1).mean()
    data['STD_10'] = data['TOTALCNT'].rolling(10, min_periods=1).std()
    data['STD_30'] = data['TOTALCNT'].rolling(30, min_periods=1).std()
    data['change_rate'] = data['TOTALCNT'].pct_change()
    data['change_rate_10'] = data['TOTALCNT'].pct_change(10)
    data['hour'] = data.index.hour
    data['dayofweek'] = data.index.dayofweek
    data['is_weekend'] = (data.index.dayofweek >= 5).astype(int)
    data['trend'] = data['MA_10'] - data['MA_30']
    
    data.fillna(method='ffill', inplace=True)
    data.fillna(0, inplace=True)
    
    # ìŠ¤ì¼€ì¼ë§
    scaler = RobustScaler()
    feature_cols = ['TOTALCNT', 'MA_10', 'MA_30', 'MA_60', 'STD_10', 'STD_30',
                   'change_rate', 'change_rate_10', 'hour', 'dayofweek', 
                   'is_weekend', 'trend']
    
    data[feature_cols + ['FUTURE']] = scaler.fit_transform(data[feature_cols + ['FUTURE']])
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    SEQ_LENGTH = 50
    X, y, spike_labels = [], [], []
    
    for i in range(len(data) - SEQ_LENGTH):
        X.append(data[feature_cols].iloc[i:i+SEQ_LENGTH].values)
        y.append(data['FUTURE'].iloc[i+SEQ_LENGTH])
        spike_labels.append(data['spike_label'].iloc[i+SEQ_LENGTH])
    
    X, y, spike_labels = np.array(X), np.array(y), np.array(spike_labels)
    
    # ë°ì´í„° ë¶„í• 
    train_size = int(0.7 * len(X))
    val_size = int(0.15 * len(X))
    
    data_dict = {
        'X_train': X[:train_size],
        'y_train': y[:train_size],
        'spike_train': spike_labels[:train_size],
        'X_val': X[train_size:train_size+val_size],
        'y_val': y[train_size:train_size+val_size],
        'spike_val': spike_labels[train_size:train_size+val_size],
        'X_test': X[train_size+val_size:],
        'y_test': y[train_size+val_size:],
        'spike_test': spike_labels[train_size+val_size:],
        'scaler': scaler,
        'feature_cols': feature_cols,
        'input_shape': (SEQ_LENGTH, len(feature_cols))
    }
    
    # ìºì‹œ ì €ì¥
    checkpoint_manager.save_data(data_dict)
    
    return data_dict

# ===================================
# 5. ëª¨ë¸ ì •ì˜
# ===================================

def build_simple_lstm(input_shape):
    """ë‹¨ìˆœ LSTM"""
    model = Sequential([
        Input(shape=input_shape),
        LSTM(64, return_sequences=True, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),
        Dropout(0.5),
        BatchNormalization(),
        LSTM(32, return_sequences=False, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),
        Dropout(0.5),
        Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),
        Dropout(0.3),
        Dense(1)
    ])
    return model

def build_gru_model(input_shape):
    """GRU ëª¨ë¸"""
    model = Sequential([
        Input(shape=input_shape),
        GRU(64, return_sequences=True, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),
        Dropout(0.5),
        GRU(32, return_sequences=False),
        Dropout(0.5),
        Dense(16, activation='relu'),
        Dropout(0.3),
        Dense(1)
    ])
    return model

def build_cnn_lstm(input_shape):
    """CNN-LSTM"""
    model = Sequential([
        Input(shape=input_shape),
        Conv1D(32, 3, activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling1D(2),
        Dropout(0.4),
        LSTM(32, return_sequences=False, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),
        Dropout(0.5),
        Dense(16, activation='relu'),
        Dropout(0.3),
        Dense(1)
    ])
    return model

def build_spike_detector(input_shape):
    """ê¸‰ë³€ ê°ì§€ê¸°"""
    model = Sequential([
        Input(shape=input_shape),
        Conv1D(32, 3, activation='relu'),
        MaxPooling1D(2),
        LSTM(32, return_sequences=False),
        Dropout(0.5),
        Dense(16, activation='relu'),
        Dropout(0.3),
        Dense(1, activation='sigmoid')
    ])
    return model

# ===================================
# 6. ì¬ì‹œì‘ ê°€ëŠ¥í•œ í•™ìŠµ í•¨ìˆ˜
# ===================================

def train_model_with_resume(model, model_name, data_dict, checkpoint_manager,
                           epochs=50, batch_size=128, resume=False):
    """ì¬ì‹œì‘ ê°€ëŠ¥í•œ í•™ìŠµ"""
    
    # ë°ì´í„° ì–¸íŒ©
    X_train = data_dict['X_train']
    y_train = data_dict['y_train']
    X_val = data_dict['X_val']
    y_val = data_dict['y_val']
    
    # ê¸‰ë³€ ê°ì§€ê¸°ì¸ ê²½ìš°
    if 'spike' in model_name:
        y_train = data_dict['spike_train']
        y_val = data_dict['spike_val']
    
    # ì¬ì‹œì‘ ì²˜ë¦¬
    start_epoch = 0
    if resume:
        start_epoch = checkpoint_manager.get_latest_epoch(model_name)
        if start_epoch > 0:
            logger.info(f"{model_name} Epoch {start_epoch}ë¶€í„° ì¬ì‹œì‘")
            checkpoint_manager.load_model_weights(model, model_name, start_epoch)
    
    # ì´ë¯¸ ì™„ë£Œëœ ê²½ìš°
    if start_epoch >= epochs:
        logger.info(f"{model_name} ì´ë¯¸ ì™„ë£Œë¨")
        return model
    
    logger.info(f"\n{'='*60}")
    logger.info(f"{model_name} í•™ìŠµ ì‹œì‘ (Epoch {start_epoch+1}/{epochs})")
    logger.info(f"{'='*60}")
    
    # ì»´íŒŒì¼
    if 'spike' in model_name:
        model.compile(
            optimizer=Adam(learning_rate=0.0001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
    else:
        model.compile(
            optimizer=Adam(learning_rate=0.0001),
            loss='mse',
            metrics=['mae']
        )
    
    # ì½œë°±
    callbacks = [
        CheckpointCallback(checkpoint_manager, model_name),
        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1),
        ModelCheckpoint(f'{checkpoint_manager.models_dir}/{model_name}_best.h5', 
                       save_best_only=True, verbose=0)
    ]
    
    # í•™ìŠµ
    try:
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            initial_epoch=start_epoch,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        # ì™„ë£Œ ìƒíƒœ ì €ì¥
        state = {
            'model_progress': {
                model_name: {
                    'completed': True,
                    'final_epoch': epochs,
                    'completed_time': datetime.now().isoformat()
                }
            }
        }
        checkpoint_manager.save_state(state)
        
    except KeyboardInterrupt:
        logger.warning(f"\n{model_name} í•™ìŠµ ì¤‘ë‹¨ë¨. ì¬ì‹œì‘ ê°€ëŠ¥!")
        state = {
            'interrupted': True,
            'interrupted_model': model_name,
            'interrupted_time': datetime.now().isoformat()
        }
        checkpoint_manager.save_state(state)
        raise
        
    except Exception as e:
        logger.error(f"{model_name} í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        state = {
            'error': str(e),
            'error_model': model_name,
            'error_time': datetime.now().isoformat()
        }
        checkpoint_manager.save_state(state)
        raise
    
    return model

# ===================================
# 7. ì•™ìƒë¸” ì˜ˆì¸¡
# ===================================

def ensemble_predict(models, spike_detector, X_test):
    """ì•™ìƒë¸” ì˜ˆì¸¡"""
    
    # ê¸‰ë³€ í™•ë¥ 
    spike_probs = spike_detector.predict(X_test, verbose=0).flatten()
    
    # ê° ëª¨ë¸ ì˜ˆì¸¡
    predictions = {}
    for name, model in models.items():
        predictions[name] = model.predict(X_test, verbose=0).flatten()
    
    # ë™ì  ê°€ì¤‘ì¹˜
    ensemble_pred = np.zeros(len(X_test))
    
    for i in range(len(X_test)):
        if spike_probs[i] > 0.5:
            weights = {'lstm': 0.2, 'gru': 0.2, 'cnn_lstm': 0.6}
        else:
            weights = {'lstm': 0.4, 'gru': 0.3, 'cnn_lstm': 0.3}
        
        for name, weight in weights.items():
            if name in predictions:
                ensemble_pred[i] += weight * predictions[name][i]
        
        if spike_probs[i] > 0.5:
            ensemble_pred[i] *= 1.05
    
    return ensemble_pred, predictions, spike_probs

# ===================================
# 8. ë©”ì¸ ì‹¤í–‰
# ===================================

def main(resume=False, reset=False):
    """ë©”ì¸ ì‹¤í–‰"""
    
    # ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì €
    checkpoint_manager = UltimateCheckpointManager()
    
    # ë¦¬ì…‹ ì²˜ë¦¬
    if reset:
        if os.path.exists(checkpoint_manager.checkpoint_dir):
            shutil.rmtree(checkpoint_manager.checkpoint_dir)
            logger.info("ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™”ë¨")
        checkpoint_manager = UltimateCheckpointManager()
    
    # ì¬ì‹œì‘ ìƒíƒœ í™•ì¸
    if resume:
        state = checkpoint_manager.load_state()
        if state:
            logger.info("="*60)
            logger.info("ì´ì „ í•™ìŠµ ì¬ê°œ")
            if 'interrupted_model' in state:
                logger.info(f"ì¤‘ë‹¨ëœ ëª¨ë¸: {state['interrupted_model']}")
            logger.info("="*60)
        else:
            logger.info("ì €ì¥ëœ ìƒíƒœ ì—†ìŒ. ì²˜ìŒë¶€í„° ì‹œì‘")
            resume = False
    
    logger.info("="*60)
    logger.info("ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ v4.0 ULTIMATE")
    logger.info("="*60)
    
    # ë°ì´í„° ë¡œë“œ
    data_dict = load_and_preprocess_data(
        'data/20240201_TO_202507281705.csv',
        checkpoint_manager,
        force_reload=not resume
    )
    
    input_shape = data_dict['input_shape']
    
    # ëª¨ë¸ ì •ì˜
    model_configs = [
        ('lstm', build_simple_lstm),
        ('gru', build_gru_model),
        ('cnn_lstm', build_cnn_lstm),
        ('spike_detector', build_spike_detector)
    ]
    
    models = {}
    
    # ê° ëª¨ë¸ í•™ìŠµ
    for model_name, build_func in model_configs:
        try:
            # ì™„ë£Œ í™•ì¸
            state = checkpoint_manager.load_state()
            if state and 'model_progress' in state:
                if state['model_progress'].get(model_name, {}).get('completed', False):
                    logger.info(f"{model_name} ì´ë¯¸ ì™„ë£Œë¨. ê±´ë„ˆëœ€")
                    # ëª¨ë¸ ë¡œë“œ
                    model = build_func(input_shape)
                    model.load_weights(f'{checkpoint_manager.models_dir}/{model_name}_best.h5')
                    if model_name != 'spike_detector':
                        models[model_name] = model
                    else:
                        spike_model = model
                    continue
            
            # ëª¨ë¸ ë¹Œë“œ
            model = build_func(input_shape)
            
            # í•™ìŠµ
            epochs = 30 if model_name == 'spike_detector' else 50
            model = train_model_with_resume(
                model, model_name, data_dict, checkpoint_manager,
                epochs=epochs, batch_size=128, resume=resume
            )
            
            if model_name != 'spike_detector':
                models[model_name] = model
            else:
                spike_model = model
                
        except KeyboardInterrupt:
            logger.warning("\ní•™ìŠµ ì¤‘ë‹¨. python model_v4_ultimate.py --resume ë¡œ ì¬ì‹œì‘ ê°€ëŠ¥")
            return
        except Exception as e:
            logger.error(f"ì˜¤ë¥˜: {str(e)}")
            logger.info("python model_v4_ultimate.py --resume ë¡œ ì¬ì‹œì‘ ì‹œë„")
            return
    
    # í‰ê°€
    logger.info("\n" + "="*60)
    logger.info("ëª¨ë¸ í‰ê°€")
    logger.info("="*60)
    
    # ì•™ìƒë¸” ì˜ˆì¸¡
    X_test = data_dict['X_test']
    y_test = data_dict['y_test']
    spike_test = data_dict['spike_test']
    scaler = data_dict['scaler']
    feature_cols = data_dict['feature_cols']
    
    ensemble_pred, _, spike_probs = ensemble_predict(models, spike_model, X_test)
    
    # ì—­ë³€í™˜
    y_test_original = scaler.inverse_transform(
        np.column_stack([np.zeros((len(y_test), len(feature_cols))), y_test])
    )[:, -1]
    
    ensemble_original = scaler.inverse_transform(
        np.column_stack([np.zeros((len(ensemble_pred), len(feature_cols))), ensemble_pred])
    )[:, -1]
    
    # ì„±ëŠ¥
    mae = mean_absolute_error(y_test_original, ensemble_original)
    rmse = np.sqrt(mean_squared_error(y_test_original, ensemble_original))
    r2 = r2_score(y_test_original, ensemble_original)
    
    logger.info(f"MAE: {mae:.2f}")
    logger.info(f"RMSE: {rmse:.2f}")
    logger.info(f"RÂ²: {r2:.4f}")
    
    # 1400+ ì„±ëŠ¥
    high_mask = y_test_original >= 1400
    if high_mask.sum() > 0:
        mae_high = mean_absolute_error(y_test_original[high_mask], ensemble_original[high_mask])
        logger.info(f"1400+ MAE: {mae_high:.2f}")
    
    # ê¸‰ë³€ ê°ì§€ ì •í™•ë„
    spike_acc = accuracy_score(spike_test, spike_probs > 0.5)
    logger.info(f"ê¸‰ë³€ ê°ì§€ ì •í™•ë„: {spike_acc:.2%}")
    
    # ì‹œê°í™”
    plt.figure(figsize=(15, 5))
    plt.plot(y_test_original[:200], label='ì‹¤ì œ', color='blue')
    plt.plot(ensemble_original[:200], label='ì˜ˆì¸¡', color='red', alpha=0.7)
    plt.axhline(y=1400, color='green', linestyle='--', alpha=0.5)
    plt.title(f'ì˜ˆì¸¡ ê²°ê³¼ (MAE: {mae:.2f})')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig('results_ultimate.png')
    plt.show()
    
    # ìµœì¢… ì €ì¥
    os.makedirs('models_ultimate', exist_ok=True)
    for name, model in models.items():
        model.save(f'models_ultimate/{name}_final.h5')
    spike_model.save('models_ultimate/spike_detector_final.h5')
    joblib.dump(scaler, 'models_ultimate/scaler.pkl')
    
    logger.info("\n" + "="*60)
    logger.info("ì™„ë£Œ!")
    logger.info(f"ìµœì¢… MAE: {mae:.2f}")
    logger.info("="*60)

# ===================================
# 9. ì‹¤í–‰
# ===================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ v4.0 ULTIMATE')
    parser.add_argument('--resume', action='store_true', help='ì´ì „ í•™ìŠµ ì¬ê°œ')
    parser.add_argument('--reset', action='store_true', help='ì´ˆê¸°í™” í›„ ì‹œì‘')
    
    args = parser.parse_args()
    
    try:
        main(resume=args.resume, reset=args.reset)
    except Exception as e:
        logger.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")
        logger.error(traceback.format_exc())
        print("\nâŒ ì˜¤ë¥˜ ë°œìƒ! --resume ì˜µì…˜ìœ¼ë¡œ ì¬ì‹œì‘ ê°€ëŠ¥")