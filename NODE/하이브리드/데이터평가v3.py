"""
ë°˜ë„ì²´ ë¬¼ë¥˜ ê¸‰ì¦ ì˜ˆì¸¡ì„ ìœ„í•œ ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ v3.0
=========================================================
ë³¸ ì‹œìŠ¤í…œì€ í•™ìŠµëœ ì´ì¤‘ ì¶œë ¥ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬
ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜ë„ì²´ íŒ¹ ê°„ ë¬¼ë¥˜ëŸ‰ì„ ì˜ˆì¸¡í•˜ê³  íŠ¹íˆ TOTALCNT > 1400
ê¸‰ì¦ êµ¬ê°„ì„ ì‚¬ì „ì— ê°ì§€í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ì´ì¤‘ ì¶œë ¥ ëª¨ë¸ ë¡œë“œ (ìˆ˜ì¹˜ ì˜ˆì¸¡ + ê¸‰ì¦ í™•ë¥ )
2. ê°œë³„ êµ¬ê°„ ë°ì´í„°ë¥¼ í™œìš©í•œ ì •ë°€ ì˜ˆì¸¡
3. ê¸‰ì¦ êµ¬ê°„ ì‚¬ì „ ê°ì§€ ë° ê²½ê³ 
4. ì‹¤ì‹œê°„ ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”

ê°œë°œì¼: 2024ë…„
ë²„ì „: 3.0 (ê¸‰ì¦ ì˜ˆì¸¡ íŠ¹í™”)
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import load_model
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
import sys
import os
import platform
from datetime import datetime, timedelta
import joblib
import logging
import warnings
import json

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')

# ===================================
# í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
# ===================================

# í•œê¸€ í°íŠ¸ ì„¤ì •
def set_korean_font():
    """ìš´ì˜ì²´ì œë³„ í•œê¸€ í°íŠ¸ ìë™ ì„¤ì •"""
    system = platform.system()
    
    if system == 'Windows':
        font_family = 'Malgun Gothic'
    elif system == 'Darwin':
        font_family = 'AppleGothic'
    else:
        font_family = 'NanumGothic'
    
    plt.rcParams['font.family'] = font_family
    plt.rcParams['axes.unicode_minus'] = False
    return True

USE_KOREAN = set_korean_font()

# CPU ëª¨ë“œ ì„¤ì •
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
tf.config.set_visible_devices([], 'GPU')

# ëœë¤ ì‹œë“œ ê³ ì •
RANDOM_SEED = 2079936
tf.random.set_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('spike_prediction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

logger.info("="*60)
logger.info("ê¸‰ì¦ ì˜ˆì¸¡ ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ v3.0 ì‹œì‘")
logger.info("="*60)

# ===================================
# ê¸‰ì¦ ì˜ˆì¸¡ ì‹œìŠ¤í…œ í´ë˜ìŠ¤
# ===================================

class SpikePredictor:
    """ê¸‰ì¦ ì˜ˆì¸¡ íŠ¹í™” ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.models = {}
        self.scaler = None
        self.config = None
        self.spike_threshold = 1400  # ê¸‰ì¦ ì„ê³„ê°’
        
    def load_models(self):
        """í•™ìŠµëœ ì´ì¤‘ ì¶œë ¥ ëª¨ë¸ ë¡œë“œ"""
        logger.info("í•™ìŠµëœ ê¸‰ì¦ ì˜ˆì¸¡ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # v3 ì´ì¤‘ ì¶œë ¥ ëª¨ë¸ë“¤ ë¡œë“œ
        model_names = ['dual_lstm', 'dual_gru', 'dual_rnn', 'dual_bilstm']
        for model_name in model_names:
            try:
                # v3 ëª¨ë¸ ê²½ë¡œ
                model_path = f'model_v3/{model_name}_final.keras'
                if os.path.exists(model_path):
                    self.models[model_name] = load_model(model_path, compile=False)
                    logger.info(f"âœ“ {model_name.upper()} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                else:
                    # ê¸°ì¡´ ëª¨ë¸ í´ë°±
                    alt_path = f'model/{model_name.replace("dual_", "")}_final_hybrid.keras'
                    if os.path.exists(alt_path):
                        logger.warning(f"âš  {model_name} v3 ëª¨ë¸ ì—†ìŒ, ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©")
            except Exception as e:
                logger.error(f"âŒ {model_name.upper()} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        try:
            scaler_paths = [
                'scaler_v3/scaler_v3.pkl',
                'scaler/standard_scaler_hybrid.pkl'
            ]
            for scaler_path in scaler_paths:
                if os.path.exists(scaler_path):
                    self.scaler = joblib.load(scaler_path)
                    logger.info("âœ“ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
                    break
        except Exception as e:
            logger.error(f"âŒ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise
        
        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        try:
            config_paths = [
                'results_v3/training_config.json',
                'results/training_config.json'
            ]
            for config_path in config_paths:
                if os.path.exists(config_path):
                    with open(config_path, 'r') as f:
                        self.config = json.load(f)
                    break
            
            if not self.config:
                self.config = {
                    'seq_length': 30,
                    'future_minutes': 10,
                    'spike_threshold': 1400,
                    'model_weights': {
                        'dual_lstm': 0.35,
                        'dual_gru': 0.25,
                        'dual_rnn': 0.15,
                        'dual_bilstm': 0.25
                    }
                }
            logger.info("âœ“ ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    
    def preprocess_data(self, data_path):
        """ê¸‰ì¦ ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬"""
        logger.info(f"ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘: {data_path}")
        
        # ë°ì´í„° ë¡œë“œ
        data = pd.read_csv(data_path)
        
        # ì‹œê°„ ì»¬ëŸ¼ ë³€í™˜
        data['CURRTIME'] = pd.to_datetime(data['CURRTIME'], format='%Y%m%d%H%M')
        data['TIME'] = pd.to_datetime(data['TIME'], format='%Y%m%d%H%M')
        
        # í•„ìš”í•œ ì»¬ëŸ¼ ì„ íƒ (ê°œë³„ êµ¬ê°„ í¬í•¨)
        required_columns = ['CURRTIME', 'TOTALCNT', 'M14AM10A', 'M10AM14A', 
                          'M14AM14B', 'M14BM14A', 'M14AM16', 'M16M14A', 'TIME']
        available_columns = [col for col in required_columns if col in data.columns]
        data = data[available_columns]
        data.set_index('CURRTIME', inplace=True)
        
        # FUTURE ì»¬ëŸ¼ ìƒì„± (10ë¶„ í›„)
        data['FUTURE'] = pd.NA
        future_minutes = self.config.get('future_minutes', 10)
        
        for i in data.index:
            future_time = i + pd.Timedelta(minutes=future_minutes)
            if (future_time <= data.index.max()) & (future_time in data.index):
                data.loc[i, 'FUTURE'] = data.loc[future_time, 'TOTALCNT']
        
        data.dropna(subset=['FUTURE'], inplace=True)
        
        # ê¸‰ì¦ ë¼ë²¨ ìƒì„±
        data['future_spike'] = (data['FUTURE'] > self.spike_threshold).astype(int)
        
        logger.info(f"ê¸‰ì¦ ë¹„ìœ¨: {data['future_spike'].mean():.2%}")
        
        # ê°œë³„ êµ¬ê°„ íŠ¹ì§• ìƒì„±
        segment_columns = ['M14AM10A', 'M10AM14A', 'M14AM14B', 'M14BM14A', 'M14AM16', 'M16M14A']
        available_segments = [col for col in segment_columns if col in data.columns]
        
        for col in available_segments:
            # ë¹„ìœ¨
            data[f'{col}_ratio'] = data[col] / (data['TOTALCNT'] + 1e-6)
            # ë³€í™”ìœ¨
            data[f'{col}_change_10'] = data[col].pct_change(10).fillna(0)
            # ì´ë™í‰ê· 
            data[f'{col}_MA5'] = data[col].rolling(window=5, min_periods=1).mean()
        
        # ê¸‰ì¦ ì‹ í˜¸ íŠ¹ì§•
        if 'M14AM14B' in data.columns:
            data['M14AM14B_spike_signal'] = (data['M14AM14B_change_10'] > 0.5).astype(int)
        if 'M16M14A' in data.columns:
            data['M16M14A_spike_signal'] = (data['M16M14A_change_10'] > 0.5).astype(int)
        
        # ê¸°ë³¸ íŠ¹ì§•
        data['hour'] = data.index.hour
        data['dayofweek'] = data.index.dayofweek
        data['is_weekend'] = (data.index.dayofweek >= 5).astype(int)
        data['MA_5'] = data['TOTALCNT'].rolling(window=5, min_periods=1).mean()
        data['MA_10'] = data['TOTALCNT'].rolling(window=10, min_periods=1).mean()
        data['MA_30'] = data['TOTALCNT'].rolling(window=30, min_periods=1).mean()
        data['STD_5'] = data['TOTALCNT'].rolling(window=5, min_periods=1).std()
        data['STD_10'] = data['TOTALCNT'].rolling(window=10, min_periods=1).std()
        data['change_rate'] = data['TOTALCNT'].pct_change()
        data['change_rate_5'] = data['TOTALCNT'].pct_change(5)
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        data = data.ffill().fillna(0)
        
        logger.info(f"ì „ì²˜ë¦¬ ì™„ë£Œ - ë°ì´í„° shape: {data.shape}")
        
        return data
    
    def scale_data(self, data):
        """ë°ì´í„° ìŠ¤ì¼€ì¼ë§"""
        # ìŠ¤ì¼€ì¼ë§í•  ì»¬ëŸ¼
        segment_columns = ['M14AM10A', 'M10AM14A', 'M14AM14B', 'M14BM14A', 'M14AM16', 'M16M14A']
        available_segments = [col for col in segment_columns if col in data.columns]
        
        scaling_columns = ['TOTALCNT', 'FUTURE'] + available_segments
        scaling_columns += [col for col in data.columns if 'MA' in col or 'STD' in col]
        scaling_columns += [f'{seg}_MA5' for seg in available_segments if f'{seg}_MA5' in data.columns]
        scaling_columns = list(set([col for col in scaling_columns if col in data.columns]))
        
        # ìŠ¤ì¼€ì¼ëŸ¬ê°€ ê¸°ëŒ€í•˜ëŠ” ì»¬ëŸ¼ í™•ì¸
        if hasattr(self.scaler, 'feature_names_in_'):
            expected_columns = list(self.scaler.feature_names_in_)
            scaling_columns = [col for col in expected_columns if col in data.columns]
        
        # ìŠ¤ì¼€ì¼ë§ ì ìš©
        scaled_data = self.scaler.transform(data[scaling_columns])
        scaled_df = pd.DataFrame(scaled_data, columns=[f'scaled_{col}' for col in scaling_columns], 
                               index=data.index)
        
        # ë¹„ìŠ¤ì¼€ì¼ íŠ¹ì§•
        non_scaled_features = [col for col in data.columns 
                             if ('ratio' in col or 'change' in col or 'signal' in col or 
                                 col in ['hour', 'dayofweek', 'is_weekend', 'future_spike'])]
        
        # ìµœì¢… ë°ì´í„°
        result = pd.concat([data[non_scaled_features], scaled_df], axis=1)
        
        # TIMEê³¼ FUTURE ì›ë³¸ê°’ ë³´ì¡´
        result['TIME'] = data['TIME']
        result['FUTURE'] = data['FUTURE']
        
        return result
    
    def create_sequences(self, data):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
        seq_length = self.config.get('seq_length', 30)
        
        # ì—°ì†ì„± í™•ì¸
        time_diff = data.index.to_series().diff()
        split_points = time_diff > pd.Timedelta(minutes=1)
        segment_ids = split_points.cumsum()
        
        # ì…ë ¥ íŠ¹ì§• ì„ íƒ
        input_features = [col for col in data.columns 
                         if col not in ['scaled_FUTURE', 'future_spike', 'TIME', 'FUTURE']]
        
        all_X = []
        all_y_reg = []
        all_y_cls = []
        all_times = []
        all_future_vals = []
        
        # ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ë¡œ ì‹œí€€ìŠ¤ ìƒì„±
        for segment_id in segment_ids.unique():
            segment = data[segment_ids == segment_id]
            
            if len(segment) > seq_length:
                X_data = segment[input_features].values
                y_reg_data = segment['scaled_FUTURE'].values if 'scaled_FUTURE' in segment.columns else segment['FUTURE'].values
                y_cls_data = segment['future_spike'].values
                time_data = segment['TIME'].values
                future_data = segment['FUTURE'].values
                
                for i in range(len(segment) - seq_length):
                    all_X.append(X_data[i:i+seq_length])
                    all_y_reg.append(y_reg_data[i+seq_length])
                    all_y_cls.append(y_cls_data[i+seq_length])
                    all_times.append(time_data[i+seq_length])
                    all_future_vals.append(future_data[i+seq_length])
        
        return (np.array(all_X), np.array(all_y_reg), np.array(all_y_cls),
                np.array(all_times), np.array(all_future_vals))
    
    def enhanced_ensemble_predict(self, X_data):
        """ê¸‰ì¦ ì˜ˆì¸¡ ê°•í™” ì•™ìƒë¸”"""
        weights = self.config.get('model_weights', {
            'dual_lstm': 0.35,
            'dual_gru': 0.25,
            'dual_rnn': 0.15,
            'dual_bilstm': 0.25
        })
        
        regression_preds = {}
        spike_preds = {}
        ensemble_reg = np.zeros(len(X_data))
        ensemble_spike = np.zeros(len(X_data))
        total_weight = 0
        
        # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡
        for model_name, model in self.models.items():
            if model is not None:
                logger.info(f"{model_name.upper()} ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
                pred = model.predict(X_data, verbose=0)
                
                # ì´ì¤‘ ì¶œë ¥ ì²˜ë¦¬
                if isinstance(pred, list) and len(pred) == 2:
                    regression_preds[model_name] = pred[0].flatten()
                    spike_preds[model_name] = pred[1].flatten()
                else:
                    # ë‹¨ì¼ ì¶œë ¥ ëª¨ë¸
                    regression_preds[model_name] = pred.flatten()
                    spike_preds[model_name] = np.zeros_like(pred.flatten())
                
                # ê°€ì¤‘ í‰ê· 
                weight = weights.get(model_name, 0.25)
                ensemble_reg += weight * regression_preds[model_name]
                ensemble_spike += weight * spike_preds[model_name]
                total_weight += weight
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        if total_weight > 0:
            ensemble_reg /= total_weight
            ensemble_spike /= total_weight
        
        # ê¸‰ì¦ í™•ë¥ ì´ ë†’ìœ¼ë©´ ì˜ˆì¸¡ê°’ ìƒí–¥ ì¡°ì •
        spike_mask = ensemble_spike > 0.7
        ensemble_reg[spike_mask] *= 1.15
        
        return ensemble_reg, ensemble_spike, regression_preds, spike_preds
    
    def inverse_scale_predictions(self, predictions):
        """ì˜ˆì¸¡ê°’ ì—­ìŠ¤ì¼€ì¼ë§"""
        if hasattr(self.scaler, 'feature_names_in_'):
            feature_names = list(self.scaler.feature_names_in_)
            n_features = len(feature_names)
            dummy = np.zeros((len(predictions), n_features))
            
            if 'FUTURE' in feature_names:
                future_idx = feature_names.index('FUTURE')
            else:
                future_idx = 0
            
            dummy[:, future_idx] = predictions
            return self.scaler.inverse_transform(dummy)[:, future_idx]
        else:
            # ê¸°ë³¸ ì—­ìŠ¤ì¼€ì¼ë§
            n_features = self.scaler.n_features_in_
            dummy = np.zeros((len(predictions), n_features))
            dummy[:, 0] = predictions
            return self.scaler.inverse_transform(dummy)[:, 0]
    
    def print_spike_prediction_details(self, predictions, spike_probs, actual_values, times):
        """ê¸‰ì¦ ì˜ˆì¸¡ ìƒì„¸ ì¶œë ¥"""
        print("\n" + "="*100)
        print("10ë¶„ í›„ ê¸‰ì¦ ì˜ˆì¸¡ ìƒì„¸ ì •ë³´")
        print("="*100)
        
        # ìµœê·¼ 20ê°œ ì˜ˆì¸¡ ê²°ê³¼
        n_display = min(20, len(predictions))
        
        print(f"\nìµœê·¼ {n_display}ê°œ ì˜ˆì¸¡ ê²°ê³¼:")
        print("-"*100)
        print(f"{'í˜„ì¬ ì‹œê°„':^20} | {'í˜„ì¬ê°’':^10} | {'10ë¶„í›„ ì˜ˆì¸¡':^12} | {'ì‹¤ì œê°’':^10} | {'ê¸‰ì¦í™•ë¥ ':^10} | {'ê¸‰ì¦ì˜ˆì¸¡':^10} | {'ì‹¤ì œê¸‰ì¦':^10}")
        print("-"*100)
        
        for i in range(-n_display, 0):
            current_time = pd.Timestamp(times[i])
            predict_time = current_time + timedelta(minutes=10)
            current_val = actual_values[i]
            predicted_val = predictions[i]
            actual_val = actual_values[i]
            spike_prob = spike_probs[i]
            spike_pred = "â˜…ì˜ˆâ˜…" if spike_prob > 0.5 else "ì•„ë‹ˆì˜¤"
            actual_spike = "â˜…ì˜ˆâ˜…" if actual_val > self.spike_threshold else "ì•„ë‹ˆì˜¤"
            
            # ê¸‰ì¦ ì˜ˆì¸¡ ë§ì¶˜ ê²½ìš° ê°•ì¡°
            if (spike_prob > 0.5 and actual_val > self.spike_threshold):
                print(f"\033[92m{current_time.strftime('%Y-%m-%d %H:%M'):^20} | {current_val:^10.0f} | "
                      f"{predicted_val:^12.0f} | {actual_val:^10.0f} | {spike_prob:^10.1%} | "
                      f"{spike_pred:^10} | {actual_spike:^10}\033[0m")
            elif (spike_prob > 0.5 and actual_val <= self.spike_threshold):
                # ì˜¤íƒ
                print(f"\033[93m{current_time.strftime('%Y-%m-%d %H:%M'):^20} | {current_val:^10.0f} | "
                      f"{predicted_val:^12.0f} | {actual_val:^10.0f} | {spike_prob:^10.1%} | "
                      f"{spike_pred:^10} | {actual_spike:^10}\033[0m")
            elif (spike_prob <= 0.5 and actual_val > self.spike_threshold):
                # ë¯¸íƒ
                print(f"\033[91m{current_time.strftime('%Y-%m-%d %H:%M'):^20} | {current_val:^10.0f} | "
                      f"{predicted_val:^12.0f} | {actual_val:^10.0f} | {spike_prob:^10.1%} | "
                      f"{spike_pred:^10} | {actual_spike:^10}\033[0m")
            else:
                print(f"{current_time.strftime('%Y-%m-%d %H:%M'):^20} | {current_val:^10.0f} | "
                      f"{predicted_val:^12.0f} | {actual_val:^10.0f} | {spike_prob:^10.1%} | "
                      f"{spike_pred:^10} | {actual_spike:^10}")
        
        print("-"*100)
        
        # ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥ ìš”ì•½
        spike_pred_mask = spike_probs[-n_display:] > 0.5
        actual_spike_mask = actual_values[-n_display:] > self.spike_threshold
        
        tp = np.sum(spike_pred_mask & actual_spike_mask)
        fp = np.sum(spike_pred_mask & ~actual_spike_mask)
        fn = np.sum(~spike_pred_mask & actual_spike_mask)
        tn = np.sum(~spike_pred_mask & ~actual_spike_mask)
        
        precision = tp / (tp + fp) * 100 if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0
        
        print(f"\nìµœê·¼ {n_display}ê°œ ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥:")
        print(f"  â€¢ ì •í™•íˆ ì˜ˆì¸¡í•œ ê¸‰ì¦: {tp}ê±´")
        print(f"  â€¢ ì˜¤íƒ (ì˜ëª»ëœ ê¸‰ì¦ ì˜ˆì¸¡): {fp}ê±´")
        print(f"  â€¢ ë¯¸íƒ (ë†“ì¹œ ê¸‰ì¦): {fn}ê±´")
        print(f"  â€¢ ì •ë°€ë„: {precision:.1f}%")
        print(f"  â€¢ ì¬í˜„ìœ¨: {recall:.1f}%")
        
        # ê°€ì¥ ìµœê·¼ ì˜ˆì¸¡ ê°•ì¡°
        print("\n" + "="*100)
        print("ê°€ì¥ ìµœê·¼ 10ë¶„ í›„ ê¸‰ì¦ ì˜ˆì¸¡")
        print("="*100)
        
        latest_time = pd.Timestamp(times[-1])
        latest_predict_time = latest_time + timedelta(minutes=10)
        
        print(f"í˜„ì¬ ì‹œê°„: {latest_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ì˜ˆì¸¡ ëŒ€ìƒ ì‹œê°„: {latest_predict_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"í˜„ì¬ ë¬¼ë¥˜ëŸ‰: {actual_values[-1]:.0f}")
        print(f"10ë¶„ í›„ ì˜ˆì¸¡ ë¬¼ë¥˜ëŸ‰: {predictions[-1]:.0f}")
        print(f"ê¸‰ì¦ í™•ë¥ : {spike_probs[-1]:.1%}")
        print(f"ê¸‰ì¦ ì˜ˆì¸¡: {'â˜…â˜…â˜… ì˜ˆ â˜…â˜…â˜…' if spike_probs[-1] > 0.5 else 'ì•„ë‹ˆì˜¤'}")
        
        if spike_probs[-1] > 0.7:
            print(f"\nğŸš¨ ê²½ê³ : 10ë¶„ í›„ ê¸‰ì¦ ë°œìƒ ê°€ëŠ¥ì„± ë§¤ìš° ë†’ìŒ! (í™•ë¥ : {spike_probs[-1]:.1%})")
            print(f"   ì˜ˆì¸¡ê°’({predictions[-1]:.0f}) > ì„ê³„ê°’({self.spike_threshold})")
        elif spike_probs[-1] > 0.5:
            print(f"\nâš ï¸  ì£¼ì˜: 10ë¶„ í›„ ê¸‰ì¦ ë°œìƒ ê°€ëŠ¥ì„± ìˆìŒ (í™•ë¥ : {spike_probs[-1]:.1%})")
        
        print("="*100 + "\n")
    
    def run_prediction(self, data_path):
        """ì „ì²´ ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        # ëª¨ë¸ ë¡œë“œ
        self.load_models()
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        processed_data = self.preprocess_data(data_path)
        
        # ìŠ¤ì¼€ì¼ë§
        scaled_data = self.scale_data(processed_data)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X_seq, y_reg_seq, y_cls_seq, time_seq, future_seq = self.create_sequences(scaled_data)
        
        logger.info(f"ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ - shape: {X_seq.shape}")
        
        # ì•™ìƒë¸” ì˜ˆì¸¡
        ensemble_reg, ensemble_spike, reg_preds, spike_preds = self.enhanced_ensemble_predict(X_seq)
        
        # ì—­ìŠ¤ì¼€ì¼ë§
        ensemble_pred_original = self.inverse_scale_predictions(ensemble_reg)
        
        # ê¸‰ì¦ ì˜ˆì¸¡ ìƒì„¸ ì¶œë ¥
        self.print_spike_prediction_details(
            ensemble_pred_original,
            ensemble_spike,
            future_seq,
            time_seq
        )
        
        # ê²°ê³¼ ì •ë¦¬
        results = {
            'predictions': ensemble_pred_original,
            'spike_probabilities': ensemble_spike,
            'actual_values': future_seq,
            'actual_spikes': y_cls_seq,
            'times': time_seq,
            'individual_regression': reg_preds,
            'individual_spike': spike_preds
        }
        
        return results
    
    def evaluate_spike_predictions(self, predictions, spike_probs, actual_values, actual_spikes):
        """ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€"""
        # ìˆ˜ì¹˜ ì˜ˆì¸¡ ì„±ëŠ¥
        mae = mean_absolute_error(actual_values, predictions)
        mse = mean_squared_error(actual_values, predictions)
        rmse = np.sqrt(mse)
        r2 = r2_score(actual_values, predictions)
        
        # ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥
        spike_pred_binary = (spike_probs > 0.5).astype(int)
        
        tp = np.sum((spike_pred_binary == 1) & (actual_spikes == 1))
        fp = np.sum((spike_pred_binary == 1) & (actual_spikes == 0))
        fn = np.sum((spike_pred_binary == 0) & (actual_spikes == 1))
        tn = np.sum((spike_pred_binary == 0) & (actual_spikes == 0))
        
        accuracy = (tp + tn) / (tp + tn + fp + fn) * 100 if (tp + tn + fp + fn) > 0 else 0
        precision = tp / (tp + fp) * 100 if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        metrics = {
            'mae': mae,
            'mse': mse,
            'rmse': rmse,
            'r2': r2,
            'spike_accuracy': accuracy,
            'spike_precision': precision,
            'spike_recall': recall,
            'spike_f1': f1,
            'spike_tp': tp,
            'spike_fp': fp,
            'spike_fn': fn,
            'spike_tn': tn
        }
        
        return metrics
    
    def save_results(self, results, metrics):
        """ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('spike_prediction_results', exist_ok=True)
        
        # ì˜ˆì¸¡ ê²°ê³¼ DataFrame ìƒì„±
        result_df = pd.DataFrame({
            'EVENT_DT': pd.Series(results['times']),
            'ACTUAL_VALUE': results['actual_values'],
            'PREDICT_DT': pd.Series(results['times']) + timedelta(minutes=10),
            'PREDICTED_VALUE': results['predictions'],
            'SPIKE_PROBABILITY': results['spike_probabilities'],
            'SPIKE_PREDICTED': (results['spike_probabilities'] > 0.5).astype(int),
            'SPIKE_ACTUAL': results['actual_spikes'],
            'ERROR': np.abs(results['actual_values'] - results['predictions'])
        })
        
        # CSV ì €ì¥
        csv_path = f'spike_prediction_results/spike_predictions_{timestamp}.csv'
        result_df.to_csv(csv_path, index=False)
        logger.info(f"ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {csv_path}")
        
        # ì„±ëŠ¥ ì§€í‘œ ì €ì¥
        metrics_path = f'spike_prediction_results/spike_metrics_{timestamp}.json'
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=4, default=str)
        logger.info(f"ì„±ëŠ¥ ì§€í‘œ ì €ì¥: {metrics_path}")
        
        return result_df
    
    def visualize_spike_results(self, results, metrics):
        """ê¸‰ì¦ ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""
        # ìƒ˜í”Œ í¬ê¸°
        sample_size = min(500, len(results['predictions']))
        
        # ê·¸ë¦¼ í¬ê¸° ì„¤ì •
        fig = plt.figure(figsize=(20, 15))
        
        # 1. ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ë¹„êµ + ê¸‰ì¦ êµ¬ê°„ í‘œì‹œ
        ax1 = plt.subplot(3, 1, 1)
        ax1.plot(results['actual_values'][:sample_size], label='ì‹¤ì œê°’', color='blue', linewidth=2)
        ax1.plot(results['predictions'][:sample_size], label='ì˜ˆì¸¡ê°’', color='red', linewidth=1.5)
        ax1.axhline(y=self.spike_threshold, color='orange', linestyle='--', 
                   label=f'ê¸‰ì¦ ì„ê³„ê°’ ({self.spike_threshold})')
        
        # ê¸‰ì¦ êµ¬ê°„ í•˜ì´ë¼ì´íŠ¸
        actual_spike_mask = results['actual_spikes'][:sample_size] == 1
        if np.any(actual_spike_mask):
            spike_indices = np.where(actual_spike_mask)[0]
            ax1.scatter(spike_indices, results['actual_values'][:sample_size][spike_indices],
                       color='darkred', s=50, marker='o', label='ì‹¤ì œ ê¸‰ì¦', zorder=5, alpha=0.7)
        
        # ì˜ˆì¸¡ëœ ê¸‰ì¦
        pred_spike_mask = results['spike_probabilities'][:sample_size] > 0.5
        if np.any(pred_spike_mask):
            pred_indices = np.where(pred_spike_mask)[0]
            ax1.scatter(pred_indices, results['predictions'][:sample_size][pred_indices],
                       color='orange', s=30, marker='^', label='ì˜ˆì¸¡ ê¸‰ì¦', zorder=4)
        
        ax1.set_title('ê¸‰ì¦ ì˜ˆì¸¡ ê²°ê³¼ (10ë¶„ í›„)', fontsize=16)
        ax1.set_xlabel('ì‹œê°„ ì¸ë±ìŠ¤', fontsize=12)
        ax1.set_ylabel('ë¬¼ë¥˜ëŸ‰', fontsize=12)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. ê¸‰ì¦ í™•ë¥  ê·¸ë˜í”„
        ax2 = plt.subplot(3, 1, 2)
        ax2.plot(results['spike_probabilities'][:sample_size], label='ê¸‰ì¦ í™•ë¥ ', color='red', linewidth=1.5)
        ax2.axhline(y=0.5, color='black', linestyle='--', alpha=0.5, label='ê²°ì • ì„ê³„ê°’ (0.5)')
        ax2.axhline(y=0.7, color='orange', linestyle='--', alpha=0.5, label='ë†’ì€ í™•ì‹  (0.7)')
        
        # ì‹¤ì œ ê¸‰ì¦ êµ¬ê°„ ë°°ê²½ìƒ‰
        for i in range(sample_size):
            if results['actual_spikes'][i] == 1:
                ax2.axvspan(i-0.5, i+0.5, alpha=0.2, color='red')
        
        ax2.set_title('ê¸‰ì¦ í™•ë¥  ì˜ˆì¸¡', fontsize=16)
        ax2.set_xlabel('ì‹œê°„ ì¸ë±ìŠ¤', fontsize=12)
        ax2.set_ylabel('í™•ë¥ ', fontsize=12)
        ax2.set_ylim(0, 1)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. ì„±ëŠ¥ ì§€í‘œ ìš”ì•½
        ax3 = plt.subplot(3, 2, 5)
        
        # ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥
        metrics_text = f"""ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥
        
ì¬í˜„ìœ¨: {metrics['spike_recall']:.1f}%
ì •ë°€ë„: {metrics['spike_precision']:.1f}%
F1-Score: {metrics['spike_f1']:.1f}%
ì •í™•ë„: {metrics['spike_accuracy']:.1f}%

ì •í™•íˆ ì˜ˆì¸¡: {metrics['spike_tp']}ê±´
ì˜¤íƒ: {metrics['spike_fp']}ê±´
ë¯¸íƒ: {metrics['spike_fn']}ê±´"""
        
        ax3.text(0.1, 0.5, metrics_text, fontsize=14, transform=ax3.transAxes,
                verticalalignment='center', family='monospace',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        ax3.axis('off')
        
        # 4. í˜¼ë™ í–‰ë ¬
        ax4 = plt.subplot(3, 2, 6)
        confusion_matrix = np.array([[metrics['spike_tn'], metrics['spike_fp']],
                                    [metrics['spike_fn'], metrics['spike_tp']]])
        
        sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['ì •ìƒ', 'ê¸‰ì¦'],
                   yticklabels=['ì •ìƒ', 'ê¸‰ì¦'],
                   ax=ax4)
        ax4.set_title('ê¸‰ì¦ ì˜ˆì¸¡ í˜¼ë™ í–‰ë ¬', fontsize=14)
        ax4.set_xlabel('ì˜ˆì¸¡', fontsize=12)
        ax4.set_ylabel('ì‹¤ì œ', fontsize=12)
        
        plt.tight_layout()
        
        # ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        plt.savefig(f'spike_prediction_results/spike_visualization_{timestamp}.png',
                   dpi=300, bbox_inches='tight')
        plt.show()

# ===================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ===================================

def main(data_path=None):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    predictor = SpikePredictor()
    
    # ë°ì´í„° ê²½ë¡œ ì„¤ì •
    if data_path is None:
        # ê¸°ë³¸ ê²½ë¡œë“¤
        possible_paths = [
            'data/20250731_to_20250806.csv',
            'data/0730to31.csv',
            'data/20240201_TO_202507281705.csv'
        ]
        for path in possible_paths:
            if os.path.exists(path):
                data_path = path
                break
        
        if data_path is None:
            logger.error("ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
    
    # ì˜ˆì¸¡ ì‹¤í–‰
    logger.info("ê¸‰ì¦ ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
    results = predictor.run_prediction(data_path)
    
    # ì„±ëŠ¥ í‰ê°€
    logger.info("ì„±ëŠ¥ í‰ê°€ ì¤‘...")
    metrics = predictor.evaluate_spike_predictions(
        results['predictions'],
        results['spike_probabilities'],
        results['actual_values'],
        results['actual_spikes']
    )
    
    # ê²°ê³¼ ì¶œë ¥
    logger.info("\n" + "="*60)
    logger.info("ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥ ìš”ì•½")
    logger.info("="*60)
    logger.info(f"[ìˆ˜ì¹˜ ì˜ˆì¸¡ ì„±ëŠ¥]")
    logger.info(f"  MAE: {metrics['mae']:.2f}")
    logger.info(f"  RMSE: {metrics['rmse']:.2f}")
    logger.info(f"  RÂ²: {metrics['r2']:.4f}")
    logger.info(f"\n[ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥]")
    logger.info(f"  ì¬í˜„ìœ¨: {metrics['spike_recall']:.1f}% {'âœ“ ëª©í‘œ ë‹¬ì„±' if metrics['spike_recall'] >= 70 else ''}")
    logger.info(f"  ì •ë°€ë„: {metrics['spike_precision']:.1f}%")
    logger.info(f"  F1-Score: {metrics['spike_f1']:.1f}%")
    logger.info(f"  ì •í™•ë„: {metrics['spike_accuracy']:.1f}%")
    
    # ê²°ê³¼ ì €ì¥
    logger.info("\nê²°ê³¼ ì €ì¥ ì¤‘...")
    result_df = predictor.save_results(results, metrics)
    
    # ì‹œê°í™”
    logger.info("ê²°ê³¼ ì‹œê°í™” ì¤‘...")
    predictor.visualize_spike_results(results, metrics)
    
    logger.info("\n" + "="*60)
    logger.info("ê¸‰ì¦ ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
    logger.info("="*60)
    
    return results, metrics, result_df

# ===================================
# ì‹¤ì‹œê°„ ì˜ˆì¸¡ í•¨ìˆ˜
# ===================================

def predict_realtime_spike(new_data_path):
    """ì‹¤ì‹œê°„ ê¸‰ì¦ ì˜ˆì¸¡"""
    predictor = SpikePredictor()
    
    # ëª¨ë¸ ë¡œë“œ
    if not predictor.models:
        predictor.load_models()
    
    try:
        # ë°ì´í„° ì „ì²˜ë¦¬
        processed_data = predictor.preprocess_data(new_data_path)
        
        # ë§ˆì§€ë§‰ 30ë¶„ ë°ì´í„°
        last_30_rows = processed_data.tail(30)
        
        if len(last_30_rows) < 30:
            logger.warning("ì‹¤ì‹œê°„ ì˜ˆì¸¡ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ìµœì†Œ 30ê°œ í•„ìš”)")
            return None
        
        # ìŠ¤ì¼€ì¼ë§
        scaled_data = predictor.scale_data(last_30_rows)
        
        # ì…ë ¥ íŠ¹ì§• ì„ íƒ
        input_features = [col for col in scaled_data.columns
                         if col not in ['scaled_FUTURE', 'future_spike', 'TIME', 'FUTURE']]
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X_realtime = scaled_data[input_features].values.reshape(1, 30, -1)
        
        # ì˜ˆì¸¡
        reg_pred, spike_pred, _, _ = predictor.enhanced_ensemble_predict(X_realtime)
        
        # ì—­ìŠ¤ì¼€ì¼ë§
        prediction = predictor.inverse_scale_predictions(reg_pred)[0]
        spike_probability = spike_pred[0]
        
        # ê¸‰ì¦ ì—¬ë¶€
        is_spike = spike_probability > 0.5
        
        # ê²°ê³¼
        current_time = processed_data.index[-1]
        predict_time = current_time + timedelta(minutes=10)
        
        result = {
            'current_time': current_time,
            'predict_time': predict_time,
            'current_value': processed_data['TOTALCNT'].iloc[-1],
            'predicted_value': prediction,
            'spike_probability': spike_probability,
            'is_spike': is_spike,
            'confidence': spike_probability if is_spike else 1 - spike_probability
        }
        
        # ì‹¤ì‹œê°„ ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("ì‹¤ì‹œê°„ ê¸‰ì¦ ì˜ˆì¸¡ ê²°ê³¼ (10ë¶„ í›„)")
        print("="*80)
        print(f"í˜„ì¬ ì‹œê°„: {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ì˜ˆì¸¡ ëŒ€ìƒ ì‹œê°„: {predict_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"í˜„ì¬ ë¬¼ë¥˜ëŸ‰: {result['current_value']:.0f}")
        print(f"10ë¶„ í›„ ì˜ˆì¸¡ ë¬¼ë¥˜ëŸ‰: {prediction:.0f}")
        print(f"ê¸‰ì¦ í™•ë¥ : {spike_probability:.1%}")
        print(f"ê¸‰ì¦ ì˜ˆì¸¡: {'â˜…â˜…â˜… ì˜ˆ â˜…â˜…â˜…' if is_spike else 'ì•„ë‹ˆì˜¤'}")
        print(f"ì˜ˆì¸¡ ì‹ ë¢°ë„: {result['confidence']:.1%}")
        
        if spike_probability > 0.7:
            print(f"\nğŸš¨ ê²½ê³ : 10ë¶„ í›„ ê¸‰ì¦ ë°œìƒ ê°€ëŠ¥ì„± ë§¤ìš° ë†’ìŒ!")
            print(f"   ì˜ˆì¸¡ê°’({prediction:.0f}) > ì„ê³„ê°’({predictor.spike_threshold})")
            print(f"   ì¦‰ì‹œ ëŒ€ì‘ ì¡°ì¹˜ í•„ìš”!")
        elif spike_probability > 0.5:
            print(f"\nâš ï¸  ì£¼ì˜: 10ë¶„ í›„ ê¸‰ì¦ ë°œìƒ ê°€ëŠ¥ì„± ìˆìŒ")
            print(f"   ëª¨ë‹ˆí„°ë§ ê°•í™” í•„ìš”")
        
        print("="*80 + "\n")
        
        return result
        
    except Exception as e:
        logger.error(f"ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
        return None

# ===================================
# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
# ===================================

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        # ëª…ë ¹ì¤„ ì¸ìë¡œ ë°ì´í„° ê²½ë¡œ ë°›ê¸°
        data_path = sys.argv[1]
        print(f"ë°ì´í„° ê²½ë¡œ: {data_path}")
        results, metrics, result_df = main(data_path)
    else:
        # ê¸°ë³¸ ì‹¤í–‰
        results, metrics, result_df = main()
    
    # ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì˜ˆì‹œ (ì˜µì…˜)
    # realtime_result = predict_realtime_spike('data/realtime_data.csv')