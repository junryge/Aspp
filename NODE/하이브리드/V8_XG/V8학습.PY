import numpy as np
import pandas as pd
import xgboost as xgb
import pickle
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import os

warnings.filterwarnings('ignore')
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def check_gpu_availability():
    """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    print("\n" + "="*80)
    print("GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸")
    print("="*80)
    
    try:
        test_X = np.random.rand(100, 10)
        test_y = np.random.rand(100)
        
        test_model = xgb.XGBRegressor(
            tree_method='gpu_hist',
            gpu_id=0,
            n_estimators=10
        )
        test_model.fit(test_X, test_y)
        
        print("âœ… GPU ì‚¬ìš© ê°€ëŠ¥! XGBoost GPU ëª¨ë“œë¡œ í•™ìŠµ")
        return 'gpu_hist', 0
        
    except Exception as e:
        print(f"âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€: {str(e)[:100]}")
        print("ğŸ’» CPU ëª¨ë“œë¡œ í•™ìŠµ")
        return 'hist', None

def create_features_100min(df, start_idx=100):
    """
    100ë¶„ ì‹œí€€ìŠ¤ë¡œ 10ë¶„ í›„ TOTALCNT ì˜ˆì¸¡ Feature ìƒì„±
    ê° ì»¬ëŸ¼ë³„ 8ê°œ í†µê³„ê°’ë§Œ ì‚¬ìš© (ì„±ê³µ ì½”ë“œ íŒ¨í„´)
    
    ì»¬ëŸ¼: M14AM14B, M14AM10A, M14AM16, TOTALCNT
    """
    
    print(f"\nğŸ“Š Feature ìƒì„± ì¤‘... (100ë¶„ ì‹œí€€ìŠ¤ â†’ 10ë¶„ í›„ ì˜ˆì¸¡)")
    
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required_cols = ['M14AM14B', 'M14AM10A', 'M14AM16', 'TOTALCNT']
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        raise ValueError(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")
    
    print(f"âœ… í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸: {required_cols}")
    
    features_list = []
    labels = []
    
    seq_totalcnt_max_list = []
    seq_totalcnt_min_list = []
    indices = []
    
    total_sequences = len(df) - start_idx - 10
    print(f"âœ… ìƒì„± ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤: {total_sequences:,}ê°œ")
    
    # Feature ìƒì„±
    for i in range(start_idx, len(df) - 10):
        if i % 10000 == 0:
            progress = (i - start_idx) / total_sequences * 100
            print(f"  ì§„í–‰ë¥ : {i-start_idx}/{total_sequences} ({progress:.1f}%)", end='\r')
        
        # 100ë¶„ ì‹œí€€ìŠ¤ ì¶”ì¶œ
        seq_m14b = df['M14AM14B'].iloc[i-100:i].values
        seq_m10a = df['M14AM10A'].iloc[i-100:i].values
        seq_m16 = df['M14AM16'].iloc[i-100:i].values
        seq_totalcnt = df['TOTALCNT'].iloc[i-100:i].values
        
        features = {}
        
        # ========== M14AM14B (8ê°œ í†µê³„ê°’) ==========
        features['m14b_mean'] = np.mean(seq_m14b)                    # 100ë¶„ í‰ê· 
        features['m14b_std'] = np.std(seq_m14b)                      # 100ë¶„ í‘œì¤€í¸ì°¨
        features['m14b_last_5_mean'] = np.mean(seq_m14b[-5:])        # ìµœê·¼ 5ë¶„ í‰ê· 
        features['m14b_max'] = np.max(seq_m14b)                      # 100ë¶„ ìµœëŒ€
        features['m14b_min'] = np.min(seq_m14b)                      # 100ë¶„ ìµœì†Œ
        features['m14b_slope'] = np.polyfit(np.arange(100), seq_m14b, 1)[0]  # ì¶”ì„¸
        features['m14b_last_10_mean'] = np.mean(seq_m14b[-10:])      # ìµœê·¼ 10ë¶„ í‰ê· 
        features['m14b_first_10_mean'] = np.mean(seq_m14b[:10])      # ì²˜ìŒ 10ë¶„ í‰ê· 
        
        # ========== M14AM10A (8ê°œ í†µê³„ê°’) ==========
        features['m10a_mean'] = np.mean(seq_m10a)
        features['m10a_std'] = np.std(seq_m10a)
        features['m10a_last_5_mean'] = np.mean(seq_m10a[-5:])
        features['m10a_max'] = np.max(seq_m10a)
        features['m10a_min'] = np.min(seq_m10a)
        features['m10a_slope'] = np.polyfit(np.arange(100), seq_m10a, 1)[0]
        features['m10a_last_10_mean'] = np.mean(seq_m10a[-10:])
        features['m10a_first_10_mean'] = np.mean(seq_m10a[:10])
        
        # ========== M14AM16 (8ê°œ í†µê³„ê°’) ==========
        features['m16_mean'] = np.mean(seq_m16)
        features['m16_std'] = np.std(seq_m16)
        features['m16_last_5_mean'] = np.mean(seq_m16[-5:])
        features['m16_max'] = np.max(seq_m16)
        features['m16_min'] = np.min(seq_m16)
        features['m16_slope'] = np.polyfit(np.arange(100), seq_m16, 1)[0]
        features['m16_last_10_mean'] = np.mean(seq_m16[-10:])
        features['m16_first_10_mean'] = np.mean(seq_m16[:10])
        
        # ========== TOTALCNT (8ê°œ í†µê³„ê°’) ==========
        features['totalcnt_mean'] = np.mean(seq_totalcnt)
        features['totalcnt_std'] = np.std(seq_totalcnt)
        features['totalcnt_last_5_mean'] = np.mean(seq_totalcnt[-5:])
        features['totalcnt_max'] = np.max(seq_totalcnt)
        features['totalcnt_min'] = np.min(seq_totalcnt)
        features['totalcnt_slope'] = np.polyfit(np.arange(100), seq_totalcnt, 1)[0]
        features['totalcnt_last_10_mean'] = np.mean(seq_totalcnt[-10:])
        features['totalcnt_first_10_mean'] = np.mean(seq_totalcnt[:10])
        
        features_list.append(features)
        
        # ë¼ë²¨: 10ë¶„ í›„ TOTALCNT ìµœëŒ€ê°’
        future_totalcnt = df['TOTALCNT'].iloc[i:i+10].values
        labels.append(np.max(future_totalcnt))
        
        # ì‹œí€€ìŠ¤ ì •ë³´ ì €ì¥
        seq_totalcnt_max_list.append(np.max(seq_totalcnt))
        seq_totalcnt_min_list.append(np.min(seq_totalcnt))
        indices.append(i)
    
    print(f"\nâœ… Feature ìƒì„± ì™„ë£Œ: {len(features_list):,}ê°œ ì‹œí€€ìŠ¤")
    
    X = pd.DataFrame(features_list)
    y = np.array(labels)
    
    seq_info = {
        'seq_max': seq_totalcnt_max_list,
        'seq_min': seq_totalcnt_min_list,
        'indices': indices
    }
    
    print(f"âœ… Feature ê°œìˆ˜: {X.shape[1]}ê°œ (4ì»¬ëŸ¼ Ã— 8ê°œ í†µê³„)")
    print(f"âœ… ë°ì´í„° ë²”ìœ„: ì¸ë±ìŠ¤ {indices[0]} ~ {indices[-1]}")
    
    return X, y, seq_info

def train_and_evaluate_complete():
    """
    100ë¶„ ì‹œí€€ìŠ¤ë¡œ 10ë¶„ í›„ TOTALCNT ì˜ˆì¸¡
    """
    print("="*80)
    print("XGBoost 100ë¶„ â†’ 10ë¶„ í›„ TOTALCNT ì˜ˆì¸¡")
    print("ì»¬ëŸ¼: M14AM14B, M14AM10A, M14AM16, TOTALCNT")
    print("Feature: ê° ì»¬ëŸ¼ë³„ 8ê°œ í†µê³„ê°’ (ì´ 32ê°œ)")
    print("="*80)
    
    # GPU í™•ì¸
    tree_method, gpu_id = check_gpu_availability()
    
    # ===== 1. ë°ì´í„° ë¡œë”© =====
    print("\n[STEP 1] ë°ì´í„° ë¡œë”©")
    print("-"*40)
    
    # CSV íŒŒì¼ ê²½ë¡œ
    csv_file = 'DATA/202502.CSV'
    print(f"ì‚¬ìš© íŒŒì¼: {csv_file}")
    
    if not os.path.exists(csv_file):
        raise FileNotFoundError(f"âŒ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_file}")
    
    df = pd.read_csv(csv_file, on_bad_lines='skip')
    print(f"âœ… ë°ì´í„° ë¡œë”©: {len(df):,}í–‰")
    print(f"âœ… ì»¬ëŸ¼: {list(df.columns)}")
    
    # ===== 2. Feature ìƒì„± =====
    print("\n[STEP 2] Feature ìƒì„± (100ë¶„ ì‹œí€€ìŠ¤)")
    print("-"*40)
    
    X, y, seq_info = create_features_100min(df)
    
    print(f"\nğŸ“Š TOTALCNT ë¶„ì„ (10ë¶„ í›„ ìµœëŒ€ê°’):")
    print(f"  í‰ê· : {y.mean():.2f}")
    print(f"  í‘œì¤€í¸ì°¨: {y.std():.2f}")
    print(f"  ìµœì†Œ: {y.min():.2f}")
    print(f"  ìµœëŒ€: {y.max():.2f}")
    print(f"  ìœ„í—˜(1700+): {np.sum(y >= 1700)}ê°œ ({np.sum(y >= 1700)/len(y)*100:.2f}%)")
    
    # ===== 3. Train/Test ë¶„í•  =====
    print("\n[STEP 3] Train/Test ë¶„í• ")
    print("-"*40)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, shuffle=True
    )
    
    print(f"âœ… í•™ìŠµ ë°ì´í„°: {X_train.shape[0]:,}ê°œ")
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]:,}ê°œ")
    
    # ===== 4. ëª¨ë¸ í•™ìŠµ =====
    print("\n[STEP 4] XGBoost ëª¨ë¸ í•™ìŠµ")
    print("-"*40)
    
    if gpu_id is not None:
        print(f"ğŸ® GPU ëª¨ë“œ: tree_method={tree_method}, gpu_id={gpu_id}")
        model = xgb.XGBRegressor(
            n_estimators=500,
            max_depth=8,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            tree_method=tree_method,
            gpu_id=gpu_id,
            random_state=42
        )
    else:
        print(f"ğŸ’» CPU ëª¨ë“œ: tree_method={tree_method}")
        model = xgb.XGBRegressor(
            n_estimators=500,
            max_depth=8,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            tree_method=tree_method,
            random_state=42,
            n_jobs=4
        )
    
    print("ğŸš€ í•™ìŠµ ì‹œì‘...")
    start_time = datetime.now()
    
    model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        verbose=50
    )
    
    elapsed = (datetime.now() - start_time).total_seconds()
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
    
    # ===== 5. ëª¨ë¸ í‰ê°€ =====
    print("\n[STEP 5] ëª¨ë¸ í‰ê°€")
    print("-"*40)
    
    # Train ì„±ëŠ¥
    y_train_pred = model.predict(X_train)
    train_mae = mean_absolute_error(y_train, y_train_pred)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    train_r2 = r2_score(y_train, y_train_pred)
    
    print(f"ğŸ“Š Train ì„±ëŠ¥:")
    print(f"  MAE:  {train_mae:.2f}")
    print(f"  RMSE: {train_rmse:.2f}")
    print(f"  RÂ²:   {train_r2:.4f}")
    
    # Test ì„±ëŠ¥
    y_test_pred = model.predict(X_test)
    test_mae = mean_absolute_error(y_test, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    test_r2 = r2_score(y_test, y_test_pred)
    
    print(f"\nğŸ“Š Test ì„±ëŠ¥:")
    print(f"  MAE:  {test_mae:.2f}")
    print(f"  RMSE: {test_rmse:.2f}")
    print(f"  RÂ²:   {test_r2:.4f}")
    
    print(f"\nğŸ“Š Train vs Test ì°¨ì´:")
    print(f"  MAE ì°¨ì´:  {abs(test_mae - train_mae):.2f}")
    print(f"  ê³¼ì í•© ì—¬ë¶€: {'âš ï¸ ê³¼ì í•©' if abs(test_mae - train_mae) > 50 else 'âœ… ì •ìƒ'}")
    
    # ===== 6. ëª¨ë¸ ì €ì¥ =====
    print("\n[STEP 6] ëª¨ë¸ ì €ì¥")
    print("-"*40)
    
    with open('xgboost_100to10_totalcnt.pkl', 'wb') as f:
        pickle.dump(model, f)
    print("âœ… ëª¨ë¸ ì €ì¥: xgboost_100to10_totalcnt.pkl")
    
    # ===== 7. ìƒì„¸ ë¶„ì„ =====
    print("\n[STEP 7] ìƒì„¸ ë¶„ì„")
    print("-"*40)
    
    # ìœ„í—˜ êµ¬ê°„ ë¶„ì„
    danger_mask_actual = y_test >= 1700
    danger_mask_pred = y_test_pred >= 1650  # ì˜ˆì¸¡ ì„ê³„ê°’
    
    danger_actual_count = np.sum(danger_mask_actual)
    danger_detected = np.sum(danger_mask_actual & danger_mask_pred)
    
    print(f"âš ï¸ ìœ„í—˜ êµ¬ê°„ (1700+) ë¶„ì„:")
    print(f"  ì‹¤ì œ ìœ„í—˜: {danger_actual_count}ê°œ")
    print(f"  ê°ì§€ ì„±ê³µ: {danger_detected}ê°œ")
    if danger_actual_count > 0:
        print(f"  ê°ì§€ìœ¨: {danger_detected/danger_actual_count*100:.1f}%")
    
    # ì˜¤ì°¨ ë¶„ì„
    errors = y_test_pred - y_test
    print(f"\nğŸ“Š ì˜¤ì°¨ ë¶„ì„:")
    print(f"  í‰ê·  ì˜¤ì°¨: {np.mean(errors):.2f}")
    print(f"  ì˜¤ì°¨ í‘œì¤€í¸ì°¨: {np.std(errors):.2f}")
    print(f"  ì˜¤ì°¨ ë²”ìœ„: [{np.min(errors):.2f}, {np.max(errors):.2f}]")
    
    # ===== 8. ì‹œê°í™” =====
    print("\n[STEP 8] ì‹œê°í™” ìƒì„±")
    print("-"*40)
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    
    # 1. ì˜ˆì¸¡ vs ì‹¤ì œ
    ax1 = axes[0, 0]
    ax1.scatter(y_test, y_test_pred, alpha=0.3, s=5, color='blue')
    ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    ax1.axhline(y=1700, color='orange', linestyle='--', alpha=0.5, label='Danger(1700)')
    ax1.axvline(x=1700, color='orange', linestyle='--', alpha=0.5)
    ax1.set_xlabel('Actual TOTALCNT')
    ax1.set_ylabel('Predicted TOTALCNT')
    ax1.set_title(f'Actual vs Predicted\nMAE={test_mae:.2f}, RÂ²={test_r2:.3f}')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. ì‹œê³„ì—´ (ì²˜ìŒ 500ê°œ)
    ax2 = axes[0, 1]
    plot_size = min(500, len(y_test))
    ax2.plot(range(plot_size), y_test[:plot_size], 'b-', label='Actual', alpha=0.7, linewidth=1)
    ax2.plot(range(plot_size), y_test_pred[:plot_size], 'r--', label='Predicted', alpha=0.7, linewidth=1)
    ax2.axhline(y=1700, color='orange', linestyle='--', label='Danger(1700)', alpha=0.5)
    ax2.set_xlabel('Time Index')
    ax2.set_ylabel('TOTALCNT')
    ax2.set_title('Time Series (First 500)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. ì˜¤ì°¨ ë¶„í¬
    ax3 = axes[0, 2]
    ax3.hist(errors, bins=50, edgecolor='black', alpha=0.7, color='skyblue')
    ax3.axvline(x=0, color='r', linestyle='--', linewidth=2, label='Zero Error')
    ax3.set_xlabel('Prediction Error')
    ax3.set_ylabel('Frequency')
    ax3.set_title(f'Error Distribution\nMean={np.mean(errors):.2f}, Std={np.std(errors):.2f}')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. ìœ„í—˜ êµ¬ê°„ ì„±ëŠ¥
    ax4 = axes[1, 0]
    normal_mask = y_test < 1700
    ax4.scatter(y_test[normal_mask], y_test_pred[normal_mask], 
               alpha=0.2, s=5, label='Normal(<1700)', color='blue')
    ax4.scatter(y_test[danger_mask_actual], y_test_pred[danger_mask_actual], 
               alpha=0.8, s=20, label='Danger(1700+)', color='red')
    ax4.plot([1200, 2000], [1200, 2000], 'k--', lw=1)
    ax4.axhline(y=1700, color='orange', linestyle='--', alpha=0.5)
    ax4.axvline(x=1700, color='orange', linestyle='--', alpha=0.5)
    ax4.set_xlabel('Actual')
    ax4.set_ylabel('Predicted')
    ax4.set_title(f'Danger Zone Performance\nDetected: {danger_detected}/{danger_actual_count}')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # 5. MAE by Value Range
    ax5 = axes[1, 1]
    bins = np.array([1000, 1200, 1400, 1600, 1800, 2000])
    bin_labels = ['1000-1200', '1200-1400', '1400-1600', '1600-1800', '1800+']
    mae_by_range = []
    count_by_range = []
    
    for i in range(len(bins)-1):
        mask = (y_test >= bins[i]) & (y_test < bins[i+1])
        count = np.sum(mask)
        count_by_range.append(count)
        if count > 0:
            mae_by_range.append(mean_absolute_error(y_test[mask], y_test_pred[mask]))
        else:
            mae_by_range.append(0)
    
    bars = ax5.bar(bin_labels, mae_by_range, color='steelblue', alpha=0.7)
    ax5.set_xlabel('TOTALCNT Range')
    ax5.set_ylabel('MAE')
    ax5.set_title('MAE by Value Range')
    ax5.tick_params(axis='x', rotation=45)
    
    # ê°œìˆ˜ í‘œì‹œ
    for i, (bar, count) in enumerate(zip(bars, count_by_range)):
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height,
                f'n={count}',
                ha='center', va='bottom', fontsize=8)
    
    ax5.grid(True, alpha=0.3)
    
    # 6. Feature ì¤‘ìš”ë„ (Top 20)
    ax6 = axes[1, 2]
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False).head(20)
    
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(feature_importance)))
    ax6.barh(range(len(feature_importance)), feature_importance['importance'].values, color=colors)
    ax6.set_yticks(range(len(feature_importance)))
    ax6.set_yticklabels(feature_importance['feature'].values, fontsize=8)
    ax6.set_xlabel('Importance')
    ax6.set_title('Top 20 Feature Importance')
    ax6.invert_yaxis()
    ax6.grid(True, alpha=0.3)
    
    plt.suptitle('100min -> 10min TOTALCNT Prediction (XGBoost)', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('xgboost_100to10_evaluation.png', dpi=150, bbox_inches='tight')
    print("âœ… ê·¸ë˜í”„ ì €ì¥: xgboost_100to10_evaluation.png")
    
    # ===== 9. Feature ì¤‘ìš”ë„ ì „ì²´ ì¶œë ¥ =====
    print("\n[STEP 9] Feature ì¤‘ìš”ë„ (ì „ì²´)")
    print("-"*40)
    
    feature_importance_full = pd.DataFrame({
        'feature': X.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(feature_importance_full.to_string(index=False))
    
    # CSV ì €ì¥
    feature_importance_full.to_csv('feature_importance.csv', index=False)
    print("\nâœ… Feature ì¤‘ìš”ë„ ì €ì¥: feature_importance.csv")
    
    # ===== 10. ìµœì¢… ìš”ì•½ =====
    print("\n" + "="*80)
    print("ìµœì¢… í‰ê°€ ìš”ì•½")
    print("="*80)
    print(f"1. ëª¨ë¸ ì„±ëŠ¥:")
    print(f"   - Train MAE: {train_mae:.2f}")
    print(f"   - Test MAE:  {test_mae:.2f}")
    print(f"   - Test RÂ²:   {test_r2:.4f}")
    
    print(f"\n2. ìœ„í—˜ êµ¬ê°„ ì„±ëŠ¥:")
    print(f"   - ì‹¤ì œ ìœ„í—˜(1700+): {danger_actual_count}ê°œ")
    print(f"   - ê°ì§€ ì„±ê³µ: {danger_detected}ê°œ ({danger_detected/danger_actual_count*100 if danger_actual_count > 0 else 0:.1f}%)")
    
    print(f"\n3. ì‹œí€€ìŠ¤ ì •ë³´:")
    print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: 100ë¶„")
    print(f"   - ì˜ˆì¸¡ ë²”ìœ„: 10ë¶„ í›„")
    print(f"   - Feature ìˆ˜: {X.shape[1]}ê°œ (4ì»¬ëŸ¼ Ã— 8í†µê³„)")
    
    print(f"\n4. ì €ì¥ íŒŒì¼:")
    print(f"   - ëª¨ë¸: xgboost_100to10_totalcnt.pkl")
    print(f"   - ê·¸ë˜í”„: xgboost_100to10_evaluation.png")
    print(f"   - Feature ì¤‘ìš”ë„: feature_importance.csv")
    
    print("\n" + "="*80)
    
    return model, feature_importance_full

# ì‹¤í–‰
if __name__ == '__main__':
    model, feature_importance = train_and_evaluate_complete()