"""
TensorFlow 2.18.0 - ì¬ì‹œì‘ ê°€ëŠ¥í•œ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (ì˜¤ë¥˜ ìˆ˜ì •íŒ)
========================================================
í•™ìŠµ ë°ì´í„°: 20240201_TO_202507281705.csv
í•™ìŠµ ì¤‘ë‹¨ ì‹œ ì¬ì‹œì‘ ê¸°ëŠ¥ ì™„ë²½ ì§€ì›

í•µì‹¬ ê¸°ëŠ¥:
1. ì²´í¬í¬ì¸íŠ¸ ìë™ ì €ì¥
2. ì¤‘ë‹¨ì ë¶€í„° ì¬ì‹œì‘
3. í•™ìŠµ ì§„í–‰ ìƒí™© ì¶”ì 
4. ëª¨ë¸ë³„ ê°œë³„ ê´€ë¦¬
5. ë°ì´í„° ì „ì²˜ë¦¬ ìƒíƒœ ë³´ì¡´

ë°ì´í„° ì „ì²˜ë¦¬:
- ì „ì²´ ê¸°ê°„: 2024-02-01 ~ 2025-07-28 (20240201_TO_202507281705.csv)
- ë‚ ì§œ í•„í„°ë§: ì˜µì…˜ (use_date_filter=False ê¸°ë³¸ê°’)
- PM ê¸°ê°„(2024-10-23) ì˜ˆì™¸ ì²˜ë¦¬
- ì •ìƒ ë²”ìœ„: 800 ~ 2500 (PM ê¸°ê°„ ì œì™¸)

êµ¬í˜„ ì™„ë£Œ:
âœ… RobustScaler ì‚¬ìš© (ì´ìƒì¹˜ì— ê°•ê±´)
âœ… ë³€í™”ìœ¨ ê¸°ë°˜ íŠ¹ì§• ê°•í™”
âœ… Attention ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€
âœ… ë‹¤ë‹¨ê³„ ì˜ˆì¸¡ + ì•™ìƒë¸”
âœ… ë™ì  ì„ê³„ê°’ í•™ìŠµ
âœ… ì¬ì‹œì‘ ê°€ëŠ¥í•œ í•™ìŠµ

ì˜¤ë¥˜ ìˆ˜ì •:
âœ… .weights.h5 í™•ì¥ìë¡œ ëª¨ë“  ì €ì¥ ê²½ë¡œ ìˆ˜ì •
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import os
import json
import pickle
from datetime import datetime
import joblib
import warnings
import gc
import traceback
import signal
import sys
import logging

# TensorFlow 2.18.0 ì„¤ì •
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
tf.config.set_visible_devices([], 'GPU')

# ë©”ëª¨ë¦¬ ìµœì í™”
tf.config.threading.set_inter_op_parallelism_threads(4)
tf.config.threading.set_intra_op_parallelism_threads(4)

# ëœë¤ ì‹œë“œ
RANDOM_SEED = 2079936
tf.random.set_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

print(f"TensorFlow Version: {tf.__version__}")
print("="*70)

# ===================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ ì‹œìŠ¤í…œ (íŒŒì¼ í˜•ì‹ ìˆ˜ì •)
# ===================================

class CheckpointManager:
    """í•™ìŠµ ìƒíƒœ ì €ì¥ ë° ë³µì› ê´€ë¦¬"""
    
    def __init__(self, checkpoint_dir='checkpoints'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        os.makedirs('model', exist_ok=True)
        
        # ê²½ë¡œ ì„¤ì •
        self.state_file = os.path.join(checkpoint_dir, 'training_state.json')
        self.data_file = os.path.join(checkpoint_dir, 'preprocessed_data.pkl')
        self.scaler_file = os.path.join(checkpoint_dir, 'scalers.pkl')
        self.history_file = os.path.join(checkpoint_dir, 'training_history.pkl')
        
        # ìƒíƒœ ì´ˆê¸°í™”
        self.state = {
            'current_model': None,
            'current_epoch': 0,
            'completed_models': [],
            'model_histories': {},
            'best_scores': {},
            'training_config': {},
            'timestamp': None
        }
        
        # Ctrl+C í•¸ë“¤ëŸ¬ ë“±ë¡
        signal.signal(signal.SIGINT, self.signal_handler)
        
    def signal_handler(self, sig, frame):
        """Ctrl+C ì‹œ ìë™ ì €ì¥"""
        print('\n\nâš ï¸  í•™ìŠµ ì¤‘ë‹¨ ê°ì§€! í˜„ì¬ ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤...')
        self.save_emergency_checkpoint()
        print('âœ… ê¸´ê¸‰ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ!')
        print('ë‹¤ì‹œ ì‹œì‘í•˜ë ¤ë©´: python script.py --resume')
        sys.exit(0)
    
    def save_emergency_checkpoint(self):
        """ê¸´ê¸‰ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        self.state['interrupted'] = True
        self.state['interrupt_time'] = datetime.now().isoformat()
        self.save_state()
    
    def save_state(self):
        """í˜„ì¬ í•™ìŠµ ìƒíƒœ ì €ì¥"""
        self.state['timestamp'] = datetime.now().isoformat()
        with open(self.state_file, 'w') as f:
            json.dump(self.state, f, indent=4)
        print(f"ğŸ“ ìƒíƒœ ì €ì¥: {self.state_file}")
    
    def load_state(self):
        """ì €ì¥ëœ í•™ìŠµ ìƒíƒœ ë¡œë“œ"""
        if os.path.exists(self.state_file):
            with open(self.state_file, 'r') as f:
                self.state = json.load(f)
            print(f"ğŸ“‚ ìƒíƒœ ë¡œë“œ: {self.state_file}")
            return True
        return False
    
    def save_data(self, X_train, y_train, X_val, y_val, X_test, y_test, 
                  y_train_orig, y_val_orig, y_test_orig, feature_cols):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        data = {
            'X_train': X_train, 'y_train': y_train,
            'X_val': X_val, 'y_val': y_val,
            'X_test': X_test, 'y_test': y_test,
            'y_train_orig': y_train_orig,
            'y_val_orig': y_val_orig,
            'y_test_orig': y_test_orig,
            'feature_cols': feature_cols,
            'shape_info': {
                'train': X_train.shape,
                'val': X_val.shape,
                'test': X_test.shape
            }
        }
        with open(self.data_file, 'wb') as f:
            pickle.dump(data, f)
        print(f"ğŸ“ ë°ì´í„° ì €ì¥: {self.data_file}")
    
    def load_data(self):
        """ì €ì¥ëœ ë°ì´í„° ë¡œë“œ"""
        if os.path.exists(self.data_file):
            with open(self.data_file, 'rb') as f:
                data = pickle.load(f)
            print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ: {self.data_file}")
            return data
        return None
    
    def save_scalers(self, target_scaler, feature_scaler):
        """ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥"""
        scalers = {
            'target_scaler': target_scaler,
            'feature_scaler': feature_scaler
        }
        with open(self.scaler_file, 'wb') as f:
            pickle.dump(scalers, f)
        print(f"ğŸ“ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {self.scaler_file}")
    
    def load_scalers(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        if os.path.exists(self.scaler_file):
            with open(self.scaler_file, 'rb') as f:
                scalers = pickle.load(f)
            print(f"ğŸ“‚ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ: {self.scaler_file}")
            return scalers
        return None
    
    def save_history(self, model_name, history):
        """í•™ìŠµ ì´ë ¥ ì €ì¥"""
        if os.path.exists(self.history_file):
            with open(self.history_file, 'rb') as f:
                all_histories = pickle.load(f)
        else:
            all_histories = {}
        
        # ê¸°ì¡´ ì´ë ¥ê³¼ ë³‘í•©
        if model_name in all_histories:
            for key in history.history.keys():
                all_histories[model_name][key].extend(history.history[key])
        else:
            all_histories[model_name] = history.history
        
        with open(self.history_file, 'wb') as f:
            pickle.dump(all_histories, f)
    
    def load_history(self):
        """í•™ìŠµ ì´ë ¥ ë¡œë“œ"""
        if os.path.exists(self.history_file):
            with open(self.history_file, 'rb') as f:
                return pickle.load(f)
        return {}
    
    def get_model_checkpoint_path(self, model_name):
        """ëª¨ë¸ë³„ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ"""
        return os.path.join(self.checkpoint_dir, f'{model_name}_checkpoint.weights.h5')
    
    def save_model_weights(self, model, model_name, epoch):
        """ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ (ìˆ˜ì •ë¨: .weights.h5 í™•ì¥ì ì‚¬ìš©)"""
        # .weights.h5 í™•ì¥ì ì‚¬ìš©
        weights_path = os.path.join(self.checkpoint_dir, f'{model_name}_weights_epoch_{epoch}.weights.h5')
        model.save_weights(weights_path)
        return weights_path
    
    def load_model_weights(self, model, model_name):
        """ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ"""
        # ë¨¼ì € ì „ì²´ ëª¨ë¸ (.keras) í™•ì¸
        keras_path = f'model/{model_name}_final.keras'
        if os.path.exists(keras_path):
            try:
                loaded_model = keras.models.load_model(keras_path, compile=False)
                model.set_weights(loaded_model.get_weights())
                print(f"ğŸ“‚ ì „ì²´ ëª¨ë¸ ë¡œë“œ: {model_name} (.keras)")
                return True
            except:
                pass
        
        # .weights.h5 íŒŒì¼ ì‹œë„
        path = os.path.join(self.checkpoint_dir, f'{model_name}_checkpoint.weights.h5')
        if os.path.exists(path):
            try:
                model.load_weights(path)
                print(f"ğŸ“‚ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ: {model_name}")
                return True
            except:
                pass
        
        # ì—í­ë³„ íŒŒì¼ í™•ì¸
        for file in os.listdir(self.checkpoint_dir):
            if file.startswith(f'{model_name}_weights_epoch_') and file.endswith('.weights.h5'):
                try:
                    path = os.path.join(self.checkpoint_dir, file)
                    model.load_weights(path)
                    print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ (ì—í­ íŒŒì¼): {model_name}")
                    return True
                except:
                    continue
        
        return False
    
    def update_progress(self, model_name, epoch, metrics=None):
        """í•™ìŠµ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
        self.state['current_model'] = model_name
        self.state['current_epoch'] = epoch
        if metrics:
            if model_name not in self.state['best_scores']:
                self.state['best_scores'][model_name] = {}
            self.state['best_scores'][model_name].update(metrics)
        self.save_state()


# ===================================
# ì»¤ìŠ¤í…€ ì½œë°±
# ===================================

class ProgressCallback(Callback):
    """í•™ìŠµ ì§„í–‰ ìƒí™© ì¶”ì  ì½œë°±"""
    
    def __init__(self, checkpoint_manager, model_name):
        super().__init__()
        self.checkpoint_manager = checkpoint_manager
        self.model_name = model_name
        self.epoch_count = 0
        
    def on_epoch_end(self, epoch, logs=None):
        self.epoch_count += 1
        
        # 5 ì—í­ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if self.epoch_count % 5 == 0:
            try:
                metrics = {
                    'loss': logs.get('loss'),
                    'val_loss': logs.get('val_loss'),
                    'mae': logs.get('mae'),
                    'val_mae': logs.get('val_mae')
                }
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                self.checkpoint_manager.update_progress(
                    self.model_name, 
                    self.epoch_count,
                    metrics
                )
                
                # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
                self.checkpoint_manager.save_model_weights(
                    self.model, 
                    self.model_name,
                    self.epoch_count
                )
                
                print(f"\nğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (Epoch {self.epoch_count})")
            except Exception as e:
                print(f"\nâš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")


# ===================================
# ë°ì´í„° ì „ì²˜ë¦¬ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ===================================

class AdvancedPreprocessor:
    """ê°œë³„ ê²½ë¡œ ë°ì´í„°ë¥¼ í™œìš©í•œ ê³ ê¸‰ ì „ì²˜ë¦¬"""
    
    def __init__(self):
        self.target_scaler = RobustScaler()
        self.feature_scaler = StandardScaler()
        self.route_scalers = {}
        self.routes = ['M14AM10A', 'M10AM14A', 'M14AM14B', 
                      'M14BM14A', 'M14AM16', 'M16M14A']
        self.is_fitted = False
        
    def load_and_process(self, file_path):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("\n[ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬]")
        
        data = pd.read_csv(file_path)
        print(f"âœ“ ì›ë³¸ ë°ì´í„°: {len(data)} í–‰")
        
        # ì‹œê°„ ë³€í™˜
        data['CURRTIME'] = pd.to_datetime(data['CURRTIME'], format='%Y%m%d%H%M')
        data['TIME'] = pd.to_datetime(data['TIME'], format='%Y%m%d%H%M')
        
        # SUM ì»¬ëŸ¼ ì œê±°
        sum_cols = [col for col in data.columns if 'SUM' in col]
        data = data.drop(columns=sum_cols, errors='ignore')
        
        # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ (ì„ íƒì )
        use_date_filter = False  # Trueë¡œ ì„¤ì •í•˜ë©´ ë‚ ì§œ í•„í„°ë§ ì ìš©
        
        if use_date_filter:
            start_date = pd.to_datetime('2024-02-01 00:00:00')
            end_date = pd.to_datetime('2024-07-27 23:59:59')
            data = data[(data['TIME'] >= start_date) & (data['TIME'] <= end_date)].reset_index(drop=True)
            print(f"âœ“ í•„í„°ë§ í›„: {len(data)} í–‰ (2024-02-01 ~ 2024-07-27)")
        else:
            print(f"âœ“ ì „ì²´ ë°ì´í„° ì‚¬ìš©: {len(data)} í–‰")
        
        # ì¸ë±ìŠ¤ ì„¤ì •
        data.set_index('CURRTIME', inplace=True)
        
        # PM ê¸°ê°„ ì„¤ì • (ì˜ˆì™¸ ì²˜ë¦¬)
        PM_start_date = pd.to_datetime('2024-10-23 00:00:00')
        PM_end_date = pd.to_datetime('2024-10-23 23:59:59')
        
        # PM ê¸°ê°„ ë°ì´í„°ì™€ ì •ìƒ ë²”ìœ„ ë°ì´í„° ë¶„ë¦¬
        within_pm = data[(data['TIME'] >= PM_start_date) & (data['TIME'] <= PM_end_date)]
        outside_pm = data[(data['TIME'] < PM_start_date) | (data['TIME'] > PM_end_date)]
        
        # PM ê¸°ê°„ì´ ì•„ë‹Œ ê²½ìš° 800-2500 ë²”ìœ„ í•„í„°ë§
        outside_pm_filtered = outside_pm[
            (outside_pm['TOTALCNT'] >= 800) & 
            (outside_pm['TOTALCNT'] <= 2500)
        ]
        
        # PM ê¸°ê°„ ë°ì´í„°ì™€ í•„í„°ë§ëœ ë°ì´í„° ë³‘í•©
        data = pd.concat([within_pm, outside_pm_filtered])
        data = data.sort_values(by='TIME')
        
        print(f"âœ“ ì´ìƒì¹˜ ì²˜ë¦¬ í›„: {len(data)} í–‰")
        print(f"âœ“ TOTALCNT ë²”ìœ„: {data['TOTALCNT'].min()} ~ {data['TOTALCNT'].max()}")
        
        high_count = (data['TOTALCNT'] >= 1400).sum()
        print(f"âœ“ 1400 ì´ìƒ: {high_count}ê°œ ({high_count/len(data)*100:.2f}%)")
        
        return data
    
    def create_all_features(self, data):
        """ëª¨ë“  íŠ¹ì§• ìƒì„±"""
        
        # FUTURE íƒ€ê²Ÿ
        data['FUTURE'] = data['TOTALCNT'].shift(-10)
        
        # ê°œë³„ ê²½ë¡œ íŠ¹ì§•
        for route in self.routes:
            if route in data.columns:
                # ë³€í™”ìœ¨
                data[f'{route}_pct_1'] = data[route].pct_change(1) * 100
                data[f'{route}_pct_5'] = data[route].pct_change(5) * 100
                
                # ì´ë™í‰ê· 
                data[f'{route}_ma_5'] = data[route].rolling(5, min_periods=1).mean()
                data[f'{route}_ma_10'] = data[route].rolling(10, min_periods=1).mean()
                
                # ë¹„ìœ¨
                data[f'{route}_ratio'] = data[route] / (data['TOTALCNT'] + 1e-10)
                
                # ìŠ¤íŒŒì´í¬
                ma = data[route].rolling(10).mean()
                data[f'{route}_spike'] = data[route] / (ma + 1e-10)
        
        # ì–‘ë°©í–¥ íë¦„ ì°¨ì´
        if 'M14AM10A' in data.columns and 'M10AM14A' in data.columns:
            data['flow_diff_M14A_M10A'] = data['M14AM10A'] - data['M10AM14A']
        
        if 'M14AM14B' in data.columns and 'M14BM14A' in data.columns:
            data['flow_diff_M14A_M14B'] = data['M14AM14B'] - data['M14BM14A']
        
        if 'M14AM16' in data.columns and 'M16M14A' in data.columns:
            data['flow_diff_M14A_M16'] = data['M14AM16'] - data['M16M14A']
        
        # TOTALCNT íŠ¹ì§•
        for lag in [1, 2, 3, 5, 10, 15, 20, 30]:
            data[f'diff_{lag}'] = data['TOTALCNT'].diff(lag)
            data[f'pct_change_{lag}'] = data['TOTALCNT'].pct_change(lag) * 100
            data[f'accel_{lag}'] = data[f'diff_{lag}'].diff(1)
        
        # ê¸‰ìƒìŠ¹ ê°ì§€
        for window in [5, 10, 20, 30]:
            ma = data['TOTALCNT'].rolling(window).mean()
            std = data['TOTALCNT'].rolling(window).std()
            data[f'z_score_{window}'] = (data['TOTALCNT'] - ma) / (std + 1e-10)
            data[f'spike_ratio_{window}'] = data['TOTALCNT'] / (ma + 1e-10)
        
        # í†µê³„ íŠ¹ì§•
        for w in [3, 5, 10, 20, 30, 60]:
            data[f'MA_{w}'] = data['TOTALCNT'].rolling(w, min_periods=1).mean()
            data[f'STD_{w}'] = data['TOTALCNT'].rolling(w, min_periods=1).std()
            data[f'MAX_{w}'] = data['TOTALCNT'].rolling(w, min_periods=1).max()
            data[f'MIN_{w}'] = data['TOTALCNT'].rolling(w, min_periods=1).min()
            data[f'RANGE_{w}'] = data[f'MAX_{w}'] - data[f'MIN_{w}']
        
        # ì‹œê°„ íŠ¹ì§•
        data['hour'] = data.index.hour
        data['dayofweek'] = data.index.dayofweek
        data['is_weekend'] = (data.index.dayofweek >= 5).astype(int)
        data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)
        data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)
        
        # NaN ì²˜ë¦¬
        data = data.fillna(method='ffill').fillna(0)
        data = data.replace([np.inf, -np.inf], 0)
        data = data.dropna(subset=['FUTURE'])
        
        print(f"âœ“ íŠ¹ì§• ìƒì„± ì™„ë£Œ: {len(data.columns)} ì»¬ëŸ¼")
        
        return data


# ===================================
# ëª¨ë¸ ì •ì˜ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ===================================

class AttentionLSTM(layers.Layer):
    """ì»¤ìŠ¤í…€ Attention LSTM ë ˆì´ì–´"""
    
    def __init__(self, units, **kwargs):
        super(AttentionLSTM, self).__init__(**kwargs)
        self.units = units
        
    def build(self, input_shape):
        self.lstm = layers.LSTM(self.units, return_sequences=True, return_state=True)
        self.attention = layers.MultiHeadAttention(num_heads=4, key_dim=self.units)
        self.layer_norm = layers.LayerNormalization()
        super(AttentionLSTM, self).build(input_shape)
        
    def call(self, inputs):
        lstm_output, state_h, state_c = self.lstm(inputs)
        attention_output = self.attention(lstm_output, lstm_output)
        normalized = self.layer_norm(lstm_output + attention_output)
        return normalized, state_h, state_c


def build_attention_model(input_shape):
    """Attention ë©”ì»¤ë‹ˆì¦˜ ëª¨ë¸"""
    
    inputs = layers.Input(shape=input_shape)
    
    # Conv1D
    conv1 = layers.Conv1D(64, 3, padding='same', activation='relu')(inputs)
    conv2 = layers.Conv1D(128, 3, padding='same', activation='relu')(conv1)
    pool1 = layers.MaxPooling1D(2)(conv2)
    
    # Bidirectional LSTM
    lstm1 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(pool1)
    dropout1 = layers.Dropout(0.2)(lstm1)
    
    lstm2 = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(dropout1)
    dropout2 = layers.Dropout(0.2)(lstm2)
    
    # Multi-Head Attention
    attention = layers.MultiHeadAttention(
        num_heads=8, 
        key_dim=256,
        dropout=0.1
    )(dropout2, dropout2)
    
    norm1 = layers.LayerNormalization()(attention + dropout2)
    
    # Global pooling
    avg_pool = layers.GlobalAveragePooling1D()(norm1)
    
    # Dense layers
    dense1 = layers.Dense(512, activation='relu')(avg_pool)
    dropout3 = layers.Dropout(0.3)(dense1)
    norm2 = layers.BatchNormalization()(dropout3)
    
    dense2 = layers.Dense(256, activation='relu')(norm2)
    dropout4 = layers.Dropout(0.2)(dense2)
    
    dense3 = layers.Dense(128, activation='relu')(dropout4)
    dense4 = layers.Dense(64, activation='relu')(dense3)
    
    outputs = layers.Dense(1, activation='linear')(dense4)
    
    return Model(inputs=inputs, outputs=outputs)


def build_lstm_model(input_shape):
    """LSTM ëª¨ë¸"""
    model = keras.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(128, return_sequences=True),
        layers.Dropout(0.2),
        layers.LSTM(256, return_sequences=True),
        layers.Dropout(0.2),
        layers.LSTM(128, return_sequences=True),
        layers.Dropout(0.2),
        layers.LSTM(64),
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(128, activation='relu'),
        layers.Dense(64, activation='relu'),
        layers.Dense(1, activation='linear')
    ])
    return model


def build_gru_model(input_shape):
    """GRU ëª¨ë¸"""
    model = keras.Sequential([
        layers.Input(shape=input_shape),
        layers.GRU(100, return_sequences=True),
        layers.Dropout(0.2),
        layers.GRU(200, return_sequences=True),
        layers.Dropout(0.2),
        layers.GRU(100, return_sequences=True),
        layers.Dropout(0.2),
        layers.GRU(50),
        layers.Dense(200, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(100, activation='relu'),
        layers.Dense(50, activation='relu'),
        layers.Dense(1, activation='linear')
    ])
    return model


def build_cnn_lstm_model(input_shape):
    """CNN-LSTM í•˜ì´ë¸Œë¦¬ë“œ"""
    inputs = layers.Input(shape=input_shape)
    
    # CNN
    conv1 = layers.Conv1D(64, 5, padding='same', activation='relu')(inputs)
    conv2 = layers.Conv1D(128, 3, padding='same', activation='relu')(conv1)
    pool = layers.MaxPooling1D(2)(conv2)
    
    # LSTM
    lstm = layers.LSTM(128, return_sequences=True)(pool)
    lstm = layers.LSTM(64)(lstm)
    
    # Dense
    dense = layers.Dense(128, activation='relu')(lstm)
    dense = layers.Dropout(0.3)(dense)
    dense = layers.Dense(64, activation='relu')(dense)
    outputs = layers.Dense(1, activation='linear')(dense)
    
    return Model(inputs=inputs, outputs=outputs)


# ===================================
# ì†ì‹¤ í•¨ìˆ˜
# ===================================

@tf.function
def custom_loss(y_true, y_pred):
    """ê¸‰ìƒìŠ¹ ì˜ˆì¸¡ ê°•í™” ì†ì‹¤ í•¨ìˆ˜"""
    
    # Huber ì†ì‹¤
    huber = tf.keras.losses.Huber(delta=1.0)
    base_loss = huber(y_true, y_pred)
    
    # ê³ ê°’ ê°€ì¤‘ì¹˜ (ì •ê·œí™”ëœ ê°’ ê¸°ì¤€)
    high_mask = tf.cast(y_true > 0.6, tf.float32)
    
    # ì €í‰ê°€ í˜ë„í‹°
    underestimate = tf.cast(y_pred < y_true, tf.float32)
    underestimate_penalty = underestimate * high_mask * 5.0
    
    # ìµœì¢… ì†ì‹¤
    total_loss = base_loss * (1 + underestimate_penalty)
    
    return tf.reduce_mean(total_loss)


# ===================================
# ì¬ì‹œì‘ ê°€ëŠ¥í•œ í•™ìŠµ ê´€ë¦¬ì
# ===================================

class ResumableTrainer:
    """ì¬ì‹œì‘ ê°€ëŠ¥í•œ í•™ìŠµ ê´€ë¦¬"""
    
    def __init__(self, checkpoint_dir='checkpoints'):
        self.checkpoint_manager = CheckpointManager(checkpoint_dir)
        self.preprocessor = AdvancedPreprocessor()
        self.models = {}
        self.histories = {}
        self.predictions = {}
        
    def prepare_data(self, file_path='data/20240201_TO_202507281705.csv', force_reload=False):
        """ë°ì´í„° ì¤€ë¹„ (ìºì‹œ í™œìš©)"""
        
        # ì €ì¥ëœ ë°ì´í„° í™•ì¸
        if not force_reload:
            saved_data = self.checkpoint_manager.load_data()
            saved_scalers = self.checkpoint_manager.load_scalers()
            
            if saved_data and saved_scalers:
                print("\nâœ… ì €ì¥ëœ ë°ì´í„° ì‚¬ìš©")
                
                # ìŠ¤ì¼€ì¼ëŸ¬ ë³µì›
                self.preprocessor.target_scaler = saved_scalers['target_scaler']
                self.preprocessor.feature_scaler = saved_scalers['feature_scaler']
                self.preprocessor.is_fitted = True
                
                return saved_data
        
        # ìƒˆë¡œ ë°ì´í„° ì²˜ë¦¬
        print("\nğŸ“Š ë°ì´í„° ìƒˆë¡œ ì²˜ë¦¬")
        
        # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        data = self.preprocessor.load_and_process(file_path)
        data = self.preprocessor.create_all_features(data)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y, y_original, feature_cols = self.create_sequences(data)
        
        # ë°ì´í„° ë¶„í• 
        train_size = int(0.7 * len(X))
        val_size = int(0.15 * len(X))
        
        X_train = X[:train_size]
        y_train = y[:train_size]
        y_train_orig = y_original[:train_size]
        
        X_val = X[train_size:train_size+val_size]
        y_val = y[train_size:train_size+val_size]
        y_val_orig = y_original[train_size:train_size+val_size]
        
        X_test = X[train_size+val_size:]
        y_test = y[train_size+val_size:]
        y_test_orig = y_original[train_size+val_size:]
        
        print(f"\n[ë°ì´í„° ë¶„í• ]")
        print(f"í•™ìŠµ: {len(X_train)}, ê²€ì¦: {len(X_val)}, í…ŒìŠ¤íŠ¸: {len(X_test)}")
        
        # ë°ì´í„° ì €ì¥
        self.checkpoint_manager.save_data(
            X_train, y_train, X_val, y_val, X_test, y_test,
            y_train_orig, y_val_orig, y_test_orig, feature_cols
        )
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        self.checkpoint_manager.save_scalers(
            self.preprocessor.target_scaler,
            self.preprocessor.feature_scaler
        )
        
        return {
            'X_train': X_train, 'y_train': y_train,
            'X_val': X_val, 'y_val': y_val,
            'X_test': X_test, 'y_test': y_test,
            'y_train_orig': y_train_orig,
            'y_val_orig': y_val_orig,
            'y_test_orig': y_test_orig,
            'feature_cols': feature_cols
        }
    
    def create_sequences(self, data, seq_length=30):
        """ì‹œí€€ìŠ¤ ìƒì„±"""
        print("\n[ì‹œí€€ìŠ¤ ìƒì„±]")
        
        feature_cols = [col for col in data.columns if col not in ['TIME', 'FUTURE']]
        target_col = 'FUTURE'
        
        # ìŠ¤ì¼€ì¼ë§
        X_scaled = self.preprocessor.feature_scaler.fit_transform(data[feature_cols])
        y_scaled = self.preprocessor.target_scaler.fit_transform(data[[target_col]])
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y, y_original = [], [], []
        for i in range(len(X_scaled) - seq_length):
            X.append(X_scaled[i:i+seq_length])
            y.append(y_scaled[i+seq_length])
            y_original.append(data[target_col].iloc[i+seq_length])
        
        X = np.array(X)
        y = np.array(y)
        y_original = np.array(y_original).reshape(-1, 1)
        
        print(f"âœ“ X shape: {X.shape}")
        print(f"âœ“ y shape: {y.shape}")
        
        return X, y, y_original, feature_cols
    
    def train_model(self, model, model_name, X_train, y_train, X_val, y_val, 
                   initial_epoch=0, epochs=200):
        """ê°œë³„ ëª¨ë¸ í•™ìŠµ (ì¬ì‹œì‘ ì§€ì›)"""
        
        print(f"\n{'='*50}")
        print(f"{model_name.upper()} ëª¨ë¸ í•™ìŠµ")
        print(f"ì‹œì‘ ì—í­: {initial_epoch}")
        print('='*50)
        
        # ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        if initial_epoch > 0:
            loaded = self.checkpoint_manager.load_model_weights(model, model_name)
            if loaded:
                print(f"âœ… Epoch {initial_epoch}ë¶€í„° ì¬ì‹œì‘")
        
        # ì»´íŒŒì¼
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss=custom_loss,
            metrics=['mae', 'mse']
        )
        
        # ì½œë°± (ìˆ˜ì •ë¨: .weights.h5 í™•ì¥ì ì‚¬ìš©)
        callbacks = [
            EarlyStopping(
                monitor='val_loss', 
                patience=30, 
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss', 
                factor=0.5, 
                patience=10, 
                min_lr=1e-6,
                verbose=1
            ),
            ModelCheckpoint(
                f'model/{model_name}_best.weights.h5',  # .weights.h5 í™•ì¥ì
                save_best_only=True,
                save_weights_only=True,
                monitor='val_loss',
                verbose=0
            ),
            ProgressCallback(self.checkpoint_manager, model_name)
        ]
        
        # í•™ìŠµ
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            initial_epoch=initial_epoch,
            epochs=epochs,
            batch_size=32,
            callbacks=callbacks,
            verbose=1
        )
        
        # í•™ìŠµ ì™„ë£Œ í›„ ì „ì²´ ëª¨ë¸ ì €ì¥ (.keras í˜•ì‹)
        model_path = f'model/{model_name}_final.keras'
        model.save(model_path)
        print(f"âœ… ì „ì²´ ëª¨ë¸ ì €ì¥: {model_path}")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ë„ í•¨ê»˜ ì €ì¥
        scaler_path = f'scaler/{model_name}_scaler.pkl'
        joblib.dump({
            'target_scaler': self.preprocessor.target_scaler,
            'feature_scaler': self.preprocessor.feature_scaler
        }, scaler_path)
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {scaler_path}")
        
        # ì´ë ¥ ì €ì¥
        self.checkpoint_manager.save_history(model_name, history)
        
        # ì™„ë£Œ ëª¨ë¸ ê¸°ë¡
        if model_name not in self.checkpoint_manager.state['completed_models']:
            self.checkpoint_manager.state['completed_models'].append(model_name)
            self.checkpoint_manager.save_state()
        
        return history
    
    def evaluate_model(self, model, X_test, y_test_orig, model_name):
        """ëª¨ë¸ í‰ê°€"""
        print(f"\n[{model_name} í‰ê°€]")
        
        # ì˜ˆì¸¡
        y_pred = model.predict(X_test, verbose=0)
        y_pred_orig = self.preprocessor.target_scaler.inverse_transform(y_pred)
        
        # ë©”íŠ¸ë¦­
        mae = mean_absolute_error(y_test_orig, y_pred_orig)
        mse = mean_squared_error(y_test_orig, y_pred_orig)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test_orig, y_pred_orig)
        
        print(f"MAE: {mae:.2f}")
        print(f"RMSE: {rmse:.2f}")
        print(f"RÂ²: {r2:.4f}")
        
        # 1400+ ì˜ˆì¸¡ ì„±ëŠ¥
        high_mask = y_test_orig.flatten() >= 1400
        if high_mask.sum() > 0:
            high_mae = mean_absolute_error(
                y_test_orig[high_mask],
                y_pred_orig[high_mask]
            )
            high_pred = (y_pred_orig.flatten() >= 1400).sum()
            high_actual = high_mask.sum()
            
            print(f"\n[1400+ ì˜ˆì¸¡]")
            print(f"MAE: {high_mae:.2f}")
            print(f"ì˜ˆì¸¡/ì‹¤ì œ: {high_pred}/{high_actual}")
        
        return y_pred_orig
    
    def train_all(self, data_path='data/20240201_TO_202507281705.csv', resume=False):
        """ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ (ì¬ì‹œì‘ ì§€ì›)"""
        
        print("\n" + "="*70)
        if resume:
            print("ğŸ“‚ ì´ì „ í•™ìŠµ ì¬ê°œ")
        else:
            print("ğŸ†• ìƒˆë¡œìš´ í•™ìŠµ ì‹œì‘")
        print("="*70)
        
        # ìƒíƒœ ë¡œë“œ
        if resume:
            success = self.checkpoint_manager.load_state()
            if not success:
                print("âš ï¸  ì €ì¥ëœ ìƒíƒœê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
                resume = False
        
        # ë°ì´í„° ì¤€ë¹„
        data = self.prepare_data(data_path, force_reload=not resume)
        
        X_train = data['X_train']
        y_train = data['y_train']
        X_val = data['X_val']
        y_val = data['y_val']
        X_test = data['X_test']
        y_test = data['y_test']
        y_test_orig = data['y_test_orig']
        
        input_shape = (X_train.shape[1], X_train.shape[2])
        
        # ëª¨ë¸ ì •ì˜
        model_configs = [
            ('lstm', build_lstm_model),
            ('gru', build_gru_model),
            ('cnn_lstm', build_cnn_lstm_model),
            ('attention', build_attention_model)
        ]
        
        # ì™„ë£Œëœ ëª¨ë¸ í™•ì¸
        completed = self.checkpoint_manager.state.get('completed_models', [])
        
        # ê° ëª¨ë¸ í•™ìŠµ
        for model_name, build_func in model_configs:
            # ì´ë¯¸ ì™„ë£Œëœ ëª¨ë¸ ìŠ¤í‚µ
            if model_name in completed:
                print(f"\nâœ… {model_name} ëª¨ë¸ì€ ì´ë¯¸ ì™„ë£Œë¨")
                
                # ëª¨ë¸ ë¡œë“œ ë° í‰ê°€ë§Œ ìˆ˜í–‰
                model = build_func(input_shape)
                self.checkpoint_manager.load_model_weights(model, model_name)
                self.models[model_name] = model
                
                pred = self.evaluate_model(model, X_test, y_test_orig, model_name)
                self.predictions[model_name] = pred
                continue
            
            # ì‹œì‘ ì—í­ í™•ì¸
            initial_epoch = 0
            if resume and self.checkpoint_manager.state.get('current_model') == model_name:
                initial_epoch = self.checkpoint_manager.state.get('current_epoch', 0)
            
            # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
            model = build_func(input_shape)
            self.models[model_name] = model
            
            try:
                history = self.train_model(
                    model, model_name, 
                    X_train, y_train, X_val, y_val,
                    initial_epoch=initial_epoch
                )
                self.histories[model_name] = history
                
                # í‰ê°€
                pred = self.evaluate_model(model, X_test, y_test_orig, model_name)
                self.predictions[model_name] = pred
                
            except KeyboardInterrupt:
                print(f"\nâš ï¸  {model_name} í•™ìŠµ ì¤‘ë‹¨!")
                raise
            except Exception as e:
                print(f"\nâŒ {model_name} í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
                traceback.print_exc()
                continue
        
        # ì•™ìƒë¸” ì˜ˆì¸¡
        if len(self.predictions) > 0:
            print("\n" + "="*70)
            print("ì•™ìƒë¸” ì˜ˆì¸¡")
            print("="*70)
            
            ensemble_pred = np.mean(list(self.predictions.values()), axis=0)
            
            mae = mean_absolute_error(y_test_orig, ensemble_pred)
            rmse = np.sqrt(mean_squared_error(y_test_orig, ensemble_pred))
            r2 = r2_score(y_test_orig, ensemble_pred)
            
            print(f"ì•™ìƒë¸” MAE: {mae:.2f}")
            print(f"ì•™ìƒë¸” RMSE: {rmse:.2f}")
            print(f"ì•™ìƒë¸” RÂ²: {r2:.4f}")
            
            self.predictions['ensemble'] = ensemble_pred
            
            # ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            np.save('model/ensemble_predictions.npy', ensemble_pred)
            print("âœ… ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: model/ensemble_predictions.npy")
        
        # ìµœì¢… ëª¨ë¸ë“¤ ì €ì¥ í™•ì¸
        print("\n" + "="*70)
        print("ì €ì¥ëœ ëª¨ë¸ í™•ì¸")
        print("="*70)
        for model_name in self.models.keys():
            keras_path = f'model/{model_name}_final.keras'
            weights_path = f'model/{model_name}_best.weights.h5'
            if os.path.exists(keras_path):
                print(f"âœ… {model_name}: {keras_path}")
            if os.path.exists(weights_path):
                print(f"âœ… {model_name} weights: {weights_path}")
        
        print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
        
        return self.models, self.predictions, y_test_orig


# ===================================
# ë©”ì¸ ì‹¤í–‰
# ===================================

def main(resume=False):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("="*70)
    print("TensorFlow 2.18.0 - ì¬ì‹œì‘ ê°€ëŠ¥í•œ í•™ìŠµ ì‹œìŠ¤í…œ")
    print("Ctrl+Cë¡œ ì¤‘ë‹¨ ì‹œ ìë™ ì €ì¥")
    print("="*70)
    
    # í•™ìŠµ ê´€ë¦¬ì ì´ˆê¸°í™”
    trainer = ResumableTrainer()
    
    # ì‹¤ì œ í•™ìŠµ ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •
    data_file = 'data/20240201_TO_202507281705.csv'
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(data_file):
        # ëŒ€ì²´ ê²½ë¡œ ì‹œë„
        alt_paths = [
            '20240201_TO_202507281705.csv',
            '../data/20240201_TO_202507281705.csv',
            './20240201_TO_202507281705.csv'
        ]
        
        for path in alt_paths:
            if os.path.exists(path):
                data_file = path
                break
        else:
            print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_file}")
            print("ë‹¤ìŒ ê²½ë¡œ ì¤‘ í•˜ë‚˜ì— íŒŒì¼ì„ ë°°ì¹˜í•˜ì„¸ìš”:")
            for path in ['data/20240201_TO_202507281705.csv'] + alt_paths:
                print(f"  - {path}")
            return None
    
    print(f"ğŸ“ í•™ìŠµ ë°ì´í„°: {data_file}")
    
    try:
        # í•™ìŠµ ì‹¤í–‰
        models, predictions, y_test = trainer.train_all(
            data_path=data_file,
            resume=resume
        )
        
        print("\n" + "="*70)
        print("ğŸ‰ ëª¨ë“  í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("="*70)
        
    except KeyboardInterrupt:
        print("\n\ní•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ë‹¤ì‹œ ì‹œì‘í•˜ë ¤ë©´: python script.py --resume")
    except Exception as e:
        print(f"\nì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        traceback.print_exc()
    
    return trainer


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ì¬ì‹œì‘ ê°€ëŠ¥í•œ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ í•™ìŠµ')
    parser.add_argument('--resume', action='store_true', help='ì´ì „ í•™ìŠµ ì¬ê°œ')
    parser.add_argument('--reset', action='store_true', help='ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ í›„ ìƒˆë¡œ ì‹œì‘')
    parser.add_argument('--data', type=str, default='data/20240201_TO_202507281705.csv', 
                       help='í•™ìŠµ ë°ì´í„° íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # ë¦¬ì…‹ ì˜µì…˜
    if args.reset:
        import shutil
        if os.path.exists('checkpoints'):
            shutil.rmtree('checkpoints')
            print("âœ… ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì™„ë£Œ")
    
    # í•™ìŠµ ì‹¤í–‰
    trainer = main(resume=args.resume)