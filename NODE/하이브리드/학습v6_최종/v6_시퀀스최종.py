"""
V6_ì‹œí€€ìŠ¤ìƒì„±_ìµœì¢…ë³¸.py - ë³‘ë ¬ì²˜ë¦¬ ì‹œí€€ìŠ¤ ìƒì„±ê¸°
ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ì„ ìœ„í•œ 100ë¶„ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
TensorFlow 2.15.0 í˜¸í™˜
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import RobustScaler
import pickle
import warnings
from datetime import datetime
import os
from multiprocessing import Pool, cpu_count
import gc
warnings.filterwarnings('ignore')

# ============================================
# ì„¤ì •
# ============================================
class Config:
    # ë°ì´í„° íŒŒì¼
    DATA_FILE = '20240201_TO_202507281705.CSV'
    
    # ì‹œí€€ìŠ¤ ì„¤ì •
    LOOKBACK = 100  # ê³¼ê±° 100ë¶„
    FORECAST = 10   # 10ë¶„ í›„ ì˜ˆì¸¡
    
    # M14 ìž„ê³„ê°’ (ë¬¼ë¥˜ëŸ‰ë³„)
    M14B_THRESHOLDS = {
        1400: 320,  # 1400ê°œ ì˜ˆì¸¡ì‹œ M14AM14B ìž„ê³„ê°’
        1500: 400,
        1600: 450,
        1700: 500
    }
    
    RATIO_THRESHOLDS = {
        1400: 4,  # M14B/M10A ë¹„ìœ¨ ìž„ê³„ê°’
        1500: 5,
        1600: 6,
        1700: 7
    }
    
    # ì €ìž¥ ê²½ë¡œ
    SAVE_PATH = './sequences_v6.npz'
    SCALER_PATH = './scalers_v6.pkl'
    
    # ë³‘ë ¬ì²˜ë¦¬ ì„¤ì •
    N_WORKERS = min(cpu_count() - 1, 8)  # ìµœëŒ€ 8ê°œ ì½”ì–´
    CHUNK_SIZE = 5000  # ì²­í¬ë‹¹ ì‹œí€€ìŠ¤ ìˆ˜

# ============================================
# ë³‘ë ¬ì²˜ë¦¬ í•¨ìˆ˜
# ============================================
def process_chunk(args):
    """ì²­í¬ ë‹¨ìœ„ë¡œ ì‹œí€€ìŠ¤ ìƒì„± (ë³‘ë ¬ì²˜ë¦¬ìš©)"""
    start_idx, end_idx, df_values, feature_cols, lookback, forecast = args
    
    X_chunk = []
    y_chunk = []
    m14_chunk = []
    
    # DataFrame ìž¬êµ¬ì„±
    df = pd.DataFrame(df_values, columns=feature_cols)
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    for i in range(start_idx, min(end_idx, len(df) - forecast)):
        if i >= lookback:
            # ì‹œê³„ì—´ ë°ì´í„° (100ë¶„)
            X_chunk.append(df.iloc[i-lookback:i].values)
            
            # íƒ€ê²Ÿ (10ë¶„ í›„ TOTALCNT)
            y_chunk.append(df['target'].iloc[i+forecast-1])
            
            # M14 íŠ¹ì§• (í˜„ìž¬ ì‹œì )
            m14_chunk.append([
                df['M14AM14B'].iloc[i],
                df['M14AM10A'].iloc[i],
                df['M14AM16'].iloc[i],
                df['ratio_14B_10A'].iloc[i] if 'ratio_14B_10A' in df else 0
            ])
    
    return (np.array(X_chunk, dtype=np.float32),
            np.array(y_chunk, dtype=np.float32),
            np.array(m14_chunk, dtype=np.float32))

def scale_feature(args):
    """íŠ¹ì§•ë³„ ìŠ¤ì¼€ì¼ë§ (ë³‘ë ¬ì²˜ë¦¬ìš©)"""
    feature_idx, feature_data = args
    scaler = RobustScaler()
    feature_flat = feature_data.reshape(-1, 1)
    scaler.fit(feature_flat)
    scaled = scaler.transform(feature_flat).reshape(feature_data.shape)
    return feature_idx, scaled, scaler

# ============================================
# íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§
# ============================================
def create_features(df, config):
    """ë‹¤ì–‘í•œ íŠ¹ì§• ìƒì„±"""
    print("\nðŸ”§ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì¤‘...")
    
    # 1. ê¸°ë³¸ M14 íŠ¹ì§• ì •ê·œí™”
    df['M14B_norm'] = df['M14AM14B'] / 600
    df['M10A_inverse'] = (100 - df['M14AM10A']) / 100
    df['M16_norm'] = df['M14AM16'] / 100
    
    # 2. ë¹„ìœ¨ íŠ¹ì§•
    df['ratio_14B_10A'] = df['M14AM14B'] / df['M14AM10A'].clip(lower=1)
    df['ratio_14B_16'] = df['M14AM14B'] / df['M14AM16'].clip(lower=1)
    df['ratio_10A_16'] = df['M14AM10A'] / df['M14AM16'].clip(lower=1)
    
    # 3. ì‹œê³„ì—´ íŠ¹ì§• (ë³€í™”ëŸ‰, ì´ë™í‰ê· , í‘œì¤€íŽ¸ì°¨)
    for col in ['current_value', 'M14AM14B', 'M14AM10A', 'M14AM16']:
        if col in df.columns:
            # ë³€í™”ëŸ‰
            df[f'{col}_change_5'] = df[col].diff(5)
            df[f'{col}_change_10'] = df[col].diff(10)
            
            # ì´ë™í‰ê· 
            df[f'{col}_ma_10'] = df[col].rolling(10, min_periods=1).mean()
            df[f'{col}_ma_30'] = df[col].rolling(30, min_periods=1).mean()
            
            # í‘œì¤€íŽ¸ì°¨
            df[f'{col}_std_10'] = df[col].rolling(10, min_periods=1).std()
    
    # 4. M14AM10A ì—­íŒ¨í„´ (í•˜ë½ ê°ì§€)
    df['M10A_drop_5'] = -df['M14AM10A'].diff(5)
    df['M10A_drop_10'] = -df['M14AM10A'].diff(10)
    
    # 5. ê¸‰ì¦ ì‹ í˜¸ (ìž„ê³„ê°’ ê¸°ë°˜)
    for level, threshold in config.M14B_THRESHOLDS.items():
        df[f'signal_{level}'] = (df['M14AM14B'] >= threshold).astype(float)
    
    # 6. ë¹„ìœ¨ ì‹ í˜¸
    for level, threshold in config.RATIO_THRESHOLDS.items():
        df[f'ratio_signal_{level}'] = (df['ratio_14B_10A'] >= threshold).astype(float)
    
    # 7. í™©ê¸ˆ íŒ¨í„´ íŠ¹ì§•
    df['golden_pattern'] = ((df['M14AM14B'] >= 350) & (df['M14AM10A'] < 70)).astype(float)
    df['spike_imminent'] = ((df['M14AM14B'] >= 400) | (df['ratio_14B_10A'] >= 5)).astype(float)
    
    # 8. í†µê³„ì  íŠ¹ì§•
    df['current_vs_ma'] = df['current_value'] / df['current_value_ma_10'].clip(lower=1)
    df['m14b_vs_ma'] = df['M14AM14B'] / df['M14AM14B_ma_10'].clip(lower=1)
    
    # 9. ì‹œê°„ íŠ¹ì§• (ì˜µì…˜)
    if 'TIME' in df.columns:
        try:
            df['hour'] = pd.to_datetime(df['TIME']).dt.hour
            df['minute'] = pd.to_datetime(df['TIME']).dt.minute
            df['is_peak'] = ((df['hour'] >= 9) & (df['hour'] <= 17)).astype(float)
        except:
            pass
    
    return df

# ============================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("="*60)
    print("ðŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ ì‹œí€€ìŠ¤ ìƒì„±ê¸° V6 ìµœì¢…ë³¸")
    print(f"ðŸ“¦ TensorFlow 2.15.0 í˜¸í™˜")
    print(f"ðŸ’» CPU ì½”ì–´: {cpu_count()}ê°œ (ì‚¬ìš©: {Config.N_WORKERS}ê°œ)")
    print("="*60)
    
    start_time = datetime.now()
    
    # ============================================
    # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    # ============================================
    print(f"\nðŸ“‚ ë°ì´í„° ë¡œë”©: {Config.DATA_FILE}")
    df = pd.read_csv(Config.DATA_FILE)
    print(f"  âœ… ë¡œë“œ ì™„ë£Œ: {len(df):,}í–‰")
    
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ ë° ìƒì„±
    if 'TOTALCNT' in df.columns:
        df['current_value'] = df['TOTALCNT']
    else:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            df['current_value'] = df[numeric_cols[0]]
        else:
            raise ValueError("ìˆ«ìží˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    # M14 ì»¬ëŸ¼ í™•ì¸
    for col in ['M14AM10A', 'M14AM14B', 'M14AM16']:
        if col not in df.columns:
            print(f"  âš ï¸ {col} ì—†ìŒ â†’ 0ìœ¼ë¡œ ì´ˆê¸°í™”")
            df[col] = 0
    
    # íƒ€ê²Ÿ ìƒì„± (10ë¶„ í›„)
    df['target'] = df['current_value'].shift(-Config.FORECAST)
    
    # ============================================
    # 2. íŠ¹ì§• ìƒì„±
    # ============================================
    df = create_features(df, Config)
    
    # íŠ¹ì§• ì»¬ëŸ¼ ì„ íƒ
    exclude_cols = ['TIME', 'CURRTIME', 'TOTALCNT']
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df = df.fillna(0)
    df = df.dropna(subset=['target'])
    
    print(f"  âœ… íŠ¹ì§• ìƒì„± ì™„ë£Œ: {len(feature_cols)}ê°œ")
    
    # ============================================
    # 3. ë³‘ë ¬ ì‹œí€€ìŠ¤ ìƒì„±
    # ============================================
    print(f"\nðŸ“Š ë³‘ë ¬ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
    
    total_sequences = len(df) - Config.LOOKBACK - Config.FORECAST
    print(f"  ì˜ˆìƒ ì‹œí€€ìŠ¤ ìˆ˜: {total_sequences:,}ê°œ")
    
    # ì²­í¬ ì¸ë±ìŠ¤ ìƒì„±
    chunk_indices = []
    for i in range(Config.LOOKBACK, len(df) - Config.FORECAST, Config.CHUNK_SIZE):
        chunk_indices.append((
            i, 
            min(i + Config.CHUNK_SIZE, len(df) - Config.FORECAST)
        ))
    
    print(f"  ì²­í¬ ìˆ˜: {len(chunk_indices)}ê°œ")
    
    # DataFrameì„ numpy arrayë¡œ ë³€í™˜
    df_values = df[feature_cols].values
    
    # ë³‘ë ¬ì²˜ë¦¬ ì¸ìž ì¤€ë¹„
    process_args = [
        (start, end, df_values, feature_cols, Config.LOOKBACK, Config.FORECAST)
        for start, end in chunk_indices
    ]
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
    print("\nâš¡ ë³‘ë ¬ ì²˜ë¦¬ ì‹œìž‘...")
    chunk_start = datetime.now()
    
    with Pool(processes=Config.N_WORKERS) as pool:
        results = pool.map(process_chunk, process_args)
    
    # ê²°ê³¼ ë³‘í•©
    print("\nðŸ“¦ ê²°ê³¼ ë³‘í•© ì¤‘...")
    X_list = [r[0] for r in results if len(r[0]) > 0]
    y_list = [r[1] for r in results if len(r[1]) > 0]
    m14_list = [r[2] for r in results if len(r[2]) > 0]
    
    X = np.concatenate(X_list, axis=0)
    y = np.concatenate(y_list, axis=0)
    m14_features = np.concatenate(m14_list, axis=0)
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del X_list, y_list, m14_list, results
    gc.collect()
    
    chunk_time = datetime.now() - chunk_start
    print(f"  âœ… ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {chunk_time}")
    
    print(f"\nðŸ“ ìƒì„±ëœ ì‹œí€€ìŠ¤:")
    print(f"  X shape: {X.shape}")
    print(f"  y shape: {y.shape}")
    print(f"  m14_features shape: {m14_features.shape}")
    print(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©: {(X.nbytes + y.nbytes + m14_features.nbytes) / 1024**3:.2f}GB")
    
    # ============================================
    # 4. ìŠ¤ì¼€ì¼ë§ (ë³‘ë ¬)
    # ============================================
    print("\nðŸ“ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (ë³‘ë ¬)...")
    
    X_scaled = X.copy()
    scalers = {}
    
    # íŠ¹ì§•ë³„ë¡œ ë¶„ë¦¬
    feature_data_list = [(i, X[:, :, i]) for i in range(X.shape[2])]
    
    # ë³‘ë ¬ ìŠ¤ì¼€ì¼ë§
    with Pool(processes=Config.N_WORKERS) as pool:
        scaling_results = pool.map(scale_feature, feature_data_list)
    
    # ê²°ê³¼ ì ìš©
    for feature_idx, scaled_data, scaler in scaling_results:
        X_scaled[:, :, feature_idx] = scaled_data
        scalers[f'feature_{feature_idx}'] = scaler
    
    print("  âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
    
    # ============================================
    # 5. ì €ìž¥
    # ============================================
    print(f"\nðŸ’¾ ë°ì´í„° ì €ìž¥ ì¤‘...")
    
    # ì‹œí€€ìŠ¤ ì••ì¶• ì €ìž¥
    np.savez_compressed(
        Config.SAVE_PATH,
        X=X_scaled,
        y=y,
        m14_features=m14_features,
        feature_names=feature_cols
    )
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ìž¥
    with open(Config.SCALER_PATH, 'wb') as f:
        pickle.dump(scalers, f)
    
    print(f"  âœ… ì‹œí€€ìŠ¤ ì €ìž¥: {Config.SAVE_PATH}")
    print(f"  âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ìž¥: {Config.SCALER_PATH}")
    
    # ============================================
    # 6. í†µê³„ ì¶œë ¥
    # ============================================
    print("\nðŸ“Š ë°ì´í„° í†µê³„:")
    print(f"  íƒ€ê²Ÿê°’ ë²”ìœ„: {y.min():.0f} ~ {y.max():.0f}")
    print(f"  í‰ê· : {y.mean():.0f}, í‘œì¤€íŽ¸ì°¨: {y.std():.0f}")
    
    print(f"\n  ë¬¼ë¥˜ëŸ‰ êµ¬ê°„ë³„ ë¶„í¬:")
    for level in [1400, 1500, 1600, 1700]:
        count = (y >= level).sum()
        ratio = (y >= level).mean()
        print(f"    {level}+ : {ratio:6.2%} ({count:,}ê°œ)")
    
    # M14 íŠ¹ì§• í†µê³„
    print(f"\n  M14 íŠ¹ì§• í†µê³„:")
    print(f"    M14AM14B í‰ê· : {m14_features[:, 0].mean():.1f}")
    print(f"    M14AM10A í‰ê· : {m14_features[:, 1].mean():.1f}")
    print(f"    M14AM16  í‰ê· : {m14_features[:, 2].mean():.1f}")
    print(f"    ë¹„ìœ¨(14B/10A) í‰ê· : {m14_features[:, 3].mean():.2f}")
    
    # í™©ê¸ˆ íŒ¨í„´ ê²€ì¶œ
    golden_pattern_count = ((m14_features[:, 0] > 300) & (m14_features[:, 1] < 80)).sum()
    print(f"\n  ðŸ† í™©ê¸ˆ íŒ¨í„´ ê°ì§€: {golden_pattern_count:,}ê°œ")
    
    total_time = datetime.now() - start_time
    
    print("\n" + "="*60)
    print("âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ!")
    print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {total_time}")
    print(f"ðŸš€ ì†ë„ í–¥ìƒ: ì•½ {Config.N_WORKERS}ë°°")
    print(f"ðŸ’¡ ë‹¤ìŒ ë‹¨ê³„: V6_í•™ìŠµ_ìµœì¢…ë³¸.py ì‹¤í–‰")
    print("="*60)

# ============================================
# Windows í˜¸í™˜ ë©”ì¸ ê°€ë“œ
# ============================================
if __name__ == '__main__':
    # Windowsì—ì„œ multiprocessing ì‚¬ìš©ì‹œ í•„ìˆ˜
    main()