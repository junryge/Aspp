# -*- coding: utf-8 -*-
"""
CSV ë°ì´í„° ì‹œí€€ìŠ¤ ê²€ì¦ ì‹œìŠ¤í…œ (ì „ì²´ ë°ì´í„° ë¶„ì„)
===============================================
ìƒ˜í”Œë§ ì—†ì´ ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ ì™„ì „ ë¶„ì„
10ë¶„ë¶€í„° 300ë¶„ê¹Œì§€ ëª¨ë“  ì‹œí€€ìŠ¤ ê¸¸ì´ ê²€ì¦
ëª¨ë¸ë³„, ì‹œê°„ë³„ ìƒì„¸ ë¶„ì„ ë° CSV ì €ì¥
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
import os

warnings.filterwarnings('ignore')

class FullSequenceVerifier:
    """CSV ë°ì´í„°ì˜ ëª¨ë“  ì‹œí€€ìŠ¤ íŒ¨í„´ ê²€ì¦ (ìƒ˜í”Œë§ ì—†ìŒ)"""
    
    def __init__(self):
        print("="*80)
        print("CSV ë°ì´í„° ì „ì²´ ì‹œí€€ìŠ¤ ê²€ì¦ ì‹œìŠ¤í…œ (ìƒ˜í”Œë§ ì—†ìŒ)")
        print("="*80)
        self.results = []
        
    def load_and_analyze_data(self, filepath):
        """ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ë¶„ì„"""
        print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {filepath}")
        
        # CSV ë¡œë“œ
        df = pd.read_csv(filepath)
        print(f"  ì›ë³¸ ë°ì´í„°: {df.shape[0]:,}í–‰, {df.shape[1]}ê°œ ì»¬ëŸ¼")
        print(f"  ì»¬ëŸ¼: {list(df.columns)}")
        
        # ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬
        if 'CURRTIME' in df.columns:
            df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                           format='%Y%m%d%H%M', errors='coerce')
            df = df.sort_values('CURRTIME').reset_index(drop=True)
            
            # ì‹œê°„ ì—°ì†ì„± ê²€ì¦
            time_diff = df['CURRTIME'].diff().dt.total_seconds() / 60  # ë¶„ ë‹¨ìœ„
            expected_interval = time_diff.mode()[0] if not time_diff.mode().empty else 1
            
            print(f"\nâ° ì‹œê°„ ì—°ì†ì„± ë¶„ì„:")
            print(f"  ì˜ˆìƒ ê°„ê²©: {expected_interval:.0f}ë¶„")
            print(f"  ì‹œê°„ ë²”ìœ„: {df['CURRTIME'].min()} ~ {df['CURRTIME'].max()}")
            
            # ëˆ„ë½ëœ ì‹œê°„ í™•ì¸
            missing_times = (time_diff > expected_interval * 1.5).sum()
            print(f"  ì‹œê°„ ëˆ„ë½: {missing_times}ê°œ êµ¬ê°„")
            
        # 0ê°’ ì œê±°
        original_count = len(df)
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        removed_zeros = original_count - len(df)
        
        print(f"\nğŸ“Š ë°ì´í„° í’ˆì§ˆ:")
        print(f"  0ê°’ ì œê±°: {removed_zeros}ê°œ")
        print(f"  ìœ íš¨ ë°ì´í„°: {len(df):,}í–‰")
        
        # ê¸°ë³¸ í†µê³„
        print(f"\nğŸ“ˆ TOTALCNT í†µê³„:")
        print(f"  ë²”ìœ„: {df['TOTALCNT'].min():.0f} ~ {df['TOTALCNT'].max():.0f}")
        print(f"  í‰ê· : {df['TOTALCNT'].mean():.1f}")
        print(f"  í‘œì¤€í¸ì°¨: {df['TOTALCNT'].std():.1f}")
        
        # ê³ ê°’ êµ¬ê°„ ë¶„í¬
        high_1651 = (df['TOTALCNT'] >= 1651).sum()
        high_1700 = (df['TOTALCNT'] >= 1700).sum()
        high_1750 = (df['TOTALCNT'] >= 1750).sum()
        
        print(f"\nğŸ¯ ê³ ê°’ êµ¬ê°„ ë¶„í¬:")
        print(f"  1651+: {high_1651}ê°œ ({high_1651/len(df)*100:.1f}%)")
        print(f"  1700+: {high_1700}ê°œ ({high_1700/len(df)*100:.1f}%)")
        print(f"  1750+: {high_1750}ê°œ ({high_1750/len(df)*100:.1f}%)")
        
        return df
    
    def analyze_sequence_detailed(self, sequence_data):
        """V6.7 ì‹œí€€ìŠ¤ ë¶„ì„ ê¸°ëŠ¥"""
        if len(sequence_data) == 0:
            return {'max': 0, 'min': 0, 'trend': 'stable', 'is_high_plateau': False,
                   'consecutive_rises': 0, 'consecutive_falls': 0, 
                   'rise_strength': 0, 'fall_strength': 0, 'volatility': 0,
                   'mean': 0, 'std': 0, 'slope': 0}
        
        # 1. ì‹œí€€ìŠ¤ ê¸°ë³¸ í†µê³„
        seq_max = np.max(sequence_data)
        seq_min = np.min(sequence_data)
        seq_mean = np.mean(sequence_data)
        seq_std = np.std(sequence_data)
        
        # 2. ê³ í‰ì› ìƒíƒœ ì²´í¬ (ìµœê·¼ 30ê°œ í‰ê· ì´ 1700 ì´ìƒ)
        recent_mean = np.mean(sequence_data[-30:]) if len(sequence_data) >= 30 else seq_mean
        is_high_plateau = recent_mean >= 1700
        
        # 3. ì—°ì† ìƒìŠ¹ ì¹´ìš´íŠ¸ ê³„ì‚°
        consecutive_rises = 0
        for i in range(len(sequence_data)-1, 0, -1):
            if sequence_data[i] > sequence_data[i-1]:
                consecutive_rises += 1
            else:
                break
        
        # 4. ì—°ì† í•˜ë½ ì¹´ìš´íŠ¸ ê³„ì‚°
        consecutive_falls = 0
        for i in range(len(sequence_data)-1, 0, -1):
            if sequence_data[i] < sequence_data[i-1]:
                consecutive_falls += 1
            else:
                break
        
        # 5. ìƒìŠ¹/í•˜ë½ ê°•ë„ ê³„ì‚°
        rise_strength = 0
        fall_strength = 0
        if len(sequence_data) >= 10:
            recent_10 = sequence_data[-10:]
            change = recent_10[-1] - recent_10[0]
            if change > 0:
                rise_strength = change
            else:
                fall_strength = abs(change)
        
        # 6. ì¶”ì„¸ ë¶„ì„
        slope = 0
        if len(sequence_data) >= 30:
            recent = sequence_data[-30:]
            x = np.arange(len(recent))
            coeffs = np.polyfit(x, recent, 1)
            slope = coeffs[0]
            
            if is_high_plateau:
                if consecutive_rises >= 10 and rise_strength > 50:
                    trend = 'extreme_rising'
                elif consecutive_falls >= 10 and fall_strength > 50:
                    trend = 'extreme_falling'
                elif slope > 1 or consecutive_rises >= 5:
                    trend = 'high_increasing'
                elif slope < -1 or consecutive_falls >= 5:
                    trend = 'high_decreasing'
                else:
                    trend = 'high_stable'
            else:
                if consecutive_rises >= 10 and rise_strength > 50:
                    trend = 'strong_rising'
                elif consecutive_falls >= 10 and fall_strength > 50:
                    trend = 'strong_falling'
                elif consecutive_rises >= 7 and rise_strength > 30:
                    trend = 'rapid_increasing'
                elif consecutive_falls >= 7 and fall_strength > 30:
                    trend = 'rapid_decreasing'
                elif slope > 2:
                    trend = 'increasing'
                elif slope < -2:
                    trend = 'decreasing'
                else:
                    trend = 'stable'
        else:
            trend = 'stable'
        
        # ë³€ë™ì„± ì§€í‘œ
        volatility = np.std(sequence_data[-10:]) if len(sequence_data) >= 10 else seq_std
        
        return {
            'max': seq_max,
            'min': seq_min,
            'mean': seq_mean,
            'std': seq_std,
            'trend': trend,
            'is_high_plateau': is_high_plateau,
            'consecutive_rises': consecutive_rises,
            'consecutive_falls': consecutive_falls,
            'rise_strength': rise_strength,
            'fall_strength': fall_strength,
            'volatility': volatility,
            'slope': slope
        }
    
    def verify_sequences(self, df):
        """ì „ì²´ ë°ì´í„° ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ê²€ì¦ ì‹¤í–‰ (ìƒ˜í”Œë§ ì—†ìŒ)"""
        print(f"\nğŸ” ì „ì²´ ë°ì´í„° ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ê²€ì¦ ì‹œì‘!")
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì • (10ë¶„ë¶€í„° 300ë¶„ê¹Œì§€)
        sequence_lengths = list(range(10, 101, 10)) + list(range(120, 301, 20))  
        print(f"  ê²€ì¦í•  ì‹œí€€ìŠ¤ ê¸¸ì´: {len(sequence_lengths)}ê°œ")
        print(f"  ê¸¸ì´ ëª©ë¡: {sequence_lengths}")
        
        verification_results = []
        
        for seq_idx, seq_len in enumerate(sequence_lengths):
            print(f"\n{'='*60}")
            print(f"ğŸ¯ [{seq_idx+1}/{len(sequence_lengths)}] ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}ë¶„ ì „ì²´ ê²€ì¦")
            print(f"{'='*60}")
            
            # ì‹œí€€ìŠ¤ë³„ ë¶„ì„ ê²°ê³¼ ì €ì¥
            seq_analyses = []
            valid_sequences = 0
            
            # ê° ì‹œì ì—ì„œ ì‹œí€€ìŠ¤ ìƒì„± ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            max_start_idx = len(df) - seq_len - 10  # 10ë¶„ í›„ ì˜ˆì¸¡ì„ ìœ„í•œ ì—¬ìœ 
            
            if max_start_idx <= 0:
                print(f"  âŒ ì‹œí€€ìŠ¤ ê¸¸ì´ {seq_len}ë¶„: ë°ì´í„° ë¶€ì¡± (í•„ìš”: {seq_len + 10}ë¶„, ë³´ìœ : {len(df)}ë¶„)")
                continue
            
            # ì „ì²´ ë°ì´í„° ë¶„ì„ (ìƒ˜í”Œë§ ì œê±°)
            total_sequences = max_start_idx
            sample_indices = list(range(seq_len, max_start_idx + seq_len))
            
            print(f"  ğŸ“Š ë¶„ì„í•  ì „ì²´ ì‹œí€€ìŠ¤: {len(sample_indices):,}ê°œ (ìƒ˜í”Œë§ ì—†ìŒ)")
            print(f"  ğŸ“ˆ ì˜ˆìƒ ë¶„ì„ ì‹œê°„: {len(sample_indices) // 1000:.1f}ì´ˆ (ëŒ€ëµ)")
            
            # ê° ëª¨ë¸ë³„ ì‹œí€€ìŠ¤ ë¶„ì„
            model_analysis = {
                'LSTM': {'high_seq_count': 0, 'trend_counts': {}, 'boost_conditions': 0},
                'GRU': {'high_seq_count': 0, 'trend_counts': {}, 'boost_conditions': 0},
                'CNN_LSTM': {'high_seq_count': 0, 'trend_counts': {}, 'boost_conditions': 0},
                'SpikeDetector': {'high_seq_count': 0, 'trend_counts': {}, 'boost_conditions': 0},
                'ExtremeNet': {'high_seq_count': 0, 'trend_counts': {}, 'boost_conditions': 0}
            }
            
            # ì‹œê°„ëŒ€ë³„ ë¶„ì„ (ì‹œê°„ë³„)
            hourly_analysis = {}
            for hour in range(24):
                hourly_analysis[hour] = {
                    'count': 0, 'high_seq_count': 0, 'trend_counts': {},
                    'avg_max': 0, 'avg_volatility': 0, 'total_max': 0, 'total_volatility': 0
                }
            
            # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸
            checkpoint_interval = max(1000, len(sample_indices) // 20)
            
            # ì „ì²´ ì‹œí€€ìŠ¤ ë¶„ì„ (ìƒ˜í”Œë§ ì—†ìŒ)
            for seq_count, idx in enumerate(sample_indices):
                # ì§„í–‰ë¥  í‘œì‹œ
                if seq_count % checkpoint_interval == 0:
                    progress = seq_count / len(sample_indices) * 100
                    print(f"    ì§„í–‰ë¥ : {progress:.1f}% ({seq_count:,}/{len(sample_indices):,})")
                
                current_time = df.iloc[idx]['CURRTIME']
                hour = current_time.hour
                
                # ì‹œí€€ìŠ¤ ë°ì´í„° ì¶”ì¶œ (TOTALCNT)
                seq_data = df.iloc[idx-seq_len:idx]['TOTALCNT'].values
                
                # ì‹œí€€ìŠ¤ ë¶„ì„
                analysis = self.analyze_sequence_detailed(seq_data)
                seq_analyses.append(analysis)
                
                # ìœ íš¨í•œ ì‹œí€€ìŠ¤ ì¹´ìš´íŠ¸
                if analysis['max'] > 0:
                    valid_sequences += 1
                    
                    # ì‹œê°„ëŒ€ë³„ ì§‘ê³„
                    hourly_analysis[hour]['count'] += 1
                    hourly_analysis[hour]['total_max'] += analysis['max']
                    hourly_analysis[hour]['total_volatility'] += analysis['volatility']
                    
                    if analysis['max'] >= 1651:
                        hourly_analysis[hour]['high_seq_count'] += 1
                    
                    trend = analysis['trend']
                    if trend not in hourly_analysis[hour]['trend_counts']:
                        hourly_analysis[hour]['trend_counts'][trend] = 0
                    hourly_analysis[hour]['trend_counts'][trend] += 1
                    
                    # ëª¨ë¸ë³„ ë¶„ì„ (ê° ëª¨ë¸ì´ ì´ ì‹œí€€ìŠ¤ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í• ì§€)
                    m14b_value = df.iloc[idx]['M14AM14B'] if 'M14AM14B' in df.columns else 300
                    
                    for model_name in model_analysis.keys():
                        # ê³ ê°’ ì‹œí€€ìŠ¤ ì¹´ìš´íŠ¸
                        if analysis['max'] >= 1651:
                            model_analysis[model_name]['high_seq_count'] += 1
                        
                        # ì¶”ì„¸ë³„ ì¹´ìš´íŠ¸
                        if trend not in model_analysis[model_name]['trend_counts']:
                            model_analysis[model_name]['trend_counts'][trend] = 0
                        model_analysis[model_name]['trend_counts'][trend] += 1
                        
                        # ExtremeNet ë¶€ìŠ¤íŒ… ì¡°ê±´ (V6.7)
                        if model_name == 'ExtremeNet':
                            if analysis['max'] >= 1651 and ('increasing' in trend or trend == 'extreme_rising'):
                                model_analysis[model_name]['boost_conditions'] += 1
                        
                        # SpikeDetector ì¡°ê±´ (ìµœê·¼ 20ë¶„ ì¤‘ì )
                        elif model_name == 'SpikeDetector':
                            if analysis['consecutive_rises'] >= 5 or analysis['rise_strength'] > 30:
                                model_analysis[model_name]['boost_conditions'] += 1
            
            # ì‹œê°„ëŒ€ë³„ í‰ê·  ê³„ì‚° (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
            for hour in hourly_analysis:
                if hourly_analysis[hour]['count'] > 0:
                    hourly_analysis[hour]['avg_max'] = hourly_analysis[hour]['total_max'] / hourly_analysis[hour]['count']
                    hourly_analysis[hour]['avg_volatility'] = hourly_analysis[hour]['total_volatility'] / hourly_analysis[hour]['count']
            
            # ì „ì²´ ë°ì´í„° ì‹œí€€ìŠ¤ ê²€ì¦ ê²°ê³¼ ì¶œë ¥
            print(f"\n  âœ… ì „ì²´ ë¶„ì„ ì™„ë£Œ: {len(sample_indices):,}ê°œ")
            print(f"\nğŸ“ˆ ì‹œí€€ìŠ¤ {seq_len}ë¶„ ì „ì²´ ê²€ì¦ ê²°ê³¼:")
            print(f"  ìœ íš¨ ì‹œí€€ìŠ¤: {valid_sequences:,}ê°œ / {len(sample_indices):,}ê°œ")
            
            if valid_sequences > 0:
                # ì „ì²´ í†µê³„
                all_max_values = [a['max'] for a in seq_analyses if a['max'] > 0]
                all_trends = [a['trend'] for a in seq_analyses if a['max'] > 0]
                all_volatilities = [a['volatility'] for a in seq_analyses if a['volatility'] > 0]
                all_consecutive_rises = [a['consecutive_rises'] for a in seq_analyses if a['max'] > 0]
                all_consecutive_falls = [a['consecutive_falls'] for a in seq_analyses if a['max'] > 0]
                
                print(f"  MAXê°’ ë²”ìœ„: {min(all_max_values):.0f} ~ {max(all_max_values):.0f}")
                print(f"  í‰ê·  MAX: {np.mean(all_max_values):.1f}")
                print(f"  í‰ê·  ë³€ë™ì„±: {np.mean(all_volatilities):.1f}")
                print(f"  í‰ê·  ì—°ì†ìƒìŠ¹: {np.mean(all_consecutive_rises):.1f}")
                print(f"  í‰ê·  ì—°ì†í•˜ë½: {np.mean(all_consecutive_falls):.1f}")
                
                # ê³ ê°’ êµ¬ê°„ ë¹„ìœ¨ (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
                high_1651_count = sum(1 for v in all_max_values if v >= 1651)
                high_1700_count = sum(1 for v in all_max_values if v >= 1700)
                high_1750_count = sum(1 for v in all_max_values if v >= 1750)
                
                print(f"  ê³ ê°’ ì‹œí€€ìŠ¤ ë¶„í¬ (ì „ì²´ {valid_sequences:,}ê°œ ì¤‘):")
                print(f"    1651+: {high_1651_count:,}ê°œ ({high_1651_count/valid_sequences*100:.2f}%)")
                print(f"    1700+: {high_1700_count:,}ê°œ ({high_1700_count/valid_sequences*100:.2f}%)")
                print(f"    1750+: {high_1750_count:,}ê°œ ({high_1750_count/valid_sequences*100:.2f}%)")
                
                # ì¶”ì„¸ ë¶„í¬ (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
                trend_counts = {}
                for trend in all_trends:
                    trend_counts[trend] = trend_counts.get(trend, 0) + 1
                
                print(f"  ì¶”ì„¸ ë¶„í¬ (ì „ì²´):")
                for trend, count in sorted(trend_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
                    print(f"    {trend}: {count:,}ê°œ ({count/valid_sequences*100:.2f}%)")
                
                # ëª¨ë¸ë³„ ë¶€ìŠ¤íŒ… ì¡°ê±´ ë¶„ì„ (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
                print(f"\nğŸ¤– ëª¨ë¸ë³„ ë¶„ì„ (ì‹œí€€ìŠ¤ {seq_len}ë¶„, ì „ì²´ ë°ì´í„°):")
                for model_name, model_data in model_analysis.items():
                    boost_ratio = model_data['boost_conditions'] / valid_sequences * 100 if valid_sequences > 0 else 0
                    high_ratio = model_data['high_seq_count'] / valid_sequences * 100 if valid_sequences > 0 else 0
                    
                    print(f"  {model_name}:")
                    print(f"    ê³ ê°’ ëŒ€ìƒ: {model_data['high_seq_count']:,}ê°œ ({high_ratio:.2f}%)")
                    print(f"    ë¶€ìŠ¤íŒ… ì¡°ê±´: {model_data['boost_conditions']:,}ê°œ ({boost_ratio:.2f}%)")
                
                # ì‹œê°„ëŒ€ë³„ ë¶„ì„ (ì „ì²´ ë°ì´í„° ê¸°ì¤€, ìƒìœ„ ì‹œê°„ëŒ€ë§Œ)
                print(f"\nâ° ì‹œê°„ëŒ€ë³„ ë¶„ì„ (ì „ì²´ ë°ì´í„°, ìƒìœ„ 5ì‹œê°„):")
                hour_summary = []
                for hour, data in hourly_analysis.items():
                    if data['count'] > 0:
                        hour_summary.append((hour, data))
                
                # ì‹œí€€ìŠ¤ ê°œìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                hour_summary.sort(key=lambda x: x[1]['count'], reverse=True)
                
                for hour, data in hour_summary[:5]:
                    high_ratio = data['high_seq_count'] / data['count'] * 100 if data['count'] > 0 else 0
                    print(f"  {hour:02d}ì‹œ: {data['count']:,}ê°œ, ê³ ê°’ {data['high_seq_count']:,}ê°œ({high_ratio:.1f}%), "
                          f"í‰ê· MAX {data['avg_max']:.0f}")
            
            # ê²°ê³¼ ì €ì¥ (CSVìš©, ì „ì²´ ë°ì´í„° ê¸°ì¤€)
            seq_result = {
                'sequence_length': seq_len,
                'total_sequences': len(sample_indices),
                'valid_sequences': valid_sequences,
                'high_sequences_1651': sum(1 for a in seq_analyses if a['max'] >= 1651),
                'high_sequences_1700': sum(1 for a in seq_analyses if a['max'] >= 1700),
                'high_sequences_1750': sum(1 for a in seq_analyses if a['max'] >= 1750),
                'avg_max': np.mean([a['max'] for a in seq_analyses if a['max'] > 0]) if valid_sequences > 0 else 0,
                'std_max': np.std([a['max'] for a in seq_analyses if a['max'] > 0]) if valid_sequences > 0 else 0,
                'min_max': np.min([a['max'] for a in seq_analyses if a['max'] > 0]) if valid_sequences > 0 else 0,
                'max_max': np.max([a['max'] for a in seq_analyses if a['max'] > 0]) if valid_sequences > 0 else 0,
                'avg_volatility': np.mean([a['volatility'] for a in seq_analyses if a['volatility'] > 0]) if valid_sequences > 0 else 0,
                'avg_consecutive_rises': np.mean([a['consecutive_rises'] for a in seq_analyses if a['max'] > 0]) if valid_sequences > 0 else 0,
                'avg_consecutive_falls': np.mean([a['consecutive_falls'] for a in seq_analyses if a['max'] > 0]) if valid_sequences > 0 else 0,
                'increasing_trend': sum(1 for a in seq_analyses if 'increasing' in a['trend']),
                'decreasing_trend': sum(1 for a in seq_analyses if 'decreasing' in a['trend']),
                'stable_trend': sum(1 for a in seq_analyses if a['trend'] == 'stable'),
                'extreme_rising': sum(1 for a in seq_analyses if a['trend'] == 'extreme_rising'),
                'extreme_falling': sum(1 for a in seq_analyses if a['trend'] == 'extreme_falling')
            }
            
            # ëª¨ë¸ë³„ ë°ì´í„° ì¶”ê°€ (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
            for model_name, model_data in model_analysis.items():
                seq_result[f'{model_name}_high_seq'] = model_data['high_seq_count']
                seq_result[f'{model_name}_boost_conditions'] = model_data['boost_conditions']
                seq_result[f'{model_name}_boost_ratio'] = model_data['boost_conditions'] / valid_sequences * 100 if valid_sequences > 0 else 0
            
            # ì‹œê°„ëŒ€ë³„ ìµœê³  ë°ì´í„° ì¶”ê°€
            if hour_summary:
                best_hour, best_data = hour_summary[0]
                seq_result['best_hour'] = best_hour
                seq_result['best_hour_count'] = best_data['count']
                seq_result['best_hour_high_count'] = best_data['high_seq_count']
                seq_result['best_hour_high_ratio'] = best_data['high_seq_count'] / best_data['count'] * 100 if best_data['count'] > 0 else 0
            
            verification_results.append(seq_result)
        
        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        results_df = pd.DataFrame(verification_results)
        
        # CSV ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f'full_sequence_verification_{timestamp}.csv'
        results_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        print(f"\n" + "="*80)
        print(f"ğŸ’¾ ì „ì²´ ë°ì´í„° ê²€ì¦ ê²°ê³¼ ì €ì¥: {output_file}")
        print(f"ğŸ“Š ì´ {len(verification_results)}ê°œ ì‹œí€€ìŠ¤ ê¸¸ì´ ì „ì²´ ê²€ì¦ ì™„ë£Œ")
        
        # ì „ì²´ ë¶„ì„ í†µê³„ ìš”ì•½
        if verification_results:
            total_analyzed = sum(r['total_sequences'] for r in verification_results)
            total_valid = sum(r['valid_sequences'] for r in verification_results)
            print(f"ğŸ¯ ì „ì²´ ë¶„ì„ ê·œëª¨: {total_analyzed:,}ê°œ ì‹œí€€ìŠ¤ (ìœ íš¨: {total_valid:,}ê°œ)")
        
        print("="*80)
        
        return results_df, output_file
    
    def generate_summary_report(self, results_df):
        """ì „ì²´ ë°ì´í„° ê¸°ë°˜ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        print(f"\nğŸ“‹ ì „ì²´ ë°ì´í„° ì‹œí€€ìŠ¤ ê²€ì¦ ìš”ì•½ ë³´ê³ ì„œ")
        print("="*60)
        
        if results_df.empty:
            print("âŒ ê²€ì¦ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # ì „ì²´ ë¶„ì„ ê·œëª¨ ì¶œë ¥
        total_sequences = results_df['total_sequences'].sum()
        total_valid = results_df['valid_sequences'].sum()
        print(f"\nğŸ” ì „ì²´ ë¶„ì„ ê·œëª¨:")
        print(f"  ì´ ë¶„ì„ ì‹œí€€ìŠ¤: {total_sequences:,}ê°œ")
        print(f"  ìœ íš¨ ì‹œí€€ìŠ¤: {total_valid:,}ê°œ")
        print(f"  ì‹œí€€ìŠ¤ ê¸¸ì´ ì¢…ë¥˜: {len(results_df)}ê°œ")
        
        # 1. ìµœì  ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶„ì„ (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
        print(f"\nğŸ† ìµœì  ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶„ì„ (ì „ì²´ ë°ì´í„° ê¸°ì¤€):")
        
        # ìœ íš¨ ì‹œí€€ìŠ¤ ë¹„ìœ¨ì´ ë†’ì€ ìˆœ
        results_df['valid_ratio'] = results_df['valid_sequences'] / results_df['total_sequences'] * 100
        
        # ê³ ê°’ ì‹œí€€ìŠ¤ ë¹„ìœ¨ (1651+)
        results_df['high_ratio_1651'] = results_df['high_sequences_1651'] / results_df['valid_sequences'] * 100
        results_df['high_ratio_1651'] = results_df['high_ratio_1651'].fillna(0)
        
        # ê·¹ê°’ ì‹œí€€ìŠ¤ ë¹„ìœ¨ (1750+)
        results_df['extreme_ratio_1750'] = results_df['high_sequences_1750'] / results_df['valid_sequences'] * 100
        results_df['extreme_ratio_1750'] = results_df['extreme_ratio_1750'].fillna(0)
        
        print(f"\n  ê³ ê°’ ì‹œí€€ìŠ¤ ë¹„ìœ¨(1651+) ìƒìœ„ 5ê°œ (ì „ì²´ ë°ì´í„°):")
        top_high = results_df.nlargest(5, 'high_ratio_1651')
        for _, row in top_high.iterrows():
            print(f"    {int(row['sequence_length'])}ë¶„: {row['high_ratio_1651']:.2f}% "
                  f"({int(row['high_sequences_1651']):,}/{int(row['valid_sequences']):,}ê°œ)")
        
        print(f"\n  ê·¹ê°’ ì‹œí€€ìŠ¤ ë¹„ìœ¨(1750+) ìƒìœ„ 3ê°œ:")
        top_extreme = results_df.nlargest(3, 'extreme_ratio_1750')
        for _, row in top_extreme.iterrows():
            if row['extreme_ratio_1750'] > 0:
                print(f"    {int(row['sequence_length'])}ë¶„: {row['extreme_ratio_1750']:.2f}% "
                      f"({int(row['high_sequences_1750']):,}ê°œ)")
        
        # 2. ëª¨ë¸ë³„ ë¶€ìŠ¤íŒ… ì¡°ê±´ ë¶„ì„ (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
        print(f"\nğŸ¤– ëª¨ë¸ë³„ ë¶€ìŠ¤íŒ… ì¡°ê±´ ë¶„ì„ (ì „ì²´ ë°ì´í„°):")
        model_names = ['ExtremeNet', 'SpikeDetector', 'LSTM', 'GRU', 'CNN_LSTM']
        
        for model in model_names:
            boost_col = f'{model}_boost_conditions'
            high_col = f'{model}_high_seq'
            
            if boost_col in results_df.columns:
                total_boost = results_df[boost_col].sum()
                total_high = results_df[high_col].sum()
                
                if total_boost > 0:
                    best_seq = results_df.loc[results_df[boost_col].idxmax()]
                    print(f"  {model}:")
                    print(f"    ì´ ë¶€ìŠ¤íŒ… ì¡°ê±´: {total_boost:,}ê°œ")
                    print(f"    ì´ ê³ ê°’ ëŒ€ìƒ: {total_high:,}ê°œ")
                    print(f"    ìµœì  ê¸¸ì´: {int(best_seq['sequence_length'])}ë¶„ "
                          f"(ë¶€ìŠ¤íŒ… {int(best_seq[boost_col]):,}ê°œ, {best_seq[f'{model}_boost_ratio']:.2f}%)")
        
        # 3. ì‹œê°„ëŒ€ ë¶„ì„ (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
        print(f"\nâ° ì‹œê°„ëŒ€ë³„ ë¶„ì„ (ì „ì²´ ë°ì´í„° ê¸°ì¤€):")
        if 'best_hour' in results_df.columns:
            hour_counts = results_df['best_hour'].value_counts().head(5)
            print(f"  ìµœë‹¤ ê³ ê°’ ì‹œê°„ëŒ€:")
            for hour, count in hour_counts.items():
                avg_high_ratio = results_df[results_df['best_hour'] == hour]['best_hour_high_ratio'].mean()
                total_sequences = results_df[results_df['best_hour'] == hour]['best_hour_count'].sum()
                print(f"    {int(hour):02d}ì‹œ: {count}ê°œ ê¸¸ì´ì—ì„œ ìµœê³ ì„±ëŠ¥, "
                      f"í‰ê·  ê³ ê°’ë¹„ìœ¨ {avg_high_ratio:.1f}%, ì´ {total_sequences:,}ê°œ ì‹œí€€ìŠ¤")
        
        # 4. ê¶Œì¥ ì‚¬í•­ (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
        print(f"\nğŸ’¡ ì „ì²´ ë°ì´í„° ê¸°ë°˜ ê¶Œì¥ ì‚¬í•­:")
        
        # í˜„ì¬ 100ë¶„ê³¼ ë¹„êµ
        current_100 = results_df[results_df['sequence_length'] == 100]
        if not current_100.empty:
            current_performance = current_100.iloc[0]
            print(f"  í˜„ì¬ 100ë¶„ ì„¤ì • (ì „ì²´ ë°ì´í„°):")
            print(f"    ì „ì²´ ì‹œí€€ìŠ¤: {int(current_performance['total_sequences']):,}ê°œ")
            print(f"    ê³ ê°’ ë¹„ìœ¨: {current_performance['high_ratio_1651']:.2f}%")
            print(f"    í‰ê·  MAX: {current_performance['avg_max']:.0f}")
            print(f"    ExtremeNet ë¶€ìŠ¤íŒ…: {int(current_performance.get('ExtremeNet_boost_conditions', 0)):,}ê°œ "
                  f"({current_performance.get('ExtremeNet_boost_ratio', 0):.2f}%)")
        
        # ìµœê³  ì„±ëŠ¥ ì‹œí€€ìŠ¤ ì¶”ì²œ
        best_overall = results_df.loc[results_df['high_ratio_1651'].idxmax()]
        print(f"\n  ğŸ¥‡ ì¶”ì²œ ì‹œí€€ìŠ¤ ê¸¸ì´: {int(best_overall['sequence_length'])}ë¶„")
        print(f"    ì „ì²´ ë¶„ì„: {int(best_overall['total_sequences']):,}ê°œ ì‹œí€€ìŠ¤")
        print(f"    ê³ ê°’ ë¹„ìœ¨: {best_overall['high_ratio_1651']:.2f}% "
              f"({int(best_overall['high_sequences_1651']):,}ê°œ)")
        print(f"    í‰ê·  MAX: {best_overall['avg_max']:.0f}")
        print(f"    ExtremeNet ë¶€ìŠ¤íŒ…: {int(best_overall.get('ExtremeNet_boost_conditions', 0)):,}ê°œ "
              f"({best_overall.get('ExtremeNet_boost_ratio', 0):.2f}%)")
        
        # í˜„ì¬ ëŒ€ë¹„ ê°œì„  íš¨ê³¼
        if not current_100.empty:
            improvement = (best_overall['high_ratio_1651'] - current_performance['high_ratio_1651']) / current_performance['high_ratio_1651'] * 100
            boost_improvement = (best_overall.get('ExtremeNet_boost_conditions', 0) - current_performance.get('ExtremeNet_boost_conditions', 0)) / max(current_performance.get('ExtremeNet_boost_conditions', 1), 1) * 100
            
            print(f"\n  ğŸ“ˆ í˜„ì¬ 100ë¶„ ëŒ€ë¹„ ê°œì„  íš¨ê³¼:")
            print(f"    ê³ ê°’ ê°ì§€ìœ¨: {improvement:+.1f}% í–¥ìƒ")
            print(f"    ExtremeNet ë¶€ìŠ¤íŒ…: {boost_improvement:+.1f}% í–¥ìƒ")
        
        # ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•
        balanced = results_df[(results_df['valid_ratio'] >= 95) & (results_df['high_ratio_1651'] >= 10)]
        if not balanced.empty:
            balanced_best = balanced.loc[balanced['high_ratio_1651'].idxmax()]
            print(f"\n  âš–ï¸ ê· í˜•ì¡íŒ ì„ íƒ: {int(balanced_best['sequence_length'])}ë¶„")
            print(f"    ì „ì²´ ë¶„ì„: {int(balanced_best['total_sequences']):,}ê°œ")
            print(f"    ê³ ê°’ ë¹„ìœ¨: {balanced_best['high_ratio_1651']:.2f}%")
            print(f"    ìœ íš¨ ë¹„ìœ¨: {balanced_best['valid_ratio']:.2f}%")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ CSV ë°ì´í„° ì „ì²´ ì‹œí€€ìŠ¤ ê²€ì¦ ì‹œì‘!")
    
    # ê²€ì¦ê¸° ìƒì„±
    verifier = FullSequenceVerifier()
    
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    data_files = [
        'data/gs.CSV',
        'gs.CSV',
        './gs.CSV',
        'gs.csv',
        './gs.csv'
    ]
    
    data_file = None
    for file in data_files:
        if os.path.exists(file):
            data_file = file
            break
    
    if not data_file:
        print("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("ì—…ë¡œë“œ ê°€ëŠ¥í•œ íŒŒì¼:")
        for file in data_files:
            print(f"  - {file}")
        return
    
    # ë°ì´í„° ë¡œë“œ ë° ë¶„ì„
    df = verifier.load_and_analyze_data(data_file)
    
    # ì „ì²´ ì‹œí€€ìŠ¤ ê²€ì¦ ì‹¤í–‰ (ìƒ˜í”Œë§ ì—†ìŒ)
    results_df, output_file = verifier.verify_sequences(df)
    
    # ì „ì²´ ë°ì´í„° ê¸°ë°˜ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
    verifier.generate_summary_report(results_df)
    
    print(f"\nâœ… ì „ì²´ ë°ì´í„° ì‹œí€€ìŠ¤ ê²€ì¦ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {output_file}")
    print(f"ğŸ“Š {len(results_df)}ê°œ ì‹œí€€ìŠ¤ ê¸¸ì´ ì „ì²´ ê²€ì¦")
    print(f"ğŸ¯ ìµœì  ì‹œí€€ìŠ¤ ê¸¸ì´ì™€ ëª¨ë¸ë³„ ë¶€ìŠ¤íŒ… ì¡°ê±´ ì™„ì „ ë¶„ì„ ì™„ë£Œ")

if __name__ == "__main__":
    main()