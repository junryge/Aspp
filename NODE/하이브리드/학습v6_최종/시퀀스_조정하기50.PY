"""
sequence_aware_data_reducer.py - ì‹œí€€ìŠ¤ í•™ìŠµìš© ë°ì´í„° ì¶•ì†Œ
100ë¶„ ì—°ì†ì„±ì„ ìœ ì§€í•˜ë©´ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ë°ì´í„° ì¶•ì†Œ
"""

import pandas as pd
import numpy as np
from datetime import datetime

# ============================================
# ì„¤ì • (ì¡°ì • ê°€ëŠ¥)
# ============================================
# íŒŒì¼ ê²½ë¡œ
INPUT_FILE = './data/20240201_TO_202507281705.CSV'
OUTPUT_FILE = './data/reduced_for_sequence.csv'

# ì‹œí€€ìŠ¤ ì„¤ì • (V6ê³¼ ë™ì¼)
LOOKBACK = 100  # ê³¼ê±° 100ë¶„
FORECAST = 10   # 10ë¶„ í›„ ì˜ˆì¸¡

# ë°ì´í„° ì¶•ì†Œ ì„¤ì •
REDUCTION_CONFIG = {
    'target_sequences': 50000,  # ëª©í‘œ ì‹œí€€ìŠ¤ ìˆ˜ (ì•½ 5ë§Œê°œ)
    'min_segment_length': 200,  # ìµœì†Œ ì—°ì† êµ¬ê°„ (100+10+ì—¬ìœ )
    'keep_important_patterns': True,  # ì¤‘ìš” íŒ¨í„´ ìš°ì„  ë³´ì¡´
    'sampling_method': 'segment'  # 'segment' or 'sliding'
}

# ì¤‘ìš” íŒ¨í„´ (ë¬¼ë¥˜ ê¸‰ì¦ êµ¬ê°„)
IMPORTANT_THRESHOLDS = {
    'high_totalcnt': 1500,  # ë†’ì€ ë¬¼ë¥˜ëŸ‰
    'high_m14b': 400,       # M14AM14B ë†’ì€ ê°’
    'spike_change': 100     # ê¸‰ê²©í•œ ë³€í™”
}

# ============================================
# ì—°ì† êµ¬ê°„ ì°¾ê¸°
# ============================================
def find_continuous_segments(df):
    """ì—°ì†ëœ ë°ì´í„° êµ¬ê°„ ì°¾ê¸°"""
    print("\nğŸ” ì—°ì† êµ¬ê°„ ë¶„ì„ ì¤‘...")
    
    # ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬
    time_col = 'TIME' if 'TIME' in df.columns else 'CURRTIME'
    df['datetime'] = pd.to_datetime(df[time_col], format='%Y%m%d%H%M', errors='coerce')
    
    # ì‹œê°„ ì°¨ì´ ê³„ì‚° (ë¶„ ë‹¨ìœ„)
    df['time_diff'] = df['datetime'].diff().dt.total_seconds() / 60
    
    # ì—°ì† êµ¬ê°„ ì°¾ê¸° (5ë¶„ ì´ìƒ ì°¨ì´ë‚˜ë©´ ëŠê¹€)
    df['segment_id'] = (df['time_diff'] > 5).cumsum()
    
    # ê° êµ¬ê°„ì˜ í¬ê¸°
    segment_sizes = df.groupby('segment_id').size()
    valid_segments = segment_sizes[segment_sizes >= REDUCTION_CONFIG['min_segment_length']]
    
    print(f"  â€¢ ì „ì²´ êµ¬ê°„ ìˆ˜: {len(segment_sizes)}")
    print(f"  â€¢ ìœ íš¨ êµ¬ê°„ ìˆ˜ (>={REDUCTION_CONFIG['min_segment_length']}ë¶„): {len(valid_segments)}")
    print(f"  â€¢ ìµœëŒ€ êµ¬ê°„ í¬ê¸°: {segment_sizes.max()}ë¶„")
    print(f"  â€¢ í‰ê·  êµ¬ê°„ í¬ê¸°: {segment_sizes.mean():.0f}ë¶„")
    
    return df, valid_segments

# ============================================
# ì¤‘ìš” íŒ¨í„´ ì ìˆ˜ ê³„ì‚°
# ============================================
def calculate_importance_score(segment_df):
    """ê° êµ¬ê°„ì˜ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
    score = 0
    
    # 1. ë†’ì€ TOTALCNT
    if 'TOTALCNT' in segment_df.columns:
        high_total = (segment_df['TOTALCNT'] >= IMPORTANT_THRESHOLDS['high_totalcnt']).sum()
        score += high_total * 10
        
        # í‰ê·  TOTALCNTë„ ê³ ë ¤
        avg_total = segment_df['TOTALCNT'].mean()
        score += (avg_total / 100)
    
    # 2. ë†’ì€ M14AM14B
    if 'M14AM14B' in segment_df.columns:
        high_m14b = (segment_df['M14AM14B'] >= IMPORTANT_THRESHOLDS['high_m14b']).sum()
        score += high_m14b * 5
        
        # M14B/M14A ë¹„ìœ¨
        if 'M14AM10A' in segment_df.columns:
            segment_df['temp_ratio'] = segment_df['M14AM14B'] / segment_df['M14AM10A'].clip(lower=1)
            high_ratio = (segment_df['temp_ratio'] >= 5).sum()
            score += high_ratio * 8
    
    # 3. ë³€ë™ì„± (ê¸‰ê²©í•œ ë³€í™”)
    if 'TOTALCNT' in segment_df.columns:
        changes = segment_df['TOTALCNT'].diff().abs()
        spikes = (changes >= IMPORTANT_THRESHOLDS['spike_change']).sum()
        score += spikes * 3
        
        # í‘œì¤€í¸ì°¨ (ë³€ë™ì„±)
        std_value = segment_df['TOTALCNT'].std()
        score += (std_value / 10)
    
    return score

# ============================================
# ì„¸ê·¸ë¨¼íŠ¸ ê¸°ë°˜ ìƒ˜í”Œë§
# ============================================
def segment_based_sampling(df, valid_segments):
    """ì—°ì† êµ¬ê°„ ë‹¨ìœ„ë¡œ ìƒ˜í”Œë§"""
    print("\nğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ ê¸°ë°˜ ìƒ˜í”Œë§ ì¤‘...")
    
    target_sequences = REDUCTION_CONFIG['target_sequences']
    segments_needed = target_sequences // 100  # ëŒ€ëµì ì¸ í•„ìš” ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜
    
    segment_scores = {}
    
    # ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ ì¤‘ìš”ë„ ê³„ì‚°
    for seg_id in valid_segments.index:
        segment_df = df[df['segment_id'] == seg_id]
        
        # ì¤‘ìš” íŒ¨í„´ ë³´ì¡´ ì˜µì…˜
        if REDUCTION_CONFIG['keep_important_patterns']:
            score = calculate_importance_score(segment_df)
        else:
            score = len(segment_df)  # ë‹¨ìˆœíˆ ê¸¸ì´ë§Œ
        
        segment_scores[seg_id] = score
    
    # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_segments = sorted(segment_scores.items(), key=lambda x: x[1], reverse=True)
    
    # ìƒìœ„ ì„¸ê·¸ë¨¼íŠ¸ ì„ íƒ
    selected_segments = []
    total_rows = 0
    
    for seg_id, score in sorted_segments:
        segment_size = valid_segments[seg_id]
        
        # ëª©í‘œ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
        if total_rows + segment_size > target_sequences * 1.5:  # ì—¬ìœ  ìˆê²Œ
            break
        
        selected_segments.append(seg_id)
        total_rows += segment_size
        
        if len(selected_segments) >= segments_needed:
            break
    
    print(f"  â€¢ ì„ íƒëœ ì„¸ê·¸ë¨¼íŠ¸: {len(selected_segments)}ê°œ")
    print(f"  â€¢ ì˜ˆìƒ ì‹œí€€ìŠ¤ ìˆ˜: ~{total_rows - LOOKBACK - FORECAST:,}ê°œ")
    
    # ì„ íƒëœ ì„¸ê·¸ë¨¼íŠ¸ì˜ ë°ì´í„°ë§Œ ì¶”ì¶œ
    reduced_df = df[df['segment_id'].isin(selected_segments)].copy()
    
    return reduced_df

# ============================================
# ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒ˜í”Œë§
# ============================================
def sliding_window_sampling(df, valid_segments):
    """ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ìƒ˜í”Œë§"""
    print("\nğŸ“Š ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒ˜í”Œë§ ì¤‘...")
    
    target_sequences = REDUCTION_CONFIG['target_sequences']
    
    # ê° ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
    sampled_indices = []
    sequences_per_segment = target_sequences // len(valid_segments)
    
    for seg_id in valid_segments.index:
        segment_df = df[df['segment_id'] == seg_id]
        segment_indices = segment_df.index.tolist()
        
        # ì´ ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤ ìˆ˜
        max_sequences = len(segment_indices) - LOOKBACK - FORECAST
        
        if max_sequences <= 0:
            continue
        
        # ìƒ˜í”Œë§ ê°„ê²© ê³„ì‚°
        n_samples = min(sequences_per_segment, max_sequences)
        step = max(1, max_sequences // n_samples)
        
        # ì‹œì‘ ì§€ì ë“¤ ì„ íƒ
        for i in range(LOOKBACK, len(segment_indices) - FORECAST, step):
            # ì‹œí€€ìŠ¤ì— í•„ìš”í•œ ì¸ë±ìŠ¤ë“¤ (100ë¶„ + 10ë¶„)
            sequence_indices = segment_indices[i-LOOKBACK:i+FORECAST]
            sampled_indices.extend(sequence_indices)
            
            if len(set(sampled_indices)) // (LOOKBACK + FORECAST) >= target_sequences:
                break
        
        if len(set(sampled_indices)) // (LOOKBACK + FORECAST) >= target_sequences:
            break
    
    # ì¤‘ë³µ ì œê±°í•˜ê³  ì •ë ¬
    sampled_indices = sorted(list(set(sampled_indices)))
    reduced_df = df.loc[sampled_indices].copy()
    
    print(f"  â€¢ ìƒ˜í”Œë§ëœ ë°ì´í„°: {len(reduced_df):,}í–‰")
    print(f"  â€¢ ì˜ˆìƒ ì‹œí€€ìŠ¤ ìˆ˜: ~{(len(reduced_df) - LOOKBACK - FORECAST):,}ê°œ")
    
    return reduced_df

# ============================================
# ë°ì´í„° ì •ë¦¬ ë° ê²€ì¦
# ============================================
def clean_and_validate(df):
    """ë°ì´í„° ì •ë¦¬ ë° ê²€ì¦"""
    print("\nâœ… ë°ì´í„° ì •ë¦¬ ë° ê²€ì¦ ì¤‘...")
    
    # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    temp_cols = ['datetime', 'time_diff', 'segment_id', 'temp_ratio']
    for col in temp_cols:
        if col in df.columns:
            df = df.drop(col, axis=1)
    
    # ì¸ë±ìŠ¤ ë¦¬ì…‹
    df = df.reset_index(drop=True)
    
    # ê²€ì¦
    print(f"  â€¢ ìµœì¢… í–‰ ìˆ˜: {len(df):,}")
    print(f"  â€¢ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
    
    # ì‹œí€€ìŠ¤ ìƒì„± ê°€ëŠ¥ í™•ì¸
    if len(df) >= LOOKBACK + FORECAST:
        max_sequences = len(df) - LOOKBACK - FORECAST
        print(f"  â€¢ ìƒì„± ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤: {max_sequences:,}ê°œ")
    else:
        print(f"  âš ï¸ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŒ! ìµœì†Œ {LOOKBACK + FORECAST}í–‰ í•„ìš”")
    
    return df

# ============================================
# í†µê³„ ë¹„êµ
# ============================================
def compare_statistics(df_original, df_reduced):
    """ì›ë³¸ê³¼ ì¶•ì†Œ ë°ì´í„° í†µê³„ ë¹„êµ"""
    print("\nğŸ“Š í†µê³„ ë¹„êµ")
    print("="*60)
    
    print(f"í¬ê¸°: {len(df_original):,} â†’ {len(df_reduced):,} ({len(df_reduced)/len(df_original)*100:.1f}%)")
    
    if 'TOTALCNT' in df_original.columns:
        print(f"\nTOTALCNT í†µê³„:")
        print(f"  â€¢ ì›ë³¸ í‰ê· : {df_original['TOTALCNT'].mean():.0f}")
        print(f"  â€¢ ì¶•ì†Œ í‰ê· : {df_reduced['TOTALCNT'].mean():.0f}")
        print(f"  â€¢ ì›ë³¸ 1500+: {(df_original['TOTALCNT'] >= 1500).mean():.1%}")
        print(f"  â€¢ ì¶•ì†Œ 1500+: {(df_reduced['TOTALCNT'] >= 1500).mean():.1%}")
    
    if 'M14AM14B' in df_original.columns:
        print(f"\nM14AM14B í†µê³„:")
        print(f"  â€¢ ì›ë³¸ í‰ê· : {df_original['M14AM14B'].mean():.0f}")
        print(f"  â€¢ ì¶•ì†Œ í‰ê· : {df_reduced['M14AM14B'].mean():.0f}")
        print(f"  â€¢ ì›ë³¸ 400+: {(df_original['M14AM14B'] >= 400).mean():.1%}")
        print(f"  â€¢ ì¶•ì†Œ 400+: {(df_reduced['M14AM14B'] >= 400).mean():.1%}")

# ============================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================
def main():
    print("="*60)
    print("ğŸš€ ì‹œí€€ìŠ¤ í•™ìŠµìš© ë°ì´í„° ì¶•ì†Œ ë„êµ¬")
    print(f"ğŸ“ ì„¤ì •: {LOOKBACK}ë¶„ ê³¼ê±° â†’ {FORECAST}ë¶„ ë¯¸ë˜ ì˜ˆì¸¡")
    print("="*60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {INPUT_FILE}")
    df = pd.read_csv(INPUT_FILE)
    df_original = df.copy()
    print(f"âœ… ì›ë³¸ í¬ê¸°: {len(df):,}í–‰")
    
    # 2. ì—°ì† êµ¬ê°„ ì°¾ê¸°
    df, valid_segments = find_continuous_segments(df)
    
    if len(valid_segments) == 0:
        print("âŒ ìœ íš¨í•œ ì—°ì† êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤!")
        return None
    
    # 3. ìƒ˜í”Œë§ ìˆ˜í–‰
    if REDUCTION_CONFIG['sampling_method'] == 'segment':
        df_reduced = segment_based_sampling(df, valid_segments)
    else:
        df_reduced = sliding_window_sampling(df, valid_segments)
    
    # 4. ë°ì´í„° ì •ë¦¬
    df_reduced = clean_and_validate(df_reduced)
    
    # 5. í†µê³„ ë¹„êµ
    compare_statistics(df_original, df_reduced)
    
    # 6. ì €ì¥
    print(f"\nğŸ’¾ ì €ì¥ ì¤‘: {OUTPUT_FILE}")
    df_reduced.to_csv(OUTPUT_FILE, index=False)
    print(f"âœ… ì €ì¥ ì™„ë£Œ!")
    
    # 7. ìµœì¢… ì•ˆë‚´
    print("\n" + "="*60)
    print("âœ¨ ì™„ë£Œ! ì´ì œ ê¸°ì¡´ V6 ì‹œí€€ìŠ¤ ìƒì„±ê¸° ì‚¬ìš© ê°€ëŠ¥")
    print("="*60)
    print(f"1. ë°ì´í„° íŒŒì¼ ê²½ë¡œë¥¼ ë³€ê²½í•˜ì„¸ìš”:")
    print(f"   DATA_FILE = '{OUTPUT_FILE}'")
    print(f"2. ê·¸ëŒ€ë¡œ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤:")
    print(f"   python V6_ì‹œí€€ìŠ¤_END.PY")
    print("\nğŸ’¡ íŒ: ë” ì¤„ì´ë ¤ë©´ target_sequencesë¥¼ 30000ìœ¼ë¡œ ì„¤ì •")
    
    return df_reduced

if __name__ == '__main__':
    result = main()