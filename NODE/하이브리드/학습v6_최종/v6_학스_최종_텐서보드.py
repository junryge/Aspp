"""
V6_í•™ìŠµ_TensorBoard_ìˆ˜ì •ì™„ë£Œ.py - TensorBoard + í•™ìŠµ ì¬ê°œ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ 5ê°œ ëª¨ë¸ ì•™ìƒë¸” í•™ìŠµ
ë””ë ‰í† ë¦¬ ìƒì„± ë¬¸ì œ í•´ê²° ë²„ì „
TensorFlow 2.15.0 + TensorBoard + Resume Training
"""

import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import json
import os
import warnings
from datetime import datetime
import pickle
import pathlib
warnings.filterwarnings('ignore')

print("="*60)
print("ğŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ì•™ìƒë¸” í•™ìŠµ V6 - TensorBoard Edition")
print(f"ğŸ“¦ TensorFlow ë²„ì „: {tf.__version__}")
print("="*60)

# ============================================
# 1. ì„¤ì •
# ============================================
class Config:
    # ì‹œí€€ìŠ¤ íŒŒì¼
    SEQUENCE_FILE = './sequences_v6.npz'
    
    # M14 ì„ê³„ê°’
    M14B_THRESHOLDS = {
        1400: 320,
        1500: 400,
        1600: 450,
        1700: 500
    }
    
    RATIO_THRESHOLDS = {
        1400: 4,
        1500: 5,
        1600: 6,
        1700: 7
    }
    
    # í•™ìŠµ ì„¤ì •
    BATCH_SIZE = 32
    EPOCHS = 100
    LEARNING_RATE = 0.001
    PATIENCE = 15
    
    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    MODEL_DIR = './models_v6/'
    CHECKPOINT_DIR = './checkpoints_v6/'
    LOG_DIR = './logs/fit/'
    
    # ê°€ì¤‘ì¹˜ ì„¤ì •
    SPIKE_WEIGHTS = {
        'normal': 1.0,
        'level_1400': 3.0,
        'level_1500': 5.0,
        'level_1600': 8.0,
        'level_1700': 10.0
    }
    
    # í•™ìŠµ ì¬ê°œ ì„¤ì •
    RESUME_TRAINING = True  # Trueë¡œ ì„¤ì •í•˜ë©´ ì´ì „ í•™ìŠµ ì´ì–´ì„œ ì§„í–‰

# ë””ë ‰í† ë¦¬ ìƒì„± (Windows í˜¸í™˜)
for dir_path in [Config.MODEL_DIR, Config.CHECKPOINT_DIR, Config.LOG_DIR]:
    pathlib.Path(dir_path).mkdir(parents=True, exist_ok=True)
    print(f"âœ… ë””ë ‰í† ë¦¬ ìƒì„±/í™•ì¸: {dir_path}")

# ============================================
# í•™ìŠµ ìƒíƒœ ì €ì¥/ë¡œë“œ í•¨ìˆ˜
# ============================================
def save_training_state(model_name, epoch, history_dict):
    """í•™ìŠµ ìƒíƒœ ì €ì¥"""
    state = {
        'epoch': epoch,
        'history': history_dict
    }
    state_path = os.path.join(Config.CHECKPOINT_DIR, f"{model_name}_state.pkl")
    with open(state_path, 'wb') as f:
        pickle.dump(state, f)

def load_training_state(model_name):
    """í•™ìŠµ ìƒíƒœ ë¡œë“œ"""
    state_file = os.path.join(Config.CHECKPOINT_DIR, f"{model_name}_state.pkl")
    if os.path.exists(state_file):
        with open(state_file, 'rb') as f:
            return pickle.load(f)
    return None

def get_initial_epoch(model_name):
    """ì‹œì‘ ì—í­ ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸°"""
    if Config.RESUME_TRAINING:
        state = load_training_state(model_name)
        if state:
            return state['epoch']
    return 0

# ============================================
# 2. ì»¤ìŠ¤í…€ ë ˆì´ì–´ ë° ì†ì‹¤ í•¨ìˆ˜
# ============================================
class WeightedLoss(tf.keras.losses.Loss):
    """ë¬¼ë¥˜ëŸ‰ êµ¬ê°„ë³„ ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜"""
    def call(self, y_true, y_pred):
        mae = tf.abs(y_true - y_pred)
        
        # êµ¬ê°„ë³„ ê°€ì¤‘ì¹˜
        weights = tf.ones_like(y_true)
        weights = tf.where(y_true >= 1700, 10.0, weights)
        weights = tf.where((y_true >= 1600) & (y_true < 1700), 8.0, weights)
        weights = tf.where((y_true >= 1500) & (y_true < 1600), 5.0, weights)
        weights = tf.where((y_true >= 1400) & (y_true < 1500), 3.0, weights)
        
        return tf.reduce_mean(mae * weights)

class M14RuleCorrection(tf.keras.layers.Layer):
    """M14 ê·œì¹™ ê¸°ë°˜ ë³´ì • ë ˆì´ì–´"""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
    def call(self, inputs):
        pred, m14_features = inputs
        
        # M14 íŠ¹ì§• ë¶„í•´
        m14b = m14_features[:, 0:1]
        m10a = m14_features[:, 1:2]
        ratio = m14_features[:, 3:4]
        
        # ê·œì¹™ ê¸°ë°˜ ë³´ì •
        # 1700+ ì‹ í˜¸
        condition_1700 = tf.logical_and(
            tf.greater_equal(m14b, 500),
            tf.greater_equal(ratio, 7)
        )
        pred = tf.where(condition_1700, tf.maximum(pred, 1700), pred)
        
        # 1600+ ì‹ í˜¸
        condition_1600 = tf.logical_and(
            tf.greater_equal(m14b, 450),
            tf.greater_equal(ratio, 6)
        )
        pred = tf.where(condition_1600, tf.maximum(pred, 1600), pred)
        
        # 1500+ ì‹ í˜¸
        condition_1500 = tf.logical_and(
            tf.greater_equal(m14b, 400),
            tf.greater_equal(ratio, 5)
        )
        pred = tf.where(condition_1500, tf.maximum(pred, 1500), pred)
        
        # 1400+ ì‹ í˜¸
        condition_1400 = tf.greater_equal(m14b, 320)
        pred = tf.where(condition_1400, tf.maximum(pred, 1400), pred)
        
        # í™©ê¸ˆ íŒ¨í„´ ë³´ì •
        golden_pattern = tf.logical_and(
            tf.greater_equal(m14b, 350),
            tf.less(m10a, 70)
        )
        pred = tf.where(golden_pattern, pred * 1.1, pred)
        
        return pred

class SpikePerformanceCallback(tf.keras.callbacks.Callback):
    """ê¸‰ì¦ ê°ì§€ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ + TensorBoard ë¡œê¹…"""
    def __init__(self, X_val, y_val, log_dir, model_name):
        self.X_val = X_val
        self.y_val = y_val
        self.writer = tf.summary.create_file_writer(log_dir)
        self.model_name = model_name
        
    def on_epoch_end(self, epoch, logs=None):
        if epoch % 10 == 0:
            pred = self.model.predict(self.X_val, verbose=0)
            if isinstance(pred, list):
                pred = pred[0]
            pred = pred.flatten()
            
            # êµ¬ê°„ë³„ Recall
            with self.writer.as_default():
                for level in [1400, 1500, 1600, 1700]:
                    mask = self.y_val >= level
                    if np.any(mask):
                        recall = np.sum((pred >= level) & mask) / np.sum(mask)
                        print(f"   {level} Recall: {recall:.2%}", end=" ")
                        tf.summary.scalar(f'{self.model_name}/recall_{level}+', recall, step=epoch)
            print()
            self.writer.flush()

class TrainingStateCallback(tf.keras.callbacks.Callback):
    """í•™ìŠµ ìƒíƒœ ì €ì¥ ì½œë°±"""
    def __init__(self, model_name):
        self.model_name = model_name
        self.history_dict = {}
        
    def on_epoch_end(self, epoch, logs=None):
        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        for key, value in logs.items():
            if key not in self.history_dict:
                self.history_dict[key] = []
            self.history_dict[key].append(value)
        
        # ìƒíƒœ ì €ì¥
        save_training_state(self.model_name, epoch + 1, self.history_dict)

# ============================================
# 3. ëª¨ë¸ ì •ì˜
# ============================================
class ModelsV6:
    
    @staticmethod
    def build_lstm_model(input_shape):
        """1. LSTM ëª¨ë¸ - ì¥ê¸° ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ"""
        inputs = tf.keras.Input(shape=input_shape, name='lstm_input')
        
        # Stacked LSTM
        lstm1 = tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)(inputs)
        lstm2 = tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)(lstm1)
        lstm3 = tf.keras.layers.LSTM(64, dropout=0.2)(lstm2)
        
        # Dense layers
        dense1 = tf.keras.layers.Dense(128, activation='relu')(lstm3)
        dropout = tf.keras.layers.Dropout(0.3)(dense1)
        dense2 = tf.keras.layers.Dense(64, activation='relu')(dropout)
        
        # Output
        output = tf.keras.layers.Dense(1, name='lstm_output')(dense2)
        
        model = tf.keras.Model(inputs=inputs, outputs=output, name='LSTM_Model')
        return model
    
    @staticmethod
    def build_enhanced_gru(input_shape):
        """2. GRU ëª¨ë¸ - ë‹¨ê¸° ë³€ë™ì„± í¬ì°©"""
        inputs = tf.keras.Input(shape=input_shape, name='gru_input')
        
        # Layer Normalization
        x = tf.keras.layers.LayerNormalization()(inputs)
        
        # Stacked GRU with residual
        gru1 = tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2)(x)
        gru2 = tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2)(gru1)
        
        # Residual connection
        residual = tf.keras.layers.Add()([gru1, gru2])
        
        # Final GRU
        gru3 = tf.keras.layers.GRU(64, dropout=0.2)(residual)
        
        # Dense layers
        dense1 = tf.keras.layers.Dense(128, activation='relu')(gru3)
        dropout = tf.keras.layers.Dropout(0.3)(dense1)
        dense2 = tf.keras.layers.Dense(64, activation='relu')(dropout)
        
        # Output
        output = tf.keras.layers.Dense(1, name='gru_output')(dense2)
        
        model = tf.keras.Model(inputs=inputs, outputs=output, name='GRU_Model')
        return model
    
    @staticmethod
    def build_cnn_lstm(input_shape):
        """3. CNN-LSTM ëª¨ë¸ - ë³µí•© íŒ¨í„´ ì¸ì‹"""
        inputs = tf.keras.Input(shape=input_shape, name='cnn_input')
        
        # Multi-scale CNN
        conv1 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)
        conv2 = tf.keras.layers.Conv1D(64, 5, activation='relu', padding='same')(inputs)
        conv3 = tf.keras.layers.Conv1D(64, 7, activation='relu', padding='same')(inputs)
        
        # Concatenate
        concat = tf.keras.layers.Concatenate()([conv1, conv2, conv3])
        
        # Batch normalization
        norm = tf.keras.layers.BatchNormalization()(concat)
        
        # LSTM
        lstm = tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)(norm)
        lstm2 = tf.keras.layers.LSTM(64, dropout=0.2)(lstm)
        
        # Dense layers
        dense1 = tf.keras.layers.Dense(128, activation='relu')(lstm2)
        dropout = tf.keras.layers.Dropout(0.3)(dense1)
        
        # Output
        output = tf.keras.layers.Dense(1, name='cnn_lstm_output')(dropout)
        
        model = tf.keras.Model(inputs=inputs, outputs=output, name='CNN_LSTM_Model')
        return model
    
    @staticmethod
    def build_spike_detector(input_shape):
        """4. Spike Detector - ì´ìƒì¹˜ ê°ì§€ ì „ë¬¸"""
        inputs = tf.keras.Input(shape=input_shape, name='spike_input')
        
        # Multi-scale CNN for pattern detection
        conv1 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)
        conv2 = tf.keras.layers.Conv1D(64, 5, activation='relu', padding='same')(inputs)
        conv3 = tf.keras.layers.Conv1D(64, 7, activation='relu', padding='same')(inputs)
        
        # Concatenate multi-scale features
        concat = tf.keras.layers.Concatenate()([conv1, conv2, conv3])
        
        # Batch normalization
        norm = tf.keras.layers.BatchNormalization()(concat)
        
        # Attention mechanism
        attention = tf.keras.layers.MultiHeadAttention(
            num_heads=4, 
            key_dim=48,
            dropout=0.2
        )(norm, norm)
        
        # BiLSTM
        lstm = tf.keras.layers.Bidirectional(
            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)
        )(attention)
        
        # Global pooling
        pooled = tf.keras.layers.GlobalAveragePooling1D()(lstm)
        
        # Dense layers
        dense1 = tf.keras.layers.Dense(256, activation='relu')(pooled)
        dropout1 = tf.keras.layers.Dropout(0.3)(dense1)
        dense2 = tf.keras.layers.Dense(128, activation='relu')(dropout1)
        dropout2 = tf.keras.layers.Dropout(0.2)(dense2)
        
        # Dual output (íšŒê·€ + ë¶„ë¥˜)
        regression_output = tf.keras.layers.Dense(1, name='spike_value')(dropout2)
        classification_output = tf.keras.layers.Dense(1, activation='sigmoid', name='spike_prob')(dropout2)
        
        model = tf.keras.Model(
            inputs=inputs,
            outputs=[regression_output, classification_output],
            name='Spike_Detector'
        )
        return model
    
    @staticmethod
    def build_rule_based_model(input_shape, m14_shape):
        """5. Rule-Based ëª¨ë¸ - ê²€ì¦ëœ í™©ê¸ˆ íŒ¨í„´"""
        # ì‹œê³„ì—´ ì…ë ¥
        time_input = tf.keras.Input(shape=input_shape, name='time_input')
        # M14 íŠ¹ì§• ì…ë ¥
        m14_input = tf.keras.Input(shape=m14_shape, name='m14_input')
        
        # ê°„ë‹¨í•œ ì‹œê³„ì—´ ì²˜ë¦¬
        lstm = tf.keras.layers.LSTM(32, dropout=0.2)(time_input)
        
        # M14 íŠ¹ì§• ì²˜ë¦¬
        m14_dense = tf.keras.layers.Dense(16, activation='relu')(m14_input)
        
        # ê²°í•©
        combined = tf.keras.layers.Concatenate()([lstm, m14_dense])
        
        # Dense layers
        dense1 = tf.keras.layers.Dense(64, activation='relu')(combined)
        dropout = tf.keras.layers.Dropout(0.2)(dense1)
        dense2 = tf.keras.layers.Dense(32, activation='relu')(dropout)
        
        # ì˜ˆì¸¡
        prediction = tf.keras.layers.Dense(1, name='rule_pred')(dense2)
        
        # M14 ê·œì¹™ ì ìš©
        corrected = M14RuleCorrection()([prediction, m14_input])
        
        model = tf.keras.Model(
            inputs=[time_input, m14_input],
            outputs=corrected,
            name='Rule_Based_Model'
        )
        return model

# ============================================
# 4. ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„
# ============================================
print("\nğŸ“‚ ì‹œí€€ìŠ¤ ë¡œë”© ì¤‘...")

# ì‹œí€€ìŠ¤ ë¡œë“œ
if not os.path.exists(Config.SEQUENCE_FILE):
    print(f"âŒ ì‹œí€€ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {Config.SEQUENCE_FILE}")
    print("ë¨¼ì € V6_ì‹œí€€ìŠ¤ìƒì„±_ìµœì¢…ë³¸.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    exit()

data = np.load(Config.SEQUENCE_FILE)
X = data['X']
y = data['y']
m14_features = data['m14_features']

print(f"  âœ… ë¡œë“œ ì™„ë£Œ!")
print(f"  X shape: {X.shape}")
print(f"  y shape: {y.shape}")
print(f"  m14_features shape: {m14_features.shape}")

# í•™ìŠµ/ê²€ì¦ ë¶„í• 
X_train, X_val, y_train, y_val, m14_train, m14_val = train_test_split(
    X, y, m14_features, test_size=0.2, random_state=42
)

# 1400+ ì—¬ë¶€ ë ˆì´ë¸” ìƒì„±
y_spike_class = (y_train >= 1400).astype(float)
y_val_spike_class = (y_val >= 1400).astype(float)

print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
print(f"  í•™ìŠµ: {X_train.shape[0]:,}ê°œ")
print(f"  ê²€ì¦: {X_val.shape[0]:,}ê°œ")
print(f"  1400+ í•™ìŠµ ë¹„ìœ¨: {y_spike_class.mean():.1%}")
print(f"  1400+ ê²€ì¦ ë¹„ìœ¨: {y_val_spike_class.mean():.1%}")

# ============================================
# 5. í•™ìŠµ íŒŒì´í”„ë¼ì¸
# ============================================
print("\n" + "="*60)
print("ğŸ‹ï¸ 5ê°œ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
print("ğŸ“Š TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬:", Config.LOG_DIR)
print("âš¡ í•™ìŠµ ì¬ê°œ ëª¨ë“œ:", "ON" if Config.RESUME_TRAINING else "OFF")
print("="*60)

models = {}
history = {}
evaluation_results = {}

# ============================================
# 5.1 LSTM ëª¨ë¸
# ============================================
print("\n1ï¸âƒ£ LSTM ëª¨ë¸ í•™ìŠµ (ì¥ê¸° ì‹œê³„ì—´ íŒ¨í„´)")

# ëª¨ë¸ ìƒì„± ë˜ëŠ” ë¡œë“œ
lstm_model = ModelsV6.build_lstm_model(X_train.shape[1:])
checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, "lstm_checkpoint.h5")

# ì´ì „ ê°€ì¤‘ì¹˜ ë¡œë“œ (ìˆìœ¼ë©´)
if Config.RESUME_TRAINING and os.path.exists(checkpoint_path):
    print("  âœ… ì´ì „ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ")
    lstm_model.load_weights(checkpoint_path)

lstm_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
    loss=WeightedLoss(),
    metrics=['mae']
)

# TensorBoard ì„¤ì •
log_dir = os.path.join(Config.LOG_DIR, "lstm", datetime.now().strftime("%Y%m%d-%H%M%S"))
os.makedirs(log_dir, exist_ok=True)

tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,
    write_graph=True,
    write_images=True,
    update_freq='epoch',
    profile_batch='500,520'
)

# ì‹œì‘ ì—í­ ê²°ì •
initial_epoch = get_initial_epoch('lstm')
print(f"  ì‹œì‘ ì—í­: {initial_epoch}")

lstm_history = lstm_model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=Config.EPOCHS,
    initial_epoch=initial_epoch,
    batch_size=Config.BATCH_SIZE,
    callbacks=[
        tf.keras.callbacks.ModelCheckpoint(
            checkpoint_path,
            save_weights_only=True,
            save_best_only=False,
            verbose=0
        ),
        tf.keras.callbacks.ModelCheckpoint(
            os.path.join(Config.MODEL_DIR, "lstm_best.h5"),
            save_best_only=True,
            monitor='val_loss',
            verbose=0
        ),
        tf.keras.callbacks.EarlyStopping(patience=Config.PATIENCE, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5),
        tensorboard_callback,
        SpikePerformanceCallback(X_val, y_val, log_dir, 'lstm'),
        TrainingStateCallback('lstm')
    ],
    verbose=1
)

models['lstm'] = lstm_model
history['lstm'] = lstm_history

# ============================================
# 5.2 GRU ëª¨ë¸
# ============================================
print("\n2ï¸âƒ£ Enhanced GRU ëª¨ë¸ í•™ìŠµ (ë‹¨ê¸° ë³€ë™ì„±)")

gru_model = ModelsV6.build_enhanced_gru(X_train.shape[1:])
checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, "gru_checkpoint.h5")

if Config.RESUME_TRAINING and os.path.exists(checkpoint_path):
    print("  âœ… ì´ì „ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ")
    gru_model.load_weights(checkpoint_path)

gru_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
    loss=WeightedLoss(),
    metrics=['mae']
)

log_dir = os.path.join(Config.LOG_DIR, "gru", datetime.now().strftime("%Y%m%d-%H%M%S"))
os.makedirs(log_dir, exist_ok=True)

tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,
    write_graph=True,
    write_images=True,
    update_freq='epoch'
)

initial_epoch = get_initial_epoch('gru')
print(f"  ì‹œì‘ ì—í­: {initial_epoch}")

gru_history = gru_model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=Config.EPOCHS,
    initial_epoch=initial_epoch,
    batch_size=Config.BATCH_SIZE,
    callbacks=[
        tf.keras.callbacks.ModelCheckpoint(
            checkpoint_path,
            save_weights_only=True,
            save_best_only=False,
            verbose=0
        ),
        tf.keras.callbacks.ModelCheckpoint(
            os.path.join(Config.MODEL_DIR, "gru_best.h5"),
            save_best_only=True,
            monitor='val_loss',
            verbose=0
        ),
        tf.keras.callbacks.EarlyStopping(patience=Config.PATIENCE, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5),
        tensorboard_callback,
        SpikePerformanceCallback(X_val, y_val, log_dir, 'gru'),
        TrainingStateCallback('gru')
    ],
    verbose=1
)

models['gru'] = gru_model
history['gru'] = gru_history

# ============================================
# 5.3 CNN-LSTM ëª¨ë¸
# ============================================
print("\n3ï¸âƒ£ CNN-LSTM ëª¨ë¸ í•™ìŠµ (ë³µí•© íŒ¨í„´ ì¸ì‹)")

cnn_lstm_model = ModelsV6.build_cnn_lstm(X_train.shape[1:])
checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, "cnn_lstm_checkpoint.h5")

if Config.RESUME_TRAINING and os.path.exists(checkpoint_path):
    print("  âœ… ì´ì „ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ")
    cnn_lstm_model.load_weights(checkpoint_path)

cnn_lstm_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
    loss=WeightedLoss(),
    metrics=['mae']
)

log_dir = os.path.join(Config.LOG_DIR, "cnn_lstm", datetime.now().strftime("%Y%m%d-%H%M%S"))
os.makedirs(log_dir, exist_ok=True)

tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,
    write_graph=True,
    write_images=True,
    update_freq='epoch'
)

initial_epoch = get_initial_epoch('cnn_lstm')
print(f"  ì‹œì‘ ì—í­: {initial_epoch}")

cnn_lstm_history = cnn_lstm_model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=Config.EPOCHS,
    initial_epoch=initial_epoch,
    batch_size=Config.BATCH_SIZE,
    callbacks=[
        tf.keras.callbacks.ModelCheckpoint(
            checkpoint_path,
            save_weights_only=True,
            save_best_only=False,
            verbose=0
        ),
        tf.keras.callbacks.ModelCheckpoint(
            os.path.join(Config.MODEL_DIR, "cnn_lstm_best.h5"),
            save_best_only=True,
            monitor='val_loss',
            verbose=0
        ),
        tf.keras.callbacks.EarlyStopping(patience=Config.PATIENCE, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5),
        tensorboard_callback,
        SpikePerformanceCallback(X_val, y_val, log_dir, 'cnn_lstm'),
        TrainingStateCallback('cnn_lstm')
    ],
    verbose=1
)

models['cnn_lstm'] = cnn_lstm_model
history['cnn_lstm'] = cnn_lstm_history

# ============================================
# 5.4 Spike Detector ëª¨ë¸
# ============================================
print("\n4ï¸âƒ£ Spike Detector ëª¨ë¸ í•™ìŠµ (ì´ìƒì¹˜ ê°ì§€)")

spike_model = ModelsV6.build_spike_detector(X_train.shape[1:])
checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, "spike_checkpoint.h5")

if Config.RESUME_TRAINING and os.path.exists(checkpoint_path):
    print("  âœ… ì´ì „ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ")
    spike_model.load_weights(checkpoint_path)

spike_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
    loss={
        'spike_value': WeightedLoss(),
        'spike_prob': 'binary_crossentropy'
    },
    loss_weights={
        'spike_value': 1.0,
        'spike_prob': 0.3
    },
    metrics=['mae']
)

log_dir = os.path.join(Config.LOG_DIR, "spike", datetime.now().strftime("%Y%m%d-%H%M%S"))
os.makedirs(log_dir, exist_ok=True)

tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,
    write_graph=True,
    write_images=True,
    update_freq='epoch'
)

initial_epoch = get_initial_epoch('spike')
print(f"  ì‹œì‘ ì—í­: {initial_epoch}")

spike_history = spike_model.fit(
    X_train, 
    [y_train, y_spike_class],
    validation_data=(X_val, [y_val, y_val_spike_class]),
    epochs=Config.EPOCHS,
    initial_epoch=initial_epoch,
    batch_size=Config.BATCH_SIZE,
    callbacks=[
        tf.keras.callbacks.ModelCheckpoint(
            checkpoint_path,
            save_weights_only=True,
            save_best_only=False,
            verbose=0
        ),
        tf.keras.callbacks.ModelCheckpoint(
            os.path.join(Config.MODEL_DIR, "spike_best.h5"),
            save_best_only=True,
            monitor='val_spike_value_loss',
            verbose=0
        ),
        tf.keras.callbacks.EarlyStopping(patience=Config.PATIENCE, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5),
        tensorboard_callback,
        TrainingStateCallback('spike')
    ],
    verbose=1
)

models['spike'] = spike_model
history['spike'] = spike_history

# ============================================
# 5.5 Rule-Based ëª¨ë¸
# ============================================
print("\n5ï¸âƒ£ Rule-Based ëª¨ë¸ í•™ìŠµ (ê²€ì¦ëœ í™©ê¸ˆ íŒ¨í„´)")

rule_model = ModelsV6.build_rule_based_model(X_train.shape[1:], m14_train.shape[1])
checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, "rule_checkpoint.h5")

if Config.RESUME_TRAINING and os.path.exists(checkpoint_path):
    print("  âœ… ì´ì „ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ")
    rule_model.load_weights(checkpoint_path)

rule_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * 0.5),
    loss=WeightedLoss(),
    metrics=['mae']
)

log_dir = os.path.join(Config.LOG_DIR, "rule", datetime.now().strftime("%Y%m%d-%H%M%S"))
os.makedirs(log_dir, exist_ok=True)

tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,
    write_graph=True,
    write_images=True,
    update_freq='epoch'
)

initial_epoch = get_initial_epoch('rule')
print(f"  ì‹œì‘ ì—í­: {initial_epoch}")

rule_history = rule_model.fit(
    [X_train, m14_train], 
    y_train,
    validation_data=([X_val, m14_val], y_val),
    epochs=50,  # Rule-basedëŠ” ë¹ ë¥´ê²Œ ìˆ˜ë ´
    initial_epoch=initial_epoch,
    batch_size=Config.BATCH_SIZE,
    callbacks=[
        tf.keras.callbacks.ModelCheckpoint(
            checkpoint_path,
            save_weights_only=True,
            save_best_only=False,
            verbose=0
        ),
        tf.keras.callbacks.ModelCheckpoint(
            os.path.join(Config.MODEL_DIR, "rule_best.h5"),
            save_best_only=True,
            monitor='val_loss',
            verbose=0
        ),
        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
        tensorboard_callback,
        TrainingStateCallback('rule')
    ],
    verbose=1
)

models['rule'] = rule_model
history['rule'] = rule_history

# ============================================
# 6. ìµœì¢… ì•™ìƒë¸” ëª¨ë¸
# ============================================
print("\n" + "="*60)
print("ğŸ¯ ìµœì¢… ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±")
print("="*60)

# ì…ë ¥ ì •ì˜
time_series_input = tf.keras.Input(shape=X_train.shape[1:], name='ensemble_time_input')
m14_input = tf.keras.Input(shape=m14_train.shape[1], name='ensemble_m14_input')

# ê° ëª¨ë¸ ì˜ˆì¸¡
lstm_pred = models['lstm'](time_series_input)
gru_pred = models['gru'](time_series_input)
cnn_lstm_pred = models['cnn_lstm'](time_series_input)
spike_pred, spike_prob = models['spike'](time_series_input)
rule_pred = models['rule']([time_series_input, m14_input])

# M14 ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜ ìƒì„±
weight_dense = tf.keras.layers.Dense(32, activation='relu')(m14_input)
weight_dense = tf.keras.layers.Dense(16, activation='relu')(weight_dense)
weights = tf.keras.layers.Dense(5, activation='softmax', name='ensemble_weights')(weight_dense)

# ê°€ì¤‘ì¹˜ ë¶„ë¦¬
w_lstm = tf.keras.layers.Lambda(lambda x: x[:, 0:1])(weights)
w_gru = tf.keras.layers.Lambda(lambda x: x[:, 1:2])(weights)
w_cnn = tf.keras.layers.Lambda(lambda x: x[:, 2:3])(weights)
w_spike = tf.keras.layers.Lambda(lambda x: x[:, 3:4])(weights)
w_rule = tf.keras.layers.Lambda(lambda x: x[:, 4:5])(weights)

# ê°€ì¤‘ í‰ê· 
weighted_lstm = tf.keras.layers.Multiply()([lstm_pred, w_lstm])
weighted_gru = tf.keras.layers.Multiply()([gru_pred, w_gru])
weighted_cnn = tf.keras.layers.Multiply()([cnn_lstm_pred, w_cnn])
weighted_spike = tf.keras.layers.Multiply()([spike_pred, w_spike])
weighted_rule = tf.keras.layers.Multiply()([rule_pred, w_rule])

# ì•™ìƒë¸” ì˜ˆì¸¡
ensemble_pred = tf.keras.layers.Add()([
    weighted_lstm, weighted_gru, weighted_cnn, 
    weighted_spike, weighted_rule
])

# ìµœì¢… M14 ê·œì¹™ ë³´ì •
final_pred = M14RuleCorrection()([ensemble_pred, m14_input])

# ì•™ìƒë¸” ëª¨ë¸ ì •ì˜
ensemble_model = tf.keras.Model(
    inputs=[time_series_input, m14_input],
    outputs=[final_pred, spike_prob],
    name='Final_Ensemble_Model'
)

# ì•™ìƒë¸” ì²´í¬í¬ì¸íŠ¸
ensemble_checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, "ensemble_checkpoint.h5")

if Config.RESUME_TRAINING and os.path.exists(ensemble_checkpoint_path):
    print("  âœ… ì´ì „ ì•™ìƒë¸” ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ")
    ensemble_model.load_weights(ensemble_checkpoint_path)

ensemble_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * 0.5),
    loss={
        'm14_rule_correction': WeightedLoss(),
        'spike_prob': 'binary_crossentropy'
    },
    loss_weights={
        'm14_rule_correction': 1.0,
        'spike_prob': 0.3
    },
    metrics=['mae']
)

log_dir = os.path.join(Config.LOG_DIR, "ensemble", datetime.now().strftime("%Y%m%d-%H%M%S"))
os.makedirs(log_dir, exist_ok=True)

tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,
    write_graph=True,
    write_images=True,
    update_freq='epoch'
)

initial_epoch = get_initial_epoch('ensemble')
print(f"  ì•™ìƒë¸” ì‹œì‘ ì—í­: {initial_epoch}")

print("\nğŸ“Š ì•™ìƒë¸” íŒŒì¸íŠœë‹...")
ensemble_history = ensemble_model.fit(
    [X_train, m14_train],
    [y_train, y_spike_class],
    validation_data=(
        [X_val, m14_val],
        [y_val, y_val_spike_class]
    ),
    epochs=20,
    initial_epoch=initial_epoch,
    batch_size=Config.BATCH_SIZE,
    callbacks=[
        tf.keras.callbacks.ModelCheckpoint(
            ensemble_checkpoint_path,
            save_weights_only=True,
            save_best_only=False,
            verbose=0
        ),
        tensorboard_callback,
        TrainingStateCallback('ensemble')
    ],
    verbose=1
)

models['ensemble'] = ensemble_model
history['ensemble'] = ensemble_history

print("\nâœ… 5ê°œ ëª¨ë¸ + ì•™ìƒë¸” í•™ìŠµ ì™„ë£Œ!")

# ============================================
# 7. í‰ê°€
# ============================================
print("\n" + "="*60)
print("ğŸ“Š ëª¨ë¸ í‰ê°€")
print("="*60)

for name, model in models.items():
    if name == 'ensemble':
        pred = model.predict([X_val, m14_val], verbose=0)[0].flatten()
    elif name == 'spike':
        pred = model.predict(X_val, verbose=0)[0].flatten()
    elif name == 'rule':
        pred = model.predict([X_val, m14_val], verbose=0).flatten()
    else:
        pred = model.predict(X_val, verbose=0).flatten()
    
    # ì „ì²´ ì„±ëŠ¥
    mae = np.mean(np.abs(y_val - pred))
    
    # êµ¬ê°„ë³„ ì„±ëŠ¥
    level_performance = {}
    for level in [1400, 1500, 1600, 1700]:
        mask = y_val >= level
        if np.any(mask):
            recall = np.sum((pred >= level) & mask) / np.sum(mask)
            level_mae = np.mean(np.abs(y_val[mask] - pred[mask]))
            level_performance[level] = {
                'recall': recall,
                'mae': level_mae,
                'count': np.sum(mask)
            }
    
    evaluation_results[name] = {
        'overall_mae': mae,
        'levels': level_performance
    }
    
    # ì¶œë ¥
    print(f"\nğŸ¯ {name.upper()} ëª¨ë¸:")
    print(f"  ì „ì²´ MAE: {mae:.2f}")
    for level, perf in level_performance.items():
        print(f"  {level}+: Recall={perf['recall']:.2%}, MAE={perf['mae']:.1f} (n={perf['count']})")

# ìµœì¢… ì„ íƒ
best_model = min(evaluation_results.keys(), key=lambda x: evaluation_results[x]['overall_mae'])
print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_model.upper()} ëª¨ë¸")
print(f"  MAE: {evaluation_results[best_model]['overall_mae']:.2f}")

# ============================================
# 8. ëª¨ë¸ ì €ì¥
# ============================================
print("\nğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘...")

for name, model in models.items():
    model_path = os.path.join(Config.MODEL_DIR, f"{name}_model.h5")
    model.save(model_path)
    print(f"  {name}_model.h5 ì €ì¥ ì™„ë£Œ")

# í‰ê°€ ê²°ê³¼ ì €ì¥
results_path = os.path.join(Config.MODEL_DIR, "evaluation_results.json")
with open(results_path, 'w') as f:
    json.dump(evaluation_results, f, indent=2, default=str)

# ì„¤ì • ì €ì¥
config_dict = {k: v for k, v in Config.__dict__.items() if not k.startswith('_')}
config_path = os.path.join(Config.MODEL_DIR, "config.json")
with open(config_path, 'w') as f:
    json.dump(config_dict, f, indent=2)

print("  ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ")

# ============================================
# 9. ì‹œê°í™”
# ============================================
print("\nğŸ“ˆ ê²°ê³¼ ì‹œê°í™” ìƒì„± ì¤‘...")

# ì‹œê°í™” ì½”ë“œëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ì§€ë§Œ ê²½ë¡œ ìˆ˜ì •
fig = plt.figure(figsize=(20, 12))

# 1-5. ê° ëª¨ë¸ í•™ìŠµ ê³¡ì„ 
for idx, (name, hist) in enumerate(history.items()):
    if idx < 5:  # ê°œë³„ ëª¨ë¸ë“¤
        ax = plt.subplot(3, 4, idx+1)
        
        if hasattr(hist, 'history'):
            if name == 'spike':
                loss = hist.history.get('spike_value_loss', [])
                val_loss = hist.history.get('val_spike_value_loss', [])
            else:
                loss = hist.history.get('loss', [])
                val_loss = hist.history.get('val_loss', [])
            
            if loss and val_loss:
                ax.plot(loss, label='Train Loss', alpha=0.8)
                ax.plot(val_loss, label='Val Loss', alpha=0.8)
        
        ax.set_title(f'{name.upper()} Learning Curve')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss')
        ax.legend()
        ax.grid(True, alpha=0.3)

# 6. ì•™ìƒë¸” í•™ìŠµ ê³¡ì„ 
ax = plt.subplot(3, 4, 6)
if 'ensemble' in history and hasattr(history['ensemble'], 'history'):
    loss = history['ensemble'].history.get('m14_rule_correction_loss', [])
    val_loss = history['ensemble'].history.get('val_m14_rule_correction_loss', [])
    if loss and val_loss:
        ax.plot(loss, label='Train Loss', alpha=0.8)
        ax.plot(val_loss, label='Val Loss', alpha=0.8)
ax.set_title('ENSEMBLE Learning Curve')
ax.set_xlabel('Epoch')
ax.set_ylabel('Loss')
ax.legend()
ax.grid(True, alpha=0.3)

# 7. ëª¨ë¸ë³„ MAE ë¹„êµ
ax = plt.subplot(3, 4, 7)
model_names = list(evaluation_results.keys())
maes = [evaluation_results[m]['overall_mae'] for m in model_names]
colors = ['blue', 'green', 'orange', 'red', 'purple', 'brown']

bars = ax.bar(model_names, maes, color=colors[:len(model_names)])
ax.set_title('Model MAE Comparison')
ax.set_ylabel('MAE')
ax.set_ylim(0, max(maes) * 1.2)

for bar, mae in zip(bars, maes):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height,
           f'{mae:.1f}', ha='center', va='bottom')

# 8-11. Recall ë¹„êµ
for idx, level in enumerate([1400, 1500, 1600, 1700]):
    ax = plt.subplot(3, 4, 8+idx)
    recalls = []
    for m in model_names:
        if level in evaluation_results[m]['levels']:
            recalls.append(evaluation_results[m]['levels'][level]['recall'] * 100)
        else:
            recalls.append(0)
    
    bars = ax.bar(model_names, recalls, color=colors[:len(model_names)])
    ax.set_title(f'{level}+ Recall Comparison (%)')
    ax.set_ylabel('Recall (%)')
    ax.set_ylim(0, 105)
    
    for bar, recall in zip(bars, recalls):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{recall:.1f}%', ha='center', va='bottom')

# 12. ì„±ëŠ¥ ìš”ì•½
ax = plt.subplot(3, 4, 12)
ax.axis('off')

summary_text = "ğŸ† Performance Summary\n" + "="*35 + "\n"
summary_text += f"Best Model: {best_model.upper()}\n"
summary_text += f"Overall MAE: {evaluation_results[best_model]['overall_mae']:.2f}\n\n"

summary_text += "Recall by Level:\n"
for level in [1400, 1500, 1600, 1700]:
    if level in evaluation_results[best_model]['levels']:
        recall = evaluation_results[best_model]['levels'][level]['recall']
        mae = evaluation_results[best_model]['levels'][level]['mae']
        summary_text += f"  {level}+: {recall:6.1%} (MAE: {mae:.1f})\n"

summary_text += f"\n5-Model Ensemble Complete!"

ax.text(0.1, 0.9, summary_text, transform=ax.transAxes,
       fontsize=11, verticalalignment='top', fontfamily='monospace')

plt.suptitle('V6 Ensemble Model Performance Analysis', fontsize=16, fontweight='bold')
plt.tight_layout()

# ì €ì¥ ê²½ë¡œ
plot_path = os.path.join(Config.MODEL_DIR, "training_results.png")
plt.savefig(plot_path, dpi=100, bbox_inches='tight')
print(f"  training_results.png ì €ì¥ ì™„ë£Œ: {plot_path}")
plt.show()

# ============================================
# 10. ìµœì¢… ì¶œë ¥
# ============================================
print("\n" + "="*60)
print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
print("="*60)
print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {Config.MODEL_DIR}")
print(f"ğŸ“‚ ì‹œí€€ìŠ¤ íŒŒì¼: {Config.SEQUENCE_FILE}")
print(f"ğŸ“Š ì²´í¬í¬ì¸íŠ¸ ìœ„ì¹˜: {Config.CHECKPOINT_DIR}")
print(f"ğŸ“ˆ TensorBoard ë¡œê·¸: {Config.LOG_DIR}")
print("\nğŸ“Š ìµœì¢… ì„±ëŠ¥:")
print(f"  ìµœê³  ëª¨ë¸: {best_model.upper()}")
print(f"  ì „ì²´ MAE: {evaluation_results[best_model]['overall_mae']:.2f}")
print("\nğŸ’¡ TensorBoard ì‹¤í–‰ ëª…ë ¹ì–´:")
print(f"  tensorboard --logdir={Config.LOG_DIR}")
print(f"  ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:6006 ì ‘ì†")
print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì ìš©")
print("="*60)

# GPU ì •ë³´ ì¶œë ¥
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    print(f"\nğŸ® GPU ì‚¬ìš©: {len(gpus)}ê°œ")
    for gpu in gpus:
        print(f"  {gpu}")
else:
    print("\nğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰ë¨")