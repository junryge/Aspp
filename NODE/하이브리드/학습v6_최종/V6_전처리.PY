import pandas as pd
import numpy as np
import os
from datetime import datetime
import glob
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class DataConsolidator:
    """ë°˜ë„ì²´ ë¬¼ë¥˜ ë°ì´í„° í†µí•© ë° ì¤‘ë³µ ì œê±°"""
    
    def __init__(self):
        # ëª¨ë“  ì»¬ëŸ¼ ìœ ì§€
        self.all_columns = [
            "CURRTIME", "TOTALCNT", "M14AM10A", "M10AM14A", "M14AM10ASUM",
            "M14AM14B", "M14BM14A", "M14AM14BSUM", "M14AM16", "M16M14A", 
            "M14AM16SUM", "TIME"
        ]
        self.pattern_columns = [
            "TOTALCNT", "M14AM10A", "M14AM14B", "M14AM16", "M14AM14BSUM"
        ]
        self.unique_patterns = set()
        
    def load_all_csv_files(self, folder_path):
        """OUTPUT_BY_DATE í´ë”ì—ì„œ ëª¨ë“  CSV íŒŒì¼ ë¡œë“œ ë° í†µí•©"""
        print("ğŸ“‚ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘...")
        all_files = glob.glob(os.path.join(folder_path, "*.csv"))
        all_files.sort()
        
        all_data = []
        file_count = 0
        
        for file in tqdm(all_files, desc="íŒŒì¼ ì½ê¸°"):
            try:
                df = pd.read_csv(file, encoding='utf-8-sig')
                
                # í•„ìš”í•œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                missing_cols = [col for col in self.all_columns if col not in df.columns]
                if missing_cols:
                    print(f"âš ï¸ {os.path.basename(file)}: ëˆ„ë½ëœ ì»¬ëŸ¼ {missing_cols}")
                    continue
                
                # íŒŒì¼ ì •ë³´ ì¶”ê°€
                df['SOURCE_FILE'] = os.path.basename(file)
                all_data.append(df)
                file_count += 1
                
            except Exception as e:
                print(f"âŒ {os.path.basename(file)} ì½ê¸° ì‹¤íŒ¨: {e}")
                continue
        
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            print(f"âœ… ì´ {len(combined_df):,}ê°œ í–‰ ë¡œë“œ ì™„ë£Œ ({file_count}ê°œ íŒŒì¼)")
            return combined_df
        else:
            raise ValueError("ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def create_pattern_hash(self, row):
        """íŒ¨í„´ì˜ ê³ ìœ  í•´ì‹œ ìƒì„± (ì¤‘ë³µ ê²€ì‚¬ìš©)"""
        # ì£¼ìš” íŒ¨í„´ ì»¬ëŸ¼ê°’ì„ ë°˜ì˜¬ë¦¼í•˜ì—¬ íŒ¨í„´ ìƒì„±
        pattern_values = []
        
        # TOTALCNTëŠ” 50 ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
        pattern_values.append(round(row['TOTALCNT'] / 50) * 50)
        
        # M14AM14BëŠ” 20 ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
        pattern_values.append(round(row['M14AM14B'] / 20) * 20)
        
        # M14AM10AëŠ” 10 ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
        pattern_values.append(round(row['M14AM10A'] / 10) * 10)
        
        # M14AM16ëŠ” 20 ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
        pattern_values.append(round(row['M14AM16'] / 20) * 20)
        
        # M14AM14BSUMëŠ” 50 ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
        pattern_values.append(round(row['M14AM14BSUM'] / 50) * 50)
        
        return tuple(pattern_values)
    
    def remove_duplicates_smart(self, df):
        """ìŠ¤ë§ˆíŠ¸í•œ ì¤‘ë³µ ì œê±° - ìœ ì‚¬ íŒ¨í„´ ì œê±°"""
        print("\nğŸ” ì¤‘ë³µ ë° ìœ ì‚¬ íŒ¨í„´ ì œê±° ì¤‘...")
        
        original_size = len(df)
        
        # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        filtered_rows = []
        seen_patterns = set()
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="íŒ¨í„´ ë¶„ì„"):
            # íŒ¨í„´ í•´ì‹œ ìƒì„±
            pattern_hash = self.create_pattern_hash(row)
            
            # ìƒˆë¡œìš´ íŒ¨í„´ì¸ ê²½ìš°ë§Œ ì¶”ê°€
            if pattern_hash not in seen_patterns:
                seen_patterns.add(pattern_hash)
                filtered_rows.append(row)
        
        # í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        filtered_df = pd.DataFrame(filtered_rows)
        
        # SOURCE_FILE ì»¬ëŸ¼ ì œê±° (ìµœì¢… íŒŒì¼ì—ëŠ” ë¶ˆí•„ìš”)
        if 'SOURCE_FILE' in filtered_df.columns:
            filtered_df = filtered_df.drop('SOURCE_FILE', axis=1)
        
        removed_count = original_size - len(filtered_df)
        print(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {removed_count:,}ê°œ ì œê±° ({removed_count/original_size*100:.1f}%)")
        print(f"   ì›ë³¸: {original_size:,}ê°œ â†’ í•„í„°ë§: {len(filtered_df):,}ê°œ")
        
        return filtered_df
    
    def validate_data(self, df):
        """ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬"""
        print("\nğŸ” ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ì¤‘...")
        
        # 1. ê²°ì¸¡ê°’ í™•ì¸
        missing_counts = df[self.all_columns].isnull().sum()
        if missing_counts.any():
            print("âš ï¸ ê²°ì¸¡ê°’ ë°œê²¬:")
            print(missing_counts[missing_counts > 0])
            
            # ê²°ì¸¡ê°’ ì œê±°
            df = df.dropna(subset=self.all_columns)
            print(f"   â†’ ê²°ì¸¡ê°’ ìˆëŠ” í–‰ ì œê±° í›„: {len(df):,}ê°œ")
        
        # 2. ì´ìƒê°’ í™•ì¸ (ìŒìˆ˜ ê°’)
        numeric_cols = ["TOTALCNT", "M14AM10A", "M14AM14B", "M14AM16", 
                       "M14AM14BSUM", "M14AM10ASUM", "M14AM16SUM"]
        
        for col in numeric_cols:
            if col in df.columns:
                negative_count = (df[col] < 0).sum()
                if negative_count > 0:
                    print(f"âš ï¸ {col}ì— ìŒìˆ˜ê°’ {negative_count}ê°œ ë°œê²¬ â†’ ì œê±°")
                    df = df[df[col] >= 0]
        
        # 3. ê·¹ë‹¨ì  ì´ìƒê°’ ì œê±° (ìƒìœ„ 0.1% ì œê±°)
        for col in ['TOTALCNT', 'M14AM14B']:
            if col in df.columns:
                q999 = df[col].quantile(0.999)
                outliers = df[col] > q999
                if outliers.sum() > 0:
                    print(f"âš ï¸ {col} ê·¹ë‹¨ ì´ìƒê°’ {outliers.sum()}ê°œ ë°œê²¬ (>{q999:.0f}) â†’ ì œê±°")
                    df = df[~outliers]
        
        print(f"âœ… ìœ íš¨ì„± ê²€ì‚¬ ì™„ë£Œ: ìµœì¢… {len(df):,}ê°œ í–‰")
        return df
    
    def add_useful_features(self, df):
        """ë¶„ì„ì— ìœ ìš©í•œ ì¶”ê°€ íŠ¹ì§• ìƒì„±"""
        print("\nğŸ”§ ì¶”ê°€ íŠ¹ì§• ìƒì„± ì¤‘...")
        
        # 1. ë¹„ìœ¨ íŠ¹ì§• (ì•ˆì „í•˜ê²Œ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
        df['M14B_M14A_RATIO'] = np.where(
            df['M14AM10A'] > 0, 
            df['M14AM14B'] / df['M14AM10A'], 
            0
        )
        
        df['M16_M14A_RATIO'] = np.where(
            df['M14AM10A'] > 0, 
            df['M14AM16'] / df['M14AM10A'], 
            0
        )
        
        # 2. ë³€í™”ìœ¨ íŠ¹ì§• (shift ì‚¬ìš©)
        df['TOTALCNT_CHANGE'] = df['TOTALCNT'].diff().fillna(0)
        df['M14AM14B_CHANGE'] = df['M14AM14B'].diff().fillna(0)
        
        # 3. ì´ë™ í‰ê·  (10ë¶„)
        df['TOTALCNT_MA10'] = df['TOTALCNT'].rolling(window=10, min_periods=1).mean()
        df['M14AM14B_MA10'] = df['M14AM14B'].rolling(window=10, min_periods=1).mean()
        
        print("âœ… ì¶”ê°€ íŠ¹ì§• ìƒì„± ì™„ë£Œ")
        return df
    
    def save_consolidated_data(self, df, output_path):
        """í†µí•©ëœ ë°ì´í„° ì €ì¥"""
        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘: {output_path}")
        
        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬ (ì›ë³¸ ì»¬ëŸ¼ ë¨¼ì €, ì¶”ê°€ íŠ¹ì§•ì€ ë’¤ì—)
        original_cols = [col for col in self.all_columns if col in df.columns]
        additional_cols = [col for col in df.columns if col not in original_cols]
        
        df = df[original_cols + additional_cols]
        
        # CSV ì €ì¥
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        # ìš”ì•½ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ“Š ìµœì¢… ë°ì´í„° ìš”ì•½:")
        print(f"- ì´ í–‰ ìˆ˜: {len(df):,}ê°œ")
        print(f"- ì´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ")
        print(f"- íŒŒì¼ í¬ê¸°: {os.path.getsize(output_path) / 1024 / 1024:.1f} MB")
        print(f"\nì£¼ìš” í†µê³„:")
        print(f"- TOTALCNT í‰ê· : {df['TOTALCNT'].mean():.0f}")
        print(f"- TOTALCNT ë²”ìœ„: {df['TOTALCNT'].min():.0f} ~ {df['TOTALCNT'].max():.0f}")
        print(f"- M14AM14B í‰ê· : {df['M14AM14B'].mean():.0f}")
        print(f"- M14AM14B > 300ì¸ ë¹„ìœ¨: {(df['M14AM14B'] > 300).mean()*100:.1f}%")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ ë°ì´í„° í†µí•© ì‹œì‘")
    print("=" * 60)
    
    # ì„¤ì •
    folder_path = "OUTPUT_BY_DATE"  # ë°ì´í„° í´ë” ê²½ë¡œ
    output_path = "semiconductor_consolidated_data.csv"  # ì¶œë ¥ íŒŒì¼ëª…
    
    # ë°ì´í„° í†µí•©ê¸° ì´ˆê¸°í™”
    consolidator = DataConsolidator()
    
    try:
        # 1. ëª¨ë“  CSV íŒŒì¼ ë¡œë“œ
        df = consolidator.load_all_csv_files(folder_path)
        
        # 2. ì¤‘ë³µ íŒ¨í„´ ì œê±°
        df = consolidator.remove_duplicates_smart(df)
        
        # 3. ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
        df = consolidator.validate_data(df)
        
        # 4. ì¶”ê°€ íŠ¹ì§• ìƒì„± (ì„ íƒì‚¬í•­)
        df = consolidator.add_useful_features(df)
        
        # 5. í†µí•© ë°ì´í„° ì €ì¥
        consolidator.save_consolidated_data(df, output_path)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ë°ì´í„° í†µí•© ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_path}")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()