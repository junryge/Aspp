"""
V6_RAM_30GB_ì™„ì „ì²´.py - RAM 30GB ì œí•œ GPU í•™ìŠµ
5ê°œ ëª¨ë¸ + ì•™ìƒë¸” + ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ ì™„ì „ í¬í•¨
"""

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import RobustScaler
import pickle
import os
import gc
import psutil
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Mixed Precision í™œì„±í™” (ë©”ëª¨ë¦¬ 50% ì ˆì•½)
tf.keras.mixed_precision.set_global_policy('mixed_float16')

print("="*60)
print("ğŸš€ V6 RAM 30GB ì œí•œ - ì™„ì „ì²´ ì‹œìŠ¤í…œ")
print("ğŸ’¾ ìµœëŒ€ RAM: 30GB")
print("ğŸ® GPU í•™ìŠµ + 5ê°œ ëª¨ë¸ + ì•™ìƒë¸”")
print("="*60)

# ============================================
# GPU ì„¤ì •
# ============================================
def setup_gpu():
    """GPU ì„¤ì •"""
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"âœ… GPU ê°ì§€: {len(gpus)}ê°œ")
            return True
        except:
            return False
    return False

has_gpu = setup_gpu()

# ============================================
# ë©”ëª¨ë¦¬ ê´€ë¦¬
# ============================================
class MemoryManager:
    """RAM 30GB ì œí•œ ê´€ë¦¬ì"""
    
    @staticmethod
    def get_usage():
        """í˜„ì¬ RAM ì‚¬ìš©ëŸ‰ GB"""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024**3
    
    @staticmethod
    def enforce_limit(max_gb=28):
        """ë©”ëª¨ë¦¬ ì œí•œ ê°•ì œ (ì•ˆì „ë§ˆì§„ 2GB)"""
        current = MemoryManager.get_usage()
        if current > max_gb:
            gc.collect()
            gc.collect()
            tf.keras.backend.clear_session()
            new = MemoryManager.get_usage()
            print(f"  ğŸ’¾ ë©”ëª¨ë¦¬ ì •ë¦¬: {current:.1f}GB â†’ {new:.1f}GB")
    
    @staticmethod
    def print_status(stage=""):
        """ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥"""
        usage = MemoryManager.get_usage()
        print(f"  [{stage}] RAM: {usage:.1f}GB")

# ============================================
# ì„¤ì •
# ============================================
class Config:
    # íŒŒì¼ ê²½ë¡œ
    SEQUENCE_FILE = './sequences_v6_gpu.npz'
    SCALER_FILE = './scalers_v6_gpu.pkl'
    MODEL_DIR = './models_v6_gpu/'
    CHECKPOINT_DIR = './checkpoints_v6_gpu/'
    
    # í•™ìŠµ ì„¤ì •
    BATCH_SIZE = 32  # ë©”ëª¨ë¦¬ ì ˆì•½
    EPOCHS = 30
    LEARNING_RATE = 0.0005
    PATIENCE = 10
    
    # M14 ì„ê³„ê°’
    M14B_THRESHOLDS = {
        1400: 320,
        1500: 400,
        1600: 450,
        1700: 500
    }
    
    # ë©”ëª¨ë¦¬ ì œí•œ
    MAX_RAM_GB = 28  # ì•ˆì „ë§ˆì§„ 2GB

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(Config.MODEL_DIR, exist_ok=True)
os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)

# ============================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
# ============================================
class CheckpointManager:
    """í•™ìŠµ ì¤‘ë‹¨/ì¬ê°œ ê´€ë¦¬"""
    
    def __init__(self):
        self.checkpoint_file = os.path.join(Config.CHECKPOINT_DIR, 'training_state.pkl')
    
    def save_state(self, completed_models, evaluation_results):
        """ìƒíƒœ ì €ì¥"""
        state = {
            'completed_models': completed_models,
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'evaluation_results': evaluation_results
        }
        
        with open(self.checkpoint_file, 'wb') as f:
            pickle.dump(state, f)
        
        print(f"  ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {completed_models}")
    
    def load_state(self):
        """ìƒíƒœ ë³µì›"""
        if not os.path.exists(self.checkpoint_file):
            return [], {}
        
        with open(self.checkpoint_file, 'rb') as f:
            state = pickle.load(f)
        
        print(f"  ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë³µì›: {state['completed_models']}")
        return state['completed_models'], state.get('evaluation_results', {})

# ============================================
# ì»¤ìŠ¤í…€ ë ˆì´ì–´ ë° ì†ì‹¤ í•¨ìˆ˜
# ============================================
@tf.keras.utils.register_keras_serializable()
class WeightedLoss(tf.keras.losses.Loss):
    """ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜"""
    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        mae = tf.abs(y_true - y_pred)
        
        # ë¬¼ë¥˜ëŸ‰ë³„ ê°€ì¤‘ì¹˜
        weights = tf.ones_like(y_true)
        weights = tf.where(y_true >= 1700, 10.0, weights)
        weights = tf.where((y_true >= 1600) & (y_true < 1700), 8.0, weights)
        weights = tf.where((y_true >= 1500) & (y_true < 1600), 5.0, weights)
        weights = tf.where((y_true >= 1400) & (y_true < 1500), 3.0, weights)
        
        return tf.reduce_mean(mae * weights)

@tf.keras.utils.register_keras_serializable()
class M14RuleCorrection(tf.keras.layers.Layer):
    """M14 ê·œì¹™ ë³´ì • ë ˆì´ì–´"""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    
    def call(self, inputs):
        pred, m14_features = inputs
        pred = tf.cast(pred, tf.float32)
        m14_features = tf.cast(m14_features, tf.float32)
        
        m14b = m14_features[:, 0:1]
        m10a = m14_features[:, 1:2]
        ratio = m14_features[:, 3:4]
        
        # ê²€ì¦ëœ ê·œì¹™
        pred = tf.where((m14b >= 500) & (ratio >= 7), tf.maximum(pred, 1700), pred)
        pred = tf.where((m14b >= 450) & (ratio >= 6), tf.maximum(pred, 1600), pred)
        pred = tf.where((m14b >= 400) & (ratio >= 5), tf.maximum(pred, 1500), pred)
        pred = tf.where(m14b >= 320, tf.maximum(pred, 1400), pred)
        
        # í™©ê¸ˆ íŒ¨í„´
        golden = (m14b >= 350) & (m10a < 70)
        pred = tf.where(golden, pred * 1.1, pred)
        
        return pred

# ============================================
# 5ê°œ ëª¨ë¸ ì •ì˜
# ============================================
class ModelsV6:
    """5ê°œ ì „ë¬¸ ëª¨ë¸"""
    
    @staticmethod
    def build_lstm(input_shape):
        """1. LSTM ëª¨ë¸"""
        model = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=input_shape),
            tf.keras.layers.LayerNormalization(),
            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.3),
            tf.keras.layers.LSTM(64, dropout=0.3),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dropout(0.4),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(1, dtype='float32')
        ], name='LSTM_Model')
        return model
    
    @staticmethod
    def build_gru(input_shape):
        """2. GRU ëª¨ë¸"""
        model = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=input_shape),
            tf.keras.layers.LayerNormalization(),
            tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2),
            tf.keras.layers.GRU(64, dropout=0.2),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dense(1, dtype='float32')
        ], name='GRU_Model')
        return model
    
    @staticmethod
    def build_cnn_lstm(input_shape):
        """3. CNN-LSTM ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape)
        
        conv1 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)
        conv2 = tf.keras.layers.Conv1D(64, 5, activation='relu', padding='same')(inputs)
        conv3 = tf.keras.layers.Conv1D(64, 7, activation='relu', padding='same')(inputs)
        
        concat = tf.keras.layers.Concatenate()([conv1, conv2, conv3])
        norm = tf.keras.layers.BatchNormalization()(concat)
        
        lstm = tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)(norm)
        lstm2 = tf.keras.layers.LSTM(64, dropout=0.2)(lstm)
        
        dense1 = tf.keras.layers.Dense(128, activation='relu')(lstm2)
        dropout = tf.keras.layers.Dropout(0.3)(dense1)
        output = tf.keras.layers.Dense(1, dtype='float32')(dropout)
        
        return tf.keras.Model(inputs=inputs, outputs=output, name='CNN_LSTM_Model')
    
    @staticmethod
    def build_spike_detector(input_shape):
        """4. Spike Detector ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape)
        
        conv1 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)
        conv2 = tf.keras.layers.Conv1D(64, 5, activation='relu', padding='same')(inputs)
        conv3 = tf.keras.layers.Conv1D(64, 7, activation='relu', padding='same')(inputs)
        
        concat = tf.keras.layers.Concatenate()([conv1, conv2, conv3])
        norm = tf.keras.layers.BatchNormalization()(concat)
        
        attention = tf.keras.layers.MultiHeadAttention(
            num_heads=4, key_dim=48, dropout=0.2
        )(norm, norm)
        
        lstm = tf.keras.layers.Bidirectional(
            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)
        )(attention)
        
        pooled = tf.keras.layers.GlobalAveragePooling1D()(lstm)
        
        dense1 = tf.keras.layers.Dense(256, activation='relu')(pooled)
        dropout1 = tf.keras.layers.Dropout(0.3)(dense1)
        dense2 = tf.keras.layers.Dense(128, activation='relu')(dropout1)
        dropout2 = tf.keras.layers.Dropout(0.2)(dense2)
        
        regression_output = tf.keras.layers.Dense(1, name='spike_value', dtype='float32')(dropout2)
        classification_output = tf.keras.layers.Dense(1, activation='sigmoid', name='spike_prob', dtype='float32')(dropout2)
        
        return tf.keras.Model(
            inputs=inputs,
            outputs=[regression_output, classification_output],
            name='Spike_Detector'
        )
    
    @staticmethod
    def build_rule_based(input_shape, m14_shape):
        """5. Rule-Based ëª¨ë¸"""
        time_input = tf.keras.Input(shape=input_shape, name='time_input')
        m14_input = tf.keras.Input(shape=m14_shape, name='m14_input')
        
        lstm = tf.keras.layers.LSTM(32, dropout=0.2)(time_input)
        m14_dense = tf.keras.layers.Dense(16, activation='relu')(m14_input)
        
        combined = tf.keras.layers.Concatenate()([lstm, m14_dense])
        
        dense1 = tf.keras.layers.Dense(64, activation='relu')(combined)
        dropout = tf.keras.layers.Dropout(0.2)(dense1)
        dense2 = tf.keras.layers.Dense(32, activation='relu')(dropout)
        
        prediction = tf.keras.layers.Dense(1, name='rule_pred', dtype='float32')(dense2)
        corrected = M14RuleCorrection()([prediction, m14_input])
        
        return tf.keras.Model(
            inputs=[time_input, m14_input],
            outputs=corrected,
            name='Rule_Based_Model'
        )

# ============================================
# ë°ì´í„° ì œë„ˆë ˆì´í„°
# ============================================
class DataGenerator(tf.keras.utils.Sequence):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ ìƒì„±"""
    
    def __init__(self, X_file, y_file, m14_file, indices, 
                 batch_size=32, model_type='normal', shuffle=True):
        # ë©”ëª¨ë¦¬ ë§µìœ¼ë¡œ ì—´ê¸°
        self.X_mmap = np.load(X_file, mmap_mode='r')
        self.y_mmap = np.load(y_file, mmap_mode='r')
        self.m14_mmap = np.load(m14_file, mmap_mode='r')
        
        self.indices = indices
        self.batch_size = batch_size
        self.model_type = model_type
        self.shuffle = shuffle
        self.on_epoch_end()
    
    def __len__(self):
        return len(self.indices) // self.batch_size
    
    def __getitem__(self, index):
        batch_idx = self.indices[index * self.batch_size:(index + 1) * self.batch_size]
        
        # ë°°ì¹˜ë§Œ ë©”ëª¨ë¦¬ì— ë¡œë“œ
        X = np.array(self.X_mmap[batch_idx], dtype=np.float16)
        y = np.array(self.y_mmap[batch_idx], dtype=np.float32)
        m14 = np.array(self.m14_mmap[batch_idx], dtype=np.float16)
        
        # ëª¨ë¸ë³„ ë°˜í™˜
        if self.model_type == 'spike':
            y_spike = (y >= 1400).astype(np.float32)
            return X, [y, y_spike]
        elif self.model_type == 'rule':
            return [X, m14], y
        elif self.model_type == 'ensemble':
            y_spike = (y >= 1400).astype(np.float32)
            return [X, m14], [y, y_spike]
        else:
            return X, y
    
    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indices)
        gc.collect()  # ì—í­ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬

# ============================================
# ë©”ì¸ ì‹¤í–‰
# ============================================
print(f"\nğŸ’¾ ì´ˆê¸° RAM: {MemoryManager.get_usage():.1f}GB")

# 1. ë°ì´í„° ì¤€ë¹„
print("\nğŸ“¦ ë°ì´í„° ì¤€ë¹„...")

# NPZë¥¼ ê°œë³„ íŒŒì¼ë¡œ ë¶„ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨)
if not os.path.exists('X_data.npy'):
    print("  NPZ â†’ NPY ë³€í™˜ ì¤‘...")
    data = np.load(Config.SEQUENCE_FILE, mmap_mode='r')
    
    # ì²­í¬ ë‹¨ìœ„ë¡œ ì €ì¥
    chunk_size = 10000
    X_shape = data['X'].shape
    y_shape = data['y_original'].shape if 'y_original' in data else data['y'].shape
    m14_shape = data['m14_features'].shape
    
    # ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ ìƒì„±
    X_mmap = np.memmap('X_data.npy', dtype='float16', mode='w+', shape=X_shape)
    y_mmap = np.memmap('y_data.npy', dtype='float32', mode='w+', shape=y_shape)
    m14_mmap = np.memmap('m14_data.npy', dtype='float16', mode='w+', shape=m14_shape)
    
    for i in range(0, X_shape[0], chunk_size):
        end = min(i + chunk_size, X_shape[0])
        X_mmap[i:end] = data['X'][i:end].astype(np.float16)
        y_mmap[i:end] = (data['y_original'][i:end] if 'y_original' in data else data['y'][i:end]).astype(np.float32)
        m14_mmap[i:end] = data['m14_features'][i:end].astype(np.float16)
        
        if i % 50000 == 0:
            print(f"    ì§„í–‰: {i}/{X_shape[0]}")
            MemoryManager.enforce_limit(Config.MAX_RAM_GB)
    
    del X_mmap, y_mmap, m14_mmap
    gc.collect()
    print("  âœ… ë³€í™˜ ì™„ë£Œ")

# ë°ì´í„° ì •ë³´ ë¡œë“œ
X_mmap = np.load('X_data.npy', mmap_mode='r')
n_samples = len(X_mmap)
input_shape = (X_mmap.shape[1], X_mmap.shape[2])

# ì¸ë±ìŠ¤ ë¶„í• 
np.random.seed(42)
indices = np.arange(n_samples)
np.random.shuffle(indices)

split = int(0.8 * n_samples)
train_idx = indices[:split]
val_idx = indices[split:]

print(f"  í•™ìŠµ: {len(train_idx):,}ê°œ")
print(f"  ê²€ì¦: {len(val_idx):,}ê°œ")

MemoryManager.print_status("ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")

# 2. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
checkpoint_manager = CheckpointManager()
completed_models, evaluation_results = checkpoint_manager.load_state()

# 3. 5ê°œ ëª¨ë¸ í•™ìŠµ
model_configs = [
    ('lstm', ModelsV6.build_lstm, input_shape, None, 'normal'),
    ('gru', ModelsV6.build_gru, input_shape, None, 'normal'),
    ('cnn_lstm', ModelsV6.build_cnn_lstm, input_shape, None, 'normal'),
    ('spike', ModelsV6.build_spike_detector, input_shape, None, 'spike'),
    ('rule', ModelsV6.build_rule_based, input_shape, (4,), 'rule')
]

for idx, (name, builder, shape1, shape2, model_type) in enumerate(model_configs):
    if name not in completed_models:
        print(f"\n{['1ï¸âƒ£','2ï¸âƒ£','3ï¸âƒ£','4ï¸âƒ£','5ï¸âƒ£'][idx]} {name.upper()} ëª¨ë¸ í•™ìŠµ")
        MemoryManager.print_status(f"{name} ì‹œì‘")
        
        # ëª¨ë¸ ìƒì„±
        if shape2:
            model = builder(shape1, shape2)
        else:
            model = builder(shape1)
        
        # ì»´íŒŒì¼
        if name == 'spike':
            model.compile(
                optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
                loss={'spike_value': WeightedLoss(), 'spike_prob': 'binary_crossentropy'},
                loss_weights={'spike_value': 1.0, 'spike_prob': 0.3},
                metrics={'spike_value': 'mae', 'spike_prob': 'accuracy'}
            )
        elif name == 'rule':
            model.compile(
                optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * 0.5),
                loss=WeightedLoss(),
                metrics=['mae']
            )
        else:
            model.compile(
                optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
                loss=WeightedLoss(),
                metrics=['mae']
            )
        
        # ì œë„ˆë ˆì´í„° ìƒì„±
        train_gen = DataGenerator(
            'X_data.npy', 'y_data.npy', 'm14_data.npy',
            train_idx, Config.BATCH_SIZE, model_type, shuffle=True
        )
        val_gen = DataGenerator(
            'X_data.npy', 'y_data.npy', 'm14_data.npy',
            val_idx, Config.BATCH_SIZE, model_type, shuffle=False
        )
        
        # ì½œë°±
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                patience=Config.PATIENCE,
                restore_best_weights=True,
                verbose=1
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                patience=5,
                factor=0.5,
                verbose=1
            ),
            tf.keras.callbacks.LambdaCallback(
                on_epoch_end=lambda epoch, logs: (
                    MemoryManager.enforce_limit(Config.MAX_RAM_GB),
                    MemoryManager.print_status(f"ì—í­ {epoch+1}")
                )
            )
        ]
        
        # í•™ìŠµ
        history = model.fit(
            train_gen,
            validation_data=val_gen,
            epochs=Config.EPOCHS,
            callbacks=callbacks,
            verbose=1,
            workers=1,
            use_multiprocessing=False
        )
        
        # ì €ì¥
        model.save(f"{Config.MODEL_DIR}{name}_final.h5")
        completed_models.append(name)
        
        # í‰ê°€
        if name == 'spike':
            eval_result = model.evaluate(val_gen, verbose=0)
            evaluation_results[name] = {'mae': eval_result[1]}
        else:
            eval_result = model.evaluate(val_gen, verbose=0)
            evaluation_results[name] = {'mae': eval_result[1] if len(eval_result) > 1 else eval_result}
        
        print(f"  âœ… {name} MAE: {evaluation_results[name]['mae']:.2f}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_manager.save_state(completed_models, evaluation_results)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model, train_gen, val_gen
        tf.keras.backend.clear_session()
        gc.collect()
        
        MemoryManager.print_status(f"{name} ì™„ë£Œ")
    else:
        print(f"\n{['1ï¸âƒ£','2ï¸âƒ£','3ï¸âƒ£','4ï¸âƒ£','5ï¸âƒ£'][idx]} {name.upper()} - ì´ë¯¸ ì™„ë£Œ âœ“")

# 4. ì•™ìƒë¸” ëª¨ë¸
if 'ensemble' not in completed_models:
    print("\nğŸ¯ ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±")
    MemoryManager.print_status("ì•™ìƒë¸” ì‹œì‘")
    
    # ê°œë³„ ëª¨ë¸ ë¡œë“œ
    models = {}
    for name in ['lstm', 'gru', 'cnn_lstm', 'spike', 'rule']:
        custom_objects = {'WeightedLoss': WeightedLoss}
        if name == 'rule':
            custom_objects['M14RuleCorrection'] = M14RuleCorrection
        
        models[name] = tf.keras.models.load_model(
            f"{Config.MODEL_DIR}{name}_final.h5",
            custom_objects=custom_objects
        )
    
    # ì•™ìƒë¸” êµ¬ì„±
    time_input = tf.keras.Input(shape=input_shape, name='ensemble_time')
    m14_input = tf.keras.Input(shape=(4,), name='ensemble_m14')
    
    # ê° ëª¨ë¸ ì˜ˆì¸¡
    lstm_pred = models['lstm'](time_input)
    gru_pred = models['gru'](time_input)
    cnn_lstm_pred = models['cnn_lstm'](time_input)
    spike_pred, spike_prob = models['spike'](time_input)
    rule_pred = models['rule']([time_input, m14_input])
    
    # ë™ì  ê°€ì¤‘ì¹˜
    weight_dense = tf.keras.layers.Dense(32, activation='relu')(m14_input)
    weight_dense = tf.keras.layers.Dense(16, activation='relu')(weight_dense)
    weights = tf.keras.layers.Dense(5, activation='softmax', dtype='float32')(weight_dense)
    
    w = [tf.keras.layers.Lambda(lambda x: x[:, i:i+1])(weights) for i in range(5)]
    
    # ê°€ì¤‘ í‰ê· 
    weighted = [
        tf.keras.layers.Multiply()([pred, weight])
        for pred, weight in zip([lstm_pred, gru_pred, cnn_lstm_pred, spike_pred, rule_pred], w)
    ]
    
    ensemble_pred = tf.keras.layers.Add()(weighted)
    final_pred = M14RuleCorrection(name='ensemble_output')([ensemble_pred, m14_input])
    spike_prob_output = tf.keras.layers.Lambda(lambda x: x, name='spike_output')(spike_prob)
    
    # ì•™ìƒë¸” ëª¨ë¸
    ensemble_model = tf.keras.Model(
        inputs=[time_input, m14_input],
        outputs=[final_pred, spike_prob_output],
        name='Ensemble_Model'
    )
    
    # ì»´íŒŒì¼
    ensemble_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * 0.5),
        loss={'ensemble_output': WeightedLoss(), 'spike_output': 'binary_crossentropy'},
        loss_weights={'ensemble_output': 1.0, 'spike_output': 0.3},
        metrics={'ensemble_output': 'mae', 'spike_output': 'accuracy'}
    )
    
    # íŒŒì¸íŠœë‹
    train_gen = DataGenerator(
        'X_data.npy', 'y_data.npy', 'm14_data.npy',
        train_idx, Config.BATCH_SIZE, 'ensemble', shuffle=True
    )
    val_gen = DataGenerator(
        'X_data.npy', 'y_data.npy', 'm14_data.npy',
        val_idx, Config.BATCH_SIZE, 'ensemble', shuffle=False
    )
    
    ensemble_model.fit(
        train_gen,
        validation_data=val_gen,
        epochs=10,  # ì§§ê²Œ
        callbacks=[
            tf.keras.callbacks.LambdaCallback(
                on_epoch_end=lambda epoch, logs: MemoryManager.enforce_limit(Config.MAX_RAM_GB)
            )
        ],
        verbose=1
    )
    
    # ì €ì¥
    ensemble_model.save(f"{Config.MODEL_DIR}ensemble_final.h5")
    
    # í‰ê°€
    eval_result = ensemble_model.evaluate(val_gen, verbose=0)
    evaluation_results['ensemble'] = {'mae': eval_result[1]}
    
    print(f"  âœ… ì•™ìƒë¸” MAE: {evaluation_results['ensemble']['mae']:.2f}")
    
    completed_models.append('ensemble')
    checkpoint_manager.save_state(completed_models, evaluation_results)
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del models, ensemble_model
    tf.keras.backend.clear_session()
    gc.collect()
    
    MemoryManager.print_status("ì•™ìƒë¸” ì™„ë£Œ")

# 5. ìµœì¢… ê²°ê³¼
print("\n" + "="*60)
print("ğŸ“Š ìµœì¢… ê²°ê³¼")
print("="*60)

for name in completed_models:
    if name in evaluation_results:
        mae = evaluation_results[name].get('mae', 'N/A')
        print(f"  {name.upper()}: MAE = {mae:.2f}" if isinstance(mae, float) else f"  {name.upper()}: {mae}")

best_model = min(evaluation_results.keys(), key=lambda x: evaluation_results[x].get('mae', float('inf')))
print(f"\nğŸ† ìµœê³  ëª¨ë¸: {best_model.upper()}")

# ê²°ê³¼ ì €ì¥
with open(f"{Config.MODEL_DIR}evaluation_results.json", 'w') as f:
    json.dump(evaluation_results, f, indent=2, default=str)

print(f"\nğŸ’¾ ìµœì¢… RAM: {MemoryManager.get_usage():.1f}GB")
print("âœ… RAM 30GB ì œí•œ ì„±ê³µ!")
print("âœ… 5ê°œ ëª¨ë¸ + ì•™ìƒë¸” ì™„ë£Œ!")
print("âœ… GPU í•™ìŠµ ì™„ë£Œ!")
print("="*60)