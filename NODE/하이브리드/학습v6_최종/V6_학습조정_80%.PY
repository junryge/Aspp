"""
V6 ê°œì„ íŒ - ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
- 100ë¶„ ë°ì´í„° ìœ ì§€
- í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°
- í•™ìŠµ íŒŒë¼ë¯¸í„° ìµœì í™”
- ê°€ì¤‘ì¹˜ ê°•í™”
- .keras í˜•ì‹ ì‚¬ìš© (TensorFlow 2.16+ í˜¸í™˜)
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight
import matplotlib.pyplot as plt
import json
import os
import pickle
import warnings
from datetime import datetime
import gc
warnings.filterwarnings('ignore')

print("="*60)
print("ğŸš€ V6 ê°œì„ íŒ - ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print("="*60)

# ============================================
# GPU ì„¤ì •
# ============================================
def setup_gpu():
    """GPU ì„¤ì • ë° í™•ì¸"""
    print("\nğŸ® GPU í™˜ê²½ í™•ì¸...")
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"âœ… GPU ê°ì§€: {len(gpus)}ê°œ")
            return True
        except Exception as e:
            print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
            return False
    else:
        print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        return False

has_gpu = setup_gpu()

# ============================================
# ê°œì„ ëœ ì„¤ì •
# ============================================
class Config:
    # ë°ì´í„° íŒŒì¼
    DATA_FILE = '/mnt/user-data/uploads/20250807_DATA.CSV'
    
    # ì‹œí€€ìŠ¤ ì„¤ì • (100ë¶„ ìœ ì§€!)
    LOOKBACK = 100  # â­ ê³¼ê±° 100ë¶„ ë°ì´í„° ì‚¬ìš©
    FORECAST = 10   # â­ 10ë¶„ í›„ ì˜ˆì¸¡
    
    # M14 ì„ê³„ê°’ (ê°œì„ ëœ ê°’)
    M14B_THRESHOLDS = {
        1300: 250,  # ì¶”ê°€
        1400: 300,  # ì¡°ì •
        1450: 350,  # ì¶”ê°€
        1500: 380,  # ì¡°ì •
        1550: 420,  # ì¶”ê°€
    }
    
    # ë¹„ìœ¨ ì„ê³„ê°’ (ê°œì„ ëœ ê°’)
    RATIO_THRESHOLDS = {
        1300: 3.5,
        1400: 4.0,
        1450: 4.5,
        1500: 5.0,
        1550: 5.5,
    }
    
    # í•™ìŠµ ì„¤ì • (ìµœì í™”)
    BATCH_SIZE = 64  # ì¤„ì„ (ë” ìì£¼ ì—…ë°ì´íŠ¸)
    EPOCHS = 100  # ì¦ê°€ (ì¶©ë¶„í•œ í•™ìŠµ)
    LEARNING_RATE = 0.001  # ì¦ê°€ (ë¹ ë¥¸ ìˆ˜ë ´)
    PATIENCE = 20  # ì¦ê°€ (ì¶©ë¶„í•œ ê¸°íšŒ)
    
    # ì €ì¥ ê²½ë¡œ
    MODEL_DIR = './models_v6_improved/'
    CHECKPOINT_DIR = './checkpoints_v6_improved/'

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(Config.MODEL_DIR, exist_ok=True)
os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)

# ============================================
# ê°•í™”ëœ ì†ì‹¤ í•¨ìˆ˜
# ============================================
class ImprovedWeightedLoss(tf.keras.losses.Loss):
    """ê°œì„ ëœ ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜"""
    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        
        mae = tf.abs(y_true - y_pred)
        
        # ê°•í™”ëœ ê°€ì¤‘ì¹˜
        weights = tf.ones_like(y_true)
        weights = tf.where(y_true >= 1550, 25.0, weights)  # 1550+ ë§¤ìš° ì¤‘ìš”
        weights = tf.where((y_true >= 1500) & (y_true < 1550), 20.0, weights)
        weights = tf.where((y_true >= 1450) & (y_true < 1500), 15.0, weights)
        weights = tf.where((y_true >= 1400) & (y_true < 1450), 10.0, weights)
        weights = tf.where((y_true >= 1350) & (y_true < 1400), 5.0, weights)
        
        # ì˜ˆì¸¡ ì˜¤ì°¨ê°€ í° ê²½ìš° ì¶”ê°€ í˜ë„í‹°
        large_error_penalty = tf.where(mae > 100, mae * 0.1, 0.0)
        
        return tf.reduce_mean(mae * weights + large_error_penalty)

# ============================================
# M14 ê·œì¹™ ë³´ì • ë ˆì´ì–´ (ê°œì„ )
# ============================================
class ImprovedM14RuleCorrection(tf.keras.layers.Layer):
    """ê°œì„ ëœ M14 ê·œì¹™ ê¸°ë°˜ ë³´ì •"""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    
    def call(self, inputs, training=None):
        pred, m14_features = inputs
        
        pred = tf.cast(pred, tf.float32)
        m14_features = tf.cast(m14_features, tf.float32)
        
        m14b = m14_features[:, 0:1]  # M14AM14B
        m10a = m14_features[:, 1:2]  # M14AM10A
        m16 = m14_features[:, 2:3]   # M14AM16
        ratio = tf.where(m10a > 0, m14b / (m10a + 1e-7), 0.0)
        
        # ì„¸ë°€í•œ ê·œì¹™ ì ìš©
        corrections = []
        
        # ê¸°ë³¸ ì„ê³„ê°’ ê·œì¹™
        corrections.append(tf.where(m14b >= 420, tf.maximum(pred, 1550.0), pred))
        corrections.append(tf.where(m14b >= 380, tf.maximum(pred, 1500.0), pred))
        corrections.append(tf.where(m14b >= 350, tf.maximum(pred, 1450.0), pred))
        corrections.append(tf.where(m14b >= 300, tf.maximum(pred, 1400.0), pred))
        
        # ë¹„ìœ¨ ê¸°ë°˜ ê·œì¹™
        corrections.append(tf.where(ratio >= 5.5, pred * 1.15, pred))
        corrections.append(tf.where(ratio >= 5.0, pred * 1.10, pred))
        corrections.append(tf.where(ratio >= 4.5, pred * 1.08, pred))
        corrections.append(tf.where(ratio >= 4.0, pred * 1.05, pred))
        
        # í™©ê¸ˆ íŒ¨í„´
        golden = (m14b >= 350) & (m10a < 70)
        corrections.append(tf.where(golden, pred * 1.2, pred))
        
        # ë³µí•© ì¡°ê±´
        spike_condition = (m14b >= 320) & (ratio >= 4.0) & (m16 < 90)
        corrections.append(tf.where(spike_condition, tf.maximum(pred, 1450.0), pred))
        
        # ëª¨ë“  ë³´ì • ì ìš©
        for correction in corrections:
            pred = correction
        
        # ìµœì¢… ë²”ìœ„ ì œí•œ
        pred = tf.clip_by_value(pred, 1200.0, 2000.0)
        
        return pred

# ============================================
# ê°œì„ ëœ ëª¨ë¸ ì •ì˜
# ============================================
class ImprovedModelsV6:
    
    @staticmethod
    def build_lstm_model(input_shape):
        """ê°œì„ ëœ LSTM"""
        model = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=input_shape),
            tf.keras.layers.LayerNormalization(),
            tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.3),  # ì¦ê°€
            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.3),
            tf.keras.layers.LSTM(64, dropout=0.3),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dense(256, activation='relu'),
            tf.keras.layers.Dropout(0.4),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dense(1)
        ], name='LSTM_Model')
        return model
    
    @staticmethod
    def build_gru_model(input_shape):
        """ê°œì„ ëœ GRU"""
        model = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=input_shape),
            tf.keras.layers.LayerNormalization(),
            tf.keras.layers.GRU(256, return_sequences=True, dropout=0.2),
            tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2),
            tf.keras.layers.GRU(64, dropout=0.2),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dense(256, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dense(1)
        ], name='GRU_Model')
        return model
    
    @staticmethod
    def build_cnn_lstm(input_shape):
        """ê°œì„ ëœ CNN-LSTM"""
        inputs = tf.keras.Input(shape=input_shape)
        
        # ë©€í‹°ìŠ¤ì¼€ì¼ CNN
        convs = []
        for kernel_size in [3, 5, 7, 9]:
            conv = tf.keras.layers.Conv1D(
                128, kernel_size, 
                activation='relu', 
                padding='same'
            )(inputs)
            conv = tf.keras.layers.BatchNormalization()(conv)
            convs.append(conv)
        
        concat = tf.keras.layers.Concatenate()(convs)
        
        # LSTM ì²˜ë¦¬
        lstm1 = tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.2)(concat)
        lstm2 = tf.keras.layers.LSTM(128, dropout=0.2)(lstm1)
        
        # Dense layers
        dense1 = tf.keras.layers.Dense(256, activation='relu')(lstm2)
        dropout1 = tf.keras.layers.Dropout(0.3)(dense1)
        dense2 = tf.keras.layers.Dense(128, activation='relu')(dropout1)
        dropout2 = tf.keras.layers.Dropout(0.2)(dense2)
        output = tf.keras.layers.Dense(1)(dropout2)
        
        return tf.keras.Model(inputs=inputs, outputs=output, name='CNN_LSTM_Model')
    
    @staticmethod
    def build_spike_detector(input_shape):
        """ê°œì„ ëœ Spike Detector"""
        inputs = tf.keras.Input(shape=input_shape)
        
        # ë©€í‹°ìŠ¤ì¼€ì¼ CNN
        convs = []
        for kernel_size in [3, 5, 7]:
            conv = tf.keras.layers.Conv1D(
                96, kernel_size, 
                activation='relu', 
                padding='same'
            )(inputs)
            convs.append(conv)
        
        concat = tf.keras.layers.Concatenate()(convs)
        norm = tf.keras.layers.BatchNormalization()(concat)
        
        # ì–´í…ì…˜
        attention = tf.keras.layers.MultiHeadAttention(
            num_heads=8, key_dim=36, dropout=0.2
        )(norm, norm)
        
        # Bidirectional LSTM
        lstm = tf.keras.layers.Bidirectional(
            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)
        )(attention)
        
        # Global pooling
        avg_pool = tf.keras.layers.GlobalAveragePooling1D()(lstm)
        max_pool = tf.keras.layers.GlobalMaxPooling1D()(lstm)
        pooled = tf.keras.layers.Concatenate()([avg_pool, max_pool])
        
        # Dense layers
        dense1 = tf.keras.layers.Dense(256, activation='relu')(pooled)
        dropout1 = tf.keras.layers.Dropout(0.3)(dense1)
        dense2 = tf.keras.layers.Dense(128, activation='relu')(dropout1)
        dropout2 = tf.keras.layers.Dropout(0.2)(dense2)
        
        # ì¶œë ¥
        regression_output = tf.keras.layers.Dense(1, name='spike_value')(dropout2)
        
        return tf.keras.Model(
            inputs=inputs,
            outputs=regression_output,
            name='Spike_Detector'
        )
    
    @staticmethod
    def build_rule_based_model(input_shape, m14_shape):
        """ê°œì„ ëœ Rule-Based"""
        time_input = tf.keras.Input(shape=input_shape, name='time_input')
        m14_input = tf.keras.Input(shape=(m14_shape,), name='m14_input')
        
        # ì‹œê³„ì—´ ì²˜ë¦¬
        lstm1 = tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.2)(time_input)
        lstm2 = tf.keras.layers.LSTM(32, dropout=0.2)(lstm1)
        
        # M14 íŠ¹ì§• ì²˜ë¦¬ (ë” ê¹Šê²Œ)
        m14_dense1 = tf.keras.layers.Dense(32, activation='relu')(m14_input)
        m14_dense2 = tf.keras.layers.Dense(16, activation='relu')(m14_dense1)
        
        # ê²°í•©
        combined = tf.keras.layers.Concatenate()([lstm2, m14_dense2])
        
        dense1 = tf.keras.layers.Dense(128, activation='relu')(combined)
        dropout1 = tf.keras.layers.Dropout(0.3)(dense1)
        dense2 = tf.keras.layers.Dense(64, activation='relu')(dropout1)
        dropout2 = tf.keras.layers.Dropout(0.2)(dense2)
        
        prediction = tf.keras.layers.Dense(1, name='rule_pred')(dropout2)
        
        # M14 ê·œì¹™ ì ìš©
        corrected = ImprovedM14RuleCorrection()([prediction, m14_input])
        
        return tf.keras.Model(
            inputs=[time_input, m14_input],
            outputs=corrected,
            name='Rule_Based_Model'
        )

# ============================================
# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ============================================
print("\nğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
df = pd.read_csv(Config.DATA_FILE)

# í•„ìš”í•œ ì»¬ëŸ¼ ì„ íƒ
feature_columns = ['M14AM10A', 'M14AM14B', 'M14AM16', 'M14AM14BSUM', 'TOTALCNT']
for col in feature_columns:
    if col not in df.columns:
        if col == 'M14AM14BSUM':
            df[col] = df['M14AM14B'] + df['M14AM10A']  # í•©ê³„ ìƒì„±
        else:
            print(f"âš ï¸ {col} ì»¬ëŸ¼ ì—†ìŒ")

# ë°ì´í„° ì •ë¦¬
df = df[feature_columns].copy()
for col in feature_columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')
df = df.dropna()

print(f"ë°ì´í„° shape: {df.shape}")
print(f"TOTALCNT ë²”ìœ„: {df['TOTALCNT'].min():.0f} ~ {df['TOTALCNT'].max():.0f}")
print(f"TOTALCNT í‰ê· : {df['TOTALCNT'].mean():.0f}")

# ============================================
# íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§
# ============================================
print("\nğŸ”§ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§...")

# ë¹„ìœ¨ íŠ¹ì§•
df['ratio_14B_10A'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
df['ratio_14B_16'] = df['M14AM14B'] / (df['M14AM16'] + 1)
df['ratio_10A_16'] = df['M14AM10A'] / (df['M14AM16'] + 1)

# ì‹œê³„ì—´ íŠ¹ì§•
for col in feature_columns:
    # ë³€í™”ëŸ‰
    df[f'{col}_diff_1'] = df[col].diff(1)
    df[f'{col}_diff_5'] = df[col].diff(5)
    df[f'{col}_diff_10'] = df[col].diff(10)
    
    # ì´ë™í‰ê· 
    df[f'{col}_ma_5'] = df[col].rolling(5, min_periods=1).mean()
    df[f'{col}_ma_10'] = df[col].rolling(10, min_periods=1).mean()
    df[f'{col}_ma_20'] = df[col].rolling(20, min_periods=1).mean()
    
    # í‘œì¤€í¸ì°¨
    df[f'{col}_std_5'] = df[col].rolling(5, min_periods=1).std()
    df[f'{col}_std_10'] = df[col].rolling(10, min_periods=1).std()

# í™©ê¸ˆ íŒ¨í„´
df['golden_pattern'] = ((df['M14AM14B'] >= 350) & (df['M14AM10A'] < 70)).astype(float)

# ê¸‰ì¦ ì‹ í˜¸
for level, threshold in Config.M14B_THRESHOLDS.items():
    df[f'signal_{level}'] = (df['M14AM14B'] >= threshold).astype(float)

for level, ratio in Config.RATIO_THRESHOLDS.items():
    df[f'ratio_signal_{level}'] = (df['ratio_14B_10A'] >= ratio).astype(float)

df = df.fillna(0)

print(f"íŠ¹ì§• ê°œìˆ˜: {len(df.columns)}ê°œ")

# ============================================
# ì‹œí€€ìŠ¤ ìƒì„± (100ë¶„!)
# ============================================
print("\nâš¡ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘... (100ë¶„ ë°ì´í„°)")

def create_sequences(data, lookback=100, forecast=10):
    """100ë¶„ ì‹œí€€ìŠ¤ ìƒì„±"""
    X, y = [], []
    
    for i in range(len(data) - lookback - forecast):
        X.append(data[i:i+lookback])
        y.append(data[i+lookback+forecast-1, df.columns.get_loc('TOTALCNT')])
    
    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)

# ì‹œí€€ìŠ¤ ìƒì„±
data = df.values
X, y = create_sequences(data, Config.LOOKBACK, Config.FORECAST)

print(f"X shape: {X.shape}")
print(f"y shape: {y.shape}")

# M14 íŠ¹ì§• ì¶”ì¶œ (í˜„ì¬ ì‹œì )
m14_features = np.zeros((len(X), 4), dtype=np.float32)
for i in range(len(X)):
    idx = i + Config.LOOKBACK
    if idx < len(df):
        m14_features[i] = [
            df['M14AM14B'].iloc[idx],
            df['M14AM10A'].iloc[idx],
            df['M14AM16'].iloc[idx],
            df['ratio_14B_10A'].iloc[idx]
        ]

# ============================================
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
# ============================================
print("\nğŸ“ ë°ì´í„° ìŠ¤ì¼€ì¼ë§...")

# X ìŠ¤ì¼€ì¼ë§
X_scaled = np.zeros_like(X)
scalers = {}

for i in range(X.shape[2]):
    scaler = RobustScaler()
    feature = X[:, :, i].reshape(-1, 1)
    X_scaled[:, :, i] = scaler.fit_transform(feature).reshape(X[:, :, i].shape)
    scalers[f'feature_{i}'] = scaler

# M14 ìŠ¤ì¼€ì¼ë§
m14_scaler = RobustScaler()
m14_features_scaled = m14_scaler.fit_transform(m14_features)

print("âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")

# ============================================
# ë°ì´í„° ë¶„í• 
# ============================================
print("\nğŸ“Š ë°ì´í„° ë¶„í• ...")

X_train, X_val, y_train, y_val, m14_train, m14_val = train_test_split(
    X_scaled, y, m14_features_scaled, 
    test_size=0.2, 
    random_state=42,
    stratify=(y >= 1400).astype(int)  # ì¸µí™” ìƒ˜í”Œë§
)

print(f"í•™ìŠµ: {X_train.shape[0]:,}ê°œ")
print(f"ê²€ì¦: {X_val.shape[0]:,}ê°œ")

# íƒ€ê²Ÿ ë¶„í¬ í™•ì¸
for level in [1300, 1400, 1450, 1500]:
    train_count = (y_train >= level).sum()
    val_count = (y_val >= level).sum()
    print(f"{level}+: í•™ìŠµ {train_count}ê°œ ({train_count/len(y_train)*100:.1f}%), "
          f"ê²€ì¦ {val_count}ê°œ ({val_count/len(y_val)*100:.1f}%)")

# ============================================
# ëª¨ë¸ í•™ìŠµ
# ============================================
print("\n" + "="*60)
print("ğŸ‹ï¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
print("="*60)

models = {}
history = {}

# í•™ìŠµ ì½œë°±
def get_callbacks(patience=15):
    return [
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=patience,
            restore_best_weights=True,
            verbose=1
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            patience=patience//2,
            factor=0.5,
            min_lr=1e-7,
            verbose=1
        ),
        tf.keras.callbacks.ModelCheckpoint(
            f'{Config.CHECKPOINT_DIR}{{epoch:02d}}-{{val_loss:.2f}}.keras',  # âœ… .h5 â†’ .keras
            save_best_only=True,
            monitor='val_loss'
        )
    ]

# 1. LSTM
print("\n1ï¸âƒ£ LSTM ëª¨ë¸ í•™ìŠµ...")
lstm_model = ImprovedModelsV6.build_lstm_model(X_train.shape[1:])
lstm_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
    loss=ImprovedWeightedLoss(),
    metrics=['mae']
)
lstm_hist = lstm_model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=Config.EPOCHS,
    batch_size=Config.BATCH_SIZE,
    callbacks=get_callbacks(),
    verbose=1
)
models['lstm'] = lstm_model
history['lstm'] = lstm_hist

# 2. GRU
print("\n2ï¸âƒ£ GRU ëª¨ë¸ í•™ìŠµ...")
gru_model = ImprovedModelsV6.build_gru_model(X_train.shape[1:])
gru_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
    loss=ImprovedWeightedLoss(),
    metrics=['mae']
)
gru_hist = gru_model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=Config.EPOCHS,
    batch_size=Config.BATCH_SIZE,
    callbacks=get_callbacks(),
    verbose=1
)
models['gru'] = gru_model
history['gru'] = gru_hist

# 3. CNN-LSTM
print("\n3ï¸âƒ£ CNN-LSTM ëª¨ë¸ í•™ìŠµ...")
cnn_lstm_model = ImprovedModelsV6.build_cnn_lstm(X_train.shape[1:])
cnn_lstm_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
    loss=ImprovedWeightedLoss(),
    metrics=['mae']
)
cnn_lstm_hist = cnn_lstm_model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=Config.EPOCHS,
    batch_size=Config.BATCH_SIZE,
    callbacks=get_callbacks(),
    verbose=1
)
models['cnn_lstm'] = cnn_lstm_model
history['cnn_lstm'] = cnn_lstm_hist

# 4. Spike Detector
print("\n4ï¸âƒ£ Spike Detector ëª¨ë¸ í•™ìŠµ...")
spike_model = ImprovedModelsV6.build_spike_detector(X_train.shape[1:])
spike_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
    loss=ImprovedWeightedLoss(),
    metrics=['mae']
)
spike_hist = spike_model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=Config.EPOCHS,
    batch_size=Config.BATCH_SIZE,
    callbacks=get_callbacks(),
    verbose=1
)
models['spike'] = spike_model
history['spike'] = spike_hist

# 5. Rule-Based
print("\n5ï¸âƒ£ Rule-Based ëª¨ë¸ í•™ìŠµ...")
rule_model = ImprovedModelsV6.build_rule_based_model(X_train.shape[1:], m14_train.shape[1])
rule_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * 0.5),
    loss=ImprovedWeightedLoss(),
    metrics=['mae']
)
rule_hist = rule_model.fit(
    [X_train, m14_train], y_train,
    validation_data=([X_val, m14_val], y_val),
    epochs=50,  # Ruleì€ ë¹¨ë¦¬ ìˆ˜ë ´
    batch_size=Config.BATCH_SIZE,
    callbacks=get_callbacks(patience=10),
    verbose=1
)
models['rule'] = rule_model
history['rule'] = rule_hist

# ============================================
# ì•™ìƒë¸” ëª¨ë¸
# ============================================
print("\n" + "="*60)
print("ğŸ¯ ì•™ìƒë¸” ëª¨ë¸ ìƒì„±")
print("="*60)

# ì•™ìƒë¸” ì…ë ¥
time_input = tf.keras.Input(shape=X_train.shape[1:], name='ensemble_time')
m14_input = tf.keras.Input(shape=(m14_train.shape[1],), name='ensemble_m14')

# ê° ëª¨ë¸ ì˜ˆì¸¡
lstm_pred = models['lstm'](time_input)
gru_pred = models['gru'](time_input)
cnn_lstm_pred = models['cnn_lstm'](time_input)
spike_pred = models['spike'](time_input)
rule_pred = models['rule']([time_input, m14_input])

# ê°€ì¤‘ í‰ê·  (ê³ ì • ê°€ì¤‘ì¹˜)
ensemble_pred = tf.keras.layers.Lambda(
    lambda x: 0.25*x[0] + 0.20*x[1] + 0.25*x[2] + 0.15*x[3] + 0.15*x[4]
)([lstm_pred, gru_pred, cnn_lstm_pred, spike_pred, rule_pred])

# ìµœì¢… ë³´ì •
final_pred = ImprovedM14RuleCorrection()([ensemble_pred, m14_input])

# ì•™ìƒë¸” ëª¨ë¸
ensemble_model = tf.keras.Model(
    inputs=[time_input, m14_input],
    outputs=final_pred,
    name='Ensemble_Model'
)

ensemble_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * 0.3),
    loss=ImprovedWeightedLoss(),
    metrics=['mae']
)

print("ğŸ“Š ì•™ìƒë¸” íŒŒì¸íŠœë‹...")
ensemble_hist = ensemble_model.fit(
    [X_train, m14_train], y_train,
    validation_data=([X_val, m14_val], y_val),
    epochs=30,
    batch_size=Config.BATCH_SIZE,
    callbacks=get_callbacks(patience=10),
    verbose=1
)

models['ensemble'] = ensemble_model
history['ensemble'] = ensemble_hist

# ============================================
# í‰ê°€
# ============================================
print("\n" + "="*60)
print("ğŸ“Š ëª¨ë¸ í‰ê°€")
print("="*60)

evaluation_results = {}

for name, model in models.items():
    # ì˜ˆì¸¡
    if name == 'ensemble' or name == 'rule':
        pred = model.predict([X_val, m14_val], verbose=0).flatten()
    else:
        pred = model.predict(X_val, verbose=0).flatten()
    
    # ì „ì²´ MAE
    mae = np.mean(np.abs(y_val - pred))
    
    # êµ¬ê°„ë³„ ì„±ëŠ¥
    level_performance = {}
    for level in [1300, 1400, 1450, 1500]:
        mask = y_val >= level
        if np.any(mask):
            # Recall: ì‹¤ì œ ê¸‰ì¦ ì¤‘ ì˜ˆì¸¡ ì„±ê³µ ë¹„ìœ¨
            recall = np.sum((pred >= level) & mask) / np.sum(mask)
            # Precision: ê¸‰ì¦ ì˜ˆì¸¡ ì¤‘ ì‹¤ì œ ê¸‰ì¦ ë¹„ìœ¨
            precision = np.sum((pred >= level) & mask) / max(np.sum(pred >= level), 1)
            # F1 Score
            f1 = 2 * (precision * recall) / max(precision + recall, 1e-7)
            # MAE
            level_mae = np.mean(np.abs(y_val[mask] - pred[mask]))
            
            level_performance[level] = {
                'recall': recall,
                'precision': precision,
                'f1': f1,
                'mae': level_mae,
                'count': np.sum(mask)
            }
    
    evaluation_results[name] = {
        'overall_mae': mae,
        'levels': level_performance
    }
    
    # ì¶œë ¥
    print(f"\nğŸ¯ {name.upper()}:")
    print(f"  ì „ì²´ MAE: {mae:.2f}")
    for level, perf in level_performance.items():
        print(f"  {level}+: Recall={perf['recall']:.2%}, "
              f"Precision={perf['precision']:.2%}, "
              f"F1={perf['f1']:.2%}, MAE={perf['mae']:.1f}")

# ìµœê³  ëª¨ë¸
best_model = min(evaluation_results.keys(), 
                 key=lambda x: evaluation_results[x]['overall_mae'])
print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_model.upper()} "
      f"(MAE: {evaluation_results[best_model]['overall_mae']:.2f})")

# ============================================
# ëª¨ë¸ ì €ì¥
# ============================================
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")

for name, model in models.items():
    save_path = f"{Config.MODEL_DIR}{name}_final.keras"  # âœ… .h5 â†’ .keras
    model.save(save_path)
    print(f"  âœ… {name} ì €ì¥: {save_path}")

# í‰ê°€ ê²°ê³¼ ì €ì¥
with open(f"{Config.MODEL_DIR}evaluation_results.json", 'w') as f:
    json.dump(evaluation_results, f, indent=2, default=str)
print(f"  âœ… í‰ê°€ ê²°ê³¼ ì €ì¥")

# ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
with open(f"{Config.MODEL_DIR}scalers.pkl", 'wb') as f:
    pickle.dump({
        'feature_scalers': scalers,
        'm14_scaler': m14_scaler
    }, f)
print(f"  âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥")

# ============================================
# ìµœì¢… ì¶œë ¥
# ============================================
print("\n" + "="*60)
print("ğŸ‰ V6 ê°œì„ íŒ í•™ìŠµ ì™„ë£Œ!")
print("="*60)
print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {Config.MODEL_DIR}")
print(f"ğŸ† ìµœê³  ëª¨ë¸: {best_model.upper()}")
print(f"ğŸ“Š ì „ì²´ MAE: {evaluation_results[best_model]['overall_mae']:.2f}")

# 100ë§Œê°œ ë°ì´í„° ì²´í¬
if len(y) >= 1000000:
    print("\n" + "="*60)
    print("ğŸ”” ì•Œë¦¼: 100ë§Œê°œ ì´ìƒ ë°ì´í„° ê°ì§€!")
    print("ğŸ“Š Patch Time Series Transformer ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    print("="*60)

print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
print("="*60)

# ë©”ëª¨ë¦¬ ì •ë¦¬
tf.keras.backend.clear_session()
gc.collect()