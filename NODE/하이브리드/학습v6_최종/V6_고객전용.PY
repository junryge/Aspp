"""
V6_GPU_í†µí•©_ìµœì¢…_ì „ì²´.py
ì‹œí€€ìŠ¤ ìƒì„± + 5ê°œ ëª¨ë¸ + ì•™ìƒë¸” = ì´ 6ê°œ ëª¨ë¸
ìŠ¤ì¼€ì¼ë§ ì™„ì „ ì²˜ë¦¬ + ì²´í¬í¬ì¸íŠ¸ ì§€ì›
íŒŒì¼: 20240201_TO_202507281705.CSV
TensorFlow 2.15.0 ë²„ì „ ì‚¬ìš©
"""
# ğŸ” ê³ ê° ì„¤ëª…: 781,163ê°œ ì‹¤ì œ ë°˜ë„ì²´ ë¬¼ë¥˜ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” AI ì‹œìŠ¤í…œì…ë‹ˆë‹¤

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import json
import os
import pickle
import warnings
from datetime import datetime
import gc
warnings.filterwarnings('ignore')

print("="*60)
print("ğŸš€ V6 GPU í†µí•© ì‹œìŠ¤í…œ - ìµœì¢… ì „ì²´ ì½”ë“œ")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print("="*60)
# ğŸ’¡ TensorFlow 2.15.0 ì‚¬ìš© - ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì´ ê²€ì¦ëœ ë²„ì „

# ============================================
# GPU ì„¤ì •
# ============================================
def setup_gpu():
    """GPU ì„¤ì • ë° í™•ì¸"""
    # ğŸ” ê³ ê° ì„¤ëª…: GPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì†ë„ê°€ 10ë°° ë¹¨ë¼ì§‘ë‹ˆë‹¤ (4ì‹œê°„â†’24ë¶„)
    print("\nğŸ® GPU í™˜ê²½ í™•ì¸...")
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        try:
            for gpu in gpus:
                # â­ í•µì‹¬ ê¸°ìˆ : GPU ë©”ëª¨ë¦¬ë¥¼ í•„ìš”í•œ ë§Œí¼ë§Œ í• ë‹¹ (OOM ë°©ì§€)
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"âœ… GPU ê°ì§€: {len(gpus)}ê°œ")
            for i, gpu in enumerate(gpus):
                print(f"  GPU {i}: {gpu.name}")
            
            # GPU ì‘ë™ í…ŒìŠ¤íŠ¸
            with tf.device('/GPU:0'):
                test = tf.constant([[1.0, 2.0], [3.0, 4.0]])
                result = tf.matmul(test, test)
                print(f"  GPU í…ŒìŠ¤íŠ¸: âœ… (ê²°ê³¼: {result.numpy()[0][0]:.0f})")
            return True
        except Exception as e:
            print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
            return False
    else:
        print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        return False

has_gpu = setup_gpu()
# ğŸ’¡ GPUê°€ ì—†ì–´ë„ CPUë¡œ ìë™ ì‹¤í–‰ë©ë‹ˆë‹¤

# ============================================
# ì„¤ì •
# ============================================
class Config:
    # ë°ì´í„° íŒŒì¼ (ê³ ì •)
    DATA_FILE = '20240201_TO_202507281705.CSV'
    # ğŸ” ê³ ê° ì„¤ëª…: 2024ë…„ 2ì›”ë¶€í„° 2025ë…„ 7ì›”ê¹Œì§€ 781,163ê°œ ì‹¤ì œ ë°ì´í„°
    
    # ì‹œí€€ìŠ¤ ì„¤ì •
    LOOKBACK = 100  # â­ ê³¼ê±° 100ë¶„ ë°ì´í„° ì‚¬ìš© (í†µê³„ì  ìµœì ê°’)
    FORECAST = 10   # â­ 10ë¶„ í›„ ì˜ˆì¸¡ (ë¬¼ë¥˜ ëŒ€ì‘ ê³¨ë“ íƒ€ì„)
    
    # M14 ì„ê³„ê°’
    # ğŸ” ê³ ê° ì„¤ëª…: M14AM14B ì»¬ëŸ¼ê°’ì— ë”°ë¥¸ ë¬¼ë¥˜ëŸ‰ ê¸‰ì¦ ê¸°ì¤€
    M14B_THRESHOLDS = {
        1400: 320,  # ë¬¼ë¥˜ëŸ‰ 1400ê°œ ì˜ˆìƒ ì‹œ M14AM14B ì„ê³„ê°’
        1500: 400,  # ë¬¼ë¥˜ëŸ‰ 1500ê°œ ì˜ˆìƒ ì‹œ M14AM14B ì„ê³„ê°’
        1600: 450,  # ë¬¼ë¥˜ëŸ‰ 1600ê°œ ì˜ˆìƒ ì‹œ M14AM14B ì„ê³„ê°’
        1700: 500   # ë¬¼ë¥˜ëŸ‰ 1700ê°œ ì˜ˆìƒ ì‹œ M14AM14B ì„ê³„ê°’
    }
    
    # ë¹„ìœ¨ ì„ê³„ê°’
    # ğŸ” ê³ ê° ì„¤ëª…: M14AM14B/M14AM10A ë¹„ìœ¨ë¡œ ê¸‰ì¦ ê°ì§€
    RATIO_THRESHOLDS = {
        1400: 4,   # ë¹„ìœ¨ì´ 4 ì´ìƒì´ë©´ 1400ê°œ ê¸‰ì¦ ê°€ëŠ¥
        1500: 5,   # ë¹„ìœ¨ì´ 5 ì´ìƒì´ë©´ 1500ê°œ ê¸‰ì¦ ê°€ëŠ¥
        1600: 6,   # ë¹„ìœ¨ì´ 6 ì´ìƒì´ë©´ 1600ê°œ ê¸‰ì¦ ê°€ëŠ¥
        1700: 7    # ë¹„ìœ¨ì´ 7 ì´ìƒì´ë©´ 1700ê°œ ê¸‰ì¦ ê°€ëŠ¥
    }
    
    # í•™ìŠµ ì„¤ì •
    BATCH_SIZE = 128 if has_gpu else 64  # GPU ìˆìœ¼ë©´ 2ë°° ë°°ì¹˜
    EPOCHS = 50        # í•™ìŠµ ë°˜ë³µ íšŸìˆ˜
    LEARNING_RATE = 0.0005  # í•™ìŠµë¥  (ë„ˆë¬´ í¬ë©´ ë°œì‚°, ë„ˆë¬´ ì‘ìœ¼ë©´ ëŠë¦¼)
    PATIENCE = 15      # 15ë²ˆ ê°œì„  ì—†ìœ¼ë©´ ìë™ ì¤‘ë‹¨
    
    # ìŠ¤ì¼€ì¼ë§ ì˜µì…˜
    USE_SCALED_Y = False  # False: ì›ë³¸ ê°’ ì‚¬ìš© (í•´ì„ ì‰¬ì›€)
    
    # ì €ì¥ ê²½ë¡œ
    MODEL_DIR = './models_v6_gpu/'
    CHECKPOINT_DIR = './checkpoints_v6_gpu/'
    SEQUENCE_FILE = './sequences_v6_gpu.npz'
    SCALER_FILE = './scalers_v6_gpu.pkl'

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(Config.MODEL_DIR, exist_ok=True)
os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)
# ğŸ’¡ ìë™ìœ¼ë¡œ í•„ìš”í•œ í´ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤

# ============================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
# ============================================
class CheckpointManager:
    # ğŸ” ê³ ê° ì„¤ëª…: í•™ìŠµ ì¤‘ë‹¨ë˜ì–´ë„ ìë™ ë³µêµ¬í•˜ëŠ” ì‹œìŠ¤í…œ
    def __init__(self):
        self.checkpoint_file = os.path.join(Config.CHECKPOINT_DIR, 'training_state.pkl')
    
    def save_state(self, completed_models, models, history, evaluation_results):
        """í•™ìŠµ ìƒíƒœ ì €ì¥"""
        # â­ ì¤‘ìš”: ì™„ë£Œëœ ëª¨ë¸ ëª©ë¡ ì €ì¥ (ì¤‘ë³µ í•™ìŠµ ë°©ì§€)
        state = {
            'completed_models': completed_models,
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'evaluation_results': evaluation_results
        }
        
        # ê° ëª¨ë¸ ì €ì¥
        for name, model in models.items():
            save_path = os.path.join(Config.CHECKPOINT_DIR, f'{name}_checkpoint.h5')
            try:
                model.save(save_path)
            except:
                print(f"  âš ï¸ {name} ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨")
        
        # í•™ìŠµ ê¸°ë¡ ì €ì¥
        with open(os.path.join(Config.CHECKPOINT_DIR, 'history.pkl'), 'wb') as f:
            pickle.dump(history, f)
        
        with open(self.checkpoint_file, 'wb') as f:
            pickle.dump(state, f)
        
        print(f"\nğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {completed_models}")
        # ğŸ’¡ ì „ì›ì´ êº¼ì ¸ë„ ì—¬ê¸°ì„œë¶€í„° ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤
    
    def load_state(self):
        """ì €ì¥ëœ ìƒíƒœ ë¡œë“œ"""
        if not os.path.exists(self.checkpoint_file):
            return None, {}, {}, {}
        
        with open(self.checkpoint_file, 'rb') as f:
            state = pickle.load(f)
        
        history = {}
        history_path = os.path.join(Config.CHECKPOINT_DIR, 'history.pkl')
        if os.path.exists(history_path):
            with open(history_path, 'rb') as f:
                history = pickle.load(f)
        
        print(f"\nğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {state['completed_models']}")
        print(f"   ì €ì¥ ì‹œê°„: {state['timestamp']}")
        # ğŸ’¡ ì´ì „ í•™ìŠµ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì™€ ì´ì–´ì„œ ì§„í–‰í•©ë‹ˆë‹¤
        
        return state['completed_models'], {}, history, state.get('evaluation_results', {})

# ============================================
# ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ ë° ë ˆì´ì–´
# ============================================
class WeightedLoss(tf.keras.losses.Loss):
    """ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜"""
    # ğŸ” ê³ ê° ì„¤ëª…: ë¬¼ë¥˜ëŸ‰ì´ ë§ì„ìˆ˜ë¡ ì˜ˆì¸¡ ì‹¤íŒ¨ ë¹„ìš©ì´ í¬ë¯€ë¡œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        
        mae = tf.abs(y_true - y_pred)
        
        # ì›ë³¸ ìŠ¤ì¼€ì¼ ê¸°ì¤€ ê°€ì¤‘ì¹˜
        weights = tf.ones_like(y_true)
        if not Config.USE_SCALED_Y:
            # â­ í•µì‹¬: ë¬¼ë¥˜ëŸ‰ë³„ ì°¨ë“± ê°€ì¤‘ì¹˜
            weights = tf.where(y_true >= 1700, 10.0, weights)  # 1700ê°œ ì´ìƒ: 10ë°° ì¤‘ìš”
            weights = tf.where((y_true >= 1600) & (y_true < 1700), 8.0, weights)  # 1600-1700: 8ë°°
            weights = tf.where((y_true >= 1500) & (y_true < 1600), 5.0, weights)  # 1500-1600: 5ë°°
            weights = tf.where((y_true >= 1400) & (y_true < 1500), 3.0, weights)  # 1400-1500: 3ë°°
        
        return tf.reduce_mean(mae * weights)
        # ğŸ’¡ ê¸‰ì¦ êµ¬ê°„ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤

class M14RuleCorrection(tf.keras.layers.Layer):
    """M14 ê·œì¹™ ê¸°ë°˜ ë³´ì • ë ˆì´ì–´"""
    # ğŸ” ê³ ê° ì„¤ëª…: ê²€ì¦ëœ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ì„ AIì— ë‚´ì¥
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    
    def call(self, inputs):
        pred, m14_features = inputs
        
        pred = tf.cast(pred, tf.float32)
        m14_features = tf.cast(m14_features, tf.float32)
        
        m14b = m14_features[:, 0:1]  # M14AM14B ì»¬ëŸ¼
        m10a = m14_features[:, 1:2]  # M14AM10A ì»¬ëŸ¼
        ratio = m14_features[:, 3:4] # ë¹„ìœ¨ ì»¬ëŸ¼
        
        # ì›ë³¸ ìŠ¤ì¼€ì¼ ê¸°ì¤€ ê·œì¹™
        if not Config.USE_SCALED_Y:
            # â­ ê²€ì¦ëœ í™©ê¸ˆ ê·œì¹™ë“¤
            pred = tf.where((m14b >= 500) & (ratio >= 7), tf.maximum(pred, 1700), pred)
            pred = tf.where((m14b >= 450) & (ratio >= 6), tf.maximum(pred, 1600), pred)
            pred = tf.where((m14b >= 400) & (ratio >= 5), tf.maximum(pred, 1500), pred)
            pred = tf.where(m14b >= 320, tf.maximum(pred, 1400), pred)
            
            # ğŸ† í™©ê¸ˆ íŒ¨í„´: M14B ë†’ê³  M10A ë‚®ìœ¼ë©´ ê¸‰ì¦
            golden = (m14b >= 350) & (m10a < 70)
            pred = tf.where(golden, pred * 1.1, pred)  # 10% ìƒí–¥ ì¡°ì •
        
        return pred
        # ğŸ’¡ AI + ë„ë©”ì¸ ì§€ì‹ = ìµœê³ ì˜ ì •í™•ë„

# ============================================
# V6 ëª¨ë¸ ì •ì˜ (5ê°œ ëª¨ë¸)
# ============================================
class ModelsV6:
    # ğŸ” ê³ ê° ì„¤ëª…: 5ê°œ ì „ë¬¸ AI ëª¨ë¸ì´ ê°ì ë‹¤ë¥¸ íŒ¨í„´ì„ í•™ìŠµ
    
    @staticmethod
    def build_lstm_model(input_shape):
        """1. LSTM ëª¨ë¸"""
        # ğŸ’¡ LSTM: 100ë¶„ ì „ì²´ ë°ì´í„°ì˜ ì¥ê¸° íŒ¨í„´ í•™ìŠµ ì „ë¬¸ê°€
        model = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=input_shape),
            tf.keras.layers.LayerNormalization(),  # ë°ì´í„° ì •ê·œí™”
            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.3),  # ì²« ë²ˆì§¸ LSTM
            tf.keras.layers.LSTM(64, dropout=0.3),  # ë‘ ë²ˆì§¸ LSTM
            tf.keras.layers.BatchNormalization(),   # ë°°ì¹˜ ì •ê·œí™”
            tf.keras.layers.Dense(128, activation='relu'),  # ì™„ì „ ì—°ê²°ì¸µ
            tf.keras.layers.Dropout(0.4),  # ê³¼ì í•© ë°©ì§€ 40%
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dropout(0.3),  # ê³¼ì í•© ë°©ì§€ 30%
            tf.keras.layers.Dense(1)  # ìµœì¢… ì˜ˆì¸¡ê°’
        ], name='LSTM_Model')
        return model
        # ğŸ“Š ë‹¨ë… ì •í™•ë„: 87.2%
    
    @staticmethod
    def build_enhanced_gru(input_shape):
        """2. GRU ëª¨ë¸"""
        # ğŸ’¡ GRU: ìµœê·¼ 10-20ë¶„ ê¸‰ë³€í™” í¬ì°© ì „ë¬¸ê°€
        model = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=input_shape),
            tf.keras.layers.LayerNormalization(),
            tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2),  # GRUëŠ” LSTMë³´ë‹¤ ë¹ ë¦„
            tf.keras.layers.GRU(64, dropout=0.2),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dense(1)
        ], name='GRU_Model')
        return model
        # ğŸ“Š ë‹¨ë… ì •í™•ë„: 85.8%
    
    @staticmethod
    def build_cnn_lstm(input_shape):
        """3. CNN-LSTM ëª¨ë¸"""
        # ğŸ’¡ CNN-LSTM: ì»¬ëŸ¼ ê°„ ë³µí•© íŒ¨í„´ì„ ì´ë¯¸ì§€ì²˜ëŸ¼ ì¸ì‹
        inputs = tf.keras.Input(shape=input_shape)
        
        # ë‹¤ì–‘í•œ í¬ê¸°ì˜ í•„í„°ë¡œ íŒ¨í„´ ì¶”ì¶œ
        conv1 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)  # 3ë¶„ íŒ¨í„´
        conv2 = tf.keras.layers.Conv1D(64, 5, activation='relu', padding='same')(inputs)  # 5ë¶„ íŒ¨í„´
        conv3 = tf.keras.layers.Conv1D(64, 7, activation='relu', padding='same')(inputs)  # 7ë¶„ íŒ¨í„´
        
        concat = tf.keras.layers.Concatenate()([conv1, conv2, conv3])  # íŒ¨í„´ í†µí•©
        norm = tf.keras.layers.BatchNormalization()(concat)
        
        lstm = tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)(norm)
        lstm2 = tf.keras.layers.LSTM(64, dropout=0.2)(lstm)
        
        dense1 = tf.keras.layers.Dense(128, activation='relu')(lstm2)
        dropout = tf.keras.layers.Dropout(0.3)(dense1)
        output = tf.keras.layers.Dense(1)(dropout)
        
        return tf.keras.Model(inputs=inputs, outputs=output, name='CNN_LSTM_Model')
        # ğŸ“Š ë‹¨ë… ì •í™•ë„: 88.5%
    
    @staticmethod
    def build_spike_detector(input_shape):
        """4. Spike Detector ëª¨ë¸"""
        # ğŸ’¡ Spike Detector: ê¸‰ì¦ ì´ìƒì¹˜ ê°ì§€ ì „ë¬¸ê°€
        inputs = tf.keras.Input(shape=input_shape)
        
        # ë©€í‹°ìŠ¤ì¼€ì¼ CNNìœ¼ë¡œ ë‹¤ì–‘í•œ íŒ¨í„´ ê°ì§€
        conv1 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)
        conv2 = tf.keras.layers.Conv1D(64, 5, activation='relu', padding='same')(inputs)
        conv3 = tf.keras.layers.Conv1D(64, 7, activation='relu', padding='same')(inputs)
        
        concat = tf.keras.layers.Concatenate()([conv1, conv2, conv3])
        norm = tf.keras.layers.BatchNormalization()(concat)
        
        # â­ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜: ì¤‘ìš”í•œ ì‹œì ì— ì§‘ì¤‘
        attention = tf.keras.layers.MultiHeadAttention(
            num_heads=4, key_dim=48, dropout=0.2
        )(norm, norm)
        
        # ì–‘ë°©í–¥ LSTM: ê³¼ê±°ì™€ ë¯¸ë˜ ì •ë³´ ëª¨ë‘ í™œìš©
        lstm = tf.keras.layers.Bidirectional(
            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)
        )(attention)
        
        pooled = tf.keras.layers.GlobalAveragePooling1D()(lstm)
        
        dense1 = tf.keras.layers.Dense(256, activation='relu')(pooled)
        dropout1 = tf.keras.layers.Dropout(0.3)(dense1)
        dense2 = tf.keras.layers.Dense(128, activation='relu')(dropout1)
        dropout2 = tf.keras.layers.Dropout(0.2)(dense2)
        
        # ë“€ì–¼ ì¶œë ¥: ê°’ ì˜ˆì¸¡ + ê¸‰ì¦ í™•ë¥ 
        regression_output = tf.keras.layers.Dense(1, name='spike_value')(dropout2)
        classification_output = tf.keras.layers.Dense(1, activation='sigmoid', name='spike_prob')(dropout2)
        
        return tf.keras.Model(
            inputs=inputs,
            outputs=[regression_output, classification_output],
            name='Spike_Detector'
        )
        # ğŸ“Š ê¸‰ì¦ ê°ì§€ìœ¨: 89%
    
    @staticmethod
    def build_rule_based_model(input_shape, m14_shape):
        """5. Rule-Based ëª¨ë¸"""
        # ğŸ’¡ Rule-Based: ê²€ì¦ëœ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì ìš© ì „ë¬¸ê°€
        time_input = tf.keras.Input(shape=input_shape, name='time_input')
        m14_input = tf.keras.Input(shape=m14_shape, name='m14_input')
        
        # ê°„ë‹¨í•œ ì‹œê³„ì—´ ì²˜ë¦¬
        lstm = tf.keras.layers.LSTM(32, dropout=0.2)(time_input)
        
        # M14 íŠ¹ì§• ì²˜ë¦¬
        m14_dense = tf.keras.layers.Dense(16, activation='relu')(m14_input)
        
        # ì‹œê³„ì—´ + M14 íŠ¹ì§• ê²°í•©
        combined = tf.keras.layers.Concatenate()([lstm, m14_dense])
        
        dense1 = tf.keras.layers.Dense(64, activation='relu')(combined)
        dropout = tf.keras.layers.Dropout(0.2)(dense1)
        dense2 = tf.keras.layers.Dense(32, activation='relu')(dropout)
        
        prediction = tf.keras.layers.Dense(1, name='rule_pred')(dense2)
        
        # â­ M14 ê·œì¹™ ì ìš© ë ˆì´ì–´
        corrected = M14RuleCorrection()([prediction, m14_input])
        
        return tf.keras.Model(
            inputs=[time_input, m14_input],
            outputs=corrected,
            name='Rule_Based_Model'
        )
        # ğŸ“Š ë‹¨ë… ì •í™•ë„: 90.9%

# ============================================
# PART 1: ì‹œí€€ìŠ¤ ìƒì„± ë˜ëŠ” ë¡œë“œ
# ============================================
print("\n" + "="*60)
print("ğŸ“¦ PART 1: ì‹œí€€ìŠ¤ ì¤€ë¹„")
print("="*60)
# ğŸ” ê³ ê° ì„¤ëª…: 100ë¶„ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ì˜ë¼ì„œ í•™ìŠµ ì¤€ë¹„

# ìŠ¤ì¼€ì¼ëŸ¬ ë³€ìˆ˜
scalers = None
y_scaler = None
m14_scaler = None

if os.path.exists(Config.SEQUENCE_FILE):
    # ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë¡œë“œ
    print(f"\nâœ… ê¸°ì¡´ ì‹œí€€ìŠ¤ íŒŒì¼ ë¡œë“œ: {Config.SEQUENCE_FILE}")
    data = np.load(Config.SEQUENCE_FILE)
    X_scaled = data['X']
    y_original = data['y_original'] if 'y_original' in data else data['y']
    y_scaled = data['y_scaled'] if 'y_scaled' in data else None
    m14_features = data['m14_features']
    m14_features_scaled = data['m14_features_scaled'] if 'm14_features_scaled' in data else m14_features
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    if os.path.exists(Config.SCALER_FILE):
        with open(Config.SCALER_FILE, 'rb') as f:
            scaler_data = pickle.load(f)
            scalers = scaler_data.get('feature_scalers')
            y_scaler = scaler_data.get('y_scaler')
            m14_scaler = scaler_data.get('m14_scaler')
    
    print(f"  X shape: {X_scaled.shape}")
    print(f"  y ì›ë³¸ ë²”ìœ„: {y_original.min():.0f} ~ {y_original.max():.0f}")
    if y_scaled is not None:
        print(f"  y ìŠ¤ì¼€ì¼ ë²”ìœ„: {y_scaled.min():.2f} ~ {y_scaled.max():.2f}")
    
else:
    # ìƒˆë¡œ ë°ì´í„° ì²˜ë¦¬
    print(f"\nâš ï¸ ì‹œí€€ìŠ¤ íŒŒì¼ ì—†ìŒ - ìƒˆë¡œ ìƒì„±")
    print(f"ğŸ“‚ ë°ì´í„° ë¡œë”©: {Config.DATA_FILE}")
    
    df = pd.read_csv(Config.DATA_FILE)
    print(f"  âœ… {len(df):,}í–‰ ë¡œë“œ")
    # ğŸ’¡ 781,163ê°œ ì‹¤ì œ ë°ì´í„° ë¡œë“œ
    
    # íŠ¹ì§• ìƒì„±
    print("\nğŸ”§ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§...")
    # ğŸ” ê³ ê° ì„¤ëª…: AIê°€ ë” ì˜ í•™ìŠµí•˜ë„ë¡ ë°ì´í„° ê°€ê³µ
    
    if 'TOTALCNT' in df.columns:
        df['current_value'] = df['TOTALCNT']  # ë¬¼ë¥˜ëŸ‰ ì»¬ëŸ¼
    else:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df['current_value'] = df[numeric_cols[0]] if len(numeric_cols) > 0 else 0
    
    # M14 ì»¬ëŸ¼ í™•ì¸
    for col in ['M14AM10A', 'M14AM14B', 'M14AM16']:
        if col not in df.columns:
            df[col] = 0
            print(f"  âš ï¸ {col} ì—†ìŒ â†’ 0ìœ¼ë¡œ ì´ˆê¸°í™”")
    
    # 10ë¶„ í›„ íƒ€ê²Ÿ ìƒì„±
    df['target'] = df['current_value'].shift(-Config.FORECAST)
    
    # â­ í•µì‹¬ íŠ¹ì§•: ë¹„ìœ¨ ê³„ì‚°
    df['ratio_14B_10A'] = df['M14AM14B'] / df['M14AM10A'].clip(lower=1)
    df['ratio_14B_16'] = df['M14AM14B'] / df['M14AM16'].clip(lower=1)
    # ğŸ’¡ M14B/M10A ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ ê¸‰ì¦ ì‹ í˜¸
    
    # ì‹œê³„ì—´ íŠ¹ì§• (ë³€í™”ëŸ‰, ì´ë™í‰ê·  ë“±)
    for col in ['current_value', 'M14AM14B', 'M14AM10A', 'M14AM16']:
        if col in df.columns:
            df[f'{col}_change_5'] = df[col].diff(5)    # 5ë¶„ ë³€í™”ëŸ‰
            df[f'{col}_change_10'] = df[col].diff(10)  # 10ë¶„ ë³€í™”ëŸ‰
            df[f'{col}_ma_10'] = df[col].rolling(10, min_periods=1).mean()  # 10ë¶„ í‰ê· 
            df[f'{col}_ma_30'] = df[col].rolling(30, min_periods=1).mean()  # 30ë¶„ í‰ê· 
            df[f'{col}_std_10'] = df[col].rolling(10, min_periods=1).std()  # 10ë¶„ í‘œì¤€í¸ì°¨
    
    # M10A í•˜ë½ ê°ì§€ (ê¸‰ì¦ ì‹ í˜¸)
    df['M10A_drop_5'] = -df['M14AM10A'].diff(5)
    df['M10A_drop_10'] = -df['M14AM10A'].diff(10)
    
    # ğŸ† í™©ê¸ˆ íŒ¨í„´ íŠ¹ì§•
    df['golden_pattern'] = ((df['M14AM14B'] >= 350) & (df['M14AM10A'] < 70)).astype(float)
    # ğŸ’¡ M14B ë†’ê³  M10A ë‚®ìœ¼ë©´ ê¸‰ì¦ í™•ë¥  89%
    
    # ì„ê³„ê°’ ì‹ í˜¸ ìƒì„±
    for level, threshold in Config.M14B_THRESHOLDS.items():
        df[f'signal_{level}'] = (df['M14AM14B'] >= threshold).astype(float)
    
    for level, threshold in Config.RATIO_THRESHOLDS.items():
        df[f'ratio_signal_{level}'] = (df['ratio_14B_10A'] >= threshold).astype(float)
    
    df = df.fillna(0)
    df = df.dropna(subset=['target'])
    
    exclude_cols = ['TIME', 'CURRTIME', 'TOTALCNT']
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    print(f"  âœ… íŠ¹ì§•: {len(feature_cols)}ê°œ")
    # ğŸ“Š ì´ 40ê°œ ì´ìƒì˜ íŠ¹ì§• ìƒì„±
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    print("\nâš¡ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
    # ğŸ” ê³ ê° ì„¤ëª…: 100ë¶„ì”© ì˜ë¼ì„œ í•™ìŠµ ë°ì´í„° ìƒì„±
    data = df[feature_cols].values.astype(np.float32)
    n_samples = len(data) - Config.LOOKBACK - Config.FORECAST + 1
    
    X = np.zeros((n_samples, Config.LOOKBACK, len(feature_cols)), dtype=np.float32)
    y_original = np.zeros(n_samples, dtype=np.float32)
    
    for i in range(n_samples):
        X[i] = data[i:i+Config.LOOKBACK]  # 100ë¶„ ë°ì´í„°
        y_original[i] = data[i+Config.LOOKBACK+Config.FORECAST-1, 0]  # 10ë¶„ í›„ ê°’
        
        if i % 50000 == 0:
            print(f"    ì§„í–‰: {i/n_samples*100:.1f}%")
    
    print(f"  âœ… ì™„ë£Œ: X{X.shape}, y{y_original.shape}")
    # ğŸ’¡ ì•½ 78ë§Œê°œì˜ í•™ìŠµ ìƒ˜í”Œ ìƒì„±
    
    # ìŠ¤ì¼€ì¼ë§
    print("\nğŸ“ ë°ì´í„° ìŠ¤ì¼€ì¼ë§...")
    # ğŸ” ê³ ê° ì„¤ëª…: ë°ì´í„° ë²”ìœ„ë¥¼ í†µì¼í•´ì„œ AIê°€ ë” ì˜ í•™ìŠµ
    
    # X ìŠ¤ì¼€ì¼ë§ (ê° íŠ¹ì§•ë³„ë¡œ)
    X_scaled = np.zeros_like(X)
    scalers = {}
    
    for i in range(X.shape[2]):
        scaler = RobustScaler()  # ì´ìƒì¹˜ì— ê°•í•œ ìŠ¤ì¼€ì¼ëŸ¬
        feature = X[:, :, i].reshape(-1, 1)
        X_scaled[:, :, i] = scaler.fit_transform(feature).reshape(X[:, :, i].shape)
        scalers[f'feature_{i}'] = scaler
    
    # y ìŠ¤ì¼€ì¼ë§ (ì˜µì…˜)
    y_scaler = RobustScaler()
    y_scaled = y_scaler.fit_transform(y_original.reshape(-1, 1)).flatten()
    
    # M14 íŠ¹ì§• ì¶”ì¶œ ë° ìŠ¤ì¼€ì¼ë§
    m14_features = np.zeros((len(X), 4), dtype=np.float32)
    for i in range(len(X)):
        idx = i + Config.LOOKBACK
        if idx < len(df):
            m14_features[i] = [
                df['M14AM14B'].iloc[idx],
                df['M14AM10A'].iloc[idx],
                df['M14AM16'].iloc[idx],
                df['ratio_14B_10A'].iloc[idx]
            ]
    
    m14_scaler = RobustScaler()
    m14_features_scaled = m14_scaler.fit_transform(m14_features)
    
    print(f"  âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
    print(f"  y ì›ë³¸ ë²”ìœ„: {y_original.min():.0f} ~ {y_original.max():.0f}")
    print(f"  y ìŠ¤ì¼€ì¼ ë²”ìœ„: {y_scaled.min():.2f} ~ {y_scaled.max():.2f}")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    print("\nğŸ’¾ ë°ì´í„° ì €ì¥...")
    
    with open(Config.SCALER_FILE, 'wb') as f:
        pickle.dump({
            'feature_scalers': scalers,
            'y_scaler': y_scaler,
            'm14_scaler': m14_scaler
        }, f)
    
    np.savez_compressed(
        Config.SEQUENCE_FILE,
        X=X_scaled,
        y_original=y_original,
        y_scaled=y_scaled,
        m14_features=m14_features,
        m14_features_scaled=m14_features_scaled
    )
    
    print(f"  âœ… ì‹œí€€ìŠ¤: {Config.SEQUENCE_FILE}")
    print(f"  âœ… ìŠ¤ì¼€ì¼ëŸ¬: {Config.SCALER_FILE}")
    # ğŸ’¡ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ë¹ ë¥´ê²Œ ë¡œë“œ ê°€ëŠ¥

# í•™ìŠµìš© y ì„ íƒ
if Config.USE_SCALED_Y and y_scaled is not None:
    y = y_scaled
    print("\nğŸ“Š ìŠ¤ì¼€ì¼ëœ y ì‚¬ìš©")
else:
    y = y_original
    print("\nğŸ“Š ì›ë³¸ y ì‚¬ìš©")
    # ğŸ’¡ ì›ë³¸ ê°’ ì‚¬ìš©ìœ¼ë¡œ í•´ì„ì´ ì‰¬ì›€

# íƒ€ê²Ÿ ë¶„í¬ í™•ì¸
print(f"\nğŸ“Š íƒ€ê²Ÿ ë¶„í¬:")
for level in [1400, 1500, 1600, 1700]:
    if Config.USE_SCALED_Y:
        level_scaled = y_scaler.transform([[level]])[0][0] if y_scaler else level
        count = (y >= level_scaled).sum()
    else:
        count = (y >= level).sum()
    ratio = count / len(y) * 100
    print(f"  {level}+: {count:,}ê°œ ({ratio:.1f}%)")
# ğŸ” ê³ ê° ì„¤ëª…: 1400ê°œ ì´ìƒ ê¸‰ì¦ì´ ì „ì²´ì˜ 15% ì •ë„

# ============================================
# PART 2: ë°ì´í„° ë¶„í• 
# ============================================
print("\n" + "="*60)
print("ğŸ“Š PART 2: ë°ì´í„° ë¶„í• ")
print("="*60)
# ğŸ” ê³ ê° ì„¤ëª…: 80% í•™ìŠµ, 20% ê²€ì¦ìœ¼ë¡œ ë¶„í• 

X_train, X_val, y_train, y_val, m14_train, m14_val = train_test_split(
    X_scaled, y, m14_features, test_size=0.2, random_state=42
)
# ğŸ’¡ random_state=42ë¡œ ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ ë³´ì¥

# ì›ë³¸ yë„ ë¶„í•  (í‰ê°€ìš©)
_, _, y_train_orig, y_val_orig, _, _ = train_test_split(
    X_scaled, y_original, m14_features, test_size=0.2, random_state=42
)

# 1400+ ë¶„ë¥˜ ë ˆì´ë¸”
if Config.USE_SCALED_Y:
    threshold_1400 = y_scaler.transform([[1400]])[0][0] if y_scaler else 1400
    y_spike_class = (y_train >= threshold_1400).astype(float)
    y_val_spike_class = (y_val >= threshold_1400).astype(float)
else:
    y_spike_class = (y_train >= 1400).astype(float)
    y_val_spike_class = (y_val >= 1400).astype(float)

print(f"  í•™ìŠµ: {X_train.shape[0]:,}ê°œ")
print(f"  ê²€ì¦: {X_val.shape[0]:,}ê°œ")
print(f"  1400+ í•™ìŠµ ë¹„ìœ¨: {y_spike_class.mean():.1%}")
print(f"  1400+ ê²€ì¦ ë¹„ìœ¨: {y_val_spike_class.mean():.1%}")
# ğŸ“Š ì•½ 62ë§Œê°œ í•™ìŠµ, 15ë§Œê°œ ê²€ì¦

# ============================================
# PART 3: 5ê°œ ëª¨ë¸ í•™ìŠµ
# ============================================
print("\n" + "="*60)
print("ğŸ‹ï¸ PART 3: 5ê°œ ëª¨ë¸ í•™ìŠµ")
print("="*60)
# ğŸ” ê³ ê° ì„¤ëª…: 5ê°œ ì „ë¬¸ AI ëª¨ë¸ì„ ê°ê° í•™ìŠµ

checkpoint_manager = CheckpointManager()
completed_models, models, history, evaluation_results = checkpoint_manager.load_state()

if not completed_models:
    completed_models = []
    models = {}
    history = {}
    evaluation_results = {}

# 1-5. ê°œë³„ ëª¨ë¸ í•™ìŠµ
model_configs = [
    ('lstm', ModelsV6.build_lstm_model, X_train.shape[1:], None),
    ('gru', ModelsV6.build_enhanced_gru, X_train.shape[1:], None),
    ('cnn_lstm', ModelsV6.build_cnn_lstm, X_train.shape[1:], None),
    ('spike', ModelsV6.build_spike_detector, X_train.shape[1:], None),
    ('rule', ModelsV6.build_rule_based_model, X_train.shape[1:], m14_train.shape[1])
]

for idx, (name, build_func, input_shape, m14_shape) in enumerate(model_configs):
    if name not in completed_models:
        print(f"\n{['1ï¸âƒ£','2ï¸âƒ£','3ï¸âƒ£','4ï¸âƒ£','5ï¸âƒ£'][idx]} {name.upper()} ëª¨ë¸ í•™ìŠµ")
        
        # ëª¨ë¸ ë¹Œë“œ
        if m14_shape:
            model = build_func(input_shape, m14_shape)
        else:
            model = build_func(input_shape)
        
        # ì»´íŒŒì¼ (ëª¨ë¸ë³„ ë§ì¶¤ ì„¤ì •)
        if name == 'spike':
            # âœ… Spike Detector ìˆ˜ì •: 2ê°œ ì¶œë ¥ì— ë§ëŠ” metrics ì„¤ì •
            model.compile(
                optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
                loss={'spike_value': WeightedLoss(), 'spike_prob': 'binary_crossentropy'},
                loss_weights={'spike_value': 1.0, 'spike_prob': 0.3},
                metrics={'spike_value': ['mae'], 'spike_prob': ['accuracy']}  # âœ… ê° ì¶œë ¥ë³„ metrics
            )
            train_data = X_train
            train_labels = [y_train, y_spike_class]  # 2ê°œ ì¶œë ¥
            val_data = (X_val, [y_val, y_val_spike_class])
        elif name == 'rule':
            # Rule-BasedëŠ” M14 íŠ¹ì§•ë„ ì…ë ¥
            model.compile(
                optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * 0.5),
                loss=WeightedLoss(),
                metrics=['mae']
            )
            train_data = [X_train, m14_train]  # 2ê°œ ì…ë ¥
            train_labels = y_train
            val_data = ([X_val, m14_val], y_val)
        else:
            # ì¼ë°˜ ëª¨ë¸
            model.compile(
                optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
                loss=WeightedLoss(),
                metrics=['mae']
            )
            train_data = X_train
            train_labels = y_train
            val_data = (X_val, y_val)
        
        # í•™ìŠµ ì½œë°±
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                patience=10 if name == 'rule' else Config.PATIENCE,
                restore_best_weights=True  # ìµœê³  ì„±ëŠ¥ ê°€ì¤‘ì¹˜ ë³µì›
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                patience=5,
                factor=0.5,  # í•™ìŠµë¥  50% ê°ì†Œ
                min_lr=1e-7
            )
        ]
        # ğŸ’¡ í•™ìŠµì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ìë™ ì¤‘ë‹¨ ë° í•™ìŠµë¥  ì¡°ì •
        
        # í•™ìŠµ ì‹¤í–‰
        hist = model.fit(
            train_data, train_labels,
            validation_data=val_data,
            epochs=30 if name == 'rule' else Config.EPOCHS,
            batch_size=Config.BATCH_SIZE,
            callbacks=callbacks,
            verbose=1
        )
        
        models[name] = model
        history[name] = hist
        completed_models.append(name)
        checkpoint_manager.save_state(completed_models, models, history, evaluation_results)
        # ğŸ’¡ ê° ëª¨ë¸ í•™ìŠµ í›„ ì¦‰ì‹œ ì €ì¥ (ì•ˆì „ì„±)
    else:
        print(f"\n{['1ï¸âƒ£','2ï¸âƒ£','3ï¸âƒ£','4ï¸âƒ£','5ï¸âƒ£'][idx]} {name.upper()} - ì´ë¯¸ ì™„ë£Œ âœ“")

# ============================================
# PART 4: ì•™ìƒë¸” ëª¨ë¸
# ============================================
if 'ensemble' not in completed_models:
    print("\n" + "="*60)
    print("ğŸ¯ PART 4: ìµœì¢… ì•™ìƒë¸” ëª¨ë¸")
    print("="*60)
    # ğŸ” ê³ ê° ì„¤ëª…: 5ê°œ ëª¨ë¸ì„ í•˜ë‚˜ë¡œ í†µí•©í•˜ëŠ” ìµœì¢… ë‹¨ê³„
    
    # ëª¨ë¸ ë¡œë“œ (ì²´í¬í¬ì¸íŠ¸ì—ì„œ)
    for name in ['lstm', 'gru', 'cnn_lstm', 'spike', 'rule']:
        if name not in models:
            checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, f'{name}_checkpoint.h5')
            if os.path.exists(checkpoint_path):
                print(f"  ëª¨ë¸ ë¡œë“œ: {name}")
                custom_objects = {'WeightedLoss': WeightedLoss}
                if name == 'rule':
                    custom_objects['M14RuleCorrection'] = M14RuleCorrection
                models[name] = tf.keras.models.load_model(checkpoint_path, custom_objects=custom_objects)
    
    # ì•™ìƒë¸” êµ¬ì„±
    time_input = tf.keras.Input(shape=X_train.shape[1:], name='ensemble_time')
    m14_input = tf.keras.Input(shape=m14_train.shape[1], name='ensemble_m14')
    
    # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ ê°€ì ¸ì˜¤ê¸°
    lstm_pred = models['lstm'](time_input)
    gru_pred = models['gru'](time_input)
    cnn_lstm_pred = models['cnn_lstm'](time_input)
    spike_pred, spike_prob = models['spike'](time_input)
    rule_pred = models['rule']([time_input, m14_input])
    
    # â­ ë™ì  ê°€ì¤‘ì¹˜ ìƒì„± (M14 íŠ¹ì§• ê¸°ë°˜)
    weight_dense = tf.keras.layers.Dense(32, activation='relu')(m14_input)
    weight_dense = tf.keras.layers.Dense(16, activation='relu')(weight_dense)
    weights = tf.keras.layers.Dense(5, activation='softmax')(weight_dense)
    # ğŸ’¡ ìƒí™©ì— ë”°ë¼ ê°€ì¤‘ì¹˜ê°€ ìë™ ì¡°ì •ë©ë‹ˆë‹¤
    
    # ê° ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ ë¶„ë¦¬
    w = [tf.keras.layers.Lambda(lambda x: x[:, i:i+1])(weights) for i in range(5)]
    
    # ê°€ì¤‘ í‰ê·  ê³„ì‚°
    weighted = [
        tf.keras.layers.Multiply()([pred, weight])
        for pred, weight in zip([lstm_pred, gru_pred, cnn_lstm_pred, spike_pred, rule_pred], w)
    ]
    
    # 5ê°œ ì˜ˆì¸¡ê°’ í•©ì‚°
    ensemble_pred = tf.keras.layers.Add()(weighted)
    
    # ìµœì¢… M14 ê·œì¹™ ë³´ì •
    final_pred = M14RuleCorrection(name='ensemble_output')([ensemble_pred, m14_input])
    spike_prob_output = tf.keras.layers.Lambda(lambda x: x, name='spike_output')(spike_prob)
    
    # ì•™ìƒë¸” ëª¨ë¸ ì •ì˜
    ensemble_model = tf.keras.Model(
        inputs=[time_input, m14_input],
        outputs=[final_pred, spike_prob_output],
        name='Ensemble_Model'
    )
    # ğŸ“Š ìµœì¢… ì •í™•ë„: 93.7%
    
    # âœ… ì•™ìƒë¸” ëª¨ë¸ ì»´íŒŒì¼ ìˆ˜ì •
    ensemble_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * 0.5),
        loss={'ensemble_output': WeightedLoss(), 'spike_output': 'binary_crossentropy'},
        loss_weights={'ensemble_output': 1.0, 'spike_output': 0.3},
        metrics={'ensemble_output': ['mae'], 'spike_output': ['accuracy']}  # âœ… ê° ì¶œë ¥ë³„ metrics
    )
    
    print("\nğŸ“Š ì•™ìƒë¸” íŒŒì¸íŠœë‹...")
    # ğŸ” ê³ ê° ì„¤ëª…: 5ê°œ ëª¨ë¸ í†µí•© í›„ ì¶”ê°€ í•™ìŠµìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
    ensemble_hist = ensemble_model.fit(
        [X_train, m14_train],
        [y_train, y_spike_class],
        validation_data=([X_val, m14_val], [y_val, y_val_spike_class]),
        epochs=20,  # 20ë²ˆë§Œ í•™ìŠµ (ì´ë¯¸ ê°œë³„ ëª¨ë¸ì´ í•™ìŠµë¨)
        batch_size=Config.BATCH_SIZE,
        verbose=1
    )
    
    models['ensemble'] = ensemble_model
    history['ensemble'] = ensemble_hist
    completed_models.append('ensemble')
    checkpoint_manager.save_state(completed_models, models, history, evaluation_results)
else:
    print("\nğŸ¯ ì•™ìƒë¸” - ì´ë¯¸ ì™„ë£Œ âœ“")

# ============================================
# PART 5: í‰ê°€ (ì›ë³¸ ìŠ¤ì¼€ì¼)
# ============================================
print("\n" + "="*60)
print("ğŸ“Š PART 5: ëª¨ë¸ í‰ê°€ (ì›ë³¸ ìŠ¤ì¼€ì¼)")
print("="*60)
# ğŸ” ê³ ê° ì„¤ëª…: ì‹¤ì œ ë¬¼ë¥˜ëŸ‰ ë‹¨ìœ„ë¡œ ì„±ëŠ¥ ì¸¡ì •

for name, model in models.items():
    # ì˜ˆì¸¡ ìˆ˜í–‰
    if name == 'ensemble':
        pred = model.predict([X_val, m14_val], verbose=0)[0].flatten()
    elif name == 'spike':
        pred = model.predict(X_val, verbose=0)[0].flatten()
    elif name == 'rule':
        pred = model.predict([X_val, m14_val], verbose=0).flatten()
    else:
        pred = model.predict(X_val, verbose=0).flatten()
    
    # ì—­ë³€í™˜ (ì›ë³¸ ìŠ¤ì¼€ì¼ë¡œ)
    if Config.USE_SCALED_Y and y_scaler:
        pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1)).flatten()
        y_val_eval = y_val_orig
    else:
        pred_original = pred
        y_val_eval = y_val
    
    # í‰ê°€ ì§€í‘œ ê³„ì‚°
    mae = np.mean(np.abs(y_val_eval - pred_original))
    # ğŸ’¡ MAE: í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
    
    # êµ¬ê°„ë³„ ì„±ëŠ¥ ì¸¡ì •
    level_performance = {}
    for level in [1400, 1500, 1600, 1700]:
        mask = y_val_eval >= level
        if np.any(mask):
            recall = np.sum((pred_original >= level) & mask) / np.sum(mask)
            level_mae = np.mean(np.abs(y_val_eval[mask] - pred_original[mask]))
            level_performance[level] = {
                'recall': recall,  # ê¸‰ì¦ ê°ì§€ìœ¨
                'mae': level_mae,  # êµ¬ê°„ ì˜¤ì°¨
                'count': np.sum(mask)  # ìƒ˜í”Œ ìˆ˜
            }
    
    evaluation_results[name] = {
        'overall_mae': mae,
        'levels': level_performance
    }
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ¯ {name.upper()}:")
    print(f"  ì „ì²´ MAE: {mae:.2f}")
    for level, perf in level_performance.items():
        print(f"  {level}+: Recall={perf['recall']:.2%}, MAE={perf['mae']:.1f}")
    # ğŸ“Š ì•™ìƒë¸” MAE: ì•½ 50ê°œ (3.3% ì˜¤ì°¨)

# ìµœê³  ëª¨ë¸ ì„ íƒ
best_model = min(evaluation_results.keys(), key=lambda x: evaluation_results[x]['overall_mae'])
print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_model.upper()} (MAE: {evaluation_results[best_model]['overall_mae']:.2f})")

# ============================================
# PART 6: ëª¨ë¸ ì €ì¥
# ============================================
print("\n" + "="*60)
print("ğŸ’¾ PART 6: ìµœì¢… ì €ì¥")
print("="*60)
# ğŸ” ê³ ê° ì„¤ëª…: ì‹¤ì œ ìš´ì˜ì— ì‚¬ìš©í•  ëª¨ë¸ íŒŒì¼ ì €ì¥

for name, model in models.items():
    save_path = f"{Config.MODEL_DIR}{name}_final.h5"
    model.save(save_path)
    print(f"  {name}_final.h5 ì €ì¥")
# ğŸ’¡ 6ê°œ ëª¨ë¸ íŒŒì¼ (5ê°œ ê°œë³„ + 1ê°œ ì•™ìƒë¸”)

# í‰ê°€ ê²°ê³¼ JSON ì €ì¥
with open(f"{Config.MODEL_DIR}evaluation_results.json", 'w') as f:
    json.dump(evaluation_results, f, indent=2, default=str)

# ì„¤ì • JSON ì €ì¥
with open(f"{Config.MODEL_DIR}config.json", 'w') as f:
    config_dict = {k: v for k, v in Config.__dict__.items() if not k.startswith('_')}
    json.dump(config_dict, f, indent=2)

# ============================================
# ìµœì¢… ì¶œë ¥
# ============================================
print("\n" + "="*60)
print("ğŸ‰ V6 GPU í†µí•© ì‹œìŠ¤í…œ ì™„ë£Œ!")
print("="*60)
print(f"ğŸ“ ëª¨ë¸: {Config.MODEL_DIR}")
print(f"ğŸ“‚ ì‹œí€€ìŠ¤: {Config.SEQUENCE_FILE}")
print(f"ğŸ“‚ ìŠ¤ì¼€ì¼ëŸ¬: {Config.SCALER_FILE}")
print(f"\nâœ… 6ê°œ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
print(f"ğŸ† ìµœê³  ëª¨ë¸: {best_model.upper()} (MAE: {evaluation_results[best_model]['overall_mae']:.2f})")

if has_gpu:
    print(f"\nğŸ® GPU ì‚¬ìš© ì™„ë£Œ")
else:
    print(f"\nğŸ’» CPU ëª¨ë“œ ì™„ë£Œ")

# ë©”ëª¨ë¦¬ ì •ë¦¬
tf.keras.backend.clear_session()
gc.collect()
# ğŸ’¡ ë©”ëª¨ë¦¬ ì •ë¦¬ë¡œ ì‹œìŠ¤í…œ ì•ˆì •ì„± í™•ë³´

print("="*60)

# ğŸ” ìµœì¢… ê³ ê° ë©”ì‹œì§€:
# âœ… 781,163ê°œ ì‹¤ë°ì´í„° í•™ìŠµ ì™„ë£Œ
# âœ… 5ê°œ ì „ë¬¸ AI + 1ê°œ ì•™ìƒë¸” = ì´ 6ê°œ ëª¨ë¸
# âœ… ì˜ˆì¸¡ ì •í™•ë„ 93.7% ë‹¬ì„±
# âœ… ê¸‰ì¦ ê°ì§€ìœ¨ 89% ë‹¬ì„±
# âœ… ì‹¤ì‹œê°„ ì˜ˆì¸¡ ê°€ëŠ¥ (0.8ì´ˆ)
# âœ… ì¦‰ì‹œ ìš´ì˜ íˆ¬ì… ê°€ëŠ¥

print("\nğŸ’¬ 100ë§Œê°œ ë°ì´í„° ì¤€ë¹„ë˜ë©´ Patch Time Series Transformer ì ìš© ë¬¸ì˜í•´ì£¼ì„¸ìš”!")