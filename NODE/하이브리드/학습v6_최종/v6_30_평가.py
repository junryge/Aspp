"""
V6 ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ (ìˆ˜ì •íŒ)
- í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
- ìƒˆë¡œìš´ í‰ê°€ ë°ì´í„°ë¡œ ì„±ëŠ¥ ì¸¡ì •
- ìƒì„¸í•œ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
- CPU ëª¨ë“œ ìµœì í™”
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import RobustScaler
import matplotlib.pyplot as plt
import seaborn as sns
import json
import pickle
import warnings
from datetime import datetime
import os
warnings.filterwarnings('ignore')

# TensorFlow ê²½ê³  ì–µì œ
import logging
logging.getLogger('tensorflow').setLevel(logging.ERROR)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

print("="*60)
print("ğŸ“Š V6 ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ (ìˆ˜ì •íŒ)")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print("="*60)

# ============================================
# GPU/CPU ì„¤ì •
# ============================================
def setup_compute():
    """ê³„ì‚° í™˜ê²½ ì„¤ì •"""
    print("\nğŸ® ê³„ì‚° í™˜ê²½ í™•ì¸...")
    
    # CPU ëª¨ë“œ ê°•ì œ (í•„ìš”ì‹œ)
    # os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
    
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"âœ… GPU ê°ì§€: {len(gpus)}ê°œ")
            return True
        except Exception as e:
            print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
            print("ğŸ’» CPU ëª¨ë“œë¡œ ì „í™˜")
            return False
    else:
        print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        return False

has_gpu = setup_compute()

# ============================================
# ì„¤ì •
# ============================================
class Config:
    # í‰ê°€ ë°ì´í„° íŒŒì¼
    EVAL_DATA_FILE = './data/20250731_to20250806.CSV'  # ì‹¤ì œ íŒŒì¼ëª…ì— ë§ê²Œ ìˆ˜ì •
    
    # í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
    MODEL_DIR = './models_v6_full_train/'
    
    # ì‹œí€€ìŠ¤ ì„¤ì •
    LOOKBACK = 100  # ê³¼ê±° 100ë¶„ ë°ì´í„°
    FORECAST = 10   # 10ë¶„ í›„ ì˜ˆì¸¡
    
    # í‰ê°€ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    EVAL_RESULT_DIR = './evaluation_results/'
    
    # ì‹œê°í™” ì €ì¥ ê²½ë¡œ
    PLOT_DIR = './evaluation_plots/'
    
    # CPU ëª¨ë“œ ë°°ì¹˜ í¬ê¸°
    BATCH_SIZE = 32  # CPUì—ì„œëŠ” ì‘ì€ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(Config.EVAL_RESULT_DIR, exist_ok=True)
os.makedirs(Config.PLOT_DIR, exist_ok=True)

# ============================================
# ì»¤ìŠ¤í…€ ë ˆì´ì–´ ì •ì˜ (ëª¨ë¸ ë¡œë“œìš©)
# ============================================
class M14RuleCorrection(tf.keras.layers.Layer):
    """M14 ê·œì¹™ ê¸°ë°˜ ë³´ì • ë ˆì´ì–´"""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    
    def call(self, inputs, training=None):
        pred, m14_features = inputs
        
        pred = tf.cast(pred, tf.float32)
        m14_features = tf.cast(m14_features, tf.float32)
        
        m14b = m14_features[:, 0:1]
        m10a = m14_features[:, 1:2]
        m16 = m14_features[:, 2:3]
        ratio = m14_features[:, 3:4] if m14_features.shape[1] > 3 else \
                tf.where(m10a > 0, m14b / (m10a + 1e-7), 0.0)
        
        # ì„ê³„ê°’ ê·œì¹™
        pred = tf.where(m14b >= 420, tf.maximum(pred, 1550.0), pred)
        pred = tf.where(m14b >= 380, tf.maximum(pred, 1500.0), pred)
        pred = tf.where(m14b >= 350, tf.maximum(pred, 1450.0), pred)
        pred = tf.where(m14b >= 300, tf.maximum(pred, 1400.0), pred)
        
        # ë¹„ìœ¨ ë³´ì •
        pred = tf.where(ratio >= 5.5, pred * 1.15, pred)
        pred = tf.where((ratio >= 5.0) & (ratio < 5.5), pred * 1.10, pred)
        pred = tf.where((ratio >= 4.5) & (ratio < 5.0), pred * 1.08, pred)
        pred = tf.where((ratio >= 4.0) & (ratio < 4.5), pred * 1.05, pred)
        
        # í™©ê¸ˆ íŒ¨í„´
        golden = (m14b >= 350) & (m10a < 70)
        pred = tf.where(golden, pred * 1.2, pred)
        
        # ë²”ìœ„ ì œí•œ
        pred = tf.clip_by_value(pred, 1200.0, 2000.0)
        
        return pred

# ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ (ëª¨ë¸ ë¡œë“œìš©)
class WeightedLoss(tf.keras.losses.Loss):
    """ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜"""
    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        
        mae = tf.abs(y_true - y_pred)
        
        # ê°€ì¤‘ì¹˜
        weights = tf.ones_like(y_true)
        weights = tf.where(y_true >= 1550, 30.0, weights)
        weights = tf.where((y_true >= 1500) & (y_true < 1550), 25.0, weights)
        weights = tf.where((y_true >= 1450) & (y_true < 1500), 20.0, weights)
        weights = tf.where((y_true >= 1400) & (y_true < 1450), 15.0, weights)
        weights = tf.where((y_true >= 1350) & (y_true < 1400), 10.0, weights)
        
        # í° ì˜¤ì°¨ í˜ë„í‹°
        large_error = tf.where(mae > 100, mae * 0.2, 0.0)
        
        return tf.reduce_mean(mae * weights + large_error)

# ============================================
# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
# ============================================
def prepare_evaluation_data(file_path):
    """í‰ê°€ ë°ì´í„° ì¤€ë¹„"""
    print(f"\nğŸ“‚ í‰ê°€ ë°ì´í„° ë¡œë“œ: {file_path}")
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(file_path)
    print(f"  ì›ë³¸ ë°ì´í„°: {len(df)}í–‰")
    
    # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸ ë° ì²˜ë¦¬
    required_columns = ['M14AM10A', 'M14AM14B', 'M14AM16', 'TOTALCNT']
    
    # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    for col in required_columns:
        if col not in df.columns:
            print(f"  âš ï¸ {col} ì»¬ëŸ¼ ì—†ìŒ - 0ìœ¼ë¡œ ì´ˆê¸°í™”")
            df[col] = 0
    
    # M14AM14BSUM ìƒì„±
    if 'M14AM14BSUM' not in df.columns:
        df['M14AM14BSUM'] = df['M14AM14B'] + df['M14AM10A']
    
    # íƒ€ê²Ÿ ìƒì„± (10ë¶„ í›„ TOTALCNT)
    df['target'] = df['TOTALCNT'].shift(-Config.FORECAST)
    
    # íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§
    print("\nğŸ”§ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§...")
    
    # ê¸°ë³¸ íŠ¹ì§•
    df['ratio_14B_10A'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
    df['ratio_14B_16'] = df['M14AM14B'] / (df['M14AM16'] + 1)
    df['ratio_10A_16'] = df['M14AM10A'] / (df['M14AM16'] + 1)
    
    # ì‹œê³„ì—´ íŠ¹ì§•
    for col in ['TOTALCNT', 'M14AM14B', 'M14AM10A', 'M14AM16']:
        if col in df.columns:
            # ë³€í™”ëŸ‰
            df[f'{col}_diff_1'] = df[col].diff(1)
            df[f'{col}_diff_5'] = df[col].diff(5)
            df[f'{col}_diff_10'] = df[col].diff(10)
            
            # ì´ë™í‰ê· 
            df[f'{col}_ma_5'] = df[col].rolling(5, min_periods=1).mean()
            df[f'{col}_ma_10'] = df[col].rolling(10, min_periods=1).mean()
            df[f'{col}_ma_20'] = df[col].rolling(20, min_periods=1).mean()
            
            # í‘œì¤€í¸ì°¨
            df[f'{col}_std_5'] = df[col].rolling(5, min_periods=1).std()
            df[f'{col}_std_10'] = df[col].rolling(10, min_periods=1).std()
    
    # í™©ê¸ˆ íŒ¨í„´
    df['golden_pattern'] = ((df['M14AM14B'] >= 350) & (df['M14AM10A'] < 70)).astype(float)
    
    # ê¸‰ì¦ ì‹ í˜¸
    thresholds_14b = [250, 300, 350, 400, 450]
    for threshold in thresholds_14b:
        df[f'signal_{threshold}'] = (df['M14AM14B'] >= threshold).astype(float)
    
    thresholds_ratio = [3.5, 4.0, 4.5, 5.0, 5.5]
    for threshold in thresholds_ratio:
        df[f'ratio_signal_{threshold}'] = (df['ratio_14B_10A'] >= threshold).astype(float)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df = df.fillna(0)
    
    # íƒ€ê²Ÿì´ ìˆëŠ” ë°ì´í„°ë§Œ ì„ íƒ
    df = df.dropna(subset=['target'])
    
    print(f"  ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}í–‰, {len(df.columns)}ê°œ íŠ¹ì§•")
    
    return df

def create_sequences(df, lookback=100, forecast=10):
    """ì‹œí€€ìŠ¤ ìƒì„± (ìˆ˜ì •ëœ ë²„ì „)"""
    print("\nâš¡ í‰ê°€ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
    
    X, y = [], []
    
    # numpy arrayë¡œ ë³€í™˜
    data_array = df.values
    
    # TOTALCNT ì»¬ëŸ¼ ì¸ë±ìŠ¤ ì°¾ê¸°
    totalcnt_idx = df.columns.get_loc('TOTALCNT')
    
    for i in range(len(data_array) - lookback - forecast + 1):
        # ì‹œí€€ìŠ¤ ì…ë ¥ (100ë¶„)
        X.append(data_array[i:i+lookback])
        
        # íƒ€ê²Ÿ (10ë¶„ í›„ TOTALCNT)
        target_idx = i + lookback + forecast - 1
        if target_idx < len(data_array):
            y.append(data_array[target_idx, totalcnt_idx])
    
    X = np.array(X, dtype=np.float32)
    y = np.array(y, dtype=np.float32)
    
    print(f"  X shape: {X.shape}")
    print(f"  y shape: {y.shape}")
    print(f"  y ë²”ìœ„: {y.min():.0f} ~ {y.max():.0f}")
    
    return X, y, df

# ============================================
# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
# ============================================
def load_models():
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
    print("\nğŸ“¦ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    models = {}
    model_names = ['lstm', 'gru', 'cnn_lstm', 'spike', 'rule', 'ensemble']
    
    # ì»¤ìŠ¤í…€ ê°ì²´ ì •ì˜
    custom_objects = {
        'M14RuleCorrection': M14RuleCorrection,
        'WeightedLoss': WeightedLoss,
    }
    
    for name in model_names:
        model_path = f"{Config.MODEL_DIR}{name}_final.keras"
        if os.path.exists(model_path):
            try:
                models[name] = tf.keras.models.load_model(
                    model_path,
                    custom_objects=custom_objects,
                    compile=False
                )
                # ì¬ì»´íŒŒì¼ (í‰ê°€ìš©)
                models[name].compile(
                    optimizer='adam',
                    loss='mae',
                    metrics=['mae']
                )
                print(f"  âœ… {name} ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                print(f"  âŒ {name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print(f"  âš ï¸ {name} ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
    
    return models

# ============================================
# í‰ê°€ í•¨ìˆ˜
# ============================================
def evaluate_models(models, X_test, y_test, m14_test):
    """ëª¨ë¸ í‰ê°€"""
    print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ì‹œì‘...")
    
    results = {}
    predictions = {}
    
    for name, model in models.items():
        print(f"\n  í‰ê°€ ì¤‘: {name.upper()}")
        
        try:
            # ì˜ˆì¸¡ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ ê°œì„ )
            if name in ['ensemble', 'rule']:
                # Ruleê³¼ Ensembleì€ ë‘ ê°œì˜ ì…ë ¥ í•„ìš”
                pred = model.predict(
                    [X_test, m14_test], 
                    batch_size=Config.BATCH_SIZE,
                    verbose=0
                ).flatten()
            else:
                # ë‚˜ë¨¸ì§€ ëª¨ë¸ì€ í•˜ë‚˜ì˜ ì…ë ¥
                pred = model.predict(
                    X_test, 
                    batch_size=Config.BATCH_SIZE,
                    verbose=0
                ).flatten()
            
            predictions[name] = pred
            
            # ì „ì²´ ì„±ëŠ¥ ì§€í‘œ
            mae = np.mean(np.abs(y_test - pred))
            mse = np.mean((y_test - pred) ** 2)
            rmse = np.sqrt(mse)
            
            # MAPE ê³„ì‚° (0 division ë°©ì§€)
            non_zero_mask = y_test != 0
            if np.any(non_zero_mask):
                mape = np.mean(np.abs((y_test[non_zero_mask] - pred[non_zero_mask]) / y_test[non_zero_mask])) * 100
            else:
                mape = 0
            
            # ì •í™•ë„ (ì˜¤ì°¨ ê¸°ì¤€)
            accuracy_50 = np.mean(np.abs(y_test - pred) <= 50) * 100
            accuracy_100 = np.mean(np.abs(y_test - pred) <= 100) * 100
            
            # êµ¬ê°„ë³„ ì„±ëŠ¥
            level_performance = {}
            for level in [1300, 1400, 1450, 1500, 1550]:
                mask = y_test >= level
                if np.any(mask):
                    # Recall: ì‹¤ì œ ê¸‰ì¦ ì¤‘ ì˜ˆì¸¡ ì„±ê³µ
                    tp = np.sum((pred >= level) & mask)
                    fn = np.sum((pred < level) & mask)
                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                    
                    # Precision: ì˜ˆì¸¡ ê¸‰ì¦ ì¤‘ ì‹¤ì œ ê¸‰ì¦
                    fp = np.sum((pred >= level) & ~mask)
                    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                    
                    # F1 Score
                    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
                    
                    # êµ¬ê°„ MAE
                    level_mae = np.mean(np.abs(y_test[mask] - pred[mask]))
                    
                    level_performance[level] = {
                        'recall': recall,
                        'precision': precision,
                        'f1': f1,
                        'mae': level_mae,
                        'count': np.sum(mask),
                        'tp': int(tp),
                        'fp': int(fp),
                        'fn': int(fn)
                    }
            
            results[name] = {
                'mae': float(mae),
                'mse': float(mse),
                'rmse': float(rmse),
                'mape': float(mape),
                'accuracy_50': float(accuracy_50),
                'accuracy_100': float(accuracy_100),
                'levels': level_performance
            }
            
            print(f"    MAE: {mae:.2f}")
            print(f"    RMSE: {rmse:.2f}")
            print(f"    MAPE: {mape:.2f}%")
            print(f"    ì •í™•ë„(Â±50): {accuracy_50:.1f}%")
            print(f"    ì •í™•ë„(Â±100): {accuracy_100:.1f}%")
            
        except Exception as e:
            print(f"    âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    return results, predictions

# ============================================
# ì‹œê°í™” í•¨ìˆ˜
# ============================================
def create_visualizations(y_test, predictions, results):
    """í‰ê°€ ê²°ê³¼ ì‹œê°í™”"""
    print("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
    
    try:
        # í°íŠ¸ ì„¤ì • (í•œê¸€ ì§€ì›)
        plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # 1-1. MAE ë¹„êµ
        ax = axes[0, 0]
        model_names = list(results.keys())
        mae_values = [results[name]['mae'] for name in model_names]
        colors = plt.cm.Set3(np.linspace(0, 1, len(model_names)))
        bars = ax.bar(model_names, mae_values, color=colors)
        ax.set_title('Model MAE Comparison', fontsize=12, fontweight='bold')
        ax.set_ylabel('MAE')
        ax.set_xticklabels(model_names, rotation=45)
        for bar, value in zip(bars, mae_values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                   f'{value:.1f}', ha='center', va='bottom')
        
        # 1-2. ì •í™•ë„ ë¹„êµ
        ax = axes[0, 1]
        acc_50 = [results[name]['accuracy_50'] for name in model_names]
        acc_100 = [results[name]['accuracy_100'] for name in model_names]
        x = np.arange(len(model_names))
        width = 0.35
        ax.bar(x - width/2, acc_50, width, label='Â±50 Accuracy', color='skyblue')
        ax.bar(x + width/2, acc_100, width, label='Â±100 Accuracy', color='lightcoral')
        ax.set_title('Model Accuracy', fontsize=12, fontweight='bold')
        ax.set_ylabel('Accuracy (%)')
        ax.set_xticks(x)
        ax.set_xticklabels(model_names, rotation=45)
        ax.legend()
        
        # 1-3. 1400+ ê¸‰ì¦ ê°ì§€ ì„±ëŠ¥
        ax = axes[0, 2]
        f1_scores = []
        for name in model_names:
            if 1400 in results[name]['levels']:
                f1_scores.append(results[name]['levels'][1400]['f1'] * 100)
            else:
                f1_scores.append(0)
        bars = ax.bar(model_names, f1_scores, color='green', alpha=0.7)
        ax.set_title('1400+ Spike Detection F1 Score', fontsize=12, fontweight='bold')
        ax.set_ylabel('F1 Score (%)')
        ax.set_xticklabels(model_names, rotation=45)
        for bar, value in zip(bars, f1_scores):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                   f'{value:.1f}%', ha='center', va='bottom')
        
        # 2. ì˜ˆì¸¡ vs ì‹¤ì œ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸)
        if results:
            best_model = min(results.keys(), key=lambda x: results[x]['mae'])
            
            # 2-1. ì‚°ì ë„
            ax = axes[1, 0]
            sample_size = min(500, len(y_test))
            sample_idx = np.random.choice(len(y_test), sample_size, replace=False)
            ax.scatter(y_test[sample_idx], predictions[best_model][sample_idx], 
                      alpha=0.5, s=10)
            ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
                   'r--', label='Perfect Prediction')
            ax.set_xlabel('Actual')
            ax.set_ylabel('Predicted')
            ax.set_title(f'{best_model.upper()} - Predicted vs Actual', fontsize=12, fontweight='bold')
            ax.legend()
            
            # 2-2. ì‹œê³„ì—´ ì˜ˆì¸¡
            ax = axes[1, 1]
            time_sample = min(200, len(y_test))
            time_range = range(time_sample)
            ax.plot(time_range, y_test[:time_sample], label='Actual', linewidth=2)
            ax.plot(time_range, predictions[best_model][:time_sample], 
                   label=f'{best_model} Predicted', linewidth=2, alpha=0.7)
            ax.set_xlabel('Time (minutes)')
            ax.set_ylabel('TOTALCNT')
            ax.set_title('Time Series Prediction Sample', fontsize=12, fontweight='bold')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # 2-3. ì˜¤ì°¨ ë¶„í¬
            ax = axes[1, 2]
            errors = predictions[best_model] - y_test
            ax.hist(errors, bins=50, color='purple', alpha=0.7, edgecolor='black')
            ax.axvline(x=0, color='red', linestyle='--', label='Error = 0')
            ax.set_xlabel('Prediction Error')
            ax.set_ylabel('Frequency')
            ax.set_title(f'{best_model.upper()} Error Distribution', fontsize=12, fontweight='bold')
            ax.legend()
            
            mean_error = np.mean(errors)
            std_error = np.std(errors)
            ax.text(0.05, 0.95, f'Mean: {mean_error:.1f}\nStd: {std_error:.1f}',
                   transform=ax.transAxes, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        plt.tight_layout()
        
        # ì €ì¥
        save_path = f'{Config.PLOT_DIR}model_evaluation_summary.png'
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"  âœ… ì‹œê°í™” ì €ì¥: {save_path}")
        
        # í‘œì‹œ (ì„ íƒì‚¬í•­)
        # plt.show()
        plt.close()
        
    except Exception as e:
        print(f"  âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def create_detailed_report(results, y_test):
    """ìƒì„¸ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
    print("\nğŸ“ ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    
    report = []
    report.append("="*80)
    report.append("V6 ëª¨ë¸ í‰ê°€ ë¦¬í¬íŠ¸")
    report.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("="*80)
    report.append("")
    
    # ë°ì´í„° ê°œìš”
    report.append("ğŸ“Š í‰ê°€ ë°ì´í„° ê°œìš”")
    report.append(f"  - ì´ ìƒ˜í”Œ ìˆ˜: {len(y_test):,}ê°œ")
    report.append(f"  - TOTALCNT ë²”ìœ„: {y_test.min():.0f} ~ {y_test.max():.0f}")
    report.append(f"  - TOTALCNT í‰ê· : {y_test.mean():.0f}")
    report.append(f"  - TOTALCNT í‘œì¤€í¸ì°¨: {y_test.std():.0f}")
    report.append("")
    
    # ê¸‰ì¦ ë°ì´í„° ë¶„í¬
    report.append("ğŸ“ˆ ê¸‰ì¦ ë°ì´í„° ë¶„í¬")
    for level in [1300, 1400, 1450, 1500, 1550]:
        count = np.sum(y_test >= level)
        ratio = count / len(y_test) * 100
        report.append(f"  - {level}+ : {count:,}ê°œ ({ratio:.1f}%)")
    report.append("")
    
    # ëª¨ë¸ë³„ ì„±ëŠ¥
    report.append("ğŸ† ëª¨ë¸ë³„ ì„±ëŠ¥ í‰ê°€")
    report.append("-"*80)
    
    if results:
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        best_model = min(results.keys(), key=lambda x: results[x]['mae'])
        
        for name, metrics in results.items():
            is_best = " â­ BEST" if name == best_model else ""
            report.append(f"\nğŸ“ {name.upper()} ëª¨ë¸{is_best}")
            report.append(f"  ì „ì²´ ì„±ëŠ¥:")
            report.append(f"    - MAE: {metrics['mae']:.2f}")
            report.append(f"    - RMSE: {metrics['rmse']:.2f}")
            report.append(f"    - MAPE: {metrics['mape']:.2f}%")
            report.append(f"    - ì •í™•ë„(Â±50): {metrics['accuracy_50']:.1f}%")
            report.append(f"    - ì •í™•ë„(Â±100): {metrics['accuracy_100']:.1f}%")
            
            report.append(f"  \n  ê¸‰ì¦ ê°ì§€ ì„±ëŠ¥:")
            for level, perf in metrics['levels'].items():
                if perf['count'] > 0:
                    report.append(f"    {level}+ ê°ì§€:")
                    report.append(f"      - Recall: {perf['recall']:.1%}")
                    report.append(f"      - Precision: {perf['precision']:.1%}")
                    report.append(f"      - F1 Score: {perf['f1']:.1%}")
                    report.append(f"      - MAE: {perf['mae']:.1f}")
                    report.append(f"      - ìƒ˜í”Œ ìˆ˜: {perf['count']:,}ê°œ")
        
        report.append("")
        report.append("="*80)
        report.append("ğŸ¯ ê²°ë¡ ")
        report.append(f"  ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model.upper()}")
        report.append(f"  MAE: {results[best_model]['mae']:.2f}")
        report.append(f"  ì •í™•ë„(Â±50): {results[best_model]['accuracy_50']:.1f}%")
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        report.append("")
        report.append("ğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­")
        
        # CNN-LSTMì´ë‚˜ Spike ì„±ëŠ¥ì´ ë‚®ì€ ê²½ìš°
        if 'cnn_lstm' in results and results['cnn_lstm']['mae'] > 1000:
            report.append("  - CNN-LSTM ëª¨ë¸ ì¬í•™ìŠµ í•„ìš” (êµ¬ì¡° ë‹¨ìˆœí™” ê¶Œì¥)")
        if 'spike' in results and results['spike']['mae'] > 1000:
            report.append("  - Spike Detector ì„ê³„ê°’ ì¬ì¡°ì • í•„ìš”")
        
        # ì•™ìƒë¸” ê°œì„  ì—¬ë¶€
        if 'ensemble' in results and 'lstm' in results:
            if results['ensemble']['mae'] > results['lstm']['mae']:
                report.append("  - ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì¬ì¡°ì • ê¶Œì¥ (CNN/Spike ì œì™¸)")
    
    report.append("="*80)
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    report_text = "\n".join(report)
    report_path = f"{Config.EVAL_RESULT_DIR}evaluation_report.txt"
    
    try:
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        print(f"  âœ… ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    except Exception as e:
        print(f"  âŒ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # ê²°ê³¼ JSON ì €ì¥
    json_path = f"{Config.EVAL_RESULT_DIR}evaluation_results.json"
    try:
        with open(json_path, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        print(f"  âœ… JSON ê²°ê³¼ ì €ì¥: {json_path}")
    except Exception as e:
        print(f"  âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}")
    
    return report_text

# ============================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================
def main():
    """ë©”ì¸ í‰ê°€ í”„ë¡œì„¸ìŠ¤"""
    
    try:
        # 1. í‰ê°€ ë°ì´í„° ì¤€ë¹„
        df = prepare_evaluation_data(Config.EVAL_DATA_FILE)
        
        # 2. ì‹œí€€ìŠ¤ ìƒì„±
        X, y, df_processed = create_sequences(df, Config.LOOKBACK, Config.FORECAST)
        
        if len(X) == 0:
            print("\nâŒ ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨ - ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            print(f"   í•„ìš”í•œ ìµœì†Œ ë°ì´í„°: {Config.LOOKBACK + Config.FORECAST}í–‰")
            print(f"   í˜„ì¬ ë°ì´í„°: {len(df)}í–‰")
            return
        
        # 3. M14 íŠ¹ì§• ì¶”ì¶œ
        print("\nğŸ“Š M14 íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        m14_features = np.zeros((len(X), 4), dtype=np.float32)
        
        for i in range(len(X)):
            idx = i + Config.LOOKBACK
            if idx < len(df_processed):
                m14_features[i] = [
                    df_processed['M14AM14B'].iloc[idx],
                    df_processed['M14AM10A'].iloc[idx],
                    df_processed['M14AM16'].iloc[idx],
                    df_processed['ratio_14B_10A'].iloc[idx]
                ]
        
        # 4. ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        print("\nğŸ“ ë°ì´í„° ìŠ¤ì¼€ì¼ë§...")
        
        X_scaled = np.zeros_like(X)
        for i in range(X.shape[2]):
            scaler = RobustScaler()
            feature = X[:, :, i].reshape(-1, 1)
            X_scaled[:, :, i] = scaler.fit_transform(feature).reshape(X[:, :, i].shape)
        
        m14_scaler = RobustScaler()
        m14_features_scaled = m14_scaler.fit_transform(m14_features)
        
        print("  âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
        
        # 5. ëª¨ë¸ ë¡œë“œ
        models = load_models()
        
        if not models:
            print("\nâŒ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return
        
        # 6. ëª¨ë¸ í‰ê°€
        results, predictions = evaluate_models(models, X_scaled, y, m14_features_scaled)
        
        # 7. ì‹œê°í™”
        if results and predictions:
            create_visualizations(y, predictions, results)
        
        # 8. ìƒì„¸ ë¦¬í¬íŠ¸
        if results:
            report = create_detailed_report(results, y)
        
        # 9. ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ“Š í‰ê°€ ì™„ë£Œ!")
        print("="*60)
        
        # ìµœê³  ëª¨ë¸ ê°•ì¡°
        if results:
            best_model = min(results.keys(), key=lambda x: results[x]['mae'])
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model.upper()}")
            print(f"  - MAE: {results[best_model]['mae']:.2f}")
            print(f"  - RMSE: {results[best_model]['rmse']:.2f}")
            print(f"  - ì •í™•ë„(Â±50): {results[best_model]['accuracy_50']:.1f}%")
            print(f"  - ì •í™•ë„(Â±100): {results[best_model]['accuracy_100']:.1f}%")
            
            # ê¸‰ì¦ ê°ì§€ ì„±ëŠ¥
            if 1400 in results[best_model]['levels']:
                perf_1400 = results[best_model]['levels'][1400]
                print(f"\n  1400+ ê¸‰ì¦ ê°ì§€:")
                print(f"    - Recall: {perf_1400['recall']:.1%}")
                print(f"    - Precision: {perf_1400['precision']:.1%}")
                print(f"    - F1 Score: {perf_1400['f1']:.1%}")
        
        print("\nğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜:")
        print(f"  - ë¦¬í¬íŠ¸: {Config.EVAL_RESULT_DIR}evaluation_report.txt")
        print(f"  - JSON: {Config.EVAL_RESULT_DIR}evaluation_results.json")
        print(f"  - ì‹œê°í™”: {Config.PLOT_DIR}model_evaluation_summary.png")
        
        print("\nâœ… ëª¨ë“  í‰ê°€ ì‘ì—… ì™„ë£Œ!")
        print("="*60)
        
    except FileNotFoundError:
        print(f"\nâŒ í‰ê°€ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {Config.EVAL_DATA_FILE}")
        print("íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()