"""
V6 ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ - TensorFlow 2.16.1 ì „ìš©
- Keras 3.0 í˜¸í™˜
- ëª¨ë¸ ì¬ìƒì„± í›„ ìˆ˜ë™ ì˜ˆì¸¡
- ê·œì¹™ ê¸°ë°˜ í‰ê°€ í¬í•¨
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import RobustScaler
import matplotlib.pyplot as plt
import json
import warnings
from datetime import datetime
import os
warnings.filterwarnings('ignore')

print("="*60)
print("ğŸ“Š V6 ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ - TF 2.16.1 ì „ìš©")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print(f"ğŸ“¦ Keras: {tf.keras.__version__}")
print("="*60)

# ============================================
# ì„¤ì •
# ============================================
class Config:
    # í‰ê°€ ë°ì´í„° íŒŒì¼
    EVAL_DATA_FILE = './data/20250731_to20250806.CSV'
    
    # í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
    MODEL_DIR = './models_v6_full_train/'
    
    # ì‹œí€€ìŠ¤ ì„¤ì •
    LOOKBACK = 100  # ê³¼ê±° 100ë¶„ ë°ì´í„°
    FORECAST = 10   # 10ë¶„ í›„ ì˜ˆì¸¡
    
    # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    EVAL_RESULT_DIR = './evaluation_results/'
    PLOT_DIR = './evaluation_plots/'
    
    # ë°°ì¹˜ í¬ê¸°
    BATCH_SIZE = 32
    
    # íŠ¹ì§• ê°œìˆ˜
    NUM_FEATURES = 47  # í•™ìŠµ ì‹œ ì‚¬ìš©í•œ íŠ¹ì§• ê°œìˆ˜

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(Config.EVAL_RESULT_DIR, exist_ok=True)
os.makedirs(Config.PLOT_DIR, exist_ok=True)

# ============================================
# TensorFlow 2.16.1ìš© ëª¨ë¸ ì¬ìƒì„±
# ============================================
def create_models_tf216():
    """TensorFlow 2.16.1ìš© ëª¨ë¸ êµ¬ì¡° ìƒì„±"""
    
    models = {}
    
    # 1. LSTM ëª¨ë¸
    lstm = tf.keras.Sequential([
        tf.keras.layers.InputLayer(shape=(Config.LOOKBACK, Config.NUM_FEATURES)),
        tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.2),
        tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2),
        tf.keras.layers.LSTM(64, dropout=0.2),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(1)
    ], name='LSTM_Model_v216')
    models['lstm'] = lstm
    
    # 2. GRU ëª¨ë¸
    gru = tf.keras.Sequential([
        tf.keras.layers.InputLayer(shape=(Config.LOOKBACK, Config.NUM_FEATURES)),
        tf.keras.layers.GRU(256, return_sequences=True, dropout=0.15),
        tf.keras.layers.GRU(128, return_sequences=True, dropout=0.15),
        tf.keras.layers.GRU(64, dropout=0.15),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(1)
    ], name='GRU_Model_v216')
    models['gru'] = gru
    
    print("âœ… TF 2.16.1ìš© ëª¨ë¸ êµ¬ì¡° ìƒì„± ì™„ë£Œ")
    
    return models

# ============================================
# ê°€ì¤‘ì¹˜ ì¶”ì¶œ ë° ì ìš© ì‹œë„
# ============================================
def try_load_weights(models):
    """ì €ì¥ëœ ëª¨ë¸ì—ì„œ ê°€ì¤‘ì¹˜ ì¶”ì¶œ ì‹œë„"""
    
    loaded_models = {}
    
    for name, model in models.items():
        weight_files = [
            f"{Config.MODEL_DIR}{name}_final.keras",
            f"{Config.MODEL_DIR}{name}_best.keras",
            f"{Config.MODEL_DIR}{name}.weights.h5",
        ]
        
        loaded = False
        for weight_file in weight_files:
            if os.path.exists(weight_file):
                try:
                    # ë°©ë²• 1: ì§ì ‘ ê°€ì¤‘ì¹˜ ë¡œë“œ
                    model.load_weights(weight_file)
                    loaded_models[name] = model
                    loaded = True
                    print(f"âœ… {name} ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ")
                    break
                except:
                    try:
                        # ë°©ë²• 2: ë ˆì´ì–´ë³„ ê°€ì¤‘ì¹˜ ë³µì‚¬
                        temp_model = tf.keras.models.load_model(weight_file, compile=False)
                        for i, layer in enumerate(model.layers):
                            if i < len(temp_model.layers):
                                try:
                                    layer.set_weights(temp_model.layers[i].get_weights())
                                except:
                                    pass
                        loaded_models[name] = model
                        loaded = True
                        print(f"âœ… {name} ê°€ì¤‘ì¹˜ ë¶€ë¶„ ë¡œë“œ")
                        break
                    except:
                        pass
        
        if not loaded:
            print(f"âš ï¸ {name} ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨ - ëœë¤ ì´ˆê¸°í™” ì‚¬ìš©")
            loaded_models[name] = model
    
    return loaded_models

# ============================================
# ê°•ë ¥í•œ ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡ê¸°
# ============================================
class RuleBasedPredictor:
    """M14 ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡ê¸°"""
    
    def __init__(self):
        self.thresholds = {
            'M14B': [250, 300, 350, 400, 450],
            'predictions': [1350, 1400, 1450, 1500, 1550],
            'ratios': [3.0, 4.0, 4.5, 5.0, 5.5],
            'adjustments': [1.02, 1.05, 1.08, 1.10, 1.15]
        }
    
    def predict(self, X, m14_features):
        """ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡"""
        predictions = []
        
        for i in range(len(X)):
            # ìµœê·¼ ê°’ë“¤
            recent_values = X[i, -20:, 0]  # ìµœê·¼ 20ê°œ TOTALCNT
            current_value = recent_values[-1]
            recent_avg = np.mean(recent_values)
            recent_trend = np.polyfit(range(len(recent_values)), recent_values, 1)[0]
            
            # M14 íŠ¹ì§•
            m14b = m14_features[i, 0] if m14_features.shape[1] > 0 else 0
            m10a = m14_features[i, 1] if m14_features.shape[1] > 1 else 1
            m16 = m14_features[i, 2] if m14_features.shape[1] > 2 else 0
            
            # ê¸°ë³¸ ì˜ˆì¸¡ (íŠ¸ë Œë“œ ê¸°ë°˜)
            base_pred = recent_avg + recent_trend * Config.FORECAST
            
            # M14B ì„ê³„ê°’ ê¸°ë°˜ ì¡°ì •
            for j, threshold in enumerate(self.thresholds['M14B']):
                if m14b >= threshold:
                    base_pred = max(base_pred, self.thresholds['predictions'][j])
            
            # ë¹„ìœ¨ ê¸°ë°˜ ì¡°ì •
            if m10a > 0:
                ratio = m14b / m10a
                for j, ratio_threshold in enumerate(self.thresholds['ratios']):
                    if ratio >= ratio_threshold:
                        base_pred *= self.thresholds['adjustments'][j]
            
            # í™©ê¸ˆ íŒ¨í„´ (M14B ë†’ê³  M10A ë‚®ìŒ)
            if m14b >= 350 and m10a < 70:
                base_pred *= 1.2
            
            # ì•ˆì •í™”
            base_pred = np.clip(base_pred, 1200, 2000)
            
            predictions.append(base_pred)
        
        return np.array(predictions)

# ============================================
# ë°ì´í„° ì „ì²˜ë¦¬
# ============================================
def prepare_data(file_path):
    """ë°ì´í„° ì¤€ë¹„"""
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë“œ: {file_path}")
    
    df = pd.read_csv(file_path)
    print(f"  ì›ë³¸: {len(df)}í–‰")
    
    # í•„ìˆ˜ ì»¬ëŸ¼
    required = ['M14AM10A', 'M14AM14B', 'M14AM16', 'TOTALCNT']
    for col in required:
        if col not in df.columns:
            df[col] = 0
    
    if 'M14AM14BSUM' not in df.columns:
        df['M14AM14BSUM'] = df['M14AM14B'] + df['M14AM10A']
    
    # íŠ¹ì§• ìƒì„±
    print("  íŠ¹ì§• ìƒì„± ì¤‘...")
    
    # ë¹„ìœ¨
    df['ratio_14B_10A'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
    df['ratio_14B_16'] = df['M14AM14B'] / (df['M14AM16'] + 1)
    
    # ì‹œê³„ì—´
    for col in ['TOTALCNT', 'M14AM14B', 'M14AM10A', 'M14AM16']:
        if col in df.columns:
            for period in [1, 5, 10]:
                df[f'{col}_diff_{period}'] = df[col].diff(period)
            for window in [5, 10, 20]:
                df[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()
                df[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std()
    
    # ì‹ í˜¸
    df['golden'] = ((df['M14AM14B'] >= 350) & (df['M14AM10A'] < 70)).astype(float)
    
    for t in [250, 300, 350, 400, 450]:
        df[f'sig_{t}'] = (df['M14AM14B'] >= t).astype(float)
    
    df = df.fillna(0)
    
    # íŠ¹ì§• ê°œìˆ˜ ë§ì¶”ê¸°
    if len(df.columns) > Config.NUM_FEATURES:
        df = df.iloc[:, :Config.NUM_FEATURES]
    elif len(df.columns) < Config.NUM_FEATURES:
        for i in range(Config.NUM_FEATURES - len(df.columns)):
            df[f'pad_{i}'] = 0
    
    print(f"  ìµœì¢…: {len(df.columns)}ê°œ íŠ¹ì§•")
    
    return df

def create_sequences(df):
    """ì‹œí€€ìŠ¤ ìƒì„±"""
    X, y = [], []
    
    data = df.values
    
    for i in range(len(data) - Config.LOOKBACK - Config.FORECAST):
        X.append(data[i:i+Config.LOOKBACK])
        # íƒ€ê²Ÿ: 10ë¶„ í›„ TOTALCNT
        y.append(data[i+Config.LOOKBACK+Config.FORECAST-1, 3])  # TOTALCNT ì¸ë±ìŠ¤
    
    X = np.array(X, dtype=np.float32)
    y = np.array(y, dtype=np.float32)
    
    return X, y

# ============================================
# í‰ê°€ í•¨ìˆ˜
# ============================================
def evaluate(y_true, y_pred, name):
    """ì„±ëŠ¥ í‰ê°€"""
    mae = np.mean(np.abs(y_true - y_pred))
    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))
    
    acc_50 = np.mean(np.abs(y_true - y_pred) <= 50) * 100
    acc_100 = np.mean(np.abs(y_true - y_pred) <= 100) * 100
    
    # ê¸‰ì¦ ê°ì§€ ì„±ëŠ¥
    spike_levels = [1400, 1450, 1500]
    spike_performance = {}
    
    for level in spike_levels:
        actual_spike = y_true >= level
        pred_spike = y_pred >= level
        
        if np.sum(actual_spike) > 0:
            recall = np.sum(actual_spike & pred_spike) / np.sum(actual_spike)
            if np.sum(pred_spike) > 0:
                precision = np.sum(actual_spike & pred_spike) / np.sum(pred_spike)
                f1 = 2 * (precision * recall) / (precision + recall + 1e-10)
            else:
                precision = 0
                f1 = 0
            
            spike_performance[level] = {
                'recall': recall * 100,
                'precision': precision * 100,
                'f1': f1 * 100
            }
    
    print(f"\nğŸ“Š {name} ì„±ëŠ¥:")
    print(f"  MAE: {mae:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  ì •í™•ë„(Â±50): {acc_50:.1f}%")
    print(f"  ì •í™•ë„(Â±100): {acc_100:.1f}%")
    
    for level, perf in spike_performance.items():
        print(f"  {level}+ ê°ì§€: F1={perf['f1']:.1f}%")
    
    return {
        'mae': mae,
        'rmse': rmse,
        'acc_50': acc_50,
        'acc_100': acc_100,
        'spike': spike_performance
    }

# ============================================
# ì‹œê°í™”
# ============================================
def visualize_results(y_true, predictions, results):
    """ê²°ê³¼ ì‹œê°í™”"""
    
    plt.figure(figsize=(15, 10))
    
    # 1. ì˜ˆì¸¡ vs ì‹¤ì œ
    plt.subplot(2, 3, 1)
    best_model = min(results.keys(), key=lambda x: results[x]['mae'])
    plt.scatter(y_true[:500], predictions[best_model][:500], alpha=0.5)
    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')
    plt.xlabel('Actual')
    plt.ylabel('Predicted')
    plt.title(f'{best_model} - Predictions')
    
    # 2. MAE ë¹„êµ
    plt.subplot(2, 3, 2)
    names = list(results.keys())
    maes = [results[n]['mae'] for n in names]
    plt.bar(names, maes)
    plt.ylabel('MAE')
    plt.title('Model Comparison')
    plt.xticks(rotation=45)
    
    # 3. ì‹œê³„ì—´
    plt.subplot(2, 3, 3)
    sample = 200
    plt.plot(y_true[:sample], label='Actual', linewidth=2)
    plt.plot(predictions[best_model][:sample], label=f'{best_model}', alpha=0.7)
    plt.legend()
    plt.title('Time Series')
    
    # 4. ì˜¤ì°¨ ë¶„í¬
    plt.subplot(2, 3, 4)
    errors = predictions[best_model] - y_true
    plt.hist(errors, bins=50, alpha=0.7)
    plt.axvline(0, color='red', linestyle='--')
    plt.xlabel('Error')
    plt.ylabel('Frequency')
    plt.title('Error Distribution')
    
    # 5. ì •í™•ë„
    plt.subplot(2, 3, 5)
    acc_50 = [results[n]['acc_50'] for n in names]
    acc_100 = [results[n]['acc_100'] for n in names]
    x = np.arange(len(names))
    plt.bar(x - 0.2, acc_50, 0.4, label='Â±50')
    plt.bar(x + 0.2, acc_100, 0.4, label='Â±100')
    plt.xticks(x, names, rotation=45)
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.title('Prediction Accuracy')
    
    # 6. ê¸‰ì¦ ê°ì§€
    plt.subplot(2, 3, 6)
    if 1400 in results[best_model].get('spike', {}):
        f1_scores = []
        for name in names:
            if 'spike' in results[name] and 1400 in results[name]['spike']:
                f1_scores.append(results[name]['spike'][1400]['f1'])
            else:
                f1_scores.append(0)
        plt.bar(names, f1_scores)
        plt.ylabel('F1 Score (%)')
        plt.title('1400+ Spike Detection')
        plt.xticks(rotation=45)
    
    plt.tight_layout()
    save_path = f'{Config.PLOT_DIR}evaluation_tf216.png'
    plt.savefig(save_path, dpi=150)
    print(f"\nğŸ“ˆ ì‹œê°í™” ì €ì¥: {save_path}")
    plt.close()

# ============================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================
def main():
    print("\nğŸš€ í‰ê°€ ì‹œì‘...")
    
    try:
        # 1. ë°ì´í„° ì¤€ë¹„
        df = prepare_data(Config.EVAL_DATA_FILE)
        X, y = create_sequences(df)
        
        print(f"\nğŸ“Š ë°ì´í„° shape:")
        print(f"  X: {X.shape}")
        print(f"  y: {y.shape}")
        print(f"  y ë²”ìœ„: {y.min():.0f} ~ {y.max():.0f}")
        
        # 2. M14 íŠ¹ì§• ì¶”ì¶œ
        m14_features = np.zeros((len(X), 4))
        m14_features[:, 0] = X[:, -1, 1]  # M14AM14B (ë§ˆì§€ë§‰ ì‹œì )
        m14_features[:, 1] = X[:, -1, 0]  # M14AM10A
        m14_features[:, 2] = X[:, -1, 2]  # M14AM16
        m14_features[:, 3] = X[:, -1, 1] / (X[:, -1, 0] + 1)  # ë¹„ìœ¨
        
        # 3. ìŠ¤ì¼€ì¼ë§
        print("\nğŸ“ ìŠ¤ì¼€ì¼ë§...")
        X_scaled = np.zeros_like(X)
        for i in range(X.shape[2]):
            scaler = RobustScaler()
            feature = X[:, :, i].reshape(-1, 1)
            X_scaled[:, :, i] = scaler.fit_transform(feature).reshape(X[:, :, i].shape)
        
        # 4. ì˜ˆì¸¡
        predictions = {}
        results = {}
        
        # 4-1. ë”¥ëŸ¬ë‹ ëª¨ë¸ ì‹œë„
        print("\nğŸ¤– ë”¥ëŸ¬ë‹ ëª¨ë¸ ì˜ˆì¸¡ ì‹œë„...")
        models = create_models_tf216()
        loaded_models = try_load_weights(models)
        
        for name, model in loaded_models.items():
            try:
                pred = model.predict(X_scaled, batch_size=Config.BATCH_SIZE, verbose=0)
                predictions[name] = pred.flatten()
                results[name] = evaluate(y, predictions[name], name)
            except Exception as e:
                print(f"  âŒ {name} ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)[:50]}")
        
        # 4-2. ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡ (í•­ìƒ ì‹¤í–‰)
        print("\nğŸ“Š ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡...")
        rule_predictor = RuleBasedPredictor()
        rule_pred = rule_predictor.predict(X, m14_features)
        predictions['rule_based'] = rule_pred
        results['rule_based'] = evaluate(y, rule_pred, 'Rule-Based')
        
        # 4-3. ë‹¨ìˆœ í‰ê·  ì˜ˆì¸¡ (ë² ì´ìŠ¤ë¼ì¸)
        print("\nğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ì˜ˆì¸¡...")
        baseline_pred = np.array([np.mean(X[i, -10:, 3]) for i in range(len(X))])
        predictions['baseline'] = baseline_pred
        results['baseline'] = evaluate(y, baseline_pred, 'Baseline')
        
        # 5. ìµœê³  ëª¨ë¸ ë° ìƒì„¸ ì„±ëŠ¥ ì¶œë ¥
        if results:
            best = min(results.keys(), key=lambda x: results[x]['mae'])
            print("\n" + "="*60)
            print(f"ğŸ† ìµœê³  ì„±ëŠ¥: {best.upper()}")
            print(f"   MAE: {results[best]['mae']:.2f}")
            print(f"   RMSE: {results[best]['rmse']:.2f}")
            print(f"   ì •í™•ë„(Â±50): {results[best]['acc_50']:.1f}%")
            print(f"   ì •í™•ë„(Â±100): {results[best]['acc_100']:.1f}%")
            print("="*60)
            
            # ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
            print("\nğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ:")
            print("-"*60)
            print(f"{'ëª¨ë¸':<15} {'MAE':<10} {'RMSE':<10} {'RÂ²':<10} {'Â±50':<10} {'Â±100':<10}")
            print("-"*60)
            
            for name, result in results.items():
                # RÂ² ê³„ì‚°
                ss_res = np.sum((y - predictions[name]) ** 2)
                ss_tot = np.sum((y - np.mean(y)) ** 2)
                r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
                
                print(f"{name:<15} {result['mae']:<10.2f} {result['rmse']:<10.2f} "
                      f"{r2:<10.3f} {result['acc_50']:<10.1f} {result['acc_100']:<10.1f}")
            print("-"*60)
        
        # 6. ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
        print("\nğŸ” ì˜ˆì¸¡ ìƒ˜í”Œ (ì²˜ìŒ 20ê°œ):")
        print("-"*80)
        print(f"{'Index':<8} {'ì‹¤ì œê°’':<10} {'Ruleì˜ˆì¸¡':<10} {'Baseline':<10} {'ì˜¤ì°¨(Rule)':<12} {'ì •í™•ë„':<10}")
        print("-"*80)
        
        for i in range(min(20, len(y))):
            actual = y[i]
            rule_pred = predictions['rule_based'][i]
            base_pred = predictions['baseline'][i]
            error = rule_pred - actual
            accuracy = "âœ…" if abs(error) <= 50 else "âš ï¸" if abs(error) <= 100 else "âŒ"
            
            print(f"{i:<8} {actual:<10.0f} {rule_pred:<10.0f} {base_pred:<10.0f} "
                  f"{error:<12.1f} {accuracy:<10}")
        
        # 7. CSV íŒŒì¼ë¡œ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        print("\nğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥ ì¤‘...")
        
        # ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ DataFrame ìƒì„±
        results_df = pd.DataFrame({
            'ì‹¤ì œê°’': y,
            'Rule_Based_ì˜ˆì¸¡': predictions['rule_based'],
            'Baseline_ì˜ˆì¸¡': predictions['baseline'],
            'Rule_ì˜¤ì°¨': predictions['rule_based'] - y,
            'Baseline_ì˜¤ì°¨': predictions['baseline'] - y,
            'Rule_ì ˆëŒ€ì˜¤ì°¨': np.abs(predictions['rule_based'] - y),
            'Baseline_ì ˆëŒ€ì˜¤ì°¨': np.abs(predictions['baseline'] - y)
        })
        
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ ì˜ˆì¸¡ê°’ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        for name in ['lstm', 'gru']:
            if name in predictions:
                results_df[f'{name.upper()}_ì˜ˆì¸¡'] = predictions[name]
                results_df[f'{name.upper()}_ì˜¤ì°¨'] = predictions[name] - y
        
        # í†µê³„ ì¶”ê°€
        results_df['50ì´ë‚´_ì •í™•'] = results_df['Rule_ì ˆëŒ€ì˜¤ì°¨'] <= 50
        results_df['100ì´ë‚´_ì •í™•'] = results_df['Rule_ì ˆëŒ€ì˜¤ì°¨'] <= 100
        
        # CSV ì €ì¥
        csv_path = f'{Config.EVAL_RESULT_DIR}prediction_results.csv'
        results_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"  âœ… CSV ì €ì¥ ì™„ë£Œ: {csv_path}")
        
        # ìš”ì•½ í†µê³„
        print("\nğŸ“ˆ ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:")
        print(f"  ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(results_df):,}ê°œ")
        print(f"  Rule-Based MAE: {results_df['Rule_ì ˆëŒ€ì˜¤ì°¨'].mean():.2f}")
        print(f"  Baseline MAE: {results_df['Baseline_ì ˆëŒ€ì˜¤ì°¨'].mean():.2f}")
        print(f"  50 ì´ë‚´ ì •í™•ë„: {results_df['50ì´ë‚´_ì •í™•'].sum():,}ê°œ ({results_df['50ì´ë‚´_ì •í™•'].mean()*100:.1f}%)")
        print(f"  100 ì´ë‚´ ì •í™•ë„: {results_df['100ì´ë‚´_ì •í™•'].sum():,}ê°œ ({results_df['100ì´ë‚´_ì •í™•'].mean()*100:.1f}%)")
        
        # ê¸‰ì¦ êµ¬ê°„ ë¶„ì„
        print("\nğŸ¯ ê¸‰ì¦ êµ¬ê°„(1400+) ì˜ˆì¸¡ ì„±ëŠ¥:")
        spike_mask = y >= 1400
        if spike_mask.sum() > 0:
            spike_actual = y[spike_mask]
            spike_rule = predictions['rule_based'][spike_mask]
            spike_mae = np.mean(np.abs(spike_actual - spike_rule))
            spike_detected = (spike_rule >= 1400).sum()
            
            print(f"  ì‹¤ì œ ê¸‰ì¦ íšŸìˆ˜: {spike_mask.sum()}íšŒ")
            print(f"  ì˜ˆì¸¡ ì„±ê³µ: {spike_detected}íšŒ ({spike_detected/spike_mask.sum()*100:.1f}%)")
            print(f"  ê¸‰ì¦ êµ¬ê°„ MAE: {spike_mae:.2f}")
        
        # 8. ê²°ê³¼ JSON ì €ì¥
        with open(f'{Config.EVAL_RESULT_DIR}results_tf216.json', 'w') as f:
            # RÂ² ì¶”ê°€
            for name in results:
                ss_res = np.sum((y - predictions[name]) ** 2)
                ss_tot = np.sum((y - np.mean(y)) ** 2)
                results[name]['r2'] = float(1 - (ss_res / ss_tot)) if ss_tot > 0 else 0
            
            json.dump(results, f, indent=2, default=float)
        
        # 9. ì‹œê°í™”
        if len(predictions) > 0:
            visualize_results(y, predictions, results)
        
        print("\nâœ… í‰ê°€ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼: {Config.EVAL_RESULT_DIR}")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()