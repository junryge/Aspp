"""
V6 ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ
- 6ê°œ ëª¨ë¸ (LSTM, GRU, CNN-LSTM, Spike, Rule-Based, Ensemble) í‰ê°€
- í‰ê°€ ë°ì´í„°: 20250731_to_20250826.csv
- ë‚ ì§œë³„, ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’ ë° ì„±ëŠ¥ ì§€í‘œ ìƒì„±
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import warnings
import json
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import gc
warnings.filterwarnings('ignore')

print("="*60)
print("ğŸ”¬ V6 ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print("="*60)

# ============================================
# GPU ì„¤ì •
# ============================================
def setup_gpu():
    """GPU ì„¤ì • ë° í™•ì¸"""
    print("\nğŸ® GPU í™˜ê²½ í™•ì¸...")
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"âœ… GPU ê°ì§€: {len(gpus)}ê°œ")
            return True
        except Exception as e:
            print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
            return False
    else:
        print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        return False

has_gpu = setup_gpu()

# ============================================
# ì„¤ì •
# ============================================
class Config:
    # í‰ê°€ ë°ì´í„°
    EVAL_DATA_FILE = '/mnt/user-data/uploads/20250731_to_20250826.csv'
    
    # ëª¨ë¸ ë””ë ‰í† ë¦¬
    MODEL_DIR = './models_v6_full_train/'
    SCALER_FILE = './scalers_v6_gpu.pkl'
    
    # ì‹œí€€ìŠ¤ ì„¤ì • (í•™ìŠµê³¼ ë™ì¼)
    LOOKBACK = 100  # ê³¼ê±° 100ë¶„
    FORECAST = 10   # 10ë¶„ í›„ ì˜ˆì¸¡
    
    # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    OUTPUT_DIR = './evaluation_results/'
    
    # í‰ê°€ ì„¤ì •
    BATCH_SIZE = 128  # í‰ê°€ì‹œ ë°°ì¹˜ í¬ê¸°
    
import os
os.makedirs(Config.OUTPUT_DIR, exist_ok=True)

# ============================================
# M14 ê·œì¹™ ë³´ì • ë ˆì´ì–´ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼)
# ============================================
class M14RuleCorrection(tf.keras.layers.Layer):
    """M14 ê·œì¹™ ê¸°ë°˜ ë³´ì •"""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    
    def call(self, inputs, training=None):
        pred, m14_features = inputs
        
        pred = tf.cast(pred, tf.float32)
        m14_features = tf.cast(m14_features, tf.float32)
        
        m14b = m14_features[:, 0:1]
        m10a = m14_features[:, 1:2]
        m16 = m14_features[:, 2:3]
        ratio = m14_features[:, 3:4]
        
        # ì„ê³„ê°’ ê·œì¹™
        pred = tf.where(m14b >= 420, tf.maximum(pred, 1550.0), pred)
        pred = tf.where(m14b >= 380, tf.maximum(pred, 1500.0), pred)
        pred = tf.where(m14b >= 350, tf.maximum(pred, 1450.0), pred)
        pred = tf.where(m14b >= 300, tf.maximum(pred, 1400.0), pred)
        
        # ë¹„ìœ¨ ë³´ì •
        pred = tf.where(ratio >= 5.5, pred * 1.15, pred)
        pred = tf.where((ratio >= 5.0) & (ratio < 5.5), pred * 1.10, pred)
        pred = tf.where((ratio >= 4.5) & (ratio < 5.0), pred * 1.08, pred)
        pred = tf.where((ratio >= 4.0) & (ratio < 4.5), pred * 1.05, pred)
        
        # í™©ê¸ˆ íŒ¨í„´
        golden = (m14b >= 350) & (m10a < 70)
        pred = tf.where(golden, pred * 1.2, pred)
        
        # ë²”ìœ„ ì œí•œ
        pred = tf.clip_by_value(pred, 1200.0, 2000.0)
        
        return pred

# ============================================
# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ============================================
print("\nğŸ“Š í‰ê°€ ë°ì´í„° ë¡œë“œ ì¤‘...")

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv(Config.EVAL_DATA_FILE)
print(f"  ë°ì´í„° í¬ê¸°: {len(df)}í–‰")

# ì‹œê°„ ì •ë³´ íŒŒì‹± (ìˆë‹¤ë©´)
if 'CURRTIME' in df.columns:
    try:
        df['datetime'] = pd.to_datetime(df['CURRTIME'], format='%Y%m%d%H%M')
    except:
        df['datetime'] = pd.to_datetime(df['CURRTIME'], format='%Y%m%d%H%M%S')
elif 'TIME' in df.columns:
    try:
        df['datetime'] = pd.to_datetime(df['TIME'], format='%Y%m%d%H%M')
    except:
        df['datetime'] = pd.to_datetime(df['TIME'], format='%Y%m%d%H%M%S')
else:
    # ì¸ë±ìŠ¤ë¥¼ ì‹œê°„ìœ¼ë¡œ ì‚¬ìš©
    df['datetime'] = pd.date_range(start='2025-07-31', periods=len(df), freq='1min')

# í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸ ë° ìƒì„±
required_columns = ['M14AM10A', 'M14AM14B', 'M14AM16', 'TOTALCNT']
for col in required_columns:
    if col not in df.columns:
        print(f"âš ï¸ {col} ì»¬ëŸ¼ ì—†ìŒ - 0ìœ¼ë¡œ ì´ˆê¸°í™”")
        df[col] = 0

# M14AM14BSUMì´ ì—†ìœ¼ë©´ ìƒì„±
if 'M14AM14BSUM' not in df.columns:
    df['M14AM14BSUM'] = df['M14AM14B'] + df['M14AM10A']

print(f"  ë‚ ì§œ ë²”ìœ„: {df['datetime'].min()} ~ {df['datetime'].max()}")
print(f"  TOTALCNT ë²”ìœ„: {df['TOTALCNT'].min():.0f} ~ {df['TOTALCNT'].max():.0f}")

# ============================================
# íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ (í•™ìŠµê³¼ ë™ì¼í•˜ê²Œ)
# ============================================
print("\nğŸ”§ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§...")

# ë¹„ìœ¨ íŠ¹ì§•
df['ratio_14B_10A'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
df['ratio_14B_16'] = df['M14AM14B'] / (df['M14AM16'] + 1)
df['ratio_10A_16'] = df['M14AM10A'] / (df['M14AM16'] + 1)

# ì‹œê³„ì—´ íŠ¹ì§•
feature_columns = ['M14AM10A', 'M14AM14B', 'M14AM16', 'M14AM14BSUM', 'TOTALCNT']
for col in feature_columns:
    # ë³€í™”ëŸ‰
    df[f'{col}_diff_1'] = df[col].diff(1)
    df[f'{col}_diff_5'] = df[col].diff(5)
    df[f'{col}_diff_10'] = df[col].diff(10)
    
    # ì´ë™í‰ê· 
    df[f'{col}_ma_5'] = df[col].rolling(5, min_periods=1).mean()
    df[f'{col}_ma_10'] = df[col].rolling(10, min_periods=1).mean()
    df[f'{col}_ma_20'] = df[col].rolling(20, min_periods=1).mean()
    
    # í‘œì¤€í¸ì°¨
    df[f'{col}_std_5'] = df[col].rolling(5, min_periods=1).std()
    df[f'{col}_std_10'] = df[col].rolling(10, min_periods=1).std()

# í™©ê¸ˆ íŒ¨í„´
df['golden_pattern'] = ((df['M14AM14B'] >= 350) & (df['M14AM10A'] < 70)).astype(float)

# ê¸‰ì¦ ì‹ í˜¸
thresholds = {1300: 250, 1400: 300, 1450: 350, 1500: 380, 1550: 420}
for level, threshold in thresholds.items():
    df[f'signal_{level}'] = (df['M14AM14B'] >= threshold).astype(float)

ratio_thresholds = {1300: 3.5, 1400: 4.0, 1450: 4.5, 1500: 5.0, 1550: 5.5}
for level, ratio in ratio_thresholds.items():
    df[f'ratio_signal_{level}'] = (df['ratio_14B_10A'] >= ratio).astype(float)

df = df.fillna(0)

print(f"  íŠ¹ì§• ê°œìˆ˜: {len(df.columns)}ê°œ")

# ============================================
# ì‹œí€€ìŠ¤ ìƒì„±
# ============================================
print("\nâš¡ í‰ê°€ìš© ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")

def create_sequences_with_info(df, lookback=100, forecast=10):
    """ì‹œí€€ìŠ¤ ìƒì„± (ë‚ ì§œ ì •ë³´ í¬í•¨)"""
    X, y = [], []
    dates = []
    m14_features = []
    
    # íŠ¹ì§• ì»¬ëŸ¼ë“¤ (datetime ì œì™¸)
    feature_cols = [col for col in df.columns if col != 'datetime']
    data = df[feature_cols].values
    
    for i in range(len(data) - lookback - forecast):
        X.append(data[i:i+lookback])
        y.append(df['TOTALCNT'].iloc[i+lookback+forecast-1])
        dates.append(df['datetime'].iloc[i+lookback+forecast-1])
        
        # M14 íŠ¹ì§• (í˜„ì¬ ì‹œì )
        idx = i + lookback
        m14_features.append([
            df['M14AM14B'].iloc[idx],
            df['M14AM10A'].iloc[idx],
            df['M14AM16'].iloc[idx],
            df['ratio_14B_10A'].iloc[idx]
        ])
    
    return (np.array(X, dtype=np.float32), 
            np.array(y, dtype=np.float32),
            np.array(m14_features, dtype=np.float32),
            dates)

# ì‹œí€€ìŠ¤ ìƒì„±
X_eval, y_eval, m14_eval, dates_eval = create_sequences_with_info(
    df, Config.LOOKBACK, Config.FORECAST
)

print(f"  X shape: {X_eval.shape}")
print(f"  y shape: {y_eval.shape}")
print(f"  m14 shape: {m14_eval.shape}")
print(f"  í‰ê°€ ìƒ˜í”Œ ìˆ˜: {len(X_eval):,}ê°œ")

# ============================================
# ìŠ¤ì¼€ì¼ë§ (í•™ìŠµê³¼ ë™ì¼í•œ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©)
# ============================================
print("\nğŸ“ ë°ì´í„° ìŠ¤ì¼€ì¼ë§...")

try:
    # ê¸°ì¡´ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    with open(Config.SCALER_FILE, 'rb') as f:
        scalers = pickle.load(f)
    
    # X ìŠ¤ì¼€ì¼ë§
    X_scaled = np.zeros_like(X_eval)
    feature_scalers = scalers.get('feature_scalers', scalers)
    
    for i in range(X_eval.shape[2]):
        if f'feature_{i}' in feature_scalers:
            scaler = feature_scalers[f'feature_{i}']
            feature = X_eval[:, :, i].reshape(-1, 1)
            X_scaled[:, :, i] = scaler.transform(feature).reshape(X_eval[:, :, i].shape)
        else:
            # ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            scaler = RobustScaler()
            feature = X_eval[:, :, i].reshape(-1, 1)
            X_scaled[:, :, i] = scaler.fit_transform(feature).reshape(X_eval[:, :, i].shape)
    
    # M14 ìŠ¤ì¼€ì¼ë§
    m14_scaler = scalers.get('m14_scaler', None)
    if m14_scaler:
        m14_scaled = m14_scaler.transform(m14_eval)
    else:
        m14_scaler = RobustScaler()
        m14_scaled = m14_scaler.fit_transform(m14_eval)
    
    print("  âœ… ê¸°ì¡´ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
    
except:
    print("  âš ï¸ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ ì—†ìŒ - ìƒˆë¡œ ìƒì„±")
    # ìƒˆë¡œìš´ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ì²˜ë¦¬
    X_scaled = np.zeros_like(X_eval)
    for i in range(X_eval.shape[2]):
        scaler = RobustScaler()
        feature = X_eval[:, :, i].reshape(-1, 1)
        X_scaled[:, :, i] = scaler.fit_transform(feature).reshape(X_eval[:, :, i].shape)
    
    m14_scaler = RobustScaler()
    m14_scaled = m14_scaler.fit_transform(m14_eval)
    print("  âœ… ìƒˆ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")

# ============================================
# ëª¨ë¸ ë¡œë“œ
# ============================================
print("\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")

models = {}
model_names = ['lstm', 'gru', 'cnn_lstm', 'spike', 'rule', 'ensemble']

for name in model_names:
    model_path = f"{Config.MODEL_DIR}{name}_final.keras"
    try:
        # ì»¤ìŠ¤í…€ ê°ì²´ì™€ í•¨ê»˜ ë¡œë“œ
        model = tf.keras.models.load_model(
            model_path,
            custom_objects={'M14RuleCorrection': M14RuleCorrection}
        )
        models[name] = model
        print(f"  âœ… {name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"  âŒ {name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

print(f"\nğŸ“Š ë¡œë“œëœ ëª¨ë¸: {len(models)}ê°œ")

# ============================================
# ëª¨ë¸ë³„ ì˜ˆì¸¡
# ============================================
print("\nğŸ”® ëª¨ë¸ë³„ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")

predictions = {}
for name, model in models.items():
    print(f"  {name} ì˜ˆì¸¡ ì¤‘...")
    
    try:
        if name in ['rule', 'ensemble']:
            # Ruleê³¼ Ensembleì€ M14 íŠ¹ì§•ë„ í•„ìš”
            pred = model.predict(
                [X_scaled, m14_scaled], 
                batch_size=Config.BATCH_SIZE,
                verbose=0
            )
        else:
            # ë‚˜ë¨¸ì§€ ëª¨ë¸ë“¤
            pred = model.predict(
                X_scaled, 
                batch_size=Config.BATCH_SIZE,
                verbose=0
            )
        
        predictions[name] = pred.flatten()
        print(f"    âœ… ì™„ë£Œ - ì˜ˆì¸¡ê°’ ë²”ìœ„: {pred.min():.0f} ~ {pred.max():.0f}")
        
    except Exception as e:
        print(f"    âŒ ì‹¤íŒ¨: {e}")
        predictions[name] = np.zeros(len(y_eval))

# ============================================
# ì„±ëŠ¥ í‰ê°€
# ============================================
print("\nğŸ“Š ì„±ëŠ¥ í‰ê°€ ì¤‘...")

def calculate_metrics(y_true, y_pred, name="Model"):
    """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    
    # ì •í™•ë„ (í‰ê·  ë°±ë¶„ìœ¨ ì˜¤ì°¨ ê¸°ë°˜)
    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-7))) * 100
    accuracy = 100 - mape
    
    # êµ¬ê°„ë³„ ì„±ëŠ¥
    level_metrics = {}
    for level in [1300, 1400, 1450, 1500, 1550]:
        mask = y_true >= level
        if np.any(mask):
            recall = np.sum((y_pred >= level) & mask) / np.sum(mask)
            precision = np.sum((y_pred >= level) & mask) / max(np.sum(y_pred >= level), 1)
            f1 = 2 * (precision * recall) / max(precision + recall, 1e-7)
            
            level_metrics[f'{level}+'] = {
                'recall': recall,
                'precision': precision,
                'f1': f1,
                'count': np.sum(mask)
            }
    
    return {
        'MAE': mae,
        'RMSE': rmse,
        'R2': r2,
        'MAPE': mape,
        'Accuracy': accuracy,
        'levels': level_metrics
    }

# ëª¨ë¸ë³„ ì„±ëŠ¥ ê³„ì‚°
model_metrics = {}
for name, pred in predictions.items():
    metrics = calculate_metrics(y_eval, pred, name)
    model_metrics[name] = metrics
    
    print(f"\nğŸ“ˆ {name.upper()} ì„±ëŠ¥:")
    print(f"  MAE: {metrics['MAE']:.2f}")
    print(f"  RMSE: {metrics['RMSE']:.2f}")
    print(f"  RÂ²: {metrics['R2']:.4f}")
    print(f"  ì •í™•ë„: {metrics['Accuracy']:.2f}%")
    
    print("  êµ¬ê°„ë³„ F1 Score:")
    for level, level_metric in metrics['levels'].items():
        print(f"    {level}: {level_metric['f1']:.3f} "
              f"(Recall: {level_metric['recall']:.3f}, "
              f"Precision: {level_metric['precision']:.3f})")

# ============================================
# ê²°ê³¼ DataFrame ìƒì„±
# ============================================
print("\nğŸ“ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì¤‘...")

# 1. ìƒì„¸ ì˜ˆì¸¡ ê²°ê³¼ (ë‚ ì§œë³„)
results_df = pd.DataFrame({
    'ë‚ ì§œ': dates_eval,
    'ì‹¤ì œê°’': y_eval
})

# ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’ ì¶”ê°€
for name, pred in predictions.items():
    results_df[f'{name}_ì˜ˆì¸¡'] = pred
    results_df[f'{name}_ì˜¤ì°¨'] = np.abs(y_eval - pred)

# ì•™ìƒë¸”ì´ ìˆìœ¼ë©´ ìµœì¢… ì˜ˆì¸¡ìœ¼ë¡œ í‘œì‹œ
if 'ensemble' in predictions:
    results_df['ìµœì¢…_ì˜ˆì¸¡'] = predictions['ensemble']
    results_df['ìµœì¢…_ì˜¤ì°¨'] = np.abs(y_eval - predictions['ensemble'])

# 2. ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½
metrics_summary = []
for name, metrics in model_metrics.items():
    summary = {
        'ëª¨ë¸': name.upper(),
        'MAE': f"{metrics['MAE']:.2f}",
        'RMSE': f"{metrics['RMSE']:.2f}",
        'RÂ²': f"{metrics['R2']:.4f}",
        'ì •í™•ë„(%)': f"{metrics['Accuracy']:.2f}",
        '1400+_F1': f"{metrics['levels'].get('1400+', {}).get('f1', 0):.3f}",
        '1500+_F1': f"{metrics['levels'].get('1500+', {}).get('f1', 0):.3f}"
    }
    metrics_summary.append(summary)

metrics_df = pd.DataFrame(metrics_summary)

# ============================================
# ê²°ê³¼ ì €ì¥
# ============================================
print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")

# 1. ìƒì„¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ (CSV)
results_file = f"{Config.OUTPUT_DIR}prediction_results_detail.csv"
results_df.to_csv(results_file, index=False, encoding='utf-8-sig')
print(f"  âœ… ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_file}")

# 2. ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ ì €ì¥
metrics_file = f"{Config.OUTPUT_DIR}model_performance_summary.csv"
metrics_df.to_csv(metrics_file, index=False, encoding='utf-8-sig')
print(f"  âœ… ì„±ëŠ¥ ìš”ì•½ ì €ì¥: {metrics_file}")

# 3. JSON í˜•íƒœë¡œë„ ì €ì¥
metrics_json_file = f"{Config.OUTPUT_DIR}model_metrics.json"
with open(metrics_json_file, 'w', encoding='utf-8') as f:
    json.dump(model_metrics, f, indent=2, ensure_ascii=False, default=str)
print(f"  âœ… ìƒì„¸ ì§€í‘œ ì €ì¥: {metrics_json_file}")

# ============================================
# ì‹œê°í™”
# ============================================
print("\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")

# 1. ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# MAE ë¹„êµ
ax = axes[0, 0]
model_names_upper = [name.upper() for name in model_metrics.keys()]
mae_values = [metrics['MAE'] for metrics in model_metrics.values()]
bars = ax.bar(model_names_upper, mae_values, color='skyblue', edgecolor='navy')
ax.set_title('ëª¨ë¸ë³„ MAE (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)', fontsize=14, weight='bold')
ax.set_ylabel('MAE')
ax.grid(axis='y', alpha=0.3)
for bar, value in zip(bars, mae_values):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
            f'{value:.1f}', ha='center', va='bottom', fontsize=10)

# RMSE ë¹„êµ
ax = axes[0, 1]
rmse_values = [metrics['RMSE'] for metrics in model_metrics.values()]
bars = ax.bar(model_names_upper, rmse_values, color='lightcoral', edgecolor='darkred')
ax.set_title('ëª¨ë¸ë³„ RMSE (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)', fontsize=14, weight='bold')
ax.set_ylabel('RMSE')
ax.grid(axis='y', alpha=0.3)
for bar, value in zip(bars, rmse_values):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
            f'{value:.1f}', ha='center', va='bottom', fontsize=10)

# RÂ² ë¹„êµ
ax = axes[1, 0]
r2_values = [metrics['R2'] for metrics in model_metrics.values()]
bars = ax.bar(model_names_upper, r2_values, color='lightgreen', edgecolor='darkgreen')
ax.set_title('ëª¨ë¸ë³„ RÂ² Score (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)', fontsize=14, weight='bold')
ax.set_ylabel('RÂ² Score')
ax.set_ylim(0, 1)
ax.grid(axis='y', alpha=0.3)
for bar, value in zip(bars, r2_values):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
            f'{value:.3f}', ha='center', va='bottom', fontsize=10)

# ì •í™•ë„ ë¹„êµ
ax = axes[1, 1]
accuracy_values = [metrics['Accuracy'] for metrics in model_metrics.values()]
bars = ax.bar(model_names_upper, accuracy_values, color='gold', edgecolor='orange')
ax.set_title('ëª¨ë¸ë³„ ì •í™•ë„ (%)', fontsize=14, weight='bold')
ax.set_ylabel('ì •í™•ë„ (%)')
ax.set_ylim(80, 100)
ax.grid(axis='y', alpha=0.3)
for bar, value in zip(bars, accuracy_values):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
            f'{value:.1f}%', ha='center', va='bottom', fontsize=10)

plt.suptitle('V6 ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ', fontsize=16, weight='bold', y=1.02)
plt.tight_layout()
performance_chart_file = f"{Config.OUTPUT_DIR}model_performance_comparison.png"
plt.savefig(performance_chart_file, dpi=150, bbox_inches='tight')
print(f"  âœ… ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ì €ì¥: {performance_chart_file}")

# 2. ì‹œê³„ì—´ ì˜ˆì¸¡ ë¹„êµ (ì²˜ìŒ 500ê°œ ìƒ˜í”Œ)
fig, ax = plt.subplots(figsize=(20, 8))

sample_size = min(500, len(y_eval))
x_axis = range(sample_size)

# ì‹¤ì œê°’
ax.plot(x_axis, y_eval[:sample_size], 'k-', label='ì‹¤ì œê°’', linewidth=2, alpha=0.8)

# ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’ (ìƒ‰ìƒ êµ¬ë¶„)
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown']
for idx, (name, pred) in enumerate(predictions.items()):
    if idx < len(colors):
        ax.plot(x_axis, pred[:sample_size], 
                color=colors[idx], alpha=0.6, linewidth=1,
                label=f'{name.upper()} ì˜ˆì¸¡')

ax.set_title('ì‹œê³„ì—´ ì˜ˆì¸¡ ë¹„êµ (ì²˜ìŒ 500ê°œ ìƒ˜í”Œ)', fontsize=14, weight='bold')
ax.set_xlabel('ì‹œê°„ ì¸ë±ìŠ¤')
ax.set_ylabel('TOTALCNT')
ax.legend(loc='upper right', ncol=3)
ax.grid(True, alpha=0.3)

timeseries_chart_file = f"{Config.OUTPUT_DIR}timeseries_prediction_comparison.png"
plt.savefig(timeseries_chart_file, dpi=150, bbox_inches='tight')
print(f"  âœ… ì‹œê³„ì—´ ì°¨íŠ¸ ì €ì¥: {timeseries_chart_file}")

# 3. ê¸‰ì¦ êµ¬ê°„ (1500+) ì˜ˆì¸¡ ì„±ëŠ¥
fig, ax = plt.subplots(figsize=(12, 8))

spike_mask = y_eval >= 1500
if np.any(spike_mask):
    spike_indices = np.where(spike_mask)[0][:100]  # ì²˜ìŒ 100ê°œ ê¸‰ì¦ ì‚¬ë¡€
    
    x_pos = range(len(spike_indices))
    width = 0.15
    
    # ì‹¤ì œê°’
    ax.bar(x_pos, y_eval[spike_indices], width, label='ì‹¤ì œê°’', color='black', alpha=0.7)
    
    # ëª¨ë¸ë³„ ì˜ˆì¸¡
    for idx, (name, pred) in enumerate(predictions.items()):
        offset = (idx + 1) * width
        ax.bar([p + offset for p in x_pos], pred[spike_indices], 
               width, label=f'{name.upper()}', alpha=0.7)
    
    ax.set_title('ê¸‰ì¦ êµ¬ê°„(1500+) ì˜ˆì¸¡ ì„±ëŠ¥ ë¹„êµ', fontsize=14, weight='bold')
    ax.set_xlabel('ìƒ˜í”Œ ì¸ë±ìŠ¤')
    ax.set_ylabel('TOTALCNT')
    ax.axhline(y=1500, color='red', linestyle='--', alpha=0.5, label='1500 ì„ê³„ê°’')
    ax.legend(loc='upper right')
    ax.grid(True, alpha=0.3)
    
    spike_chart_file = f"{Config.OUTPUT_DIR}spike_prediction_comparison.png"
    plt.savefig(spike_chart_file, dpi=150, bbox_inches='tight')
    print(f"  âœ… ê¸‰ì¦ ì˜ˆì¸¡ ì°¨íŠ¸ ì €ì¥: {spike_chart_file}")

plt.close('all')

# ============================================
# ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
# ============================================
print("\nğŸ“„ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")

report_file = f"{Config.OUTPUT_DIR}evaluation_report.txt"
with open(report_file, 'w', encoding='utf-8') as f:
    f.write("="*80 + "\n")
    f.write("V6 ëª¨ë¸ í‰ê°€ ë¦¬í¬íŠ¸\n")
    f.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write("="*80 + "\n\n")
    
    f.write(f"1. í‰ê°€ ë°ì´í„° ì •ë³´\n")
    f.write(f"   - ë°ì´í„° íŒŒì¼: {Config.EVAL_DATA_FILE}\n")
    f.write(f"   - í‰ê°€ ìƒ˜í”Œ ìˆ˜: {len(y_eval):,}ê°œ\n")
    f.write(f"   - ë‚ ì§œ ë²”ìœ„: {dates_eval[0]} ~ {dates_eval[-1]}\n")
    f.write(f"   - TOTALCNT ë²”ìœ„: {y_eval.min():.0f} ~ {y_eval.max():.0f}\n")
    f.write(f"   - TOTALCNT í‰ê· : {y_eval.mean():.1f}\n\n")
    
    f.write("2. ëª¨ë¸ë³„ ì¢…í•© ì„±ëŠ¥\n")
    f.write("-"*60 + "\n")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
    best_mae_model = min(model_metrics.keys(), key=lambda x: model_metrics[x]['MAE'])
    best_accuracy_model = max(model_metrics.keys(), key=lambda x: model_metrics[x]['Accuracy'])
    
    for name, metrics in model_metrics.items():
        is_best_mae = "ğŸ†" if name == best_mae_model else "  "
        is_best_acc = "ğŸ†" if name == best_accuracy_model else "  "
        
        f.write(f"\n{is_best_mae} {name.upper()} ëª¨ë¸:\n")
        f.write(f"   - MAE: {metrics['MAE']:.2f}\n")
        f.write(f"   - RMSE: {metrics['RMSE']:.2f}\n")
        f.write(f"   - RÂ² Score: {metrics['R2']:.4f}\n")
        f.write(f"   {is_best_acc} ì •í™•ë„: {metrics['Accuracy']:.2f}%\n")
        
        f.write("   êµ¬ê°„ë³„ F1 Score:\n")
        for level, level_metric in metrics['levels'].items():
            f.write(f"     â€¢ {level}: {level_metric['f1']:.3f} "
                   f"(Recall: {level_metric['recall']:.3f}, "
                   f"Precision: {level_metric['precision']:.3f}, "
                   f"ìƒ˜í”Œìˆ˜: {level_metric['count']})\n")
    
    f.write("\n" + "="*80 + "\n")
    f.write("3. ì£¼ìš” ë°œê²¬ì‚¬í•­\n")
    f.write("-"*60 + "\n")
    
    # ì•™ìƒë¸” ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒë¥ 
    if 'ensemble' in model_metrics:
        ensemble_mae = model_metrics['ensemble']['MAE']
        single_models = ['lstm', 'gru', 'cnn_lstm', 'spike', 'rule']
        avg_single_mae = np.mean([model_metrics[m]['MAE'] for m in single_models if m in model_metrics])
        improvement = ((avg_single_mae - ensemble_mae) / avg_single_mae) * 100
        
        f.write(f"\nâœ… ì•™ìƒë¸” íš¨ê³¼:\n")
        f.write(f"   - ì•™ìƒë¸” MAE: {ensemble_mae:.2f}\n")
        f.write(f"   - ë‹¨ì¼ëª¨ë¸ í‰ê·  MAE: {avg_single_mae:.2f}\n")
        f.write(f"   - ì„±ëŠ¥ í–¥ìƒ: {improvement:.1f}%\n")
    
    # ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥
    f.write(f"\nâœ… ê¸‰ì¦ êµ¬ê°„ ì˜ˆì¸¡ ì„±ëŠ¥ (1500+ ê¸°ì¤€):\n")
    for name, metrics in model_metrics.items():
        if '1500+' in metrics['levels']:
            f1 = metrics['levels']['1500+']['f1']
            recall = metrics['levels']['1500+']['recall']
            f.write(f"   - {name.upper()}: F1={f1:.3f}, Recall={recall:.3f}\n")
    
    f.write("\n" + "="*80 + "\n")
    f.write("4. ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­\n")
    f.write("-"*60 + "\n")
    f.write(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_mae_model.upper()} (MAE: {model_metrics[best_mae_model]['MAE']:.2f})\n")
    f.write(f"ğŸ¯ ìµœê³  ì •í™•ë„ ëª¨ë¸: {best_accuracy_model.upper()} ({model_metrics[best_accuracy_model]['Accuracy']:.2f}%)\n")
    
    if 'ensemble' in model_metrics and best_mae_model == 'ensemble':
        f.write("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:\n")
        f.write("   1. ì•™ìƒë¸” ëª¨ë¸ì´ ìµœê³  ì„±ëŠ¥ì„ ë³´ì´ë¯€ë¡œ ì‹¤ì œ ìš´ì˜ì— í™œìš© ê¶Œì¥\n")
        f.write("   2. ê¸‰ì¦ êµ¬ê°„ ì˜ˆì¸¡ì´ ì¤‘ìš”í•œ ê²½ìš° Recallì´ ë†’ì€ ëª¨ë¸ ì„ íƒ\n")
        f.write("   3. ì‹¤ì‹œê°„ ì²˜ë¦¬ê°€ ì¤‘ìš”í•œ ê²½ìš° ë‹¨ì¼ ëª¨ë¸ ì¤‘ ì„ íƒ ê³ ë ¤\n")
    
    f.write("\n" + "="*80 + "\n")

print(f"  âœ… í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")

# ============================================
# ìµœì¢… ìš”ì•½ ì¶œë ¥
# ============================================
print("\n" + "="*60)
print("ğŸ¯ V6 ëª¨ë¸ í‰ê°€ ì™„ë£Œ!")
print("="*60)

print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ (MAE ê¸°ì¤€):")
sorted_models = sorted(model_metrics.items(), key=lambda x: x[1]['MAE'])
for rank, (name, metrics) in enumerate(sorted_models, 1):
    print(f"  {rank}ìœ„. {name.upper()}: MAE={metrics['MAE']:.2f}, "
          f"ì •í™•ë„={metrics['Accuracy']:.2f}%")

print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {Config.OUTPUT_DIR}")
print("  - prediction_results_detail.csv: ë‚ ì§œë³„ ìƒì„¸ ì˜ˆì¸¡ê°’")
print("  - model_performance_summary.csv: ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½")
print("  - model_metrics.json: ìƒì„¸ í‰ê°€ ì§€í‘œ")
print("  - evaluation_report.txt: ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸")
print("  - ì‹œê°í™” ì°¨íŠ¸ 3ê°œ")

# 100ë§Œê°œ ë°ì´í„° ì²´í¬
total_train_samples = 781163  # ì´ì „ í•™ìŠµ ë°ì´í„°
total_eval_samples = len(y_eval)
total_samples = total_train_samples + total_eval_samples

if total_samples >= 1000000:
    print("\n" + "="*60)
    print("ğŸ”” ì•Œë¦¼: ì´ 100ë§Œê°œ ì´ìƒ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"   í•™ìŠµ: {total_train_samples:,}ê°œ + í‰ê°€: {total_eval_samples:,}ê°œ")
    print(f"   = ì´ {total_samples:,}ê°œ")
    print("ğŸ“Š Patch Time Series Transformer ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    print("   ë” ë†’ì€ ì„±ëŠ¥ì„ ì›í•˜ì‹œë©´ ì•Œë ¤ì£¼ì„¸ìš”!")
    print("="*60)

print("\nâœ… ëª¨ë“  í‰ê°€ ì‘ì—… ì™„ë£Œ!")
print("="*60)

# ë©”ëª¨ë¦¬ ì •ë¦¬
tf.keras.backend.clear_session()
gc.collect()