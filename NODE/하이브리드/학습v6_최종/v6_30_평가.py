"""
V6 ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ (ëª¨ë¸ ë¡œë“œ ê°œì„ íŒ)
- ì»¤ìŠ¤í…€ ë ˆì´ì–´/ì†ì‹¤í•¨ìˆ˜ ì™„ë²½ ì§€ì›
- ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜ í•´ê²°
- CPU ëª¨ë“œ ìµœì í™”
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import RobustScaler
import matplotlib.pyplot as plt
import json
import warnings
from datetime import datetime
import os
warnings.filterwarnings('ignore')

# TensorFlow ê²½ê³  ì–µì œ
import logging
logging.getLogger('tensorflow').setLevel(logging.ERROR)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

print("="*60)
print("ğŸ“Š V6 ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ (ë¡œë“œ ê°œì„ íŒ)")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print("="*60)

# ============================================
# ì„¤ì •
# ============================================
class Config:
    # í‰ê°€ ë°ì´í„° íŒŒì¼
    EVAL_DATA_FILE = './data/20250731_to20250806.CSV'
    
    # í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
    MODEL_DIR = './models_v6_full_train/'
    
    # ì‹œí€€ìŠ¤ ì„¤ì •
    LOOKBACK = 100  # ê³¼ê±° 100ë¶„ ë°ì´í„°
    FORECAST = 10   # 10ë¶„ í›„ ì˜ˆì¸¡
    
    # í‰ê°€ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    EVAL_RESULT_DIR = './evaluation_results/'
    
    # ì‹œê°í™” ì €ì¥ ê²½ë¡œ
    PLOT_DIR = './evaluation_plots/'
    
    # CPU ëª¨ë“œ ë°°ì¹˜ í¬ê¸°
    BATCH_SIZE = 32

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(Config.EVAL_RESULT_DIR, exist_ok=True)
os.makedirs(Config.PLOT_DIR, exist_ok=True)

# ============================================
# ì»¤ìŠ¤í…€ ë ˆì´ì–´ ë° ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
# ============================================

@tf.keras.utils.register_keras_serializable()
class M14RuleCorrection(tf.keras.layers.Layer):
    """M14 ê·œì¹™ ê¸°ë°˜ ë³´ì • ë ˆì´ì–´"""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    
    def call(self, inputs, training=None):
        if isinstance(inputs, list):
            pred, m14_features = inputs
        else:
            # ë‹¨ì¼ ì…ë ¥ì¸ ê²½ìš°
            return inputs
        
        pred = tf.cast(pred, tf.float32)
        m14_features = tf.cast(m14_features, tf.float32)
        
        # m14_features shape í™•ì¸
        if len(m14_features.shape) == 1:
            m14_features = tf.expand_dims(m14_features, axis=0)
        
        # íŠ¹ì§• ì¶”ì¶œ
        if m14_features.shape[-1] >= 1:
            m14b = m14_features[:, 0:1]
        else:
            m14b = tf.zeros_like(pred)
            
        if m14_features.shape[-1] >= 2:
            m10a = m14_features[:, 1:2]
        else:
            m10a = tf.ones_like(pred)
            
        if m14_features.shape[-1] >= 3:
            m16 = m14_features[:, 2:3]
        else:
            m16 = tf.ones_like(pred)
            
        if m14_features.shape[-1] >= 4:
            ratio = m14_features[:, 3:4]
        else:
            ratio = tf.where(m10a > 0, m14b / (m10a + 1e-7), tf.zeros_like(pred))
        
        # ì„ê³„ê°’ ê·œì¹™ ì ìš©
        pred = tf.where(m14b >= 420, tf.maximum(pred, 1550.0), pred)
        pred = tf.where(m14b >= 380, tf.maximum(pred, 1500.0), pred)
        pred = tf.where(m14b >= 350, tf.maximum(pred, 1450.0), pred)
        pred = tf.where(m14b >= 300, tf.maximum(pred, 1400.0), pred)
        
        # ë¹„ìœ¨ ë³´ì •
        pred = tf.where(ratio >= 5.5, pred * 1.15, pred)
        pred = tf.where((ratio >= 5.0) & (ratio < 5.5), pred * 1.10, pred)
        pred = tf.where((ratio >= 4.5) & (ratio < 5.0), pred * 1.08, pred)
        pred = tf.where((ratio >= 4.0) & (ratio < 4.5), pred * 1.05, pred)
        
        # í™©ê¸ˆ íŒ¨í„´
        golden = (m14b >= 350) & (m10a < 70)
        pred = tf.where(golden, pred * 1.2, pred)
        
        # ë²”ìœ„ ì œí•œ
        pred = tf.clip_by_value(pred, 1200.0, 2000.0)
        
        return pred
    
    def get_config(self):
        config = super().get_config()
        return config

@tf.keras.utils.register_keras_serializable()
class ImprovedM14RuleCorrection(M14RuleCorrection):
    """ê°œì„ ëœ M14 ê·œì¹™ ë³´ì • (í˜¸í™˜ì„±ìš©)"""
    pass

@tf.keras.utils.register_keras_serializable()
class WeightedLoss(tf.keras.losses.Loss):
    """ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜"""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        
        mae = tf.abs(y_true - y_pred)
        
        weights = tf.ones_like(y_true)
        weights = tf.where(y_true >= 1550, 30.0, weights)
        weights = tf.where((y_true >= 1500) & (y_true < 1550), 25.0, weights)
        weights = tf.where((y_true >= 1450) & (y_true < 1500), 20.0, weights)
        weights = tf.where((y_true >= 1400) & (y_true < 1450), 15.0, weights)
        weights = tf.where((y_true >= 1350) & (y_true < 1400), 10.0, weights)
        
        large_error = tf.where(mae > 100, mae * 0.2, 0.0)
        
        return tf.reduce_mean(mae * weights + large_error)
    
    def get_config(self):
        return super().get_config()

@tf.keras.utils.register_keras_serializable()
class ImprovedWeightedLoss(WeightedLoss):
    """ê°œì„ ëœ ê°€ì¤‘ì¹˜ ì†ì‹¤ (í˜¸í™˜ì„±ìš©)"""
    pass

# ============================================
# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)
# ============================================
def load_models():
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ (ê°•í™”ëœ ë²„ì „)"""
    print("\nğŸ“¦ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    models = {}
    model_names = ['lstm', 'gru', 'cnn_lstm', 'spike', 'rule', 'ensemble']
    
    # ëª¨ë“  ê°€ëŠ¥í•œ ì»¤ìŠ¤í…€ ê°ì²´ ì •ì˜
    custom_objects = {
        'M14RuleCorrection': M14RuleCorrection,
        'ImprovedM14RuleCorrection': ImprovedM14RuleCorrection,
        'WeightedLoss': WeightedLoss,
        'ImprovedWeightedLoss': ImprovedWeightedLoss,
        # Lambda ë ˆì´ì–´ ì²˜ë¦¬
        'tf': tf,
        'Lambda': tf.keras.layers.Lambda,
    }
    
    for name in model_names:
        print(f"\n  ì‹œë„ ì¤‘: {name}")
        
        # ê°€ëŠ¥í•œ íŒŒì¼ í™•ì¥ìë“¤ ì‹œë„
        possible_paths = [
            f"{Config.MODEL_DIR}{name}_final.keras",
            f"{Config.MODEL_DIR}{name}_best.keras",
            f"{Config.MODEL_DIR}{name}_final.h5",
            f"{Config.MODEL_DIR}{name}_best.h5",
        ]
        
        model_loaded = False
        for model_path in possible_paths:
            if os.path.exists(model_path):
                print(f"    íŒŒì¼ ë°œê²¬: {model_path}")
                try:
                    # ë°©ë²• 1: ì¼ë°˜ ë¡œë“œ
                    model = tf.keras.models.load_model(
                        model_path,
                        custom_objects=custom_objects,
                        compile=False
                    )
                    
                    # ì¬ì»´íŒŒì¼
                    model.compile(
                        optimizer='adam',
                        loss='mae',
                        metrics=['mae']
                    )
                    
                    models[name] = model
                    model_loaded = True
                    print(f"    âœ… {name} ëª¨ë¸ ë¡œë“œ ì„±ê³µ (ë°©ë²• 1)")
                    break
                    
                except Exception as e1:
                    print(f"    âš ï¸ ë°©ë²• 1 ì‹¤íŒ¨: {str(e1)[:100]}...")
                    
                    try:
                        # ë°©ë²• 2: ê°€ì¤‘ì¹˜ë§Œ ë¡œë“œ (êµ¬ì¡° ì¬ìƒì„±)
                        print(f"    ë°©ë²• 2 ì‹œë„ ì¤‘ (ê°€ì¤‘ì¹˜ë§Œ ë¡œë“œ)...")
                        model = recreate_model_structure(name)
                        if model:
                            model.load_weights(model_path)
                            models[name] = model
                            model_loaded = True
                            print(f"    âœ… {name} ëª¨ë¸ ë¡œë“œ ì„±ê³µ (ë°©ë²• 2)")
                            break
                    except Exception as e2:
                        print(f"    âš ï¸ ë°©ë²• 2ë„ ì‹¤íŒ¨: {str(e2)[:100]}...")
        
        if not model_loaded:
            print(f"    âŒ {name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - ìŠ¤í‚µ")
    
    print(f"\nâœ… ì´ {len(models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return models

def recreate_model_structure(model_name):
    """ëª¨ë¸ êµ¬ì¡° ì¬ìƒì„± (ê°€ì¤‘ì¹˜ ë¡œë“œìš©)"""
    try:
        if model_name == 'lstm':
            return build_lstm_model((100, 59))  # input_shape
        elif model_name == 'gru':
            return build_gru_model((100, 59))
        elif model_name == 'cnn_lstm':
            return build_cnn_lstm((100, 59))
        elif model_name == 'spike':
            return build_spike_detector((100, 59))
        elif model_name == 'rule':
            return build_rule_based_model((100, 59), 4)
        elif model_name == 'ensemble':
            return None  # ì•™ìƒë¸”ì€ ë³µì¡í•´ì„œ ìŠ¤í‚µ
    except:
        return None

# ============================================
# ê°„ë‹¨í•œ ëª¨ë¸ êµ¬ì¡° ì •ì˜ (ì¬ìƒì„±ìš©)
# ============================================
def build_lstm_model(input_shape):
    """LSTM ëª¨ë¸ êµ¬ì¡°"""
    model = tf.keras.Sequential([
        tf.keras.layers.InputLayer(input_shape=input_shape),
        tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.2),
        tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2),
        tf.keras.layers.LSTM(64, dropout=0.2),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(1)
    ], name='LSTM_Model')
    model.compile(optimizer='adam', loss='mae', metrics=['mae'])
    return model

def build_gru_model(input_shape):
    """GRU ëª¨ë¸ êµ¬ì¡°"""
    model = tf.keras.Sequential([
        tf.keras.layers.InputLayer(input_shape=input_shape),
        tf.keras.layers.GRU(256, return_sequences=True, dropout=0.15),
        tf.keras.layers.GRU(128, return_sequences=True, dropout=0.15),
        tf.keras.layers.GRU(64, dropout=0.15),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(1)
    ], name='GRU_Model')
    model.compile(optimizer='adam', loss='mae', metrics=['mae'])
    return model

def build_cnn_lstm(input_shape):
    """CNN-LSTM ëª¨ë¸ êµ¬ì¡°"""
    inputs = tf.keras.Input(shape=input_shape)
    
    convs = []
    for kernel_size in [3, 5, 7, 9]:
        conv = tf.keras.layers.Conv1D(128, kernel_size, activation='relu', padding='same')(inputs)
        convs.append(conv)
    
    concat = tf.keras.layers.Concatenate()(convs)
    lstm1 = tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.15)(concat)
    lstm2 = tf.keras.layers.LSTM(128, dropout=0.15)(lstm1)
    dense1 = tf.keras.layers.Dense(256, activation='relu')(lstm2)
    dense2 = tf.keras.layers.Dense(128, activation='relu')(dense1)
    output = tf.keras.layers.Dense(1)(dense2)
    
    model = tf.keras.Model(inputs=inputs, outputs=output, name='CNN_LSTM_Model')
    model.compile(optimizer='adam', loss='mae', metrics=['mae'])
    return model

def build_spike_detector(input_shape):
    """Spike Detector êµ¬ì¡°"""
    inputs = tf.keras.Input(shape=input_shape)
    
    convs = []
    for kernel_size in [3, 5, 7]:
        conv = tf.keras.layers.Conv1D(96, kernel_size, activation='relu', padding='same')(inputs)
        convs.append(conv)
    
    concat = tf.keras.layers.Concatenate()(convs)
    lstm = tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.15)
    )(concat)
    
    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(lstm)
    max_pool = tf.keras.layers.GlobalMaxPooling1D()(lstm)
    pooled = tf.keras.layers.Concatenate()([avg_pool, max_pool])
    
    dense1 = tf.keras.layers.Dense(256, activation='relu')(pooled)
    dense2 = tf.keras.layers.Dense(128, activation='relu')(dense1)
    output = tf.keras.layers.Dense(1, name='spike_value')(dense2)
    
    model = tf.keras.Model(inputs=inputs, outputs=output, name='Spike_Detector')
    model.compile(optimizer='adam', loss='mae', metrics=['mae'])
    return model

def build_rule_based_model(input_shape, m14_shape):
    """Rule-Based ëª¨ë¸ êµ¬ì¡°"""
    time_input = tf.keras.Input(shape=input_shape, name='time_input')
    m14_input = tf.keras.Input(shape=(m14_shape,), name='m14_input')
    
    lstm1 = tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.15)(time_input)
    lstm2 = tf.keras.layers.LSTM(32, dropout=0.15)(lstm1)
    
    m14_dense = tf.keras.layers.Dense(16, activation='relu')(m14_input)
    
    combined = tf.keras.layers.Concatenate()([lstm2, m14_dense])
    dense1 = tf.keras.layers.Dense(128, activation='relu')(combined)
    dense2 = tf.keras.layers.Dense(64, activation='relu')(dense1)
    prediction = tf.keras.layers.Dense(1)(dense2)
    
    corrected = M14RuleCorrection()([prediction, m14_input])
    
    model = tf.keras.Model(
        inputs=[time_input, m14_input],
        outputs=corrected,
        name='Rule_Based_Model'
    )
    model.compile(optimizer='adam', loss='mae', metrics=['mae'])
    return model

# ============================================
# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
# ============================================
def prepare_evaluation_data(file_path):
    """í‰ê°€ ë°ì´í„° ì¤€ë¹„"""
    print(f"\nğŸ“‚ í‰ê°€ ë°ì´í„° ë¡œë“œ: {file_path}")
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(file_path)
    print(f"  ì›ë³¸ ë°ì´í„°: {len(df)}í–‰")
    
    # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
    required_columns = ['M14AM10A', 'M14AM14B', 'M14AM16', 'TOTALCNT']
    
    for col in required_columns:
        if col not in df.columns:
            print(f"  âš ï¸ {col} ì»¬ëŸ¼ ì—†ìŒ - 0ìœ¼ë¡œ ì´ˆê¸°í™”")
            df[col] = 0
    
    # M14AM14BSUM ìƒì„±
    if 'M14AM14BSUM' not in df.columns:
        df['M14AM14BSUM'] = df['M14AM14B'] + df['M14AM10A']
    
    # íƒ€ê²Ÿ ìƒì„±
    df['target'] = df['TOTALCNT'].shift(-Config.FORECAST)
    
    # íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§
    print("\nğŸ”§ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§...")
    
    # ê¸°ë³¸ íŠ¹ì§•
    df['ratio_14B_10A'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
    df['ratio_14B_16'] = df['M14AM14B'] / (df['M14AM16'] + 1)
    df['ratio_10A_16'] = df['M14AM10A'] / (df['M14AM16'] + 1)
    
    # ì‹œê³„ì—´ íŠ¹ì§•
    for col in ['TOTALCNT', 'M14AM14B', 'M14AM10A', 'M14AM16']:
        if col in df.columns:
            df[f'{col}_diff_1'] = df[col].diff(1)
            df[f'{col}_diff_5'] = df[col].diff(5)
            df[f'{col}_diff_10'] = df[col].diff(10)
            df[f'{col}_ma_5'] = df[col].rolling(5, min_periods=1).mean()
            df[f'{col}_ma_10'] = df[col].rolling(10, min_periods=1).mean()
            df[f'{col}_ma_20'] = df[col].rolling(20, min_periods=1).mean()
            df[f'{col}_std_5'] = df[col].rolling(5, min_periods=1).std()
            df[f'{col}_std_10'] = df[col].rolling(10, min_periods=1).std()
    
    # í™©ê¸ˆ íŒ¨í„´
    df['golden_pattern'] = ((df['M14AM14B'] >= 350) & (df['M14AM10A'] < 70)).astype(float)
    
    # ê¸‰ì¦ ì‹ í˜¸
    for threshold in [250, 300, 350, 400, 450]:
        df[f'signal_{threshold}'] = (df['M14AM14B'] >= threshold).astype(float)
    
    for threshold in [3.5, 4.0, 4.5, 5.0, 5.5]:
        df[f'ratio_signal_{threshold}'] = (df['ratio_14B_10A'] >= threshold).astype(float)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df = df.fillna(0)
    df = df.dropna(subset=['target'])
    
    print(f"  ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}í–‰, {len(df.columns)}ê°œ íŠ¹ì§•")
    
    return df

def create_sequences(df, lookback=100, forecast=10):
    """ì‹œí€€ìŠ¤ ìƒì„±"""
    print("\nâš¡ í‰ê°€ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
    
    X, y = [], []
    data_array = df.values
    totalcnt_idx = df.columns.get_loc('TOTALCNT')
    
    for i in range(len(data_array) - lookback - forecast + 1):
        X.append(data_array[i:i+lookback])
        target_idx = i + lookback + forecast - 1
        if target_idx < len(data_array):
            y.append(data_array[target_idx, totalcnt_idx])
    
    X = np.array(X, dtype=np.float32)
    y = np.array(y, dtype=np.float32)
    
    print(f"  X shape: {X.shape}")
    print(f"  y shape: {y.shape}")
    print(f"  y ë²”ìœ„: {y.min():.0f} ~ {y.max():.0f}")
    
    return X, y, df

# ============================================
# í‰ê°€ í•¨ìˆ˜
# ============================================
def evaluate_models(models, X_test, y_test, m14_test):
    """ëª¨ë¸ í‰ê°€"""
    print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ì‹œì‘...")
    
    results = {}
    predictions = {}
    
    for name, model in models.items():
        print(f"\n  í‰ê°€ ì¤‘: {name.upper()}")
        
        try:
            # ì˜ˆì¸¡
            if name in ['ensemble', 'rule']:
                pred = model.predict([X_test, m14_test], batch_size=Config.BATCH_SIZE, verbose=0).flatten()
            else:
                pred = model.predict(X_test, batch_size=Config.BATCH_SIZE, verbose=0).flatten()
            
            predictions[name] = pred
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            mae = np.mean(np.abs(y_test - pred))
            mse = np.mean((y_test - pred) ** 2)
            rmse = np.sqrt(mse)
            
            non_zero_mask = y_test != 0
            if np.any(non_zero_mask):
                mape = np.mean(np.abs((y_test[non_zero_mask] - pred[non_zero_mask]) / y_test[non_zero_mask])) * 100
            else:
                mape = 0
            
            accuracy_50 = np.mean(np.abs(y_test - pred) <= 50) * 100
            accuracy_100 = np.mean(np.abs(y_test - pred) <= 100) * 100
            
            results[name] = {
                'mae': float(mae),
                'rmse': float(rmse),
                'mape': float(mape),
                'accuracy_50': float(accuracy_50),
                'accuracy_100': float(accuracy_100),
                'levels': {}
            }
            
            print(f"    MAE: {mae:.2f}")
            print(f"    RMSE: {rmse:.2f}")
            print(f"    MAPE: {mape:.2f}%")
            print(f"    ì •í™•ë„(Â±50): {accuracy_50:.1f}%")
            print(f"    ì •í™•ë„(Â±100): {accuracy_100:.1f}%")
            
        except Exception as e:
            print(f"    âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
    
    return results, predictions

# ============================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================
def main():
    """ë©”ì¸ í‰ê°€ í”„ë¡œì„¸ìŠ¤"""
    
    try:
        # 1. í‰ê°€ ë°ì´í„° ì¤€ë¹„
        df = prepare_evaluation_data(Config.EVAL_DATA_FILE)
        
        # 2. ì‹œí€€ìŠ¤ ìƒì„±
        X, y, df_processed = create_sequences(df, Config.LOOKBACK, Config.FORECAST)
        
        if len(X) == 0:
            print("\nâŒ ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨ - ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return
        
        # 3. M14 íŠ¹ì§• ì¶”ì¶œ
        print("\nğŸ“Š M14 íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        m14_features = np.zeros((len(X), 4), dtype=np.float32)
        
        for i in range(len(X)):
            idx = i + Config.LOOKBACK
            if idx < len(df_processed):
                m14_features[i] = [
                    df_processed['M14AM14B'].iloc[idx],
                    df_processed['M14AM10A'].iloc[idx],
                    df_processed['M14AM16'].iloc[idx],
                    df_processed['ratio_14B_10A'].iloc[idx]
                ]
        
        # 4. ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        print("\nğŸ“ ë°ì´í„° ìŠ¤ì¼€ì¼ë§...")
        
        X_scaled = np.zeros_like(X)
        for i in range(X.shape[2]):
            scaler = RobustScaler()
            feature = X[:, :, i].reshape(-1, 1)
            X_scaled[:, :, i] = scaler.fit_transform(feature).reshape(X[:, :, i].shape)
        
        m14_scaler = RobustScaler()
        m14_features_scaled = m14_scaler.fit_transform(m14_features)
        
        print("  âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
        
        # 5. ëª¨ë¸ ë¡œë“œ
        models = load_models()
        
        if not models:
            print("\nâŒ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ í•´ê²° ë°©ë²•:")
            print("  1. ëª¨ë¸ íŒŒì¼ ê²½ë¡œ í™•ì¸: " + Config.MODEL_DIR)
            print("  2. ëª¨ë¸ íŒŒì¼ í™•ì¥ì í™•ì¸ (.keras, .h5)")
            print("  3. í•™ìŠµ ì½”ë“œ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ ìƒì„±")
            return
        
        # 6. ëª¨ë¸ í‰ê°€
        results, predictions = evaluate_models(models, X_scaled, y, m14_features_scaled)
        
        # 7. ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ“Š í‰ê°€ ì™„ë£Œ!")
        print("="*60)
        
        if results:
            best_model = min(results.keys(), key=lambda x: results[x]['mae'])
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model.upper()}")
            print(f"  - MAE: {results[best_model]['mae']:.2f}")
            print(f"  - RMSE: {results[best_model]['rmse']:.2f}")
            print(f"  - ì •í™•ë„(Â±50): {results[best_model]['accuracy_50']:.1f}%")
            print(f"  - ì •í™•ë„(Â±100): {results[best_model]['accuracy_100']:.1f}%")
        
        # 8. ê²°ê³¼ ì €ì¥
        if results:
            json_path = f"{Config.EVAL_RESULT_DIR}evaluation_results.json"
            with open(json_path, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            print(f"\nğŸ“ JSON ê²°ê³¼ ì €ì¥: {json_path}")
        
        print("\nâœ… ëª¨ë“  í‰ê°€ ì‘ì—… ì™„ë£Œ!")
        print("="*60)
        
    except FileNotFoundError:
        print(f"\nâŒ í‰ê°€ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {Config.EVAL_DATA_FILE}")
    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()