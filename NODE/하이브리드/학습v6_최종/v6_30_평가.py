"""
V6 ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ
- í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
- ìƒˆë¡œìš´ í‰ê°€ ë°ì´í„°ë¡œ ì„±ëŠ¥ ì¸¡ì •
- ìƒì„¸í•œ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import RobustScaler
import matplotlib.pyplot as plt
import seaborn as sns
import json
import pickle
import warnings
from datetime import datetime
import os
warnings.filterwarnings('ignore')

print("="*60)
print("ğŸ“Š V6 ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print("="*60)

# ============================================
# GPU ì„¤ì •
# ============================================
def setup_gpu():
    """GPU ì„¤ì • ë° í™•ì¸"""
    print("\nğŸ® GPU í™˜ê²½ í™•ì¸...")
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"âœ… GPU ê°ì§€: {len(gpus)}ê°œ")
            return True
        except Exception as e:
            print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
            return False
    else:
        print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        return False

has_gpu = setup_gpu()

# ============================================
# ì„¤ì •
# ============================================
class Config:
    # í‰ê°€ ë°ì´í„° íŒŒì¼
    EVAL_DATA_FILE = './data/20250731_to20250826.csv'
    
    # í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
    MODEL_DIR = './models_v6_full_train/'
    
    # ì‹œí€€ìŠ¤ ì„¤ì •
    LOOKBACK = 100  # ê³¼ê±° 100ë¶„ ë°ì´í„°
    FORECAST = 10   # 10ë¶„ í›„ ì˜ˆì¸¡
    
    # í‰ê°€ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    EVAL_RESULT_DIR = './evaluation_results/'
    
    # ì‹œê°í™” ì €ì¥ ê²½ë¡œ
    PLOT_DIR = './evaluation_plots/'

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(Config.EVAL_RESULT_DIR, exist_ok=True)
os.makedirs(Config.PLOT_DIR, exist_ok=True)

# ============================================
# ì»¤ìŠ¤í…€ ë ˆì´ì–´ ì •ì˜ (ëª¨ë¸ ë¡œë“œìš©)
# ============================================
class M14RuleCorrection(tf.keras.layers.Layer):
    """M14 ê·œì¹™ ê¸°ë°˜ ë³´ì • ë ˆì´ì–´"""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    
    def call(self, inputs, training=None):
        pred, m14_features = inputs
        
        pred = tf.cast(pred, tf.float32)
        m14_features = tf.cast(m14_features, tf.float32)
        
        m14b = m14_features[:, 0:1]
        m10a = m14_features[:, 1:2]
        m16 = m14_features[:, 2:3]
        ratio = m14_features[:, 3:4] if m14_features.shape[1] > 3 else \
                tf.where(m10a > 0, m14b / (m10a + 1e-7), 0.0)
        
        # ì„ê³„ê°’ ê·œì¹™
        pred = tf.where(m14b >= 420, tf.maximum(pred, 1550.0), pred)
        pred = tf.where(m14b >= 380, tf.maximum(pred, 1500.0), pred)
        pred = tf.where(m14b >= 350, tf.maximum(pred, 1450.0), pred)
        pred = tf.where(m14b >= 300, tf.maximum(pred, 1400.0), pred)
        
        # ë¹„ìœ¨ ë³´ì •
        pred = tf.where(ratio >= 5.5, pred * 1.15, pred)
        pred = tf.where((ratio >= 5.0) & (ratio < 5.5), pred * 1.10, pred)
        pred = tf.where((ratio >= 4.5) & (ratio < 5.0), pred * 1.08, pred)
        pred = tf.where((ratio >= 4.0) & (ratio < 4.5), pred * 1.05, pred)
        
        # í™©ê¸ˆ íŒ¨í„´
        golden = (m14b >= 350) & (m10a < 70)
        pred = tf.where(golden, pred * 1.2, pred)
        
        # ë²”ìœ„ ì œí•œ
        pred = tf.clip_by_value(pred, 1200.0, 2000.0)
        
        return pred

# ============================================
# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
# ============================================
def prepare_evaluation_data(file_path):
    """í‰ê°€ ë°ì´í„° ì¤€ë¹„"""
    print(f"\nğŸ“‚ í‰ê°€ ë°ì´í„° ë¡œë“œ: {file_path}")
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(file_path)
    print(f"  ì›ë³¸ ë°ì´í„°: {len(df)}í–‰")
    
    # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸ ë° ì²˜ë¦¬
    required_columns = ['M14AM10A', 'M14AM14B', 'M14AM16', 'TOTALCNT']
    
    # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    for col in required_columns:
        if col not in df.columns:
            print(f"  âš ï¸ {col} ì»¬ëŸ¼ ì—†ìŒ - 0ìœ¼ë¡œ ì´ˆê¸°í™”")
            df[col] = 0
    
    # M14AM14BSUM ìƒì„±
    if 'M14AM14BSUM' not in df.columns:
        df['M14AM14BSUM'] = df['M14AM14B'] + df['M14AM10A']
    
    # íƒ€ê²Ÿ ìƒì„± (10ë¶„ í›„ TOTALCNT)
    df['target'] = df['TOTALCNT'].shift(-Config.FORECAST)
    
    # íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§
    print("\nğŸ”§ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§...")
    
    # ê¸°ë³¸ íŠ¹ì§•
    df['ratio_14B_10A'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
    df['ratio_14B_16'] = df['M14AM14B'] / (df['M14AM16'] + 1)
    df['ratio_10A_16'] = df['M14AM10A'] / (df['M14AM16'] + 1)
    
    # ì‹œê³„ì—´ íŠ¹ì§•
    for col in ['TOTALCNT', 'M14AM14B', 'M14AM10A', 'M14AM16']:
        # ë³€í™”ëŸ‰
        df[f'{col}_diff_1'] = df[col].diff(1)
        df[f'{col}_diff_5'] = df[col].diff(5)
        df[f'{col}_diff_10'] = df[col].diff(10)
        
        # ì´ë™í‰ê· 
        df[f'{col}_ma_5'] = df[col].rolling(5, min_periods=1).mean()
        df[f'{col}_ma_10'] = df[col].rolling(10, min_periods=1).mean()
        df[f'{col}_ma_20'] = df[col].rolling(20, min_periods=1).mean()
        
        # í‘œì¤€í¸ì°¨
        df[f'{col}_std_5'] = df[col].rolling(5, min_periods=1).std()
        df[f'{col}_std_10'] = df[col].rolling(10, min_periods=1).std()
    
    # í™©ê¸ˆ íŒ¨í„´
    df['golden_pattern'] = ((df['M14AM14B'] >= 350) & (df['M14AM10A'] < 70)).astype(float)
    
    # ê¸‰ì¦ ì‹ í˜¸
    thresholds_14b = [250, 300, 350, 400, 450]
    for threshold in thresholds_14b:
        df[f'signal_{threshold}'] = (df['M14AM14B'] >= threshold).astype(float)
    
    thresholds_ratio = [3.5, 4.0, 4.5, 5.0, 5.5]
    for threshold in thresholds_ratio:
        df[f'ratio_signal_{threshold}'] = (df['ratio_14B_10A'] >= threshold).astype(float)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df = df.fillna(0)
    
    # íƒ€ê²Ÿì´ ìˆëŠ” ë°ì´í„°ë§Œ ì„ íƒ
    df = df.dropna(subset=['target'])
    
    print(f"  ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}í–‰, {len(df.columns)}ê°œ íŠ¹ì§•")
    
    return df

def create_sequences(data, lookback=100, forecast=10):
    """ì‹œí€€ìŠ¤ ìƒì„±"""
    X, y = [], []
    
    for i in range(len(data) - lookback - forecast):
        X.append(data[i:i+lookback])
        y.append(data[i+lookback+forecast-1, data.columns.get_loc('TOTALCNT')])
    
    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)

# ============================================
# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
# ============================================
def load_models():
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
    print("\nğŸ“¦ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    models = {}
    model_names = ['lstm', 'gru', 'cnn_lstm', 'spike', 'rule', 'ensemble']
    
    # ì»¤ìŠ¤í…€ ê°ì²´ ì •ì˜
    custom_objects = {
        'M14RuleCorrection': M14RuleCorrection,
    }
    
    for name in model_names:
        model_path = f"{Config.MODEL_DIR}{name}_final.keras"
        if os.path.exists(model_path):
            try:
                models[name] = tf.keras.models.load_model(
                    model_path,
                    custom_objects=custom_objects,
                    compile=False
                )
                print(f"  âœ… {name} ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                print(f"  âŒ {name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print(f"  âš ï¸ {name} ëª¨ë¸ íŒŒì¼ ì—†ìŒ")
    
    return models

# ============================================
# í‰ê°€ í•¨ìˆ˜
# ============================================
def evaluate_models(models, X_test, y_test, m14_test):
    """ëª¨ë¸ í‰ê°€"""
    print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ì‹œì‘...")
    
    results = {}
    predictions = {}
    
    for name, model in models.items():
        print(f"\n  í‰ê°€ ì¤‘: {name.upper()}")
        
        # ì˜ˆì¸¡
        try:
            if name in ['ensemble', 'rule']:
                pred = model.predict([X_test, m14_test], verbose=0).flatten()
            else:
                pred = model.predict(X_test, verbose=0).flatten()
            
            predictions[name] = pred
            
            # ì „ì²´ ì„±ëŠ¥ ì§€í‘œ
            mae = np.mean(np.abs(y_test - pred))
            mse = np.mean((y_test - pred) ** 2)
            rmse = np.sqrt(mse)
            mape = np.mean(np.abs((y_test - pred) / (y_test + 1e-7))) * 100
            
            # ì •í™•ë„ (ì˜¤ì°¨ ê¸°ì¤€)
            accuracy_50 = np.mean(np.abs(y_test - pred) <= 50) * 100
            accuracy_100 = np.mean(np.abs(y_test - pred) <= 100) * 100
            
            # êµ¬ê°„ë³„ ì„±ëŠ¥
            level_performance = {}
            for level in [1300, 1400, 1450, 1500, 1550]:
                mask = y_test >= level
                if np.any(mask):
                    # Recall: ì‹¤ì œ ê¸‰ì¦ ì¤‘ ì˜ˆì¸¡ ì„±ê³µ
                    tp = np.sum((pred >= level) & mask)
                    fn = np.sum((pred < level) & mask)
                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                    
                    # Precision: ì˜ˆì¸¡ ê¸‰ì¦ ì¤‘ ì‹¤ì œ ê¸‰ì¦
                    fp = np.sum((pred >= level) & ~mask)
                    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                    
                    # F1 Score
                    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
                    
                    # êµ¬ê°„ MAE
                    level_mae = np.mean(np.abs(y_test[mask] - pred[mask]))
                    
                    level_performance[level] = {
                        'recall': recall,
                        'precision': precision,
                        'f1': f1,
                        'mae': level_mae,
                        'count': np.sum(mask),
                        'tp': tp,
                        'fp': fp,
                        'fn': fn
                    }
            
            results[name] = {
                'mae': mae,
                'mse': mse,
                'rmse': rmse,
                'mape': mape,
                'accuracy_50': accuracy_50,
                'accuracy_100': accuracy_100,
                'levels': level_performance
            }
            
            print(f"    MAE: {mae:.2f}")
            print(f"    RMSE: {rmse:.2f}")
            print(f"    MAPE: {mape:.2f}%")
            print(f"    ì •í™•ë„(Â±50): {accuracy_50:.1f}%")
            print(f"    ì •í™•ë„(Â±100): {accuracy_100:.1f}%")
            
        except Exception as e:
            print(f"    âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
    
    return results, predictions

# ============================================
# ì‹œê°í™” í•¨ìˆ˜
# ============================================
def create_visualizations(y_test, predictions, results):
    """í‰ê°€ ê²°ê³¼ ì‹œê°í™”"""
    print("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
    
    # 1. ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
    plt.figure(figsize=(15, 10))
    
    # 1-1. MAE ë¹„êµ
    plt.subplot(2, 3, 1)
    model_names = list(results.keys())
    mae_values = [results[name]['mae'] for name in model_names]
    colors = plt.cm.Set3(np.linspace(0, 1, len(model_names)))
    bars = plt.bar(model_names, mae_values, color=colors)
    plt.title('ëª¨ë¸ë³„ MAE (í‰ê·  ì ˆëŒ€ ì˜¤ì°¨)', fontsize=12, fontweight='bold')
    plt.ylabel('MAE')
    plt.xticks(rotation=45)
    for bar, value in zip(bars, mae_values):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                f'{value:.1f}', ha='center', va='bottom')
    
    # 1-2. ì •í™•ë„ ë¹„êµ
    plt.subplot(2, 3, 2)
    acc_50 = [results[name]['accuracy_50'] for name in model_names]
    acc_100 = [results[name]['accuracy_100'] for name in model_names]
    x = np.arange(len(model_names))
    width = 0.35
    plt.bar(x - width/2, acc_50, width, label='Â±50 ì •í™•ë„', color='skyblue')
    plt.bar(x + width/2, acc_100, width, label='Â±100 ì •í™•ë„', color='lightcoral')
    plt.title('ëª¨ë¸ë³„ ì˜ˆì¸¡ ì •í™•ë„', fontsize=12, fontweight='bold')
    plt.ylabel('ì •í™•ë„ (%)')
    plt.xticks(x, model_names, rotation=45)
    plt.legend()
    
    # 1-3. 1400+ ê¸‰ì¦ ê°ì§€ ì„±ëŠ¥
    plt.subplot(2, 3, 3)
    f1_scores = []
    for name in model_names:
        if 1400 in results[name]['levels']:
            f1_scores.append(results[name]['levels'][1400]['f1'] * 100)
        else:
            f1_scores.append(0)
    bars = plt.bar(model_names, f1_scores, color='green', alpha=0.7)
    plt.title('1400+ ê¸‰ì¦ ê°ì§€ F1 Score', fontsize=12, fontweight='bold')
    plt.ylabel('F1 Score (%)')
    plt.xticks(rotation=45)
    for bar, value in zip(bars, f1_scores):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                f'{value:.1f}%', ha='center', va='bottom')
    
    # 2. ì˜ˆì¸¡ vs ì‹¤ì œ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸)
    best_model = min(results.keys(), key=lambda x: results[x]['mae'])
    
    plt.subplot(2, 3, 4)
    sample_size = min(500, len(y_test))
    sample_idx = np.random.choice(len(y_test), sample_size, replace=False)
    plt.scatter(y_test[sample_idx], predictions[best_model][sample_idx], 
               alpha=0.5, s=10)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
            'r--', label='ì™„ë²½í•œ ì˜ˆì¸¡')
    plt.xlabel('ì‹¤ì œê°’')
    plt.ylabel('ì˜ˆì¸¡ê°’')
    plt.title(f'{best_model.upper()} - ì˜ˆì¸¡ vs ì‹¤ì œ', fontsize=12, fontweight='bold')
    plt.legend()
    
    # 3. ì‹œê³„ì—´ ì˜ˆì¸¡ (ìƒ˜í”Œ)
    plt.subplot(2, 3, 5)
    time_sample = 200
    time_range = range(time_sample)
    plt.plot(time_range, y_test[:time_sample], label='ì‹¤ì œ', linewidth=2)
    plt.plot(time_range, predictions[best_model][:time_sample], 
            label=f'{best_model} ì˜ˆì¸¡', linewidth=2, alpha=0.7)
    plt.xlabel('ì‹œê°„ (ë¶„)')
    plt.ylabel('TOTALCNT')
    plt.title('ì‹œê³„ì—´ ì˜ˆì¸¡ ìƒ˜í”Œ (200ë¶„)', fontsize=12, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 4. ì˜¤ì°¨ ë¶„í¬
    plt.subplot(2, 3, 6)
    errors = predictions[best_model] - y_test
    plt.hist(errors, bins=50, color='purple', alpha=0.7, edgecolor='black')
    plt.axvline(x=0, color='red', linestyle='--', label='ì˜¤ì°¨ = 0')
    plt.xlabel('ì˜ˆì¸¡ ì˜¤ì°¨')
    plt.ylabel('ë¹ˆë„')
    plt.title(f'{best_model.upper()} ì˜¤ì°¨ ë¶„í¬', fontsize=12, fontweight='bold')
    plt.legend()
    
    mean_error = np.mean(errors)
    std_error = np.std(errors)
    plt.text(0.05, 0.95, f'í‰ê· : {mean_error:.1f}\ní‘œì¤€í¸ì°¨: {std_error:.1f}',
            transform=plt.gca().transAxes, verticalalignment='top')
    
    plt.tight_layout()
    plt.savefig(f'{Config.PLOT_DIR}model_evaluation_summary.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"  âœ… ì‹œê°í™” ì €ì¥: {Config.PLOT_DIR}model_evaluation_summary.png")

def create_detailed_report(results, y_test):
    """ìƒì„¸ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
    print("\nğŸ“ ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    
    report = []
    report.append("="*80)
    report.append("V6 ëª¨ë¸ í‰ê°€ ë¦¬í¬íŠ¸")
    report.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("="*80)
    report.append("")
    
    # ë°ì´í„° ê°œìš”
    report.append("ğŸ“Š í‰ê°€ ë°ì´í„° ê°œìš”")
    report.append(f"  - ì´ ìƒ˜í”Œ ìˆ˜: {len(y_test):,}ê°œ")
    report.append(f"  - TOTALCNT ë²”ìœ„: {y_test.min():.0f} ~ {y_test.max():.0f}")
    report.append(f"  - TOTALCNT í‰ê· : {y_test.mean():.0f}")
    report.append(f"  - TOTALCNT í‘œì¤€í¸ì°¨: {y_test.std():.0f}")
    report.append("")
    
    # ê¸‰ì¦ ë°ì´í„° ë¶„í¬
    report.append("ğŸ“ˆ ê¸‰ì¦ ë°ì´í„° ë¶„í¬")
    for level in [1300, 1400, 1450, 1500, 1550]:
        count = np.sum(y_test >= level)
        ratio = count / len(y_test) * 100
        report.append(f"  - {level}+ : {count:,}ê°œ ({ratio:.1f}%)")
    report.append("")
    
    # ëª¨ë¸ë³„ ì„±ëŠ¥
    report.append("ğŸ† ëª¨ë¸ë³„ ì„±ëŠ¥ í‰ê°€")
    report.append("-"*80)
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
    best_model = min(results.keys(), key=lambda x: results[x]['mae'])
    
    for name, metrics in results.items():
        is_best = " â­ BEST" if name == best_model else ""
        report.append(f"\nğŸ“ {name.upper()} ëª¨ë¸{is_best}")
        report.append(f"  ì „ì²´ ì„±ëŠ¥:")
        report.append(f"    - MAE: {metrics['mae']:.2f}")
        report.append(f"    - RMSE: {metrics['rmse']:.2f}")
        report.append(f"    - MAPE: {metrics['mape']:.2f}%")
        report.append(f"    - ì •í™•ë„(Â±50): {metrics['accuracy_50']:.1f}%")
        report.append(f"    - ì •í™•ë„(Â±100): {metrics['accuracy_100']:.1f}%")
        
        report.append(f"  \n  ê¸‰ì¦ ê°ì§€ ì„±ëŠ¥:")
        for level, perf in metrics['levels'].items():
            if perf['count'] > 0:
                report.append(f"    {level}+ ê°ì§€:")
                report.append(f"      - Recall: {perf['recall']:.1%}")
                report.append(f"      - Precision: {perf['precision']:.1%}")
                report.append(f"      - F1 Score: {perf['f1']:.1%}")
                report.append(f"      - MAE: {perf['mae']:.1f}")
                report.append(f"      - ìƒ˜í”Œ ìˆ˜: {perf['count']:,}ê°œ")
    
    report.append("")
    report.append("="*80)
    report.append("ğŸ¯ ê²°ë¡ ")
    report.append(f"  ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model.upper()}")
    report.append(f"  MAE: {results[best_model]['mae']:.2f}")
    report.append(f"  ì •í™•ë„(Â±50): {results[best_model]['accuracy_50']:.1f}%")
    
    # ê°œì„  ê¶Œì¥ì‚¬í•­
    report.append("")
    report.append("ğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­")
    
    # CNN-LSTMì´ë‚˜ Spike ì„±ëŠ¥ì´ ë‚®ì€ ê²½ìš°
    if 'cnn_lstm' in results and results['cnn_lstm']['mae'] > 1000:
        report.append("  - CNN-LSTM ëª¨ë¸ ì¬í•™ìŠµ í•„ìš” (êµ¬ì¡° ë‹¨ìˆœí™” ê¶Œì¥)")
    if 'spike' in results and results['spike']['mae'] > 1000:
        report.append("  - Spike Detector ì„ê³„ê°’ ì¬ì¡°ì • í•„ìš”")
    
    # ì•™ìƒë¸” ê°œì„  ì—¬ë¶€
    if 'ensemble' in results and 'lstm' in results:
        if results['ensemble']['mae'] > results['lstm']['mae']:
            report.append("  - ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì¬ì¡°ì • ê¶Œì¥")
    
    report.append("="*80)
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    report_text = "\n".join(report)
    report_path = f"{Config.EVAL_RESULT_DIR}evaluation_report.txt"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    print(f"  âœ… ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    
    # ê²°ê³¼ JSON ì €ì¥
    json_path = f"{Config.EVAL_RESULT_DIR}evaluation_results.json"
    with open(json_path, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"  âœ… JSON ê²°ê³¼ ì €ì¥: {json_path}")
    
    return report_text

# ============================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================
def main():
    """ë©”ì¸ í‰ê°€ í”„ë¡œì„¸ìŠ¤"""
    
    try:
        # 1. í‰ê°€ ë°ì´í„° ì¤€ë¹„
        df = prepare_evaluation_data(Config.EVAL_DATA_FILE)
        
        # 2. ì‹œí€€ìŠ¤ ìƒì„±
        print("\nâš¡ í‰ê°€ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        data = df.values
        X, y = create_sequences(df, Config.LOOKBACK, Config.FORECAST)
        
        print(f"  X shape: {X.shape}")
        print(f"  y shape: {y.shape}")
        
        # 3. M14 íŠ¹ì§• ì¶”ì¶œ
        m14_features = np.zeros((len(X), 4), dtype=np.float32)
        for i in range(len(X)):
            idx = i + Config.LOOKBACK
            if idx < len(df):
                m14_features[i] = [
                    df['M14AM14B'].iloc[idx],
                    df['M14AM10A'].iloc[idx],
                    df['M14AM16'].iloc[idx],
                    df['ratio_14B_10A'].iloc[idx]
                ]
        
        # 4. ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        print("\nğŸ“ ë°ì´í„° ìŠ¤ì¼€ì¼ë§...")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
        X_scaled = np.zeros_like(X)
        for i in range(X.shape[2]):
            scaler = RobustScaler()
            feature = X[:, :, i].reshape(-1, 1)
            X_scaled[:, :, i] = scaler.fit_transform(feature).reshape(X[:, :, i].shape)
        
        m14_scaler = RobustScaler()
        m14_features_scaled = m14_scaler.fit_transform(m14_features)
        
        print("  âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
        
        # 5. ëª¨ë¸ ë¡œë“œ
        models = load_models()
        
        if not models:
            print("\nâŒ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return
        
        # 6. ëª¨ë¸ í‰ê°€
        results, predictions = evaluate_models(models, X_scaled, y, m14_features_scaled)
        
        # 7. ì‹œê°í™”
        create_visualizations(y, predictions, results)
        
        # 8. ìƒì„¸ ë¦¬í¬íŠ¸
        report = create_detailed_report(results, y)
        
        # 9. ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ“Š í‰ê°€ ì™„ë£Œ!")
        print("="*60)
        
        # ìµœê³  ëª¨ë¸ ê°•ì¡°
        if results:
            best_model = min(results.keys(), key=lambda x: results[x]['mae'])
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model.upper()}")
            print(f"  - MAE: {results[best_model]['mae']:.2f}")
            print(f"  - RMSE: {results[best_model]['rmse']:.2f}")
            print(f"  - ì •í™•ë„(Â±50): {results[best_model]['accuracy_50']:.1f}%")
            print(f"  - ì •í™•ë„(Â±100): {results[best_model]['accuracy_100']:.1f}%")
            
            # ê¸‰ì¦ ê°ì§€ ì„±ëŠ¥
            if 1400 in results[best_model]['levels']:
                perf_1400 = results[best_model]['levels'][1400]
                print(f"\n  1400+ ê¸‰ì¦ ê°ì§€:")
                print(f"    - Recall: {perf_1400['recall']:.1%}")
                print(f"    - Precision: {perf_1400['precision']:.1%}")
                print(f"    - F1 Score: {perf_1400['f1']:.1%}")
        
        print("\nğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜:")
        print(f"  - ë¦¬í¬íŠ¸: {Config.EVAL_RESULT_DIR}evaluation_report.txt")
        print(f"  - JSON: {Config.EVAL_RESULT_DIR}evaluation_results.json")
        print(f"  - ì‹œê°í™”: {Config.PLOT_DIR}model_evaluation_summary.png")
        
        print("\nâœ… ëª¨ë“  í‰ê°€ ì‘ì—… ì™„ë£Œ!")
        print("="*60)
        
    except FileNotFoundError:
        print(f"\nâŒ í‰ê°€ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {Config.EVAL_DATA_FILE}")
        print("íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()