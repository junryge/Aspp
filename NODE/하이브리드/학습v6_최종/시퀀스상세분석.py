# -*- coding: utf-8 -*-
"""
ì™„ì „í•œ ì „ì²´ ë°ì´í„° ì‹œí€€ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ (í•œê¸€ ê¹¨ì§ í•´ê²°)
===============================================
ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ ì „ì²´ ë¶„ì„í•˜ì—¬ ì •í™•í•œ í†µê³„ ìƒì„±
í•œê¸€ í°íŠ¸ ìë™ ì„¤ì • ë° ê¹¨ì§ ë°©ì§€
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
import os
import pickle
import json
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler

warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì • (ìš´ì˜ì²´ì œë³„ ìë™ ê°ì§€)
import platform
import matplotlib.font_manager as fm

def set_korean_font():
    """í•œê¸€ í°íŠ¸ ìë™ ì„¤ì •"""
    system = platform.system()
    
    if system == 'Windows':
        # Windows í•œê¸€ í°íŠ¸
        font_list = ['Malgun Gothic', 'Microsoft YaHei', 'SimHei', 'Gulim', 'Dotum']
    elif system == 'Darwin':  # macOS
        # macOS í•œê¸€ í°íŠ¸
        font_list = ['AppleGothic', 'Noto Sans CJK KR', 'Nanum Gothic', 'Helvetica']
    else:  # Linux
        # Linux í•œê¸€ í°íŠ¸ (Docker/Server í™˜ê²½)
        font_list = ['Noto Sans CJK KR', 'NanumGothic', 'UnDotum', 'DejaVu Sans']
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ í°íŠ¸ ì°¾ê¸°
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    for font_name in font_list:
        if font_name in available_fonts:
            plt.rcParams['font.family'] = font_name
            plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
            print(f"âœ… í•œê¸€ í°íŠ¸ ì„¤ì •: {font_name}")
            return font_name
    
    # í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ ì„¤ì • + ì˜ë¬¸ ë ˆì´ë¸” ì‚¬ìš©
    plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['axes.unicode_minus'] = False
    print("âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜ë¬¸ ë ˆì´ë¸”ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    return None

# í•œê¸€ í°íŠ¸ ì„¤ì • ì‹¤í–‰
korean_font = set_korean_font()

class FullDataSequenceAnalyzer:
    """ì „ì²´ ë°ì´í„° ì‹œí€€ìŠ¤ ë¶„ì„ê¸° - ìƒ˜í”Œë§ ì—†ìŒ, í•œê¸€ ê¹¨ì§ í•´ê²°"""
    
    def __init__(self):
        print("="*80)
        print("ì „ì²´ ë°ì´í„° ì‹œí€€ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ (ìƒ˜í”Œë§ ì—†ìŒ)")
        print("="*80)
        
        # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('./checkpoints', exist_ok=True)
        os.makedirs('./scalers', exist_ok=True)
        os.makedirs('./visualizations', exist_ok=True)
        
        self.checkpoint_path = './checkpoints/full_sequence_analysis_state.pkl'
        self.state = self.load_checkpoint()
        
        # í•œê¸€/ì˜ë¬¸ ë ˆì´ë¸” ì„¤ì •
        self.labels = self._get_labels()
        
    def _get_labels(self):
        """í•œê¸€ í°íŠ¸ ì§€ì› ì—¬ë¶€ì— ë”°ë¥¸ ë ˆì´ë¸” ì„¤ì •"""
        if korean_font:
            # í•œê¸€ ë ˆì´ë¸”
            return {
                'sequence_length': 'ì‹œí€€ìŠ¤ ê¸¸ì´ (ë¶„)',
                'high_value_ratio': 'ê³ ê°’ ì‹œí€€ìŠ¤ ë¹„ìœ¨ (%)',
                'high_value_detection': 'ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ê³ ê°’ ê°ì§€ìœ¨',
                'avg_max_value': 'í‰ê·  ìµœëŒ€ê°’',
                'avg_max_title': 'ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ í‰ê·  ìµœëŒ€ê°’',
                'volatility': 'í‰ê·  ë³€ë™ì„±',
                'volatility_title': 'ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë³€ë™ì„±',
                'performance_score': 'ì¢…í•© ì„±ëŠ¥ ì ìˆ˜',
                'performance_title': 'ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì¢…í•© ì„±ëŠ¥',
                'current_100min': 'í˜„ì¬ 100ë¶„',
                'optimal': 'ìµœì ',
                'hour': 'ì‹œê°„',
                'avg_totalcnt': 'í‰ê·  TOTALCNT',
                'hourly_avg': 'ì‹œê°„ëŒ€ë³„ í‰ê· ê°’',
                'high_ratio': 'ê³ ê°’ ë¹„ìœ¨ (%)',
                'hourly_high_ratio': 'ì‹œê°„ëŒ€ë³„ ê³ ê°’ ë¹„ìœ¨',
                'std_dev': 'í‘œì¤€í¸ì°¨',
                'hourly_volatility': 'ì‹œê°„ëŒ€ë³„ ë³€ë™ì„±',
                'sample_count': 'ìƒ˜í”Œ ìˆ˜',
                'hourly_distribution': 'ì‹œê°„ëŒ€ë³„ ë°ì´í„° ë¶„í¬'
            }
        else:
            # ì˜ë¬¸ ë ˆì´ë¸”
            return {
                'sequence_length': 'Sequence Length (min)',
                'high_value_ratio': 'High Value Sequence Ratio (%)',
                'high_value_detection': 'High Value Detection Rate by Sequence Length',
                'avg_max_value': 'Average Max Value',
                'avg_max_title': 'Average Max Value by Sequence Length',
                'volatility': 'Average Volatility',
                'volatility_title': 'Volatility by Sequence Length',
                'performance_score': 'Overall Performance Score',
                'performance_title': 'Overall Performance by Sequence Length',
                'current_100min': 'Current 100min',
                'optimal': 'Optimal',
                'hour': 'Hour',
                'avg_totalcnt': 'Average TOTALCNT',
                'hourly_avg': 'Hourly Average Values',
                'high_ratio': 'High Value Ratio (%)',
                'hourly_high_ratio': 'Hourly High Value Ratio',
                'std_dev': 'Standard Deviation',
                'hourly_volatility': 'Hourly Volatility',
                'sample_count': 'Sample Count',
                'hourly_distribution': 'Hourly Data Distribution'
            }
    
    def load_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        if os.path.exists(self.checkpoint_path):
            try:
                with open(self.checkpoint_path, 'rb') as f:
                    state = pickle.load(f)
                print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œë¨ - Step {state.get('step', 0)} ì™„ë£Œ")
                return state
            except:
                print("âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨ - ìƒˆë¡œ ì‹œì‘")
        
        return {'step': 0, 'sequence_lengths': [], 'results': []}
    
    def save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        with open(self.checkpoint_path, 'wb') as f:
            pickle.dump(self.state, f)
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ë¨ - Step {self.state['step']}")
    
    def step1_load_data(self, filepath=None):
        """Step 1: ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ë¶„ì„"""
        print("\n" + "="*60)
        print("ğŸ“‚ Step 1: ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ë¶„ì„")
        print("="*60)
        
        # ë°ì´í„° íŒŒì¼ ì°¾ê¸°
        if filepath is None:
            data_files = [
                'data/gs.CSV',
                'gs.CSV',
                './gs.CSV',
                'gs.csv',
                './gs.csv'
            ]
            
            filepath = None
            for file in data_files:
                if os.path.exists(file):
                    filepath = file
                    break
            
            if filepath is None:
                print("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
                return False
        
        print(f"ğŸ“‚ ë°ì´í„° ë¡œë”©: {filepath}")
        
        # CSV ë¡œë“œ
        df = pd.read_csv(filepath)
        print(f"  ì›ë³¸ ë°ì´í„°: {df.shape[0]:,}í–‰, {df.shape[1]}ê°œ ì»¬ëŸ¼")
        print(f"  ì»¬ëŸ¼: {list(df.columns)}")
        
        # ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬
        if 'CURRTIME' in df.columns:
            df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                           format='%Y%m%d%H%M', errors='coerce')
            df = df.sort_values('CURRTIME').reset_index(drop=True)
            
            # ì‹œê°„ ì—°ì†ì„± ê²€ì¦
            time_diff = df['CURRTIME'].diff().dt.total_seconds() / 60
            expected_interval = time_diff.mode()[0] if not time_diff.mode().empty else 1
            
            print(f"\nâ° ì‹œê°„ ì—°ì†ì„± ë¶„ì„:")
            print(f"  ì˜ˆìƒ ê°„ê²©: {expected_interval:.0f}ë¶„")
            print(f"  ì‹œê°„ ë²”ìœ„: {df['CURRTIME'].min()} ~ {df['CURRTIME'].max()}")
            
            # ëˆ„ë½ëœ ì‹œê°„ í™•ì¸
            missing_times = (time_diff > expected_interval * 1.5).sum()
            print(f"  ì‹œê°„ ëˆ„ë½: {missing_times}ê°œ êµ¬ê°„")
            
        # 0ê°’ ì œê±°
        original_count = len(df)
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        removed_zeros = original_count - len(df)
        
        print(f"\nğŸ“Š ë°ì´í„° í’ˆì§ˆ:")
        print(f"  0ê°’ ì œê±°: {removed_zeros}ê°œ")
        print(f"  ìœ íš¨ ë°ì´í„°: {len(df):,}í–‰")
        
        # ê¸°ë³¸ í†µê³„
        print(f"\nğŸ“ˆ TOTALCNT í†µê³„:")
        print(f"  ë²”ìœ„: {df['TOTALCNT'].min():.0f} ~ {df['TOTALCNT'].max():.0f}")
        print(f"  í‰ê· : {df['TOTALCNT'].mean():.1f}")
        print(f"  í‘œì¤€í¸ì°¨: {df['TOTALCNT'].std():.1f}")
        print(f"  ì¤‘ì•™ê°’: {df['TOTALCNT'].median():.1f}")
        print(f"  25% ë¶„ìœ„: {df['TOTALCNT'].quantile(0.25):.1f}")
        print(f"  75% ë¶„ìœ„: {df['TOTALCNT'].quantile(0.75):.1f}")
        
        # ê³ ê°’ êµ¬ê°„ ë¶„í¬
        thresholds = [1200, 1400, 1500, 1600, 1651, 1700, 1750, 1800]
        print(f"\nğŸ¯ ì„ê³„ê°’ë³„ ë¶„í¬:")
        for threshold in thresholds:
            count = (df['TOTALCNT'] >= threshold).sum()
            percentage = count / len(df) * 100
            print(f"  {threshold}+: {count:4d}ê°œ ({percentage:5.1f}%)")
        
        # M14AM14B ë¶„í¬ (V6.7 ê¸°ì¤€)
        if 'M14AM14B' in df.columns:
            print(f"\nğŸ“Š M14AM14B ë¶„í¬:")
            m14b_thresholds = [200, 250, 300, 350, 400, 450, 500]
            for threshold in m14b_thresholds:
                count = (df['M14AM14B'] >= threshold).sum()
                percentage = count / len(df) * 100
                print(f"  {threshold}+: {count:4d}ê°œ ({percentage:5.1f}%)")
        
        # ìƒíƒœ ì €ì¥
        self.state['df'] = df
        self.state['step'] = 1
        self.save_checkpoint()
        
        return True
    
    def step2_analyze_sequence_patterns(self):
        """Step 2: ì „ì²´ ì‹œí€€ìŠ¤ íŒ¨í„´ ë¶„ì„ (ìƒ˜í”Œë§ ì—†ìŒ)"""
        if self.state['step'] < 1:
            print("âŒ Step 1ì„ ë¨¼ì € ì™„ë£Œí•´ì£¼ì„¸ìš”.")
            return False
            
        print("\n" + "="*60)
        print("ğŸ” Step 2: ì „ì²´ ì‹œí€€ìŠ¤ íŒ¨í„´ ë¶„ì„ (ì „ì²´ ë°ì´í„°)")
        print("="*60)
        
        df = self.state['df']
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •
        sequence_lengths = list(range(10, 101, 10)) + list(range(120, 301, 20))
        print(f"  ë¶„ì„í•  ì‹œí€€ìŠ¤ ê¸¸ì´: {len(sequence_lengths)}ê°œ")
        print(f"  ê¸¸ì´ ë²”ìœ„: {min(sequence_lengths)}ë¶„ ~ {max(sequence_lengths)}ë¶„")
        
        # ê° ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì „ì²´ ë¶„ì„
        sequence_analysis = []
        
        for seq_idx, seq_len in enumerate(sequence_lengths):
            print(f"\nğŸ“ [{seq_idx+1}/{len(sequence_lengths)}] ì‹œí€€ìŠ¤ ê¸¸ì´ {seq_len}ë¶„ - ì „ì²´ ë¶„ì„...")
            
            # ìƒì„± ê°€ëŠ¥í•œ ì „ì²´ ì‹œí€€ìŠ¤ ìˆ˜
            max_sequences = len(df) - seq_len - 10
            if max_sequences <= 0:
                print(f"  âŒ ë°ì´í„° ë¶€ì¡± (í•„ìš”: {seq_len + 10}ë¶„)")
                continue
            
            print(f"  ğŸ“Š ì „ì²´ ì‹œí€€ìŠ¤ ìˆ˜: {max_sequences:,}ê°œ (ìƒ˜í”Œë§ ì—†ìŒ)")
            
            # ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ í†µê³„ ìˆ˜ì§‘
            seq_stats = {
                'length': seq_len,
                'max_possible': max_sequences,
                'total_analyzed': max_sequences,  # ì „ì²´ ë¶„ì„
                'seq_max_values': [],
                'seq_min_values': [],
                'seq_mean_values': [],
                'seq_std_values': [],
                'trend_counts': {'increasing': 0, 'decreasing': 0, 'stable': 0},
                'high_value_sequences': 0,
                'extreme_sequences': 0,
                'volatility_scores': [],
                'consecutive_rises': [],
                'consecutive_falls': []
            }
            
            # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸
            checkpoint_interval = max(1000, max_sequences // 20)
            
            # ì „ì²´ ì‹œí€€ìŠ¤ ë¶„ì„
            for i, idx in enumerate(range(seq_len, max_sequences + seq_len)):
                # ì§„í–‰ë¥  í‘œì‹œ
                if i % checkpoint_interval == 0:
                    progress = i / max_sequences * 100
                    print(f"    ì§„í–‰ë¥ : {progress:.1f}% ({i:,}/{max_sequences:,})")
                
                # ì‹œí€€ìŠ¤ ë°ì´í„° ì¶”ì¶œ
                seq_data = df.iloc[idx-seq_len:idx]['TOTALCNT'].values
                
                # ê¸°ë³¸ í†µê³„
                seq_max = np.max(seq_data)
                seq_min = np.min(seq_data)
                seq_mean = np.mean(seq_data)
                seq_std = np.std(seq_data)
                
                seq_stats['seq_max_values'].append(seq_max)
                seq_stats['seq_min_values'].append(seq_min)
                seq_stats['seq_mean_values'].append(seq_mean)
                seq_stats['seq_std_values'].append(seq_std)
                seq_stats['volatility_scores'].append(seq_std)
                
                # ê³ ê°’ ì‹œí€€ìŠ¤ ì²´í¬
                if seq_max >= 1651:
                    seq_stats['high_value_sequences'] += 1
                if seq_max >= 1750:
                    seq_stats['extreme_sequences'] += 1
                
                # ì—°ì† ìƒìŠ¹/í•˜ë½ ê³„ì‚°
                consecutive_rises = 0
                consecutive_falls = 0
                
                for j in range(len(seq_data)-1, 0, -1):
                    if seq_data[j] > seq_data[j-1]:
                        consecutive_rises += 1
                    else:
                        break
                        
                for j in range(len(seq_data)-1, 0, -1):
                    if seq_data[j] < seq_data[j-1]:
                        consecutive_falls += 1
                    else:
                        break
                
                seq_stats['consecutive_rises'].append(consecutive_rises)
                seq_stats['consecutive_falls'].append(consecutive_falls)
                
                # ì¶”ì„¸ ë¶„ì„
                if len(seq_data) >= 20:
                    x = np.arange(len(seq_data))
                    slope = np.polyfit(x, seq_data, 1)[0]
                    
                    if slope > 1:
                        seq_stats['trend_counts']['increasing'] += 1
                    elif slope < -1:
                        seq_stats['trend_counts']['decreasing'] += 1
                    else:
                        seq_stats['trend_counts']['stable'] += 1
                else:
                    seq_stats['trend_counts']['stable'] += 1
            
            # ì „ì²´ ë¶„ì„ ì™„ë£Œ í†µê³„ ìš”ì•½
            print(f"  âœ… ì „ì²´ ë¶„ì„ ì™„ë£Œ: {max_sequences:,}ê°œ")
            print(f"  ğŸ“Š ì „ì²´ í†µê³„:")
            print(f"    MAX ë²”ìœ„: {min(seq_stats['seq_max_values']):.0f} ~ {max(seq_stats['seq_max_values']):.0f}")
            print(f"    í‰ê·  MAX: {np.mean(seq_stats['seq_max_values']):.1f}")
            print(f"    ê³ ê°’(1651+): {seq_stats['high_value_sequences']}ê°œ ({seq_stats['high_value_sequences']/max_sequences*100:.1f}%)")
            print(f"    ê·¹ê°’(1750+): {seq_stats['extreme_sequences']}ê°œ ({seq_stats['extreme_sequences']/max_sequences*100:.1f}%)")
            print(f"    í‰ê·  ë³€ë™ì„±: {np.mean(seq_stats['volatility_scores']):.1f}")
            print(f"    í‰ê·  ì—°ì†ìƒìŠ¹: {np.mean(seq_stats['consecutive_rises']):.1f}")
            print(f"    í‰ê·  ì—°ì†í•˜ë½: {np.mean(seq_stats['consecutive_falls']):.1f}")
            print(f"    ì¦ê°€ ì¶”ì„¸: {seq_stats['trend_counts']['increasing']}ê°œ")
            print(f"    ê°ì†Œ ì¶”ì„¸: {seq_stats['trend_counts']['decreasing']}ê°œ")
            print(f"    ì•ˆì • ì¶”ì„¸: {seq_stats['trend_counts']['stable']}ê°œ")
            
            sequence_analysis.append(seq_stats)
        
        # ìƒíƒœ ì €ì¥
        self.state['sequence_analysis'] = sequence_analysis
        self.state['sequence_lengths'] = sequence_lengths
        self.state['step'] = 2
        self.save_checkpoint()
        
        return True
    
    def step3_model_specific_analysis(self):
        """Step 3: ì „ì²´ ë°ì´í„° ê¸°ë°˜ ëª¨ë¸ë³„ ìƒì„¸ ë¶„ì„"""
        if self.state['step'] < 2:
            print("âŒ Step 2ë¥¼ ë¨¼ì € ì™„ë£Œí•´ì£¼ì„¸ìš”.")
            return False
            
        print("\n" + "="*60)
        print("ğŸ¤– Step 3: ì „ì²´ ë°ì´í„° ê¸°ë°˜ ëª¨ë¸ë³„ ìƒì„¸ ë¶„ì„")
        print("="*60)
        
        df = self.state['df']
        sequence_analysis = self.state['sequence_analysis']
        
        # ëª¨ë¸ ì •ì˜
        models = {
            'LSTM': {'focus': 'ì¥ê¸° íŒ¨í„´', 'optimal_seq': 100, 'weight': 0.25},
            'GRU': {'focus': 'ë‹¨ê¸° ë³€í™”', 'optimal_seq': 60, 'weight': 0.20},
            'CNN_LSTM': {'focus': 'ë³µí•© íŒ¨í„´', 'optimal_seq': 80, 'weight': 0.25},
            'SpikeDetector': {'focus': 'ê¸‰ë³€ ê°ì§€', 'optimal_seq': 20, 'weight': 0.15},
            'ExtremeNet': {'focus': 'ê·¹ë‹¨ê°’', 'optimal_seq': 200, 'weight': 0.15}
        }
        
        model_analysis = {}
        
        for seq_stats in sequence_analysis:
            seq_len = seq_stats['length']
            total_sequences = seq_stats['total_analyzed']
            
            print(f"\nğŸ“ ì‹œí€€ìŠ¤ {seq_len}ë¶„ ({total_sequences:,}ê°œ ì „ì²´) - ëª¨ë¸ë³„ ë¶„ì„:")
            
            # ê° ëª¨ë¸ë³„ ë¶„ì„
            for model_name, model_info in models.items():
                print(f"\n  ğŸ¤– {model_name} ({model_info['focus']}) - ê°€ì¤‘ì¹˜: {model_info['weight']}:")
                
                if model_name not in model_analysis:
                    model_analysis[model_name] = {
                        'sequences': [],
                        'boost_conditions': [],
                        'performance_estimates': [],
                        'detailed_stats': []
                    }
                
                # ëª¨ë¸ë³„ ë¶€ìŠ¤íŒ… ì¡°ê±´ ê³„ì‚° (ì „ì²´ ë°ì´í„° ê¸°ë°˜)
                boost_count = 0
                performance_score = 0
                detailed_stats = {}
                
                # ExtremeNet ë¶„ì„ (V6.7 ì¡°ê±´)
                if model_name == 'ExtremeNet':
                    high_seq = seq_stats['high_value_sequences']
                    inc_trend = seq_stats['trend_counts']['increasing']
                    total = seq_stats['total_analyzed']
                    
                    # V6.7 ë¶€ìŠ¤íŒ… ì¡°ê±´: high_value + increasing trend
                    boost_count = int(high_seq * inc_trend / total * 0.4)  # ì „ì²´ ë°ì´í„°ì´ë¯€ë¡œ ë” ì •í™•í•œ ì¶”ì •
                    performance_score = (high_seq / total * 100) * 1.2  # ê³ ê°’ ê°ì§€ íŠ¹í™”
                    
                    detailed_stats = {
                        'high_sequences': high_seq,
                        'high_ratio': high_seq / total * 100,
                        'increasing_trends': inc_trend,
                        'boost_ratio': boost_count / total * 100
                    }
                    
                    print(f"    ì „ì²´ ê³ ê°’ ì‹œí€€ìŠ¤: {high_seq:,}ê°œ ({high_seq/total*100:.2f}%)")
                    print(f"    ì „ì²´ ì¦ê°€ ì¶”ì„¸: {inc_trend:,}ê°œ ({inc_trend/total*100:.2f}%)")
                    print(f"    V6.7 ë¶€ìŠ¤íŒ… ì˜ˆìƒ: {boost_count:,}ê°œ ({boost_count/total*100:.2f}%)")
                    print(f"    ì„±ëŠ¥ ì ìˆ˜: {performance_score:.1f}%")
                
                # SpikeDetector ë¶„ì„
                elif model_name == 'SpikeDetector':
                    avg_volatility = np.mean(seq_stats['volatility_scores'])
                    avg_consecutive_rises = np.mean(seq_stats['consecutive_rises'])
                    high_volatility_count = sum(1 for v in seq_stats['volatility_scores'] if v > 30)
                    total = seq_stats['total_analyzed']
                    
                    # ë³€ë™ì„± + ì—°ì† ìƒìŠ¹ ê¸°ë°˜ ë¶€ìŠ¤íŒ…
                    boost_count = high_volatility_count + seq_stats['trend_counts']['increasing']
                    performance_score = min(95, (avg_volatility / 40 + avg_consecutive_rises / 5) * 30)
                    
                    detailed_stats = {
                        'avg_volatility': avg_volatility,
                        'high_volatility_sequences': high_volatility_count,
                        'avg_consecutive_rises': avg_consecutive_rises,
                        'boost_ratio': boost_count / total * 100
                    }
                    
                    print(f"    ì „ì²´ í‰ê·  ë³€ë™ì„±: {avg_volatility:.1f}")
                    print(f"    ê³ ë³€ë™ì„± ì‹œí€€ìŠ¤: {high_volatility_count:,}ê°œ ({high_volatility_count/total*100:.2f}%)")
                    print(f"    í‰ê·  ì—°ì†ìƒìŠ¹: {avg_consecutive_rises:.1f}íšŒ")
                    print(f"    ë¶€ìŠ¤íŒ… ì˜ˆìƒ: {boost_count:,}ê°œ ({boost_count/total*100:.2f}%)")
                    print(f"    ì„±ëŠ¥ ì ìˆ˜: {performance_score:.1f}%")
                
                # LSTM/GRU/CNN-LSTM ë¶„ì„
                else:
                    optimal_seq = model_info['optimal_seq']
                    length_penalty = abs(seq_len - optimal_seq) / optimal_seq
                    base_performance = 75
                    
                    # ì‹œí€€ìŠ¤ ê¸¸ì´ ìµœì í™” ì ìˆ˜
                    performance_score = base_performance * (1 - length_penalty * 0.4)
                    
                    # ì „ì²´ ë°ì´í„°ì—ì„œ í•´ë‹¹ ëª¨ë¸ì— ìœ ë¦¬í•œ íŒ¨í„´ ìˆ˜
                    if model_name == 'LSTM' and seq_len >= 80:
                        performance_score += 10  # ì¥ê¸° íŒ¨í„´ ë³´ë„ˆìŠ¤
                    elif model_name == 'GRU' and 40 <= seq_len <= 80:
                        performance_score += 8   # ì¤‘ê¸° íŒ¨í„´ ë³´ë„ˆìŠ¤
                    elif model_name == 'CNN_LSTM' and 60 <= seq_len <= 100:
                        performance_score += 9   # ë³µí•© íŒ¨í„´ ë³´ë„ˆìŠ¤
                    
                    performance_score = max(50, min(95, performance_score))
                    boost_count = 0  # ê¸°ë³¸ ëª¨ë¸ë“¤ì€ íŠ¹ë³„í•œ ë¶€ìŠ¤íŒ… ì—†ìŒ
                    
                    detailed_stats = {
                        'optimal_length': optimal_seq,
                        'current_length': seq_len,
                        'length_penalty': length_penalty,
                        'base_performance': base_performance
                    }
                    
                    print(f"    ìµœì  ê¸¸ì´: {optimal_seq}ë¶„ (í˜„ì¬: {seq_len}ë¶„)")
                    print(f"    ê¸¸ì´ í˜ë„í‹°: {length_penalty:.2f}")
                    print(f"    ì„±ëŠ¥ ì ìˆ˜: {performance_score:.1f}%")
                    print(f"    ë¶€ìŠ¤íŒ…: ì—†ìŒ (ê¸°ë³¸ ëª¨ë¸)")
                
                # ê²°ê³¼ ì €ì¥
                model_analysis[model_name]['sequences'].append(seq_len)
                model_analysis[model_name]['boost_conditions'].append(boost_count)
                model_analysis[model_name]['performance_estimates'].append(performance_score)
                model_analysis[model_name]['detailed_stats'].append(detailed_stats)
        
        # ìƒíƒœ ì €ì¥
        self.state['model_analysis'] = model_analysis
        self.state['step'] = 3
        self.save_checkpoint()
        
        return True
    
    def step4_hourly_analysis(self):
        """Step 4: ì „ì²´ ë°ì´í„° ì‹œê°„ëŒ€ë³„ ìƒì„¸ ë¶„ì„"""
        if self.state['step'] < 2:
            print("âŒ Step 2ë¥¼ ë¨¼ì € ì™„ë£Œí•´ì£¼ì„¸ìš”.")
            return False
            
        print("\n" + "="*60)
        print("â° Step 4: ì „ì²´ ë°ì´í„° ì‹œê°„ëŒ€ë³„ ìƒì„¸ ë¶„ì„")
        print("="*60)
        
        df = self.state['df']
        
        # ì‹œê°„ëŒ€ë³„ ì „ì²´ ë¶„ì„
        hourly_stats = {}
        
        for hour in range(24):
            hour_data = df[df['CURRTIME'].dt.hour == hour]
            
            if len(hour_data) == 0:
                continue
                
            stats = {
                'hour': hour,
                'total_samples': len(hour_data),
                'avg_totalcnt': hour_data['TOTALCNT'].mean(),
                'std_totalcnt': hour_data['TOTALCNT'].std(),
                'max_totalcnt': hour_data['TOTALCNT'].max(),
                'min_totalcnt': hour_data['TOTALCNT'].min(),
                'median_totalcnt': hour_data['TOTALCNT'].median(),
                'q25_totalcnt': hour_data['TOTALCNT'].quantile(0.25),
                'q75_totalcnt': hour_data['TOTALCNT'].quantile(0.75),
                'high_value_count': (hour_data['TOTALCNT'] >= 1651).sum(),
                'extreme_value_count': (hour_data['TOTALCNT'] >= 1750).sum(),
                'high_ratio': (hour_data['TOTALCNT'] >= 1651).sum() / len(hour_data) * 100,
                'extreme_ratio': (hour_data['TOTALCNT'] >= 1750).sum() / len(hour_data) * 100
            }
            
            # M14AM14B ë¶„ì„ (ìˆëŠ” ê²½ìš°)
            if 'M14AM14B' in hour_data.columns:
                stats['avg_m14b'] = hour_data['M14AM14B'].mean()
                stats['std_m14b'] = hour_data['M14AM14B'].std()
                stats['high_m14b_count'] = (hour_data['M14AM14B'] >= 300).sum()
                stats['high_m14b_ratio'] = (hour_data['M14AM14B'] >= 300).sum() / len(hour_data) * 100
            
            # ë³€ë™ì„± ì§€í‘œ
            if len(hour_data) > 1:
                hour_data_sorted = hour_data.sort_values('CURRTIME')
                changes = hour_data_sorted['TOTALCNT'].diff().dropna()
                stats['avg_change'] = changes.mean()
                stats['volatility'] = changes.std()
                stats['positive_changes'] = (changes > 0).sum()
                stats['negative_changes'] = (changes < 0).sum()
            
            hourly_stats[hour] = stats
        
        # ì‹œê°„ëŒ€ë³„ ì „ì²´ ìš”ì•½ ì¶œë ¥
        print(f"\nğŸ“Š ì‹œê°„ëŒ€ë³„ ì „ì²´ TOTALCNT í†µê³„:")
        print(f"{'ì‹œê°„':>4} {'ì „ì²´ìˆ˜':>7} {'í‰ê· ':>6} {'ì¤‘ì•™ê°’':>6} {'ìµœëŒ€':>6} {'ê³ ê°’ìˆ˜':>6} {'ê³ ê°’ë¹„ìœ¨':>7} {'ë³€ë™ì„±':>6}")
        print("-" * 65)
        
        for hour in range(24):
            if hour in hourly_stats:
                stats = hourly_stats[hour]
                volatility = stats.get('volatility', 0)
                print(f"{hour:2d}ì‹œ {stats['total_samples']:7d} {stats['avg_totalcnt']:6.0f} "
                      f"{stats['median_totalcnt']:6.0f} {stats['max_totalcnt']:6.0f} "
                      f"{stats['high_value_count']:6d} {stats['high_ratio']:6.1f}% {volatility:6.1f}")
        
        # ìƒíƒœ ì €ì¥
        self.state['hourly_stats'] = hourly_stats
        self.state['step'] = 4
        self.save_checkpoint()
        
        return True
    
    def step5_visualization(self):
        """Step 5: ìƒì„¸ ì‹œê°í™” (í•œê¸€ ê¹¨ì§ ë°©ì§€)"""
        if self.state['step'] < 3:
            print("âŒ Step 3ê¹Œì§€ ë¨¼ì € ì™„ë£Œí•´ì£¼ì„¸ìš”.")
            return False
            
        print("\n" + "="*60)
        print("ğŸ“Š Step 5: ìƒì„¸ ì‹œê°í™” (í•œê¸€ ê¹¨ì§ ë°©ì§€)")
        print("="*60)
        
        # 1. ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì„±ëŠ¥ ë¹„êµ
        self._plot_sequence_performance()
        
        # 2. ëª¨ë¸ë³„ ì„±ëŠ¥ íˆíŠ¸ë§µ
        self._plot_model_heatmap()
        
        # 3. ì‹œê°„ëŒ€ë³„ ë¶„ì„
        if 'hourly_stats' in self.state:
            self._plot_hourly_analysis()
        
        # 4. ë°ì´í„° ë¶„í¬
        self._plot_data_distribution()
        
        self.state['step'] = 5
        self.save_checkpoint()
        
        return True
    
    def _plot_sequence_performance(self):
        """ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì„±ëŠ¥ ì‹œê°í™” (í•œê¸€ ì§€ì›)"""
        sequence_analysis = self.state['sequence_analysis']
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # ë°ì´í„° ì¤€ë¹„
        lengths = [s['length'] for s in sequence_analysis]
        high_ratios = [s['high_value_sequences']/s['total_analyzed']*100 for s in sequence_analysis]
        avg_maxes = [np.mean(s['seq_max_values']) for s in sequence_analysis]
        avg_volatilities = [np.mean(s['volatility_scores']) for s in sequence_analysis]
        
        # 1. ê³ ê°’ ë¹„ìœ¨ vs ì‹œí€€ìŠ¤ ê¸¸ì´
        axes[0,0].plot(lengths, high_ratios, 'bo-', linewidth=2, markersize=6)
        axes[0,0].axhline(y=12.9, color='r', linestyle='--', alpha=0.7, 
                         label=f"{self.labels['current_100min']} (12.9%)")
        axes[0,0].set_xlabel(self.labels['sequence_length'])
        axes[0,0].set_ylabel(self.labels['high_value_ratio'])
        axes[0,0].set_title(self.labels['high_value_detection'])
        axes[0,0].grid(True, alpha=0.3)
        axes[0,0].legend()
        
        # 2. í‰ê·  MAX ê°’
        axes[0,1].plot(lengths, avg_maxes, 'go-', linewidth=2, markersize=6)
        axes[0,1].axhline(y=1497, color='r', linestyle='--', alpha=0.7, 
                         label=f"{self.labels['current_100min']} (1497)")
        axes[0,1].set_xlabel(self.labels['sequence_length'])
        axes[0,1].set_ylabel(self.labels['avg_max_value'])
        axes[0,1].set_title(self.labels['avg_max_title'])
        axes[0,1].grid(True, alpha=0.3)
        axes[0,1].legend()
        
        # 3. ë³€ë™ì„±
        axes[1,0].plot(lengths, avg_volatilities, 'ro-', linewidth=2, markersize=6)
        axes[1,0].set_xlabel(self.labels['sequence_length'])
        axes[1,0].set_ylabel(self.labels['volatility'])
        axes[1,0].set_title(self.labels['volatility_title'])
        axes[1,0].grid(True, alpha=0.3)
        
        # 4. ì¢…í•© ì„±ëŠ¥ ìŠ¤ì½”ì–´
        performance_scores = []
        for i, length in enumerate(lengths):
            high_weight = 0.7
            stability_weight = 0.3
            
            # ì •ê·œí™”
            norm_high = high_ratios[i] / max(high_ratios) if max(high_ratios) > 0 else 0
            norm_stability = 1 - (avg_volatilities[i] - min(avg_volatilities)) / (max(avg_volatilities) - min(avg_volatilities)) if (max(avg_volatilities) - min(avg_volatilities)) > 0 else 0.5
            
            score = high_weight * norm_high + stability_weight * norm_stability
            performance_scores.append(score * 100)
        
        axes[1,1].plot(lengths, performance_scores, 'mo-', linewidth=2, markersize=6)
        if performance_scores:
            best_idx = np.argmax(performance_scores)
            axes[1,1].scatter([lengths[best_idx]], [performance_scores[best_idx]], 
                             c='red', s=100, zorder=5, label=f'{self.labels["optimal"]}: {lengths[best_idx]}min')
        axes[1,1].set_xlabel(self.labels['sequence_length'])
        axes[1,1].set_ylabel(self.labels['performance_score'])
        axes[1,1].set_title(self.labels['performance_title'])
        axes[1,1].grid(True, alpha=0.3)
        axes[1,1].legend()
        
        plt.tight_layout()
        plt.savefig('./visualizations/sequence_performance.png', dpi=150, bbox_inches='tight')
        plt.show()
        print("âœ… ì‹œí€€ìŠ¤ ì„±ëŠ¥ ë¶„ì„ ì €ì¥: ./visualizations/sequence_performance.png")
    
    def _plot_model_heatmap(self):
        """ëª¨ë¸ë³„ ì„±ëŠ¥ íˆíŠ¸ë§µ (í•œê¸€ ì§€ì›)"""
        model_analysis = self.state['model_analysis']
        
        # ë°ì´í„° ì¤€ë¹„
        models = list(model_analysis.keys())
        sequences = model_analysis[models[0]]['sequences']
        
        # ì„±ëŠ¥ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        performance_matrix = []
        for model in models:
            performance_matrix.append(model_analysis[model]['performance_estimates'])
        
        # íˆíŠ¸ë§µ ìƒì„±
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # 1. ì„±ëŠ¥ íˆíŠ¸ë§µ
        im1 = axes[0].imshow(performance_matrix, cmap='RdYlBu_r', aspect='auto')
        axes[0].set_xticks(range(len(sequences)))
        axes[0].set_xticklabels([f'{s}min' for s in sequences], rotation=45)
        axes[0].set_yticks(range(len(models)))
        axes[0].set_yticklabels(models)
        axes[0].set_title('Model Performance by Sequence Length')
        
        # ê°’ í‘œì‹œ
        for i in range(len(models)):
            for j in range(len(sequences)):
                text = axes[0].text(j, i, f'{performance_matrix[i][j]:.1f}%',
                                   ha="center", va="center", 
                                   color="black" if performance_matrix[i][j] < 70 else "white")
        
        plt.colorbar(im1, ax=axes[0], label='Performance Score (%)')
        
        # 2. ë¶€ìŠ¤íŒ… ì¡°ê±´ íˆíŠ¸ë§µ
        boost_matrix = []
        for model in models:
            boost_matrix.append(model_analysis[model]['boost_conditions'])
        
        im2 = axes[1].imshow(boost_matrix, cmap='Oranges', aspect='auto')
        axes[1].set_xticks(range(len(sequences)))
        axes[1].set_xticklabels([f'{s}min' for s in sequences], rotation=45)
        axes[1].set_yticks(range(len(models)))
        axes[1].set_yticklabels(models)
        axes[1].set_title('Boost Conditions by Model')
        
        # ê°’ í‘œì‹œ
        for i in range(len(models)):
            for j in range(len(sequences)):
                text = axes[1].text(j, i, f'{boost_matrix[i][j]}',
                                   ha="center", va="center", color="black")
        
        plt.colorbar(im2, ax=axes[1], label='Boost Condition Count')
        
        plt.tight_layout()
        plt.savefig('./visualizations/model_heatmap.png', dpi=150, bbox_inches='tight')
        plt.show()
        print("âœ… ëª¨ë¸ íˆíŠ¸ë§µ ì €ì¥: ./visualizations/model_heatmap.png")
    
    def _plot_hourly_analysis(self):
        """ì‹œê°„ëŒ€ë³„ ë¶„ì„ ì‹œê°í™” (í•œê¸€ ì§€ì›)"""
        hourly_stats = self.state['hourly_stats']
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        hours = sorted(hourly_stats.keys())
        avg_values = [hourly_stats[h]['avg_totalcnt'] for h in hours]
        high_ratios = [hourly_stats[h]['high_ratio'] for h in hours]
        std_values = [hourly_stats[h]['std_totalcnt'] for h in hours]
        sample_counts = [hourly_stats[h]['total_samples'] for h in hours]
        
        # 1. ì‹œê°„ëŒ€ë³„ í‰ê· ê°’
        axes[0,0].bar(hours, avg_values, color='skyblue', alpha=0.7)
        axes[0,0].set_xlabel(self.labels['hour'])
        axes[0,0].set_ylabel(self.labels['avg_totalcnt'])
        axes[0,0].set_title(self.labels['hourly_avg'])
        axes[0,0].grid(True, alpha=0.3)
        
        # 2. ì‹œê°„ëŒ€ë³„ ê³ ê°’ ë¹„ìœ¨
        axes[0,1].bar(hours, high_ratios, color='orange', alpha=0.7)
        axes[0,1].set_xlabel(self.labels['hour'])
        axes[0,1].set_ylabel(self.labels['high_ratio'])
        axes[0,1].set_title(self.labels['hourly_high_ratio'])
        axes[0,1].grid(True, alpha=0.3)
        
        # 3. ì‹œê°„ëŒ€ë³„ ë³€ë™ì„±
        axes[1,0].bar(hours, std_values, color='green', alpha=0.7)
        axes[1,0].set_xlabel(self.labels['hour'])
        axes[1,0].set_ylabel(self.labels['std_dev'])
        axes[1,0].set_title(self.labels['hourly_volatility'])
        axes[1,0].grid(True, alpha=0.3)
        
        # 4. ì‹œê°„ëŒ€ë³„ ë°ì´í„° ìˆ˜
        axes[1,1].bar(hours, sample_counts, color='purple', alpha=0.7)
        axes[1,1].set_xlabel(self.labels['hour'])
        axes[1,1].set_ylabel(self.labels['sample_count'])
        axes[1,1].set_title(self.labels['hourly_distribution'])
        axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('./visualizations/hourly_analysis.png', dpi=150, bbox_inches='tight')
        plt.show()
        print("âœ… ì‹œê°„ëŒ€ ë¶„ì„ ì €ì¥: ./visualizations/hourly_analysis.png")
    
    def _plot_data_distribution(self):
        """ë°ì´í„° ë¶„í¬ ì‹œê°í™” (í•œê¸€ ì§€ì›)"""
        df = self.state['df']
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        
        # 1. TOTALCNT íˆìŠ¤í† ê·¸ë¨
        axes[0,0].hist(df['TOTALCNT'], bins=50, alpha=0.7, color='blue')
        axes[0,0].axvline(x=1651, color='r', linestyle='--', label='1651 (V6.7 Threshold)')
        axes[0,0].axvline(x=1700, color='orange', linestyle='--', label='1700 (High Value)')
        axes[0,0].set_xlabel('TOTALCNT')
        axes[0,0].set_ylabel('Frequency')
        axes[0,0].set_title('TOTALCNT Distribution')
        axes[0,0].legend()
        axes[0,0].grid(True, alpha=0.3)
        
        # 2. ì‹œê³„ì—´ í”Œë¡¯ (ìƒ˜í”Œ)
        sample_data = df.iloc[::100]  # 100ê°œë§ˆë‹¤ ìƒ˜í”Œë§
        axes[0,1].plot(sample_data.index, sample_data['TOTALCNT'], alpha=0.7)
        axes[0,1].axhline(y=1651, color='r', linestyle='--', alpha=0.5)
        axes[0,1].axhline(y=1700, color='orange', linestyle='--', alpha=0.5)
        axes[0,1].set_xlabel('Index')
        axes[0,1].set_ylabel('TOTALCNT')
        axes[0,1].set_title('Time Series Data (Sampled)')
        axes[0,1].grid(True, alpha=0.3)
        
        # 3. ë°•ìŠ¤í”Œë¡¯ (êµ¬ê°„ë³„)
        ranges = [
            df[df['TOTALCNT'] < 1400]['TOTALCNT'],
            df[(df['TOTALCNT'] >= 1400) & (df['TOTALCNT'] < 1500)]['TOTALCNT'],
            df[(df['TOTALCNT'] >= 1500) & (df['TOTALCNT'] < 1651)]['TOTALCNT'],
            df[df['TOTALCNT'] >= 1651]['TOTALCNT']
        ]
        axes[0,2].boxplot(ranges, labels=['<1400', '1400-1500', '1500-1651', '1651+'])
        axes[0,2].set_ylabel('TOTALCNT')
        axes[0,2].set_title('Distribution by Range')
        axes[0,2].grid(True, alpha=0.3)
        
        # 4. M14AM14B vs TOTALCNT (ìˆëŠ” ê²½ìš°)
        if 'M14AM14B' in df.columns:
            sample_df = df.iloc[::50]  # ìƒ˜í”Œë§
            scatter = axes[1,0].scatter(sample_df['M14AM14B'], sample_df['TOTALCNT'], 
                                      c=sample_df['TOTALCNT'], cmap='viridis', alpha=0.6)
            axes[1,0].axhline(y=1651, color='r', linestyle='--', alpha=0.5)
            axes[1,0].axvline(x=300, color='orange', linestyle='--', alpha=0.5)
            axes[1,0].set_xlabel('M14AM14B')
            axes[1,0].set_ylabel('TOTALCNT')
            axes[1,0].set_title('M14AM14B vs TOTALCNT')
            plt.colorbar(scatter, ax=axes[1,0])
        
        # 5. ì¼ë³„ í‰ê· 
        df['date'] = df['CURRTIME'].dt.date
        daily_stats = df.groupby('date')['TOTALCNT'].agg(['mean', 'max', 'std']).reset_index()
        axes[1,1].plot(daily_stats['date'], daily_stats['mean'], 'o-', label='Mean')
        axes[1,1].plot(daily_stats['date'], daily_stats['max'], 's-', label='Max')
        axes[1,1].fill_between(daily_stats['date'], 
                              daily_stats['mean'] - daily_stats['std'],
                              daily_stats['mean'] + daily_stats['std'],
                              alpha=0.3, label='Â±1 Std')
        axes[1,1].tick_params(axis='x', rotation=45)
        axes[1,1].set_ylabel('TOTALCNT')
        axes[1,1].set_title('Daily Statistics')
        axes[1,1].legend()
        axes[1,1].grid(True, alpha=0.3)
        
        # 6. ëˆ„ì ë¶„í¬í•¨ìˆ˜
        sorted_data = np.sort(df['TOTALCNT'])
        cumulative = np.arange(1, len(sorted_data) + 1) / len(sorted_data)
        axes[1,2].plot(sorted_data, cumulative)
        axes[1,2].axvline(x=1651, color='r', linestyle='--', alpha=0.7, label='1651')
        axes[1,2].axvline(x=1700, color='orange', linestyle='--', alpha=0.7, label='1700')
        axes[1,2].set_xlabel('TOTALCNT')
        axes[1,2].set_ylabel('Cumulative Probability')
        axes[1,2].set_title('Cumulative Distribution Function')
        axes[1,2].legend()
        axes[1,2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('./visualizations/data_distribution.png', dpi=150, bbox_inches='tight')
        plt.show()
        print("âœ… ë°ì´í„° ë¶„í¬ ì €ì¥: ./visualizations/data_distribution.png")
    
    def step6_export_results(self):
        """Step 6: ì „ì²´ ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
        if self.state['step'] < 3:
            print("âŒ Step 3ê¹Œì§€ ë¨¼ì € ì™„ë£Œí•´ì£¼ì„¸ìš”.")
            return False
            
        print("\n" + "="*60)
        print("ğŸ“¤ Step 6: ì „ì²´ ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°")
        print("="*60)
        
        # ì „ì²´ ë¶„ì„ ê²°ê³¼ CSV ìƒì„±
        results = []
        
        if 'sequence_analysis' in self.state and 'model_analysis' in self.state:
            sequence_analysis = self.state['sequence_analysis']
            model_analysis = self.state['model_analysis']
            
            for seq_stats in sequence_analysis:
                seq_len = seq_stats['length']
                total_analyzed = seq_stats['total_analyzed']
                
                result = {
                    'sequence_length': seq_len,
                    'total_analyzed': total_analyzed,
                    'avg_max': np.mean(seq_stats['seq_max_values']),
                    'std_max': np.std(seq_stats['seq_max_values']),
                    'min_max': np.min(seq_stats['seq_max_values']),
                    'max_max': np.max(seq_stats['seq_max_values']),
                    'avg_volatility': np.mean(seq_stats['volatility_scores']),
                    'std_volatility': np.std(seq_stats['volatility_scores']),
                    'high_sequences_1651': seq_stats['high_value_sequences'],
                    'high_ratio_percent': seq_stats['high_value_sequences'] / total_analyzed * 100,
                    'extreme_sequences_1750': seq_stats['extreme_sequences'],
                    'extreme_ratio_percent': seq_stats['extreme_sequences'] / total_analyzed * 100,
                    'increasing_trend': seq_stats['trend_counts']['increasing'],
                    'decreasing_trend': seq_stats['trend_counts']['decreasing'],
                    'stable_trend': seq_stats['trend_counts']['stable'],
                    'avg_consecutive_rises': np.mean(seq_stats['consecutive_rises']),
                    'max_consecutive_rises': np.max(seq_stats['consecutive_rises']),
                    'avg_consecutive_falls': np.mean(seq_stats['consecutive_falls']),
                    'max_consecutive_falls': np.max(seq_stats['consecutive_falls'])
                }
                
                # ëª¨ë¸ë³„ ë°ì´í„° ì¶”ê°€
                for model_name in model_analysis:
                    sequences = model_analysis[model_name]['sequences']
                    if seq_len in sequences:
                        idx = sequences.index(seq_len)
                        result[f'{model_name}_performance'] = model_analysis[model_name]['performance_estimates'][idx]
                        result[f'{model_name}_boost_conditions'] = model_analysis[model_name]['boost_conditions'][idx]
                        result[f'{model_name}_boost_ratio'] = model_analysis[model_name]['boost_conditions'][idx] / total_analyzed * 100
                
                results.append(result)
        
        # DataFrame ìƒì„± ë° ì €ì¥
        results_df = pd.DataFrame(results)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_file = f'full_sequence_analysis_{timestamp}.csv'
        results_df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        # ì‹œê°„ëŒ€ë³„ ê²°ê³¼ ì €ì¥
        if 'hourly_stats' in self.state:
            hourly_results = []
            for hour, stats in self.state['hourly_stats'].items():
                hourly_results.append({
                    'hour': hour,
                    **stats
                })
            
            hourly_df = pd.DataFrame(hourly_results)
            hourly_csv = f'hourly_analysis_{timestamp}.csv'
            hourly_df.to_csv(hourly_csv, index=False, encoding='utf-8-sig')
        
        print(f"âœ… ì „ì²´ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"  ğŸ“Š ì‹œí€€ìŠ¤ ë¶„ì„: {csv_file}")
        if 'hourly_stats' in self.state:
            print(f"  â° ì‹œê°„ëŒ€ ë¶„ì„: {hourly_csv}")
        print(f"  ğŸ’¾ ì²´í¬í¬ì¸íŠ¸: {self.checkpoint_path}")
        
        self.state['step'] = 6
        self.save_checkpoint()
        
        return True
    
    def generate_final_report(self):
        """ì „ì²´ ë°ì´í„° ê¸°ë°˜ ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        if self.state['step'] < 3:
            print("âŒ ë¶„ì„ì´ ì¶©ë¶„íˆ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
            
        print("\n" + "="*80)
        print("ğŸ“‹ ì „ì²´ ë°ì´í„° ê¸°ë°˜ ìµœì¢… ë¶„ì„ ë³´ê³ ì„œ")
        print("="*80)
        
        sequence_analysis = self.state.get('sequence_analysis', [])
        model_analysis = self.state.get('model_analysis', {})
        hourly_stats = self.state.get('hourly_stats', {})
        
        # ì „ì²´ ë°ì´í„° ìš”ì•½
        if sequence_analysis:
            total_data_analyzed = sum(s['total_analyzed'] for s in sequence_analysis)
            print(f"\nğŸ“Š ì „ì²´ ë¶„ì„ ê·œëª¨:")
            print(f"  ë¶„ì„ëœ ì´ ì‹œí€€ìŠ¤: {total_data_analyzed:,}ê°œ")
            print(f"  ì‹œí€€ìŠ¤ ê¸¸ì´ ì¢…ë¥˜: {len(sequence_analysis)}ê°œ")
            
        # 1. ìµœì  ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶„ì„ (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
        print(f"\nğŸ† ìµœì  ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶„ì„ (ì „ì²´ ë°ì´í„°):")
        if sequence_analysis:
            # ê³ ê°’ ë¹„ìœ¨ ê¸°ì¤€
            best_high_ratio = max(sequence_analysis, key=lambda x: x['high_value_sequences']/x['total_analyzed'])
            print(f"  ğŸ¥‡ ê³ ê°’ ê°ì§€ ìµœì : {best_high_ratio['length']}ë¶„")
            print(f"    ì „ì²´ ì‹œí€€ìŠ¤: {best_high_ratio['total_analyzed']:,}ê°œ")
            print(f"    ê³ ê°’ ì‹œí€€ìŠ¤: {best_high_ratio['high_value_sequences']:,}ê°œ")
            print(f"    ê³ ê°’ ë¹„ìœ¨: {best_high_ratio['high_value_sequences']/best_high_ratio['total_analyzed']*100:.2f}%")
            print(f"    í‰ê·  MAX: {np.mean(best_high_ratio['seq_max_values']):.0f}")
            
            # í˜„ì¬ 100ë¶„ê³¼ ë¹„êµ
            current_100 = next((s for s in sequence_analysis if s['length'] == 100), None)
            if current_100:
                current_ratio = current_100['high_value_sequences']/current_100['total_analyzed']*100
                best_ratio = best_high_ratio['high_value_sequences']/best_high_ratio['total_analyzed']*100
                improvement = (best_ratio - current_ratio) / current_ratio * 100
                
                print(f"\n  í˜„ì¬ 100ë¶„ ëŒ€ë¹„ (ì „ì²´ ë°ì´í„°):")
                print(f"    í˜„ì¬ 100ë¶„: {current_ratio:.2f}% ({current_100['high_value_sequences']:,}/{current_100['total_analyzed']:,}ê°œ)")
                print(f"    ìµœì  ê¸¸ì´: {best_ratio:.2f}% ({best_high_ratio['high_value_sequences']:,}/{best_high_ratio['total_analyzed']:,}ê°œ)")
                print(f"    ì„±ëŠ¥ ê°œì„ : {improvement:+.1f}%")
        
        # 2. ëª¨ë¸ë³„ ìµœì  ì¡°ê±´ (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
        print(f"\nğŸ¤– ëª¨ë¸ë³„ ìµœì  ì¡°ê±´ (ì „ì²´ ë°ì´í„° ê¸°ì¤€):")
        for model_name, model_data in model_analysis.items():
            if model_data['performance_estimates']:
                best_idx = np.argmax(model_data['performance_estimates'])
                best_seq = model_data['sequences'][best_idx]
                best_performance = model_data['performance_estimates'][best_idx]
                best_boost = model_data['boost_conditions'][best_idx]
                
                # í•´ë‹¹ ì‹œí€€ìŠ¤ì˜ ì „ì²´ ë¶„ì„ ì •ë³´
                seq_info = next((s for s in sequence_analysis if s['length'] == best_seq), None)
                total_analyzed = seq_info['total_analyzed'] if seq_info else 0
                
                print(f"  {model_name}:")
                print(f"    ìµœì  ê¸¸ì´: {best_seq}ë¶„ (ì „ì²´ {total_analyzed:,}ê°œ ì‹œí€€ìŠ¤ ë¶„ì„)")
                print(f"    ì„±ëŠ¥ ì ìˆ˜: {best_performance:.1f}%")
                print(f"    ë¶€ìŠ¤íŒ… ì¡°ê±´: {best_boost:,}ê°œ")
                if total_analyzed > 0:
                    print(f"    ë¶€ìŠ¤íŒ… ë¹„ìœ¨: {best_boost/total_analyzed*100:.2f}%")
        
        print(f"\nğŸ“Š ì „ì²´ ë°ì´í„° ë¶„ì„ ì™„ë£Œ - ì´ {len(sequence_analysis)}ê°œ ì‹œí€€ìŠ¤ ê¸¸ì´ ì™„ì „ ê²€ì¦")
        print("="*80)
        
        return True
    
    def reset_analysis(self):
        """ë¶„ì„ ì´ˆê¸°í™”"""
        if os.path.exists(self.checkpoint_path):
            os.remove(self.checkpoint_path)
        self.state = {'step': 0, 'sequence_lengths': [], 'results': []}
        print("ğŸ”„ ë¶„ì„ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = FullDataSequenceAnalyzer()
    
    while True:
        print(f"\n{'='*60}")
        print("ğŸ“‹ ì „ì²´ ë°ì´í„° ì‹œí€€ìŠ¤ ë¶„ì„ ë©”ë‰´ (í•œê¸€ ê¹¨ì§ í•´ê²°)")
        print(f"í˜„ì¬ ì§„í–‰ ë‹¨ê³„: Step {analyzer.state['step']}")
        print(f"{'='*60}")
        
        print("\nğŸ” ì „ì²´ ë¶„ì„ ë‹¨ê³„:")
        print("1. Step 1: ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ë¶„ì„")
        print("2. Step 2: ì „ì²´ ì‹œí€€ìŠ¤ íŒ¨í„´ ë¶„ì„ (ìƒ˜í”Œë§ ì—†ìŒ)")  
        print("3. Step 3: ì „ì²´ ë°ì´í„° ê¸°ë°˜ ëª¨ë¸ë³„ ë¶„ì„")
        print("4. Step 4: ì „ì²´ ë°ì´í„° ì‹œê°„ëŒ€ë³„ ë¶„ì„")
        print("5. Step 5: ì‹œê°í™” (í•œê¸€ ê¹¨ì§ ë°©ì§€)")
        print("6. Step 6: ì „ì²´ ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°")
        
        print("\nğŸ“Š ë„êµ¬:")
        print("7. ì „ì²´ ë°ì´í„° ê¸°ë°˜ ìµœì¢… ë³´ê³ ì„œ")
        print("8. ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ í™•ì¸")
        print("9. ë¶„ì„ ì´ˆê¸°í™”")
        print("0. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒ (0-9): ")
        
        if choice == '1':
            analyzer.step1_load_data()
        elif choice == '2':
            analyzer.step2_analyze_sequence_patterns()
        elif choice == '3':
            analyzer.step3_model_specific_analysis()
        elif choice == '4':
            analyzer.step4_hourly_analysis()
        elif choice == '5':
            analyzer.step5_visualization()
        elif choice == '6':
            analyzer.step6_export_results()
        elif choice == '7':
            analyzer.generate_final_report()
        elif choice == '8':
            print(f"\nğŸ“Š ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ:")
            print(f"  ì™„ë£Œ ë‹¨ê³„: Step {analyzer.state['step']}")
            if 'df' in analyzer.state:
                print(f"  ë¡œë“œëœ ë°ì´í„°: {len(analyzer.state['df']):,}í–‰")
            if 'sequence_analysis' in analyzer.state:
                print(f"  ë¶„ì„ëœ ì‹œí€€ìŠ¤: {len(analyzer.state['sequence_analysis'])}ê°œ")
                total_analyzed = sum(s['total_analyzed'] for s in analyzer.state['sequence_analysis'])
                print(f"  ì´ ë¶„ì„ ì‹œí€€ìŠ¤: {total_analyzed:,}ê°œ (ì „ì²´)")
            if 'model_analysis' in analyzer.state:
                print(f"  ë¶„ì„ëœ ëª¨ë¸: {len(analyzer.state['model_analysis'])}ê°œ")
        elif choice == '9':
            confirm = input("âš ï¸ ëª¨ë“  ë¶„ì„ ë°ì´í„°ê°€ ì‚­ì œë©ë‹ˆë‹¤. ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
            if confirm.lower() == 'y':
                analyzer.reset_analysis()
        elif choice == '0':
            print("ì „ì²´ ë°ì´í„° ë¶„ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        else:
            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()