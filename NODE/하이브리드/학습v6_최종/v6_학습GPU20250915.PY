"""
V6_GPU_í†µí•©_ì „ì²´_ìµœì¢….py
ì‹œí€€ìŠ¤ ìƒì„± + 5ê°œ ëª¨ë¸ + ì•™ìƒë¸” = ì´ 6ê°œ ëª¨ë¸
ì²´í¬í¬ì¸íŠ¸ ì§€ì› - ì¤‘ë‹¨/ì¬ì‹œì‘ ê°€ëŠ¥
íŒŒì¼: 20240201_TO_202507281705.CSV
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import json
import os
import pickle
import warnings
from datetime import datetime
import gc
warnings.filterwarnings('ignore')

print("="*60)
print("ğŸš€ V6 GPU í†µí•© ì‹œìŠ¤í…œ - ì „ì²´ ì½”ë“œ")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print("="*60)

# ============================================
# GPU ì„¤ì •
# ============================================
def setup_gpu():
    """GPU ì„¤ì • ë° í™•ì¸"""
    print("\nğŸ® GPU í™˜ê²½ í™•ì¸...")
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"âœ… GPU ê°ì§€: {len(gpus)}ê°œ")
            for i, gpu in enumerate(gpus):
                print(f"  GPU {i}: {gpu.name}")
            return True
        except Exception as e:
            print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
            return False
    else:
        print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        return False

has_gpu = setup_gpu()

# ============================================
# ì„¤ì •
# ============================================
class Config:
    # ë°ì´í„° íŒŒì¼ (ê³ ì •)
    DATA_FILE = '20240201_TO_202507281705.CSV'
    
    # ì‹œí€€ìŠ¤ ì„¤ì •
    LOOKBACK = 100
    FORECAST = 10
    
    # M14 ì„ê³„ê°’
    M14B_THRESHOLDS = {
        1400: 320,
        1500: 400,
        1600: 450,
        1700: 500
    }
    
    RATIO_THRESHOLDS = {
        1400: 4,
        1500: 5,
        1600: 6,
        1700: 7
    }
    
    # í•™ìŠµ ì„¤ì •
    BATCH_SIZE = 128 if has_gpu else 64
    EPOCHS = 50
    LEARNING_RATE = 0.0005
    PATIENCE = 15
    
    # ì €ì¥ ê²½ë¡œ
    MODEL_DIR = './models_v6_gpu/'
    CHECKPOINT_DIR = './checkpoints_v6_gpu/'
    SEQUENCE_FILE = './sequences_v6_gpu.npz'

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(Config.MODEL_DIR, exist_ok=True)
os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)

# ============================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
# ============================================
class CheckpointManager:
    def __init__(self):
        self.checkpoint_file = os.path.join(Config.CHECKPOINT_DIR, 'training_state.pkl')
    
    def save_state(self, completed_models, models, history, evaluation_results):
        """í•™ìŠµ ìƒíƒœ ì €ì¥"""
        state = {
            'completed_models': completed_models,
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'evaluation_results': evaluation_results
        }
        
        # ëª¨ë¸ ì €ì¥
        for name, model in models.items():
            save_path = os.path.join(Config.CHECKPOINT_DIR, f'{name}_checkpoint.h5')
            model.save(save_path)
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        with open(os.path.join(Config.CHECKPOINT_DIR, 'history.pkl'), 'wb') as f:
            pickle.dump(history, f)
        
        # ìƒíƒœ ì €ì¥
        with open(self.checkpoint_file, 'wb') as f:
            pickle.dump(state, f)
        
        print(f"\nğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {completed_models}")
    
    def load_state(self):
        """ì €ì¥ëœ ìƒíƒœ ë¡œë“œ"""
        if not os.path.exists(self.checkpoint_file):
            return None, {}, {}, {}
        
        with open(self.checkpoint_file, 'rb') as f:
            state = pickle.load(f)
        
        # íˆìŠ¤í† ë¦¬ ë¡œë“œ
        history = {}
        history_path = os.path.join(Config.CHECKPOINT_DIR, 'history.pkl')
        if os.path.exists(history_path):
            with open(history_path, 'rb') as f:
                history = pickle.load(f)
        
        print(f"\nğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {state['completed_models']}")
        print(f"   ì €ì¥ ì‹œê°„: {state['timestamp']}")
        
        return state['completed_models'], {}, history, state.get('evaluation_results', {})

# ============================================
# ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ ë° ë ˆì´ì–´
# ============================================
class WeightedLoss(tf.keras.losses.Loss):
    """ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜"""
    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        
        mae = tf.abs(y_true - y_pred)
        
        weights = tf.ones_like(y_true)
        weights = tf.where(y_true >= 1700, 10.0, weights)
        weights = tf.where((y_true >= 1600) & (y_true < 1700), 8.0, weights)
        weights = tf.where((y_true >= 1500) & (y_true < 1600), 5.0, weights)
        weights = tf.where((y_true >= 1400) & (y_true < 1500), 3.0, weights)
        
        return tf.reduce_mean(mae * weights)

class M14RuleCorrection(tf.keras.layers.Layer):
    """M14 ê·œì¹™ ê¸°ë°˜ ë³´ì • ë ˆì´ì–´"""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    
    def call(self, inputs):
        pred, m14_features = inputs
        
        pred = tf.cast(pred, tf.float32)
        m14_features = tf.cast(m14_features, tf.float32)
        
        m14b = m14_features[:, 0:1]
        m10a = m14_features[:, 1:2]
        ratio = m14_features[:, 3:4]
        
        # ê·œì¹™ ê¸°ë°˜ ë³´ì •
        pred = tf.where((m14b >= 500) & (ratio >= 7), tf.maximum(pred, 1700), pred)
        pred = tf.where((m14b >= 450) & (ratio >= 6), tf.maximum(pred, 1600), pred)
        pred = tf.where((m14b >= 400) & (ratio >= 5), tf.maximum(pred, 1500), pred)
        pred = tf.where(m14b >= 320, tf.maximum(pred, 1400), pred)
        
        # í™©ê¸ˆ íŒ¨í„´
        golden = (m14b >= 350) & (m10a < 70)
        pred = tf.where(golden, pred * 1.1, pred)
        
        return pred

# ============================================
# V6 ëª¨ë¸ ì •ì˜ (5ê°œ ëª¨ë¸)
# ============================================
class ModelsV6:
    
    @staticmethod
    def build_lstm_model(input_shape):
        """1. LSTM ëª¨ë¸"""
        model = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=input_shape),
            tf.keras.layers.LayerNormalization(),
            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3),
            tf.keras.layers.LSTM(64, dropout=0.3, recurrent_dropout=0.3),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dropout(0.4),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(1)
        ], name='LSTM_Model')
        return model
    
    @staticmethod
    def build_enhanced_gru(input_shape):
        """2. GRU ëª¨ë¸"""
        model = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=input_shape),
            tf.keras.layers.LayerNormalization(),
            tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2),
            tf.keras.layers.GRU(64, dropout=0.2),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dense(1)
        ], name='GRU_Model')
        return model
    
    @staticmethod
    def build_cnn_lstm(input_shape):
        """3. CNN-LSTM ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape)
        
        # Multi-scale CNN
        conv1 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)
        conv2 = tf.keras.layers.Conv1D(64, 5, activation='relu', padding='same')(inputs)
        conv3 = tf.keras.layers.Conv1D(64, 7, activation='relu', padding='same')(inputs)
        
        # Concatenate
        concat = tf.keras.layers.Concatenate()([conv1, conv2, conv3])
        norm = tf.keras.layers.BatchNormalization()(concat)
        
        # LSTM
        lstm = tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)(norm)
        lstm2 = tf.keras.layers.LSTM(64, dropout=0.2)(lstm)
        
        # Dense
        dense1 = tf.keras.layers.Dense(128, activation='relu')(lstm2)
        dropout = tf.keras.layers.Dropout(0.3)(dense1)
        output = tf.keras.layers.Dense(1)(dropout)
        
        return tf.keras.Model(inputs=inputs, outputs=output, name='CNN_LSTM_Model')
    
    @staticmethod
    def build_spike_detector(input_shape):
        """4. Spike Detector ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape)
        
        # Multi-scale CNN
        conv1 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)
        conv2 = tf.keras.layers.Conv1D(64, 5, activation='relu', padding='same')(inputs)
        conv3 = tf.keras.layers.Conv1D(64, 7, activation='relu', padding='same')(inputs)
        
        # Concatenate
        concat = tf.keras.layers.Concatenate()([conv1, conv2, conv3])
        norm = tf.keras.layers.BatchNormalization()(concat)
        
        # Attention
        attention = tf.keras.layers.MultiHeadAttention(
            num_heads=4, 
            key_dim=48,
            dropout=0.2
        )(norm, norm)
        
        # BiLSTM
        lstm = tf.keras.layers.Bidirectional(
            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)
        )(attention)
        
        # Global pooling
        pooled = tf.keras.layers.GlobalAveragePooling1D()(lstm)
        
        # Dense layers
        dense1 = tf.keras.layers.Dense(256, activation='relu')(pooled)
        dropout1 = tf.keras.layers.Dropout(0.3)(dense1)
        dense2 = tf.keras.layers.Dense(128, activation='relu')(dropout1)
        dropout2 = tf.keras.layers.Dropout(0.2)(dense2)
        
        # Dual output
        regression_output = tf.keras.layers.Dense(1, name='spike_value')(dropout2)
        classification_output = tf.keras.layers.Dense(1, activation='sigmoid', name='spike_prob')(dropout2)
        
        return tf.keras.Model(
            inputs=inputs,
            outputs=[regression_output, classification_output],
            name='Spike_Detector'
        )
    
    @staticmethod
    def build_rule_based_model(input_shape, m14_shape):
        """5. Rule-Based ëª¨ë¸"""
        time_input = tf.keras.Input(shape=input_shape, name='time_input')
        m14_input = tf.keras.Input(shape=m14_shape, name='m14_input')
        
        # ì‹œê³„ì—´ ì²˜ë¦¬
        lstm = tf.keras.layers.LSTM(32, dropout=0.2)(time_input)
        
        # M14 íŠ¹ì§• ì²˜ë¦¬
        m14_dense = tf.keras.layers.Dense(16, activation='relu')(m14_input)
        
        # ê²°í•©
        combined = tf.keras.layers.Concatenate()([lstm, m14_dense])
        
        # Dense layers
        dense1 = tf.keras.layers.Dense(64, activation='relu')(combined)
        dropout = tf.keras.layers.Dropout(0.2)(dense1)
        dense2 = tf.keras.layers.Dense(32, activation='relu')(dropout)
        
        # ì˜ˆì¸¡
        prediction = tf.keras.layers.Dense(1, name='rule_pred')(dense2)
        
        # M14 ê·œì¹™ ì ìš©
        corrected = M14RuleCorrection()([prediction, m14_input])
        
        return tf.keras.Model(
            inputs=[time_input, m14_input],
            outputs=corrected,
            name='Rule_Based_Model'
        )

# ============================================
# PART 1: ì‹œí€€ìŠ¤ ìƒì„± ë˜ëŠ” ë¡œë“œ
# ============================================
print("\n" + "="*60)
print("ğŸ“¦ PART 1: ì‹œí€€ìŠ¤ ì¤€ë¹„")
print("="*60)

if os.path.exists(Config.SEQUENCE_FILE):
    print(f"\nâœ… ê¸°ì¡´ ì‹œí€€ìŠ¤ íŒŒì¼ ë¡œë“œ: {Config.SEQUENCE_FILE}")
    data = np.load(Config.SEQUENCE_FILE)
    X_scaled = data['X']
    y = data['y']
    m14_features = data['m14_features']
    print(f"  ë¡œë“œ ì™„ë£Œ: X{X_scaled.shape}, y{y.shape}")
    
else:
    print(f"\nâš ï¸ ì‹œí€€ìŠ¤ íŒŒì¼ ì—†ìŒ - ìƒˆë¡œ ìƒì„±")
    print(f"ğŸ“‚ ë°ì´í„° ë¡œë”©: {Config.DATA_FILE}")
    
    df = pd.read_csv(Config.DATA_FILE)
    print(f"  âœ… {len(df):,}í–‰ ë¡œë“œ")
    
    # íŠ¹ì§• ìƒì„±
    print("\nğŸ”§ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§...")
    
    # ê¸°ë³¸ ì»¬ëŸ¼
    if 'TOTALCNT' in df.columns:
        df['current_value'] = df['TOTALCNT']
    else:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df['current_value'] = df[numeric_cols[0]] if len(numeric_cols) > 0 else 0
    
    # M14 ì»¬ëŸ¼
    for col in ['M14AM10A', 'M14AM14B', 'M14AM16']:
        if col not in df.columns:
            df[col] = 0
            print(f"  âš ï¸ {col} ì—†ìŒ â†’ 0ìœ¼ë¡œ ì´ˆê¸°í™”")
    
    # íƒ€ê²Ÿ ìƒì„±
    df['target'] = df['current_value'].shift(-Config.FORECAST)
    
    # ë¹„ìœ¨ íŠ¹ì§•
    df['ratio_14B_10A'] = df['M14AM14B'] / df['M14AM10A'].clip(lower=1)
    df['ratio_14B_16'] = df['M14AM14B'] / df['M14AM16'].clip(lower=1)
    
    # ì‹œê³„ì—´ íŠ¹ì§•
    for col in ['current_value', 'M14AM14B', 'M14AM10A', 'M14AM16']:
        if col in df.columns:
            df[f'{col}_change_5'] = df[col].diff(5)
            df[f'{col}_change_10'] = df[col].diff(10)
            df[f'{col}_ma_10'] = df[col].rolling(10, min_periods=1).mean()
            df[f'{col}_ma_30'] = df[col].rolling(30, min_periods=1).mean()
            df[f'{col}_std_10'] = df[col].rolling(10, min_periods=1).std()
    
    # M14AM10A ì—­íŒ¨í„´
    df['M10A_drop_5'] = -df['M14AM10A'].diff(5)
    df['M10A_drop_10'] = -df['M14AM10A'].diff(10)
    
    # í™©ê¸ˆ íŒ¨í„´
    df['golden_pattern'] = ((df['M14AM14B'] >= 350) & (df['M14AM10A'] < 70)).astype(float)
    
    # ê¸‰ì¦ ì‹ í˜¸
    for level, threshold in Config.M14B_THRESHOLDS.items():
        df[f'signal_{level}'] = (df['M14AM14B'] >= threshold).astype(float)
    
    # ë¹„ìœ¨ ì‹ í˜¸
    for level, threshold in Config.RATIO_THRESHOLDS.items():
        df[f'ratio_signal_{level}'] = (df['ratio_14B_10A'] >= threshold).astype(float)
    
    # NaN ì²˜ë¦¬
    df = df.fillna(0)
    df = df.dropna(subset=['target'])
    
    # íŠ¹ì§• ì„ íƒ
    exclude_cols = ['TIME', 'CURRTIME', 'TOTALCNT']
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    print(f"  âœ… íŠ¹ì§•: {len(feature_cols)}ê°œ")
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    print("\nâš¡ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
    data = df[feature_cols].values.astype(np.float32)
    n_samples = len(data) - Config.LOOKBACK - Config.FORECAST + 1
    
    X = np.zeros((n_samples, Config.LOOKBACK, len(feature_cols)), dtype=np.float32)
    y = np.zeros(n_samples, dtype=np.float32)
    
    for i in range(n_samples):
        X[i] = data[i:i+Config.LOOKBACK]
        y[i] = data[i+Config.LOOKBACK+Config.FORECAST-1, 0]  # target ì»¬ëŸ¼
        
        if i % 50000 == 0:
            print(f"    ì§„í–‰: {i/n_samples*100:.1f}%")
    
    print(f"  âœ… ì™„ë£Œ: X{X.shape}, y{y.shape}")
    
    # Xë§Œ ìŠ¤ì¼€ì¼ë§ (yëŠ” ì›ë³¸ ìœ ì§€!)
    print("\nğŸ“ X ë°ì´í„°ë§Œ ìŠ¤ì¼€ì¼ë§...")
    X_scaled = np.zeros_like(X)
    scalers = {}
    
    for i in range(X.shape[2]):
        scaler = RobustScaler()
        feature = X[:, :, i].reshape(-1, 1)
        X_scaled[:, :, i] = scaler.fit_transform(feature).reshape(X[:, :, i].shape)
        scalers[f'feature_{i}'] = scaler
    
    # M14 íŠ¹ì§• ì¶”ì¶œ
    m14_features = np.zeros((len(X), 4), dtype=np.float32)
    for i in range(len(X)):
        idx = i + Config.LOOKBACK
        if idx < len(df):
            m14_features[i] = [
                df['M14AM14B'].iloc[idx],
                df['M14AM10A'].iloc[idx],
                df['M14AM16'].iloc[idx],
                df['ratio_14B_10A'].iloc[idx]
            ]
    
    # ì‹œí€€ìŠ¤ ì €ì¥
    print("\nğŸ’¾ ì‹œí€€ìŠ¤ ì €ì¥...")
    np.savez_compressed(
        Config.SEQUENCE_FILE,
        X=X_scaled,
        y=y,
        m14_features=m14_features
    )
    print(f"  âœ… ì €ì¥: {Config.SEQUENCE_FILE}")

# íƒ€ê²Ÿ ë¶„í¬ í™•ì¸
print(f"\nğŸ“Š íƒ€ê²Ÿ ë¶„í¬:")
for level in [1400, 1500, 1600, 1700]:
    count = (y >= level).sum()
    ratio = count / len(y) * 100
    print(f"  {level}+: {count:,}ê°œ ({ratio:.1f}%)")

# ============================================
# PART 2: ë°ì´í„° ë¶„í• 
# ============================================
print("\n" + "="*60)
print("ğŸ“Š ë°ì´í„° ë¶„í• ")
print("="*60)

X_train, X_val, y_train, y_val, m14_train, m14_val = train_test_split(
    X_scaled, y, m14_features, test_size=0.2, random_state=42
)

y_spike_class = (y_train >= 1400).astype(float)
y_val_spike_class = (y_val >= 1400).astype(float)

print(f"  í•™ìŠµ: {X_train.shape[0]:,}ê°œ")
print(f"  ê²€ì¦: {X_val.shape[0]:,}ê°œ")
print(f"  1400+ í•™ìŠµ ë¹„ìœ¨: {y_spike_class.mean():.1%}")
print(f"  1400+ ê²€ì¦ ë¹„ìœ¨: {y_val_spike_class.mean():.1%}")

# ============================================
# PART 3: 5ê°œ ëª¨ë¸ í•™ìŠµ
# ============================================
print("\n" + "="*60)
print("ğŸ‹ï¸ PART 3: 5ê°œ ëª¨ë¸ í•™ìŠµ")
print("="*60)

# ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
checkpoint_manager = CheckpointManager()
completed_models, models, history, evaluation_results = checkpoint_manager.load_state()

if not completed_models:
    completed_models = []
    models = {}
    history = {}
    evaluation_results = {}

# 1. LSTM ëª¨ë¸
if 'lstm' not in completed_models:
    print("\n1ï¸âƒ£ LSTM ëª¨ë¸ í•™ìŠµ (ì¥ê¸° ì‹œê³„ì—´ íŒ¨í„´)")
    
    lstm_model = ModelsV6.build_lstm_model(X_train.shape[1:])
    lstm_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
        loss=WeightedLoss(),
        metrics=['mae']
    )
    
    lstm_history = lstm_model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=Config.EPOCHS,
        batch_size=Config.BATCH_SIZE,
        callbacks=[
            tf.keras.callbacks.EarlyStopping(
                patience=Config.PATIENCE,
                restore_best_weights=True
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                patience=5,
                factor=0.5
            )
        ],
        verbose=1
    )
    
    models['lstm'] = lstm_model
    history['lstm'] = lstm_history
    completed_models.append('lstm')
    checkpoint_manager.save_state(completed_models, models, history, evaluation_results)
else:
    print("\n1ï¸âƒ£ LSTM - ì´ë¯¸ ì™„ë£Œ âœ“")

# 2. GRU ëª¨ë¸
if 'gru' not in completed_models:
    print("\n2ï¸âƒ£ GRU ëª¨ë¸ í•™ìŠµ (ë‹¨ê¸° ë³€ë™ì„±)")
    
    gru_model = ModelsV6.build_enhanced_gru(X_train.shape[1:])
    gru_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
        loss=WeightedLoss(),
        metrics=['mae']
    )
    
    gru_history = gru_model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=Config.EPOCHS,
        batch_size=Config.BATCH_SIZE,
        callbacks=[
            tf.keras.callbacks.EarlyStopping(
                patience=Config.PATIENCE,
                restore_best_weights=True
            )
        ],
        verbose=1
    )
    
    models['gru'] = gru_model
    history['gru'] = gru_history
    completed_models.append('gru')
    checkpoint_manager.save_state(completed_models, models, history, evaluation_results)
else:
    print("\n2ï¸âƒ£ GRU - ì´ë¯¸ ì™„ë£Œ âœ“")

# 3. CNN-LSTM ëª¨ë¸
if 'cnn_lstm' not in completed_models:
    print("\n3ï¸âƒ£ CNN-LSTM ëª¨ë¸ í•™ìŠµ (ë³µí•© íŒ¨í„´ ì¸ì‹)")
    
    cnn_lstm_model = ModelsV6.build_cnn_lstm(X_train.shape[1:])
    cnn_lstm_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
        loss=WeightedLoss(),
        metrics=['mae']
    )
    
    cnn_lstm_history = cnn_lstm_model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=Config.EPOCHS,
        batch_size=Config.BATCH_SIZE,
        callbacks=[
            tf.keras.callbacks.EarlyStopping(
                patience=Config.PATIENCE,
                restore_best_weights=True
            )
        ],
        verbose=1
    )
    
    models['cnn_lstm'] = cnn_lstm_model
    history['cnn_lstm'] = cnn_lstm_history
    completed_models.append('cnn_lstm')
    checkpoint_manager.save_state(completed_models, models, history, evaluation_results)
else:
    print("\n3ï¸âƒ£ CNN-LSTM - ì´ë¯¸ ì™„ë£Œ âœ“")

# 4. Spike Detector ëª¨ë¸
if 'spike' not in completed_models:
    print("\n4ï¸âƒ£ Spike Detector ëª¨ë¸ í•™ìŠµ (ì´ìƒì¹˜ ê°ì§€)")
    
    spike_model = ModelsV6.build_spike_detector(X_train.shape[1:])
    spike_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
        loss={
            'spike_value': WeightedLoss(),
            'spike_prob': 'binary_crossentropy'
        },
        loss_weights={
            'spike_value': 1.0,
            'spike_prob': 0.3
        },
        metrics=['mae']
    )
    
    spike_history = spike_model.fit(
        X_train, 
        [y_train, y_spike_class],
        validation_data=(X_val, [y_val, y_val_spike_class]),
        epochs=Config.EPOCHS,
        batch_size=Config.BATCH_SIZE,
        callbacks=[
            tf.keras.callbacks.EarlyStopping(
                patience=Config.PATIENCE,
                restore_best_weights=True
            )
        ],
        verbose=1
    )
    
    models['spike'] = spike_model
    history['spike'] = spike_history
    completed_models.append('spike')
    checkpoint_manager.save_state(completed_models, models, history, evaluation_results)
else:
    print("\n4ï¸âƒ£ Spike Detector - ì´ë¯¸ ì™„ë£Œ âœ“")

# 5. Rule-Based ëª¨ë¸
if 'rule' not in completed_models:
    print("\n5ï¸âƒ£ Rule-Based ëª¨ë¸ í•™ìŠµ (ê²€ì¦ëœ í™©ê¸ˆ íŒ¨í„´)")
    
    rule_model = ModelsV6.build_rule_based_model(X_train.shape[1:], m14_train.shape[1])
    rule_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * 0.5),
        loss=WeightedLoss(),
        metrics=['mae']
    )
    
    rule_history = rule_model.fit(
        [X_train, m14_train], 
        y_train,
        validation_data=([X_val, m14_val], y_val),
        epochs=30,  # Rule-basedëŠ” ë¹ ë¥´ê²Œ ìˆ˜ë ´
        batch_size=Config.BATCH_SIZE,
        callbacks=[
            tf.keras.callbacks.EarlyStopping(
                patience=10,
                restore_best_weights=True
            )
        ],
        verbose=1
    )
    
    models['rule'] = rule_model
    history['rule'] = rule_history
    completed_models.append('rule')
    checkpoint_manager.save_state(completed_models, models, history, evaluation_results)
else:
    print("\n5ï¸âƒ£ Rule-Based - ì´ë¯¸ ì™„ë£Œ âœ“")

# ============================================
# PART 4: ì•™ìƒë¸” ëª¨ë¸
# ============================================
if 'ensemble' not in completed_models:
    print("\n" + "="*60)
    print("ğŸ¯ PART 4: ìµœì¢… ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±")
    print("="*60)
    
    # ê°œë³„ ëª¨ë¸ ë¡œë“œ (í•„ìš”ì‹œ)
    for name in ['lstm', 'gru', 'cnn_lstm', 'spike', 'rule']:
        if name not in models:
            checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, f'{name}_checkpoint.h5')
            if os.path.exists(checkpoint_path):
                print(f"  ëª¨ë¸ ë¡œë“œ: {name}")
                custom_objects = {'WeightedLoss': WeightedLoss}
                if name == 'rule':
                    custom_objects['M14RuleCorrection'] = M14RuleCorrection
                models[name] = tf.keras.models.load_model(checkpoint_path, custom_objects=custom_objects)
    
    # ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±
    time_series_input = tf.keras.Input(shape=X_train.shape[1:], name='ensemble_time_input')
    m14_input = tf.keras.Input(shape=m14_train.shape[1], name='ensemble_m14_input')
    
    # ê° ëª¨ë¸ ì˜ˆì¸¡
    lstm_pred = models['lstm'](time_series_input)
    gru_pred = models['gru'](time_series_input)
    cnn_lstm_pred = models['cnn_lstm'](time_series_input)
    spike_pred, spike_prob = models['spike'](time_series_input)
    rule_pred = models['rule']([time_series_input, m14_input])
    
    # M14 ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜ ìƒì„±
    weight_dense = tf.keras.layers.Dense(32, activation='relu')(m14_input)
    weight_dense = tf.keras.layers.Dense(16, activation='relu')(weight_dense)
    weights = tf.keras.layers.Dense(5, activation='softmax', name='ensemble_weights')(weight_dense)
    
    # ê°€ì¤‘ì¹˜ ë¶„ë¦¬
    w_lstm = tf.keras.layers.Lambda(lambda x: x[:, 0:1])(weights)
    w_gru = tf.keras.layers.Lambda(lambda x: x[:, 1:2])(weights)
    w_cnn = tf.keras.layers.Lambda(lambda x: x[:, 2:3])(weights)
    w_spike = tf.keras.layers.Lambda(lambda x: x[:, 3:4])(weights)
    w_rule = tf.keras.layers.Lambda(lambda x: x[:, 4:5])(weights)
    
    # ê°€ì¤‘ í‰ê· 
    weighted_lstm = tf.keras.layers.Multiply()([lstm_pred, w_lstm])
    weighted_gru = tf.keras.layers.Multiply()([gru_pred, w_gru])
    weighted_cnn = tf.keras.layers.Multiply()([cnn_lstm_pred, w_cnn])
    weighted_spike = tf.keras.layers.Multiply()([spike_pred, w_spike])
    weighted_rule = tf.keras.layers.Multiply()([rule_pred, w_rule])
    
    # ì•™ìƒë¸” ì˜ˆì¸¡
    ensemble_pred = tf.keras.layers.Add()([
        weighted_lstm, weighted_gru, weighted_cnn, 
        weighted_spike, weighted_rule
    ])
    
    # ìµœì¢… M14 ê·œì¹™ ë³´ì •
    final_pred = M14RuleCorrection(name='ensemble_prediction')([ensemble_pred, m14_input])
    
    # spike_prob ì¶œë ¥
    spike_prob_output = tf.keras.layers.Lambda(lambda x: x, name='spike_probability')(spike_prob)
    
    # ì•™ìƒë¸” ëª¨ë¸ ì •ì˜
    ensemble_model = tf.keras.Model(
        inputs=[time_series_input, m14_input],
        outputs=[final_pred, spike_prob_output],
        name='Final_Ensemble_Model'
    )
    
    # ì»´íŒŒì¼
    ensemble_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * 0.5),
        loss={
            'ensemble_prediction': WeightedLoss(),
            'spike_probability': 'binary_crossentropy'
        },
        loss_weights={
            'ensemble_prediction': 1.0,
            'spike_probability': 0.3
        },
        metrics=['mae']
    )
    
    print("\nğŸ“Š ì•™ìƒë¸” íŒŒì¸íŠœë‹...")
    ensemble_history = ensemble_model.fit(
        [X_train, m14_train],
        [y_train, y_spike_class],
        validation_data=(
            [X_val, m14_val],
            [y_val, y_val_spike_class]
        ),
        epochs=20,
        batch_size=Config.BATCH_SIZE,
        verbose=1
    )
    
    models['ensemble'] = ensemble_model
    history['ensemble'] = ensemble_history
    completed_models.append('ensemble')
    checkpoint_manager.save_state(completed_models, models, history, evaluation_results)
    
    print("\nâœ… ì•™ìƒë¸” ëª¨ë¸ ì™„ì„±!")
else:
    print("\nğŸ¯ ì•™ìƒë¸” ëª¨ë¸ - ì´ë¯¸ ì™„ë£Œ âœ“")

# ============================================
# PART 5: í‰ê°€
# ============================================
print("\n" + "="*60)
print("ğŸ“Š PART 5: ëª¨ë¸ í‰ê°€")
print("="*60)

for name, model in models.items():
    # ì˜ˆì¸¡
    if name == 'ensemble':
        pred = model.predict([X_val, m14_val], verbose=0)[0].flatten()
    elif name == 'spike':
        pred = model.predict(X_val, verbose=0)[0].flatten()
    elif name == 'rule':
        pred = model.predict([X_val, m14_val], verbose=0).flatten()
    else:
        pred = model.predict(X_val, verbose=0).flatten()
    
    # ì „ì²´ ì„±ëŠ¥
    mae = np.mean(np.abs(y_val - pred))
    
    # êµ¬ê°„ë³„ ì„±ëŠ¥
    level_performance = {}
    for level in [1400, 1500, 1600, 1700]:
        mask = y_val >= level
        if np.any(mask):
            recall = np.sum((pred >= level) & mask) / np.sum(mask)
            level_mae = np.mean(np.abs(y_val[mask] - pred[mask]))
            level_performance[level] = {
                'recall': recall,
                'mae': level_mae,
                'count': np.sum(mask)
            }
    
    evaluation_results[name] = {
        'overall_mae': mae,
        'levels': level_performance
    }
    
    # ì¶œë ¥
    print(f"\nğŸ¯ {name.upper()} ëª¨ë¸:")
    print(f"  ì „ì²´ MAE: {mae:.2f}")
    for level, perf in level_performance.items():
        print(f"  {level}+: Recall={perf['recall']:.2%}, MAE={perf['mae']:.1f} (n={perf['count']})")

# ìµœê³  ëª¨ë¸ ì„ íƒ
best_model = min(evaluation_results.keys(), key=lambda x: evaluation_results[x]['overall_mae'])
print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_model.upper()} ëª¨ë¸")
print(f"  MAE: {evaluation_results[best_model]['overall_mae']:.2f}")

# ============================================
# PART 6: ëª¨ë¸ ì €ì¥
# ============================================
print("\n" + "="*60)
print("ğŸ’¾ PART 6: ëª¨ë¸ ì €ì¥")
print("="*60)

for name, model in models.items():
    save_path = f"{Config.MODEL_DIR}{name}_v6_gpu.h5"
    model.save(save_path)
    print(f"  {name}_v6_gpu.h5 ì €ì¥ ì™„ë£Œ")

# í‰ê°€ ê²°ê³¼ ì €ì¥
with open(f"{Config.MODEL_DIR}evaluation_results.json", 'w') as f:
    json.dump(evaluation_results, f, indent=2, default=str)

# ì„¤ì • ì €ì¥
config_dict = {k: v for k, v in Config.__dict__.items() if not k.startswith('_')}
with open(f"{Config.MODEL_DIR}config.json", 'w') as f:
    json.dump(config_dict, f, indent=2)

print("\n  ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ")

# ============================================
# PART 7: ì‹œê°í™”
# ============================================
print("\n" + "="*60)
print("ğŸ“ˆ PART 7: ê²°ê³¼ ì‹œê°í™”")
print("="*60)

fig = plt.figure(figsize=(20, 12))

# í•™ìŠµ ê³¡ì„ 
for idx, (name, hist) in enumerate(history.items()):
    if idx < 6:
        ax = plt.subplot(3, 3, idx+1)
        
        if hasattr(hist, 'history'):
            if name == 'spike':
                loss = hist.history.get('spike_value_loss', [])
                val_loss = hist.history.get('val_spike_value_loss', [])
            elif name == 'ensemble':
                loss = hist.history.get('ensemble_prediction_loss', [])
                val_loss = hist.history.get('val_ensemble_prediction_loss', [])
            else:
                loss = hist.history.get('loss', [])
                val_loss = hist.history.get('val_loss', [])
            
            if loss and val_loss:
                ax.plot(loss, label='Train Loss', alpha=0.8)
                ax.plot(val_loss, label='Val Loss', alpha=0.8)
                ax.set_title(f'{name.upper()} Learning Curve')
                ax.set_xlabel('Epoch')
                ax.set_ylabel('Loss')
                ax.legend()
                ax.grid(True, alpha=0.3)

# MAE ë¹„êµ
ax = plt.subplot(3, 3, 7)
model_names = list(evaluation_results.keys())
maes = [evaluation_results[m]['overall_mae'] for m in model_names]
colors = ['blue', 'green', 'orange', 'red', 'purple', 'brown']

bars = ax.bar(model_names, maes, color=colors[:len(model_names)])
ax.set_title('Model MAE Comparison')
ax.set_ylabel('MAE')
ax.set_ylim(0, max(maes) * 1.2)

for bar, mae in zip(bars, maes):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height,
           f'{mae:.1f}', ha='center', va='bottom')

# 1400+ Recall ë¹„êµ
ax = plt.subplot(3, 3, 8)
recalls_1400 = []
for m in model_names:
    if 1400 in evaluation_results[m]['levels']:
        recalls_1400.append(evaluation_results[m]['levels'][1400]['recall'] * 100)
    else:
        recalls_1400.append(0)

bars = ax.bar(model_names, recalls_1400, color=colors[:len(model_names)])
ax.set_title('1400+ Recall Comparison (%)')
ax.set_ylabel('Recall (%)')
ax.set_ylim(0, 105)

for bar, recall in zip(bars, recalls_1400):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height,
           f'{recall:.1f}%', ha='center', va='bottom')

# ì„±ëŠ¥ ìš”ì•½
ax = plt.subplot(3, 3, 9)
ax.axis('off')

summary_text = "ğŸ† Performance Summary\n" + "="*35 + "\n"
summary_text += f"Best Model: {best_model.upper()}\n"
summary_text += f"Overall MAE: {evaluation_results[best_model]['overall_mae']:.2f}\n\n"

summary_text += "Recall by Level:\n"
for level in [1400, 1500, 1600, 1700]:
    if level in evaluation_results[best_model]['levels']:
        recall = evaluation_results[best_model]['levels'][level]['recall']
        mae = evaluation_results[best_model]['levels'][level]['mae']
        summary_text += f"  {level}+: {recall:6.1%} (MAE: {mae:.1f})\n"

summary_text += f"\n6ê°œ ëª¨ë¸ ì•™ìƒë¸” ì™„ì„±!"

ax.text(0.1, 0.9, summary_text, transform=ax.transAxes,
       fontsize=11, verticalalignment='top', fontfamily='monospace')

plt.suptitle('V6 GPU ì•™ìƒë¸” ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig(f"{Config.MODEL_DIR}training_results.png", dpi=100, bbox_inches='tight')
print("  training_results.png ì €ì¥ ì™„ë£Œ")
plt.show()

# ============================================
# ìµœì¢… ì¶œë ¥
# ============================================
print("\n" + "="*60)
print("ğŸ‰ V6 GPU í†µí•© ì‹œìŠ¤í…œ ì™„ë£Œ!")
print("="*60)
print(f"ğŸ“ ëª¨ë¸ ì €ì¥: {Config.MODEL_DIR}")
print(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸: {Config.CHECKPOINT_DIR}")
print(f"ğŸ“‚ ì‹œí€€ìŠ¤: {Config.SEQUENCE_FILE}")
print("\nğŸ“Š ìµœì¢… ì„±ëŠ¥:")
print(f"  ìµœê³  ëª¨ë¸: {best_model.upper()}")
print(f"  ì „ì²´ MAE: {evaluation_results[best_model]['overall_mae']:.2f}")

# GPU ì •ë³´
if has_gpu:
    gpus = tf.config.list_physical_devices('GPU')
    print(f"\nğŸ® GPU ì‚¬ìš©: {len(gpus)}ê°œ")
else:
    print("\nğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰ë¨")

print("\nâœ… 6ê°œ ëª¨ë¸ (5ê°œ + ì•™ìƒë¸”) ì¤€ë¹„ ì™„ë£Œ!")
print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì ìš©")

# ë©”ëª¨ë¦¬ ì •ë¦¬
tf.keras.backend.clear_session()
gc.collect()

print("="*60)