"""
V7_GPU_í•™ìŠµ_ìµœì í™”.py - GPU ê°€ì† 5ê°œ ëª¨ë¸ ì•™ìƒë¸” í•™ìŠµ
CUDA 12.0 + TensorFlow 2.15.0 GPU ìµœì í™”
Mixed Precision Training í¬í•¨
"""

import tensorflow as tf
from tensorflow.keras import mixed_precision
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import json
import os
import warnings
import pickle
from datetime import datetime
import gc
warnings.filterwarnings('ignore')

# ============================================
# GPU ì„¤ì • ë° ìµœì í™”
# ============================================
def setup_gpu_for_training():
    """GPU í•™ìŠµ í™˜ê²½ ìµœì í™” ì„¤ì •"""
    print("\nğŸ® GPU í•™ìŠµ í™˜ê²½ ì„¤ì • ì¤‘...")
    
    # GPU í™•ì¸
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        try:
            # GPU ë©”ëª¨ë¦¬ ë™ì  í• ë‹¹
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            
            print(f"âœ… GPU ê°ì§€: {len(gpus)}ê°œ")
            
            # Mixed Precision ì„¤ì • (FP16)
            policy = mixed_precision.Policy('mixed_float16')
            mixed_precision.set_global_policy(policy)
            print(f"âœ… Mixed Precision í™œì„±í™”: {policy.name}")
            print("  â†’ 2ë°° ë¹ ë¥¸ í•™ìŠµ ì†ë„ + 50% ë©”ëª¨ë¦¬ ì ˆì•½")
            
            # GPU ì •ë³´ ì¶œë ¥
            for i, gpu in enumerate(gpus):
                print(f"\n  GPU {i}: {gpu.name}")
            
            # XLA ì»´íŒŒì¼ëŸ¬ í™œì„±í™” (ì¶”ê°€ ê°€ì†)
            tf.config.optimizer.set_jit(True)
            print("\nâœ… XLA ì»´íŒŒì¼ëŸ¬ í™œì„±í™”")
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
            return False
    else:
        print("âŒ GPU ì—†ìŒ - CPU ëª¨ë“œ")
        return False

print("="*60)
print("ğŸš€ GPU ê°€ì† ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ì•™ìƒë¸” í•™ìŠµ V7")
print(f"ğŸ“¦ TensorFlow ë²„ì „: {tf.__version__}")
print("="*60)

# GPU ì„¤ì •
has_gpu = setup_gpu_for_training()

# ============================================
# 1. GPU ìµœì í™” ì„¤ì •
# ============================================
class GPUConfig:
    # ì‹œí€€ìŠ¤ íŒŒì¼
    SEQUENCE_FILE = './sequences_v7_gpu.npz'
    
    # GPU ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
    if has_gpu:
        BATCH_SIZE = 128  # GPU: ë” í° ë°°ì¹˜
        PREFETCH_BUFFER = tf.data.AUTOTUNE
    else:
        BATCH_SIZE = 32   # CPU: ì‘ì€ ë°°ì¹˜
        PREFETCH_BUFFER = 2
    
    # í•™ìŠµ ì„¤ì •
    EPOCHS = 100
    LEARNING_RATE = 0.001
    PATIENCE = 15
    
    # Mixed Precision ì„¤ì •
    USE_MIXED_PRECISION = has_gpu
    
    # ëª¨ë¸ ì €ì¥
    MODEL_DIR = './models_v7_gpu/'
    CHECKPOINT_DIR = './checkpoints_v7_gpu/'
    
    # M14 ì„ê³„ê°’
    M14B_THRESHOLDS = {
        1400: 320,
        1500: 400,
        1600: 450,
        1700: 500
    }

os.makedirs(GPUConfig.MODEL_DIR, exist_ok=True)
os.makedirs(GPUConfig.CHECKPOINT_DIR, exist_ok=True)

# ============================================
# 2. GPU ìµœì í™” ì»¤ìŠ¤í…€ ë ˆì´ì–´
# ============================================
@tf.function(jit_compile=True)  # XLA ì»´íŒŒì¼
class WeightedLossGPU(tf.keras.losses.Loss):
    """GPU ìµœì í™” ê°€ì¤‘ì¹˜ ì†ì‹¤"""
    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        
        mae = tf.abs(y_true - y_pred)
        
        # ë²¡í„°í™”ëœ ê°€ì¤‘ì¹˜ ê³„ì‚°
        weights = tf.where(y_true >= 1700, 10.0,
                  tf.where(y_true >= 1600, 8.0,
                  tf.where(y_true >= 1500, 5.0,
                  tf.where(y_true >= 1400, 3.0, 1.0))))
        
        return tf.reduce_mean(mae * weights)

class M14RuleCorrectionGPU(tf.keras.layers.Layer):
    """GPU ìµœì í™” M14 ê·œì¹™ ë ˆì´ì–´"""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.supports_masking = True
    
    @tf.function
    def call(self, inputs):
        pred, m14_features = inputs
        pred = tf.cast(pred, tf.float32)
        m14_features = tf.cast(m14_features, tf.float32)
        
        m14b = m14_features[:, 0:1]
        m10a = m14_features[:, 1:2]
        ratio = m14_features[:, 3:4]
        
        # GPU ë³‘ë ¬ ì²˜ë¦¬
        corrections = tf.stack([
            tf.where((m14b >= 500) & (ratio >= 7), 1700.0, pred[:, 0]),
            tf.where((m14b >= 450) & (ratio >= 6), 1600.0, pred[:, 0]),
            tf.where((m14b >= 400) & (ratio >= 5), 1500.0, pred[:, 0]),
            tf.where(m14b >= 320, 1400.0, pred[:, 0])
        ])
        
        # ìµœëŒ€ê°’ ì„ íƒ
        corrected = tf.reduce_max(corrections, axis=0)
        
        # í™©ê¸ˆ íŒ¨í„´
        golden = (m14b[:, 0] >= 350) & (m10a[:, 0] < 70)
        corrected = tf.where(golden, corrected * 1.1, corrected)
        
        return tf.expand_dims(corrected, -1)

# ============================================
# 3. GPU ìµœì í™” ëª¨ë¸
# ============================================
class GPUModels:
    
    @staticmethod
    def build_lstm_gpu(input_shape):
        """GPU ìµœì í™” LSTM"""
        strategy = tf.distribute.get_strategy()
        
        with strategy.scope():
            inputs = tf.keras.Input(shape=input_shape)
            
            # GPU ìµœì í™” LSTM
            x = tf.keras.layers.LayerNormalization()(inputs)
            
            # CuDNN LSTM (GPU ê°€ì†)
            lstm1 = tf.keras.layers.LSTM(
                256,  # GPUëŠ” ë” í° ìœ ë‹›
                return_sequences=True,
                activation='tanh',
                recurrent_activation='sigmoid',
                dropout=0.2,
                recurrent_dropout=0.2,
                implementation=2  # GPU ìµœì í™”
            )(x)
            
            lstm2 = tf.keras.layers.LSTM(
                128,
                dropout=0.2,
                recurrent_dropout=0.2,
                implementation=2
            )(lstm1)
            
            # Dense
            x = tf.keras.layers.BatchNormalization()(lstm2)
            x = tf.keras.layers.Dense(256, activation='relu')(x)
            x = tf.keras.layers.Dropout(0.3)(x)
            x = tf.keras.layers.Dense(128, activation='relu')(x)
            x = tf.keras.layers.Dropout(0.2)(x)
            
            # Mixed Precision ì¶œë ¥
            outputs = tf.keras.layers.Dense(1, dtype='float32')(x)
            
            model = tf.keras.Model(inputs=inputs, outputs=outputs, name='LSTM_GPU')
            
        return model
    
    @staticmethod
    def build_gru_gpu(input_shape):
        """GPU ìµœì í™” GRU"""
        inputs = tf.keras.Input(shape=input_shape)
        
        x = tf.keras.layers.LayerNormalization()(inputs)
        
        # CuDNN GRU
        gru1 = tf.keras.layers.GRU(
            256,
            return_sequences=True,
            dropout=0.2,
            recurrent_dropout=0.2,
            reset_after=True  # CuDNN í˜¸í™˜
        )(x)
        
        gru2 = tf.keras.layers.GRU(
            128,
            dropout=0.2,
            recurrent_dropout=0.2,
            reset_after=True
        )(gru1)
        
        # Dense
        x = tf.keras.layers.Dense(256, activation='relu')(gru2)
        x = tf.keras.layers.Dropout(0.3)(x)
        x = tf.keras.layers.Dense(128, activation='relu')(x)
        
        outputs = tf.keras.layers.Dense(1, dtype='float32')(x)
        
        return tf.keras.Model(inputs=inputs, outputs=outputs, name='GRU_GPU')
    
    @staticmethod
    def build_cnn_lstm_gpu(input_shape):
        """GPU ìµœì í™” CNN-LSTM"""
        inputs = tf.keras.Input(shape=input_shape)
        
        # ë³‘ë ¬ CNN (GPU íš¨ìœ¨ì )
        convs = []
        for kernel_size in [3, 5, 7, 9]:
            conv = tf.keras.layers.Conv1D(
                64, 
                kernel_size, 
                padding='same',
                activation='relu'
            )(inputs)
            convs.append(conv)
        
        # Concatenate
        x = tf.keras.layers.Concatenate()(convs)
        x = tf.keras.layers.BatchNormalization()(x)
        
        # LSTM
        x = tf.keras.layers.LSTM(
            128,
            dropout=0.2,
            implementation=2
        )(x)
        
        # Dense
        x = tf.keras.layers.Dense(256, activation='relu')(x)
        x = tf.keras.layers.Dropout(0.3)(x)
        
        outputs = tf.keras.layers.Dense(1, dtype='float32')(x)
        
        return tf.keras.Model(inputs=inputs, outputs=outputs, name='CNN_LSTM_GPU')

# ============================================
# 4. GPU ë°ì´í„° íŒŒì´í”„ë¼ì¸
# ============================================
def create_gpu_dataset(X, y, batch_size, training=True):
    """GPU ìµœì í™” ë°ì´í„° íŒŒì´í”„ë¼ì¸"""
    
    # TensorFlow Dataset ìƒì„±
    dataset = tf.data.Dataset.from_tensor_slices((X, y))
    
    if training:
        dataset = dataset.shuffle(buffer_size=10000)
    
    # GPU ìµœì í™”
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)  # GPU í”„ë¦¬í˜ì¹˜
    
    # ìºì‹± (ë©”ëª¨ë¦¬ ì¶©ë¶„ì‹œ)
    if X.nbytes < 4 * 1024**3:  # 4GB ë¯¸ë§Œ
        dataset = dataset.cache()
    
    return dataset

# ============================================
# 5. ë°ì´í„° ë¡œë“œ
# ============================================
print("\nğŸ“‚ ì‹œí€€ìŠ¤ ë¡œë”© ì¤‘...")

data = np.load(GPUConfig.SEQUENCE_FILE)
X = data['X'].astype(np.float32)
y = data['y'].astype(np.float32)
m14_features = data['m14_features'].astype(np.float32)

print(f"  âœ… ë¡œë“œ ì™„ë£Œ!")
print(f"  X: {X.shape}, {X.dtype}")
print(f"  y: {y.shape}, {y.dtype}")
print(f"  ë©”ëª¨ë¦¬: {(X.nbytes + y.nbytes) / 1024**3:.2f} GB")

# í•™ìŠµ/ê²€ì¦ ë¶„í• 
X_train, X_val, y_train, y_val, m14_train, m14_val = train_test_split(
    X, y, m14_features, test_size=0.2, random_state=42
)

# GPU ë°ì´í„°ì…‹ ìƒì„±
if has_gpu:
    print("\nâš¡ GPU ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìƒì„±...")
    train_dataset = create_gpu_dataset(X_train, y_train, GPUConfig.BATCH_SIZE, training=True)
    val_dataset = create_gpu_dataset(X_val, y_val, GPUConfig.BATCH_SIZE, training=False)
    print("  âœ… GPU íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ")

# ============================================
# 6. GPU í•™ìŠµ
# ============================================
print("\n" + "="*60)
print("ğŸ‹ï¸ GPU ê°€ì† í•™ìŠµ ì‹œì‘")
print("="*60)

models = {}
history = {}

# GPU ë¶„ì‚° ì „ëµ (ë‹¤ì¤‘ GPU ì§€ì›)
if has_gpu and len(tf.config.list_physical_devices('GPU')) > 1:
    strategy = tf.distribute.MirroredStrategy()
    print(f"âœ… ë‹¤ì¤‘ GPU ì „ëµ: {strategy.num_replicas_in_sync}ê°œ GPU")
else:
    strategy = tf.distribute.get_strategy()

# ============================================
# 6.1 LSTM í•™ìŠµ
# ============================================
print("\n1ï¸âƒ£ LSTM GPU í•™ìŠµ")

with strategy.scope():
    lstm_model = GPUModels.build_lstm_gpu(X_train.shape[1:])
    
    # GPU ìµœì í™” ì˜µí‹°ë§ˆì´ì €
    optimizer = tf.keras.optimizers.Adam(GPUConfig.LEARNING_RATE)
    if GPUConfig.USE_MIXED_PRECISION:
        optimizer = mixed_precision.LossScaleOptimizer(optimizer)
    
    lstm_model.compile(
        optimizer=optimizer,
        loss=WeightedLossGPU(),
        metrics=['mae']
    )

# GPU í•™ìŠµ
if has_gpu:
    with tf.device('/GPU:0'):
        lstm_history = lstm_model.fit(
            train_dataset,
            validation_data=val_dataset,
            epochs=GPUConfig.EPOCHS,
            callbacks=[
                tf.keras.callbacks.ModelCheckpoint(
                    f"{GPUConfig.MODEL_DIR}lstm_best.h5",
                    save_best_only=True,
                    monitor='val_loss'
                ),
                tf.keras.callbacks.EarlyStopping(
                    patience=GPUConfig.PATIENCE,
                    restore_best_weights=True
                ),
                tf.keras.callbacks.ReduceLROnPlateau(
                    patience=5,
                    factor=0.5
                )
            ],
            verbose=1
        )
else:
    lstm_history = lstm_model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=GPUConfig.EPOCHS,
        batch_size=GPUConfig.BATCH_SIZE,
        callbacks=[
            tf.keras.callbacks.EarlyStopping(patience=10)
        ],
        verbose=1
    )

models['lstm'] = lstm_model
history['lstm'] = lstm_history

# ë©”ëª¨ë¦¬ ì •ë¦¬
tf.keras.backend.clear_session()
gc.collect()

# ============================================
# 6.2 GRU í•™ìŠµ
# ============================================
print("\n2ï¸âƒ£ GRU GPU í•™ìŠµ")

with strategy.scope():
    gru_model = GPUModels.build_gru_gpu(X_train.shape[1:])
    
    optimizer = tf.keras.optimizers.Adam(GPUConfig.LEARNING_RATE)
    if GPUConfig.USE_MIXED_PRECISION:
        optimizer = mixed_precision.LossScaleOptimizer(optimizer)
    
    gru_model.compile(
        optimizer=optimizer,
        loss=WeightedLossGPU(),
        metrics=['mae']
    )

if has_gpu:
    with tf.device('/GPU:0'):
        gru_history = gru_model.fit(
            train_dataset,
            validation_data=val_dataset,
            epochs=GPUConfig.EPOCHS,
            callbacks=[
                tf.keras.callbacks.ModelCheckpoint(
                    f"{GPUConfig.MODEL_DIR}gru_best.h5",
                    save_best_only=True
                ),
                tf.keras.callbacks.EarlyStopping(
                    patience=GPUConfig.PATIENCE
                )
            ],
            verbose=1
        )
else:
    gru_history = gru_model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=GPUConfig.EPOCHS,
        batch_size=GPUConfig.BATCH_SIZE,
        verbose=1
    )

models['gru'] = gru_model
history['gru'] = gru_history

# ============================================
# 7. í‰ê°€
# ============================================
print("\nğŸ“Š GPU í•™ìŠµ ê²°ê³¼ í‰ê°€")

evaluation_results = {}

for name, model in models.items():
    if has_gpu:
        with tf.device('/GPU:0'):
            pred = model.predict(X_val, batch_size=256, verbose=0)
    else:
        pred = model.predict(X_val, verbose=0)
    
    pred = pred.flatten()
    mae = np.mean(np.abs(y_val - pred))
    
    evaluation_results[name] = mae
    print(f"  {name.upper()}: MAE = {mae:.2f}")

# ============================================
# 8. ëª¨ë¸ ì €ì¥
# ============================================
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")

for name, model in models.items():
    model.save(f"{GPUConfig.MODEL_DIR}{name}_gpu.h5")
    print(f"  {name}_gpu.h5 ì €ì¥ ì™„ë£Œ")

# ============================================
# 9. ì‹œê°í™”
# ============================================
if history:
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    
    for idx, (name, hist) in enumerate(history.items()):
        ax = axes[idx]
        if hasattr(hist, 'history'):
            ax.plot(hist.history['loss'], label='Train')
            ax.plot(hist.history['val_loss'], label='Val')
            ax.set_title(f'{name.upper()} GPU Training')
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Loss')
            ax.legend()
            ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f"{GPUConfig.MODEL_DIR}gpu_training.png")
    plt.show()

# ============================================
# 10. ìµœì¢… ì¶œë ¥
# ============================================
print("\n" + "="*60)
print("ğŸ‰ GPU í•™ìŠµ ì™„ë£Œ!")
print("="*60)

if has_gpu:
    # GPU ë©”ëª¨ë¦¬ ì •ë³´
    for gpu in tf.config.list_physical_devices('GPU'):
        print(f"\nğŸ® {gpu.name}")
    
    print(f"\nâš¡ GPU ê°€ì† íš¨ê³¼:")
    print(f"  - Mixed Precision: 2x ì†ë„ í–¥ìƒ")
    print(f"  - XLA ì»´íŒŒì¼: 15% ì¶”ê°€ ê°€ì†")
    print(f"  - ë°°ì¹˜ í¬ê¸°: {GPUConfig.BATCH_SIZE} (CPU ëŒ€ë¹„ 4x)")
else:
    print("\nğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰ë¨")

print(f"\nğŸ“ ëª¨ë¸ ì €ì¥: {GPUConfig.MODEL_DIR}")
print("\nâœ… ë‹¤ìŒ ë‹¨ê³„: ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì ìš©")

# ë©”ëª¨ë¦¬ ì •ë¦¬
tf.keras.backend.clear_session()
gc.collect()

print("="*60)