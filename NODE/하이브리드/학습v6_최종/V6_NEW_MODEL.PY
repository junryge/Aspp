"""
ğŸš€ Ultimate 1700+ Predictor v8.0 - Production Ready
==================================================
- Infinity/NaN ì˜¤ë¥˜ ì™„ì „ í•´ê²°
- ì „ì²´ êµ¬ê°„ë³„ í‰ê°€ (ì¼ë°˜, ì¤‘ê°„, 1700+)
- ì•ˆì •ì ì¸ ë°ì´í„° ì²˜ë¦¬
- [ìˆ˜ì •] EnsembleBase ëª¨ë¸ êµ¬ì¡° ì˜¤ë¥˜ í•´ê²°
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import *
from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
import warnings
import logging
from datetime import datetime
import os
import json

warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# TensorFlow ì„¤ì •
tf.random.set_seed(42)
np.random.seed(42)

print("\n" + "="*80)
print("ğŸš€ Ultimate 1700+ Predictor v8.0")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print("="*80)

# ========================================
# ì•ˆì „í•œ ë°ì´í„° ì²˜ë¦¬
# ========================================

class SafeDataProcessor:
    """Infinity/NaN ì•ˆì „ ì²˜ë¦¬"""
    
    def __init__(self, seq_len=100, pred_len=10):
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.scaler = MinMaxScaler()  # RobustScaler ëŒ€ì‹  MinMaxScaler ì‚¬ìš©
        self.target_scaler = MinMaxScaler()
        
    def load_and_process(self, filepath):
        """ë°ì´í„° ë¡œë“œ - ì•ˆì „ ì²˜ë¦¬"""
        logger.info("ğŸ“‚ ë°ì´í„° ë¡œë”©...")
        
        # CSV ë¡œë“œ
        df = pd.read_csv(filepath)
        logger.info(f"âœ… ì›ë³¸ ë°ì´í„°: {df.shape}")
        
        # ì‹œê°„ ë³€í™˜
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'], format='%Y%m%d%H%M', errors='coerce')
        df['TIME'] = pd.to_datetime(df['TIME'], format='%Y%m%d%H%M', errors='coerce')
        
        # NaT ì œê±°
        df = df.dropna(subset=['CURRTIME', 'TIME'])
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # Infinityì™€ NaN ì²´í¬ ë° ì œê±°
        logger.info("ğŸ” ì´ìƒì¹˜ ì²´í¬...")
        for col in numeric_cols:
            # Infinityë¥¼ NaNìœ¼ë¡œ ë³€ê²½
            df[col] = df[col].replace([np.inf, -np.inf], np.nan)
            
            # NaNì„ ì¤‘ì•™ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
            if df[col].isna().sum() > 0:
                median_val = df[col].median()
                if pd.isna(median_val):
                    median_val = 0
                df[col] = df[col].fillna(median_val)
                logger.info(f"   {col}: {df[col].isna().sum()}ê°œ NaN â†’ {median_val:.2f}ë¡œ ëŒ€ì²´")
        
        # ê·¹ë‹¨ì  ì´ìƒì¹˜ ì œê±° (IQR ë°©ë²•)
        for col in ['TOTALCNT']:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower = Q1 - 3 * IQR
            upper = Q3 + 3 * IQR
            
            before = len(df)
            df = df[(df[col] >= lower) & (df[col] <= upper)]
            after = len(df)
            
            if before != after:
                logger.info(f"   {col}: {before-after}ê°œ ê·¹ë‹¨ ì´ìƒì¹˜ ì œê±°")
        
        # íƒ€ê²Ÿ ìƒì„±
        df['TARGET'] = df['TOTALCNT'].shift(-self.pred_len)
        df['IS_1700'] = (df['TARGET'] >= 1700).astype(int)
        
        # íŠ¹ì„± ìƒì„± (ì•ˆì „í•˜ê²Œ)
        df = self._create_safe_features(df)
        
        # ìµœì¢… NaN ì œê±°
        df = df.dropna()
        
        logger.info(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {df.shape}")
        logger.info(f"âœ… 1700+ ë¹„ìœ¨: {df['IS_1700'].mean():.2%} ({df['IS_1700'].sum()}ê°œ)")
        
        return df
    
    def _create_safe_features(self, df):
        """ì•ˆì „í•œ íŠ¹ì„± ìƒì„±"""
        logger.info("âš™ï¸ íŠ¹ì„± ìƒì„±...")
        
        # ì´ë™í‰ê·  (ì•ˆì „í•œ ë²”ìœ„)
        for window in [10, 30]:
            df[f'MA_{window}'] = df['TOTALCNT'].rolling(window, min_periods=1).mean()
            df[f'MA_{window}'].fillna(df['TOTALCNT'].mean(), inplace=True)
        
        # ë³€í™”ìœ¨ (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
        df['CHANGE'] = df['TOTALCNT'].diff().fillna(0)
        df['PCT_CHANGE'] = df['TOTALCNT'].pct_change().fillna(0)
        
        # ê·¹ë‹¨ê°’ ë°©ì§€
        df['PCT_CHANGE'] = df['PCT_CHANGE'].clip(-1, 1)
        
        # ë¹„ìœ¨ ê³„ì‚° (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
        if 'M14AM14B' in df.columns and 'M14AM10A' in df.columns:
            df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)  # +1ë¡œ 0 ë°©ì§€
            df['RATIO'] = df['RATIO'].clip(0, 20)  # ê·¹ë‹¨ê°’ ì œí•œ
        
        # ì‹œê°„ íŠ¹ì„±
        df['HOUR'] = df['CURRTIME'].dt.hour
        df['DAYOFWEEK'] = df['CURRTIME'].dt.dayofweek
        df['IS_DAWN'] = df['HOUR'].isin([4, 5, 6]).astype(int)
        
        # ì„ê³„ê°’ ì‹ í˜¸
        if 'M14AM14B' in df.columns:
            df['M14B_HIGH'] = (df['M14AM14B'] > df['M14AM14B'].quantile(0.9)).astype(int)
        
        # ëª¨ë“  ê°’ ì²´í¬
        for col in df.select_dtypes(include=[np.number]).columns:
            # Infinity ì²´í¬
            df[col] = df[col].replace([np.inf, -np.inf], df[col].median())
            # NaN ì²´í¬
            df[col] = df[col].fillna(df[col].median())
            
        return df
    
    def create_sequences(self, df):
        """ì•ˆì „í•œ ì‹œí€€ìŠ¤ ìƒì„±"""
        logger.info("ğŸ”„ ì‹œí€€ìŠ¤ ìƒì„±...")
        
        # íŠ¹ì„± ì„ íƒ (ë¬¸ì œ ë  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼ ì œì™¸)
        exclude_cols = ['CURRTIME', 'TIME', 'TARGET', 'IS_1700']
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        # ìˆ˜ì¹˜í˜•ë§Œ ì„ íƒ
        feature_cols = [col for col in feature_cols if df[col].dtype in [np.float64, np.int64, np.float32, np.int32]]
        
        logger.info(f"   ì‚¬ìš©í•  íŠ¹ì„±: {len(feature_cols)}ê°œ")
        
        # ë°ì´í„° ë³µì‚¬
        data = df[feature_cols].copy()
        targets = df['TARGET'].copy()
        is_1700 = df['IS_1700'].copy()
        
        # ìŠ¤ì¼€ì¼ë§ ì „ ì²´í¬
        logger.info("   ìŠ¤ì¼€ì¼ë§ ì „ ì²´í¬...")
        data = data.replace([np.inf, -np.inf], np.nan)
        data = data.fillna(data.median())
        
        # ìŠ¤ì¼€ì¼ë§
        try:
            data_scaled = self.scaler.fit_transform(data)
            targets_scaled = self.target_scaler.fit_transform(targets.values.reshape(-1, 1)).flatten()
        except Exception as e:
            logger.error(f"âŒ ìŠ¤ì¼€ì¼ë§ ì˜¤ë¥˜: {e}")
            # ëŒ€ì²´ ë°©ë²•: ìˆ˜ë™ ì •ê·œí™”
            data_scaled = (data - data.mean()) / (data.std() + 1e-7)
            targets_scaled = (targets - targets.mean()) / (targets.std() + 1e-7)
            data_scaled = data_scaled.fillna(0).values
            targets_scaled = targets_scaled.fillna(0).values
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y, y_binary = [], [], []
        
        for i in range(len(data_scaled) - self.seq_len - self.pred_len):
            seq = data_scaled[i:i+self.seq_len]
            
            # ì‹œí€€ìŠ¤ ì²´í¬
            if not np.any(np.isnan(seq)) and not np.any(np.isinf(seq)):
                X.append(seq)
                y.append(targets_scaled[i+self.seq_len])
                y_binary.append(is_1700.iloc[i+self.seq_len])
        
        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.float32)
        y_binary = np.array(y_binary, dtype=np.float32)
        
        # ìµœì¢… ì²´í¬
        logger.info("   ìµœì¢… ì²´í¬...")
        logger.info(f"   X: NaN={np.isnan(X).sum()}, Inf={np.isinf(X).sum()}")
        logger.info(f"   y: NaN={np.isnan(y).sum()}, Inf={np.isinf(y).sum()}")
        
        logger.info(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {X.shape}")
        
        return X, y, y_binary, feature_cols

# ========================================
# ì•ˆì •ì ì¸ ëª¨ë¸
# ========================================

class StableModels:
    """ì•ˆì •ì ì¸ ëª¨ë¸ êµ¬í˜„"""
    
    def __init__(self, input_shape):
        self.input_shape = input_shape
        self.seq_len = input_shape[0]
        self.n_features = input_shape[1]
    
    def build_stable_lstm(self):
        """ì•ˆì •ì ì¸ LSTM"""
        inputs = Input(shape=self.input_shape)
        
        # Batch Normalizationìœ¼ë¡œ ì•ˆì •í™”
        x = BatchNormalization()(inputs)
        
        # LSTM layers with careful dropout
        x = LSTM(128, return_sequences=True, 
                kernel_regularizer=keras.regularizers.l2(0.01))(x)
        x = Dropout(0.2)(x)
        x = BatchNormalization()(x)
        
        x = LSTM(64, return_sequences=False,
                kernel_regularizer=keras.regularizers.l2(0.01))(x)
        x = Dropout(0.2)(x)
        x = BatchNormalization()(x)
        
        # Dense layers
        x = Dense(32, activation='relu',
                 kernel_regularizer=keras.regularizers.l2(0.01))(x)
        x = Dropout(0.1)(x)
        
        # Output with sigmoid to limit range
        outputs = Dense(1, activation='sigmoid')(x)  # 0-1 ë²”ìœ„ë¡œ ì œí•œ
        
        model = Model(inputs, outputs, name='StableLSTM')
        return model
    
    def build_spike_detector(self):
        """ê¸‰ë³€ ê°ì§€ ëª¨ë¸"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # CNN layers
        x = Conv1D(64, 3, padding='same', activation='relu')(x)
        x = BatchNormalization()(x)
        x = MaxPooling1D(2)(x)
        
        x = Conv1D(32, 3, padding='same', activation='relu')(x)
        x = BatchNormalization()(x)
        
        # LSTM
        x = LSTM(64, return_sequences=False)(x)
        x = BatchNormalization()(x)
        
        x = Dense(32, activation='relu')(x)
        x = Dropout(0.2)(x)
        
        outputs = Dense(1, activation='sigmoid')(x)
        
        model = Model(inputs, outputs, name='SpikeDetector')
        return model
    
    def build_extreme_net(self):
        """ê·¹ë‹¨ê°’ ì „ë¬¸ ëª¨ë¸"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # GRU backbone
        x = GRU(128, return_sequences=True)(x)
        x = Dropout(0.2)(x)
        x = GRU(64, return_sequences=False)(x)
        x = Dropout(0.2)(x)
        
        # Two branches
        normal_branch = Dense(32, activation='relu')(x)
        normal_out = Dense(1, activation='linear')(normal_branch)
        
        extreme_branch = Dense(32, activation='relu')(x)
        extreme_factor = Dense(1, activation='sigmoid')(extreme_branch)
        
        # Combine
        outputs = Add()([normal_out, 
                        Multiply()([normal_out, extreme_factor])])
        outputs = Activation('sigmoid')(outputs)  # ìµœì¢… ë²”ìœ„ ì œí•œ
        
        model = Model(inputs, outputs, name='ExtremeNet')
        return model
    
    def build_ensemble_base(self):
        """ì•™ìƒë¸”ìš© ë² ì´ìŠ¤ ëª¨ë¸"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)

        # [ìˆ˜ì •ëœ ë¶€ë¶„] 3D ì‹œí€€ìŠ¤ ì…ë ¥ì„ 2Dë¡œ ë³€í™˜í•˜ì—¬ Dense Layerì— ì „ë‹¬í•©ë‹ˆë‹¤.
        x = GlobalAveragePooling1D()(x)
        
        # Simple but effective
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.3)(x)
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.3)(x)
        x = Dense(32, activation='relu')(x)
        x = Dropout(0.2)(x)
        
        outputs = Dense(1, activation='sigmoid')(x)
        
        model = Model(inputs, outputs, name='EnsembleBase')
        return model

# ========================================
# í‰ê°€ ì‹œìŠ¤í…œ
# ========================================

class ComprehensiveEvaluator:
    """ì „ì²´ êµ¬ê°„ í‰ê°€"""
    
    def __init__(self, target_scaler):
        self.target_scaler = target_scaler
        self.results = {}
    
    def evaluate(self, model, X_test, y_test, y_binary, model_name):
        """ì¢…í•© í‰ê°€"""
        
        # ì˜ˆì¸¡
        y_pred = model.predict(X_test, batch_size=256, verbose=0).flatten()
        
        # Clip predictions to valid range
        y_pred = np.clip(y_pred, 0, 1)
        
        # ì—­ë³€í™˜
        y_test_real = self.target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
        y_pred_real = self.target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()
        
        # ===== ì „ì²´ ì„±ëŠ¥ =====
        mae_total = np.mean(np.abs(y_test_real - y_pred_real))
        rmse_total = np.sqrt(np.mean((y_test_real - y_pred_real) ** 2))
        
        # ===== êµ¬ê°„ë³„ ë¶„ì„ =====
        metrics = {
            'total': {'mae': mae_total, 'rmse': rmse_total}
        }
        
        # êµ¬ê°„ ì •ì˜
        ranges = [
            ('low', 0, 1200),
            ('normal', 1200, 1400),
            ('medium', 1400, 1600),
            ('high', 1600, 1700),
            ('extreme', 1700, 9999)
        ]
        
        for name, low, high in ranges:
            mask = (y_test_real >= low) & (y_test_real < high)
            if mask.sum() > 0:
                mae = np.mean(np.abs(y_test_real[mask] - y_pred_real[mask]))
                rmse = np.sqrt(np.mean((y_test_real[mask] - y_pred_real[mask]) ** 2))
                count = mask.sum()
                metrics[name] = {
                    'mae': mae,
                    'rmse': rmse,
                    'count': count,
                    'percentage': (count / len(y_test_real)) * 100
                }
        
        # ===== 1700+ íŠ¹ë³„ ë¶„ì„ =====
        actual_1700 = y_test_real >= 1700
        pred_1700 = y_pred_real >= 1700
        
        tp = np.sum(pred_1700 & actual_1700)
        fp = np.sum(pred_1700 & ~actual_1700)
        fn = np.sum(~pred_1700 & actual_1700)
        tn = np.sum(~pred_1700 & ~actual_1700)
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        # ===== ì •ìƒ êµ¬ê°„ ì˜¤íƒ =====
        normal_mask = (y_test_real >= 1200) & (y_test_real < 1400)
        if normal_mask.sum() > 0:
            false_alarm = np.sum(y_pred_real[normal_mask] >= 1700)
            false_alarm_rate = false_alarm / normal_mask.sum()
        else:
            false_alarm_rate = 0
        
        # ê²°ê³¼ ì €ì¥
        result = {
            'metrics': metrics,
            'precision_1700': precision,
            'recall_1700': recall,
            'f1_1700': f1,
            'false_alarm_rate': false_alarm_rate,
            'confusion_matrix': {'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn}
        }
        
        self.results[model_name] = result
        
        # ì¶œë ¥
        self._print_results(model_name, result)
        
        return result
    
    def _print_results(self, model_name, result):
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š {model_name} í‰ê°€ ê²°ê³¼")
        print(f"{'='*60}")
        
        print("\n[ì „ì²´ ì„±ëŠ¥]")
        print(f"  MAE: {result['metrics']['total']['mae']:.2f}")
        print(f"  RMSE: {result['metrics']['total']['rmse']:.2f}")
        
        print("\n[êµ¬ê°„ë³„ ì„±ëŠ¥]")
        for range_name in ['low', 'normal', 'medium', 'high', 'extreme']:
            if range_name in result['metrics']:
                m = result['metrics'][range_name]
                print(f"  {range_name:8s}: MAE={m['mae']:6.2f}, "
                      f"Count={m['count']:4d} ({m['percentage']:5.2f}%)")
        
        print("\n[1700+ ì˜ˆì¸¡ ì„±ëŠ¥]")
        print(f"  Precision: {result['precision_1700']:.2%}")
        print(f"  Recall: {result['recall_1700']:.2%}")
        print(f"  F1 Score: {result['f1_1700']:.2%}")
        
        print(f"\n[ì •ìƒâ†’1700+ ì˜¤íƒë¥ ]")
        print(f"  False Alarm Rate: {result['false_alarm_rate']:.2%}")
        
        # ì¢…í•© ì ìˆ˜
        balanced_score = (
            result['f1_1700'] * 0.4 +  # 1700+ ì˜ˆì¸¡
            (1 - result['false_alarm_rate']) * 0.3 +  # ì˜¤íƒ ë°©ì§€
            (1 - result['metrics']['normal']['mae']/100) * 0.3  # ì •ìƒ êµ¬ê°„ ì •í™•ë„
        )
        print(f"\n[ì¢…í•© ì ìˆ˜]")
        print(f"  Balanced Score: {balanced_score:.2%}")

# ========================================
# ë©”ì¸ ì‹¤í–‰
# ========================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("\nğŸš€ ì‹œì‘...")
    
    # 1. ë°ì´í„° ì²˜ë¦¬
    processor = SafeDataProcessor(seq_len=100, pred_len=10)
    
    # ë°ì´í„° ê²½ë¡œ í™•ì¸ (ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”)
    data_paths = [
        'data/20240201_TO_202507281705.csv',
        '/mnt/user-data/uploads/gs.CSV',
        '/mnt/user-data/uploads/20250807_DATA.CSV',
        'V6_NEW_í•™ìŠµ.pyì™€ ë™ì¼í•œ ìœ„ì¹˜ì— ìˆëŠ” ë°ì´í„° íŒŒì¼.csv' # ì˜ˆì‹œ ê²½ë¡œ
    ]
    
    data_path = None
    for path in data_paths:
        if os.path.exists(path):
            data_path = path
            break
    
    if data_path is None:
        logger.error("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! 'data_paths' ë³€ìˆ˜ ë‚´ì˜ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    df = processor.load_and_process(data_path)
    X, y, y_binary, feature_cols = processor.create_sequences(df)
    
    # 2. ë°ì´í„° ë¶„í• 
    train_size = int(0.7 * len(X))
    val_size = int(0.15 * len(X))
    
    X_train, y_train = X[:train_size], y[:train_size]
    X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]
    X_test = X[train_size+val_size:]
    y_test = y[train_size+val_size:]
    y_test_binary = y_binary[train_size+val_size:]
    
    print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
    print(f"  Train: {X_train.shape}")
    print(f"  Val: {X_val.shape}")
    print(f"  Test: {X_test.shape}")
    
    # 3. ëª¨ë¸ ìƒì„±
    model_builder = StableModels(input_shape=(100, X.shape[2]))
    
    models = {
        'StableLSTM': model_builder.build_stable_lstm(),
        'SpikeDetector': model_builder.build_spike_detector(),
        'ExtremeNet': model_builder.build_extreme_net(),
        'EnsembleBase': model_builder.build_ensemble_base()
    }
    
    # 4. í‰ê°€ì ìƒì„±
    evaluator = ComprehensiveEvaluator(processor.target_scaler)
    
    # 5. ê° ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    for name, model in models.items():
        print(f"\n{'='*60}")
        print(f"ğŸ¯ {name} í•™ìŠµ")
        print(f"{'='*60}")
        
        # ì»´íŒŒì¼
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',  # ì•ˆì •ì ì¸ MSE ì‚¬ìš©
            metrics=['mae']
        )
        
        # í•™ìŠµ
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=20,
            batch_size=256,
            callbacks=[
                EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)
            ],
            verbose=1
        )
        
        # í‰ê°€
        evaluator.evaluate(model, X_test, y_test, y_test_binary, name)
    
    # 6. ìµœì¢… ìˆœìœ„
    print("\n" + "="*80)
    print("ğŸ† ìµœì¢… ìˆœìœ„")
    print("="*80)
    
    # F1 ê¸°ì¤€ ì •ë ¬
    sorted_models = sorted(
        evaluator.results.items(),
        key=lambda x: x[1]['f1_1700'],
        reverse=True
    )
    
    for i, (name, result) in enumerate(sorted_models, 1):
        print(f"{i}. {name:15s} | F1: {result['f1_1700']:.2%} | "
              f"ì •ìƒMAE: {result['metrics']['normal']['mae']:.2f} | "
              f"ì˜¤íƒë¥ : {result['false_alarm_rate']:.2%}")
    
    print("\nâœ… ì™„ë£Œ!")
    
    return evaluator.results

if __name__ == "__main__":
    results = main()
