"""
ğŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ - 78ë§Œê°œ ë°ì´í„° ì™„ì „íŒ
=========================================
ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ìµœì í™”
- ë°°ì¹˜ ì²˜ë¦¬
- ë©”ëª¨ë¦¬ íš¨ìœ¨í™”
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import *
from sklearn.preprocessing import RobustScaler, MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, confusion_matrix
import warnings
import os
import gc
import pickle
from datetime import datetime

warnings.filterwarnings('ignore')
tf.random.set_seed(42)
np.random.seed(42)

# GPU ë©”ëª¨ë¦¬ ì„¤ì •
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)

print("="*80)
print("ğŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ - 78ë§Œê°œ ë°ì´í„° ë²„ì „")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print(f"ğŸ”§ Device: {'GPU' if gpus else 'CPU'}")
print("="*80)

# ë°ì´í„° ì²˜ë¦¬ (ëŒ€ìš©ëŸ‰ ìµœì í™”)
class DataProcessor:
    def __init__(self, seq_len=100, pred_len=10):
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.feature_scaler = RobustScaler()
        self.target_scaler = MinMaxScaler()
        self.scaler_fitted = False
        
    def save_scalers(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥"""
        with open('scalers.pkl', 'wb') as f:
            pickle.dump({
                'feature_scaler': self.feature_scaler,
                'target_scaler': self.target_scaler
            }, f)
        print("ğŸ’¾ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ")
    
    def load_scalers(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        if os.path.exists('scalers.pkl'):
            with open('scalers.pkl', 'rb') as f:
                scalers = pickle.load(f)
                self.feature_scaler = scalers['feature_scaler']
                self.target_scaler = scalers['target_scaler']
                self.scaler_fitted = True
            print("ğŸ“‚ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
            return True
        return False
        
    def load_and_process(self, filepath):
        """ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¡œë“œ"""
        print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {filepath}")
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸°
        df = pd.read_csv(filepath, low_memory=False)
        print(f"âœ… ë°ì´í„°: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        for col in df.columns:
            if df[col].dtype == 'float64':
                df[col] = df[col].astype('float32')
            elif df[col].dtype == 'int64':
                df[col] = df[col].astype('int32')
        
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                       format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        print(f"\nğŸ“Š TOTALCNT ë¶„í¬:")
        print(f"  ìµœì†Œ: {df['TOTALCNT'].min():,}")
        print(f"  í‰ê· : {df['TOTALCNT'].mean():,.0f}")
        print(f"  ìµœëŒ€: {df['TOTALCNT'].max():,}")
        
        normal = (df['TOTALCNT'] < 1400).sum()
        warning = ((df['TOTALCNT'] >= 1400) & (df['TOTALCNT'] < 1700)).sum()
        critical = (df['TOTALCNT'] >= 1700).sum()
        
        print(f"\nğŸ“Š êµ¬ê°„ ë¶„í¬:")
        print(f"  ì •ìƒ: {normal:,}ê°œ ({normal/len(df)*100:.2f}%)")
        print(f"  ì£¼ì˜: {warning:,}ê°œ ({warning/len(df)*100:.2f}%)")
        print(f"  ì‹¬ê°: {critical:,}ê°œ ({critical/len(df)*100:.2f}%)")
        
        return df
    
    def create_features(self, df):
        """íŠ¹ì„± ìƒì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
        print("\nâš™ï¸ íŠ¹ì„± ìƒì„±...")
        
        # í™©ê¸ˆ íŒ¨í„´
        df['RATIO'] = (df['M14AM14B'] / (df['M14AM10A'] + 1)).astype('float32')
        df['GOLDEN_1'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype('float32')
        df['GOLDEN_2'] = (df['M14AM14BSUM'] > 500).astype('float32')
        
        # ì‹œê°„ (NaN ì²˜ë¦¬)
        df['HOUR'] = df['CURRTIME'].dt.hour
        df['HOUR'] = df['HOUR'].fillna(0).astype('float32')  # NaNì„ 0ìœ¼ë¡œ ì±„ìš°ê³  float32ë¡œ
        df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24).astype('float32')
        df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24).astype('float32')
        
        # ì´ë™í‰ê·  (íš¨ìœ¨ì  ê³„ì‚°)
        print("  ì´ë™í‰ê·  ê³„ì‚°...")
        for w in [10, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean().astype('float32')
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0).astype('float32')
        
        # ë³€í™”ìœ¨
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0).astype('float32')
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0).astype('float32')
        
        # íƒ€ê²Ÿ
        df['TARGET'] = df['TOTALCNT'].shift(-self.pred_len)
        df['LEVEL'] = pd.cut(df['TARGET'], 
                            bins=[-np.inf, 1400, 1700, np.inf], 
                            labels=[0, 1, 2])
        
        print(f"âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(df.columns)}ê°œ ì»¬ëŸ¼")
        return df
    
    def create_sequences(self, df, batch_size=10000):
        """ëŒ€ìš©ëŸ‰ ì‹œí€€ìŠ¤ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)"""
        print("\nğŸ”„ ì‹œí€€ìŠ¤ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)...")
        df = df.dropna()
        
        # ë°ì´í„° ì²´í¬
        if len(df) == 0:
            print("âŒ ì—ëŸ¬: ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            return None, None, None
        
        features = [
            'TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M14AM10A', 'M14AM16',
            'M14AM16SUM', 'M10AM14A', 'M14BM14A', 'M16M14A', 'M14AM10ASUM',
            'RATIO', 'GOLDEN_1', 'GOLDEN_2',
            'HOUR_SIN', 'HOUR_COS',
            'MA_10', 'MA_30', 'STD_10', 'STD_30',
            'CHANGE_1', 'CHANGE_10'
        ]
        
        features = [f for f in features if f in df.columns]
        print(f"  íŠ¹ì„±: {len(features)}ê°œ")
        
        X_data = df[features].values.astype('float32')
        y_reg = df['TARGET'].values.astype('float32')
        y_cls = df['LEVEL'].values  # íƒ€ì… ë³€í™˜ ì œê±°
        
        print(f"  ë°ì´í„° í¬ê¸°: {X_data.shape}")
        
        # ìŠ¤ì¼€ì¼ë§
        print("  ìŠ¤ì¼€ì¼ëŸ¬ ì²˜ë¦¬...")
        if not self.scaler_fitted or len(X_data) > 0:
            # í•­ìƒ fit_transform ì‚¬ìš© (ì•ˆì „)
            X_scaled = self.feature_scaler.fit_transform(X_data)
            y_reg_scaled = self.target_scaler.fit_transform(y_reg.reshape(-1, 1)).flatten()
            self.scaler_fitted = True
            self.save_scalers()
        else:
            print("âŒ ì—ëŸ¬: ìŠ¤ì¼€ì¼ë§í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return None, None, None
        
        # ë°°ì¹˜ë³„ ì‹œí€€ìŠ¤ ìƒì„±
        total_sequences = len(X_scaled) - self.seq_len
        print(f"  ì´ ì‹œí€€ìŠ¤: {total_sequences:,}ê°œ")
        
        X_list, y_r_list, y_c_list = [], [], []
        
        for start_idx in range(0, total_sequences, batch_size):
            end_idx = min(start_idx + batch_size, total_sequences)
            
            batch_X = []
            batch_y_r = []
            batch_y_c = []
            
            for i in range(start_idx, end_idx):
                batch_X.append(X_scaled[i:i+self.seq_len])
                batch_y_r.append(y_reg_scaled[i+self.seq_len-1])
                # NaN ì²´í¬ í›„ ë³€í™˜
                if pd.isna(y_cls[i+self.seq_len-1]):
                    batch_y_c.append(0)  # NaNì€ 0ìœ¼ë¡œ ì²˜ë¦¬
                else:
                    batch_y_c.append(int(y_cls[i+self.seq_len-1]))
            
            X_list.append(np.array(batch_X, dtype='float32'))
            y_r_list.append(np.array(batch_y_r, dtype='float32'))
            y_c_list.append(np.array(batch_y_c, dtype='int32'))
            
            if (end_idx % 50000) == 0:
                print(f"    ì²˜ë¦¬: {end_idx:,}/{total_sequences:,}")
                gc.collect()
        
        # í•©ì¹˜ê¸°
        X = np.concatenate(X_list, axis=0)
        y_r = np.concatenate(y_r_list, axis=0)
        y_c = np.concatenate(y_c_list, axis=0)
        
        print(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {X.shape}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del X_list, y_r_list, y_c_list
        gc.collect()
        
        return X, y_r, y_c

# ===== 5ê°œ ëª¨ë¸ ì •ì˜ (ë™ì¼) =====

def build_stable_lstm(input_shape):
    """1. StableLSTM"""
    inputs = Input(shape=input_shape)
    x = Bidirectional(LSTM(128, return_sequences=True))(inputs)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    x = Bidirectional(LSTM(64))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    x = Dense(64, activation='relu')(x)
    x = BatchNormalization()(x)
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    return Model(inputs, [out_reg, out_cls], name='StableLSTM')

def build_patch_tst(input_shape):
    """2. PatchTST (ê°„ì†Œí™”)"""
    inputs = Input(shape=input_shape)
    x = Dense(128)(inputs)
    x = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
    x = LayerNormalization()(x)
    x = GlobalAveragePooling1D()(x)
    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    return Model(inputs, [out_reg, out_cls], name='PatchTST')

def build_spike_detector(input_shape):
    """3. SpikeDetector"""
    inputs = Input(shape=input_shape)
    recent = Lambda(lambda x: x[:, -20:, :])(inputs)
    x = Conv1D(64, 3, activation='relu', padding='same')(recent)
    x = BatchNormalization()(x)
    x = MaxPooling1D(2)(x)
    x = GRU(64)(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    x = Dense(64, activation='relu')(x)
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    return Model(inputs, [out_reg, out_cls], name='SpikeDetector')

def build_extreme_net(input_shape):
    """4. ExtremeNet"""
    inputs = Input(shape=input_shape)
    normal = LSTM(64, return_sequences=True)(inputs)
    normal = LSTM(32)(normal)
    extreme = MultiHeadAttention(num_heads=4, key_dim=16)(inputs, inputs)
    extreme = GlobalAveragePooling1D()(extreme)
    x = Concatenate()([normal, extreme])
    x = BatchNormalization()(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.3)(x)
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    return Model(inputs, [out_reg, out_cls], name='ExtremeNet')

def build_ensemble_base(input_shape):
    """5. EnsembleBase"""
    inputs = Input(shape=input_shape)
    avg_pool = GlobalAveragePooling1D()(inputs)
    max_pool = GlobalMaxPooling1D()(inputs)
    x = Concatenate()([avg_pool, max_pool])
    x = BatchNormalization()(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)
    x = Dense(64, activation='relu')(x)
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    return Model(inputs, [out_reg, out_cls], name='EnsembleBase')

# í•™ìŠµ ë° í‰ê°€ (ì²´í¬í¬ì¸íŠ¸ í¬í•¨)
def train_and_evaluate(model, X_train, y_train, X_val, y_val, X_test, y_test, processor):
    
    # ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ (í´ë˜ìŠ¤ ë¶ˆê· í˜•)
    def weighted_crossentropy(y_true, y_pred):
        weights = tf.constant([1.0, 5.0, 200.0])
        scce = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
        weight_map = tf.gather(weights, tf.cast(y_true, tf.int32))
        return tf.reduce_mean(scce * weight_map)
    
    model.compile(
        optimizer=Adam(0.001),
        loss=['mse', weighted_crossentropy],
        loss_weights=[0.3, 0.7],
        metrics={'regression': 'mae', 'classification': 'accuracy'}
    )
    
    print(f"\n{'='*60}")
    print(f"ğŸ¯ {model.name} í•™ìŠµ (78ë§Œê°œ ë°ì´í„°)")
    print(f"{'='*60}")
    
    # ì²´í¬í¬ì¸íŠ¸ ì½œë°±
    checkpoint_dir = 'checkpoints'
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    callbacks = [
        ModelCheckpoint(
            f'{checkpoint_dir}/{model.name}_best.keras',
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        ),
        EarlyStopping(
            patience=5, 
            restore_best_weights=True, 
            verbose=1
        ),
        ReduceLROnPlateau(
            factor=0.5, 
            patience=3, 
            min_lr=1e-6,
            verbose=1
        ),
        TensorBoard(
            log_dir=f'logs/{model.name}',
            histogram_freq=0
        )
    ]
    
    # í•™ìŠµ (ë°°ì¹˜ í¬ê¸° ì¦ê°€)
    history = model.fit(
        X_train,
        {'regression': y_train[0], 'classification': y_train[1]},
        validation_data=(X_val, {'regression': y_val[0], 'classification': y_val[1]}),
        epochs=30,  # ë” ë§ì€ ì—í­
        batch_size=512,  # ë” í° ë°°ì¹˜
        callbacks=callbacks,
        verbose=1
    )
    
    print(f"\nğŸ“Š {model.name} í‰ê°€ ê²°ê³¼:")
    
    # ì˜ˆì¸¡ (ë°°ì¹˜ ì²˜ë¦¬)
    preds = model.predict(X_test, batch_size=1024, verbose=0)
    y_reg_pred = preds[0].flatten()
    y_cls_pred = np.argmax(preds[1], axis=1)
    
    # ì—­ë³€í™˜
    y_reg_true_orig = processor.target_scaler.inverse_transform(
        y_test[0].reshape(-1, 1)).flatten()
    y_reg_pred_orig = processor.target_scaler.inverse_transform(
        y_reg_pred.reshape(-1, 1)).flatten()
    
    # ë©”íŠ¸ë¦­
    mae = mean_absolute_error(y_reg_true_orig, y_reg_pred_orig)
    rmse = np.sqrt(mean_squared_error(y_reg_true_orig, y_reg_pred_orig))
    r2 = r2_score(y_reg_true_orig, y_reg_pred_orig)
    acc = accuracy_score(y_test[1], y_cls_pred)
    
    # í˜¼ë™ í–‰ë ¬
    cm = confusion_matrix(y_test[1], y_cls_pred)
    
    print(f"\n[íšŒê·€ ì„±ëŠ¥]")
    print(f"  MAE:  {mae:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  RÂ²:   {r2:.4f}")
    
    print(f"\n[ë¶„ë¥˜ ì„±ëŠ¥]")
    print(f"  ì „ì²´ ì •í™•ë„: {acc:.2%}")
    
    print(f"\n[í˜¼ë™ í–‰ë ¬]")
    print(f"  ì •ìƒâ†’ì •ìƒ: {cm[0,0]:,}ê°œ")
    print(f"  ì£¼ì˜â†’ì£¼ì˜: {cm[1,1]:,}ê°œ" if len(cm) > 1 else "")
    print(f"  ì‹¬ê°â†’ì‹¬ê°: {cm[2,2]:,}ê°œ" if len(cm) > 2 else "")
    
    # 1700+ ê°ì§€ìœ¨
    if 2 in y_test[1]:
        critical_mask = y_test[1] == 2
        critical_recall = (y_cls_pred[critical_mask] == 2).mean()
        print(f"\n[1700+ ê°ì§€ìœ¨]: {critical_recall:.2%}")
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs('models', exist_ok=True)
    model.save(f'models/{model.name}_final.keras')
    print(f"\nğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: models/{model.name}_final.keras")
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'MAE': mae, 
        'RMSE': rmse, 
        'R2': r2, 
        'Accuracy': acc,
        'Train_Size': len(X_train),
        'Val_Size': len(X_val),
        'Test_Size': len(X_test)
    }
    
    # í•™ìŠµ ì´ë ¥ ì €ì¥
    with open(f'history/{model.name}_history.pkl', 'wb') as f:
        pickle.dump(history.history, f)
    
    return results

# ë©”ì¸ ì‹¤í–‰
def main():
    # ë°ì´í„° ê²½ë¡œ
    data_paths = [
        '/mnt/user-data/uploads/gs.CSV',  # 78ë§Œê°œ ë°ì´í„°
        'data/full_data.csv',
        'data.csv'
    ]
    
    data_path = None
    for path in data_paths:
        if os.path.exists(path):
            data_path = path
            print(f"âœ… ë°ì´í„° ë°œê²¬: {path}")
            break
    
    if not data_path:
        print("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('checkpoints', exist_ok=True)
    os.makedirs('models', exist_ok=True)
    os.makedirs('logs', exist_ok=True)
    os.makedirs('history', exist_ok=True)
    
    # ë°ì´í„° ì²˜ë¦¬
    processor = DataProcessor()
    
    # ìŠ¤ì¼€ì¼ëŸ¬ëŠ” ìƒˆë¡œ í•™ìŠµ (78ë§Œê°œ ë°ì´í„°ì— ë§ê²Œ)
    # processor.load_scalers()  # ì œê±°
    
    # ë°ì´í„° ë¡œë“œ
    df = processor.load_and_process(data_path)
    df = processor.create_features(df)
    
    # ì‹œí€€ìŠ¤ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
    X, y_reg, y_cls = processor.create_sequences(df, batch_size=10000)
    
    # ë°ì´í„° ì²´í¬
    if X is None:
        print("âŒ ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨!")
        return
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del df
    gc.collect()
    
    # ë°ì´í„° ë¶„í•  (78ë§Œê°œ ê¸°ì¤€)
    n_train = int(0.7 * len(X))  # ì•½ 54ë§Œê°œ
    n_val = int(0.15 * len(X))   # ì•½ 11ë§Œê°œ
    # n_test = ë‚˜ë¨¸ì§€             # ì•½ 11ë§Œê°œ
    
    print(f"\nğŸ“Š ë°ì´í„° ë¶„í•  (78ë§Œê°œ ê¸°ì¤€):")
    print(f"  ì „ì²´: {len(X):,}ê°œ")
    print(f"  Train: {n_train:,}ê°œ (70%)")
    print(f"  Val: {n_val:,}ê°œ (15%)")
    print(f"  Test: {len(X) - n_train - n_val:,}ê°œ (15%)")
    
    X_train = X[:n_train]
    X_val = X[n_train:n_train+n_val]
    X_test = X[n_train+n_val:]
    
    y_train = (y_reg[:n_train], y_cls[:n_train])
    y_val = (y_reg[n_train:n_train+n_val], y_cls[n_train:n_train+n_val])
    y_test = (y_reg[n_train+n_val:], y_cls[n_train+n_val:])
    
    print(f"\nğŸ“Š ì‹¤ì œ shape:")
    print(f"  X_train: {X_train.shape}")
    print(f"  X_val:   {X_val.shape}")
    print(f"  X_test:  {X_test.shape}")
    
    # 5ê°œ ëª¨ë¸ ìƒì„±
    input_shape = (X.shape[1], X.shape[2])
    
    models = {
        'StableLSTM': build_stable_lstm(input_shape),
        'PatchTST': build_patch_tst(input_shape),
        'SpikeDetector': build_spike_detector(input_shape),
        'ExtremeNet': build_extreme_net(input_shape),
        'EnsembleBase': build_ensemble_base(input_shape)
    }
    
    print(f"\nâœ… 5ê°œ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ:")
    for name in models.keys():
        print(f"  - {name}")
    
    # í•™ìŠµ ë° í‰ê°€
    results = {}
    for name, model in models.items():
        try:
            results[name] = train_and_evaluate(
                model, X_train, y_train, X_val, y_val, X_test, y_test, processor
            )
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            tf.keras.backend.clear_session()
            gc.collect()
            
        except Exception as e:
            print(f"âŒ {name} í•™ìŠµ ì‹¤íŒ¨: {e}")
            continue
    
    # ìµœì¢… ê²°ê³¼
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… ì„±ëŠ¥ ë¹„êµ (78ë§Œê°œ ë°ì´í„°)")
    print("="*80)
    
    results_df = pd.DataFrame(results).T
    print("\n", results_df.round(3))
    
    # ìµœê³  ì„±ëŠ¥
    print("\nğŸ† ìµœê³  ì„±ëŠ¥:")
    print(f"  MAE ìµœì €: {results_df['MAE'].idxmin()} ({results_df['MAE'].min():.2f})")
    print(f"  RMSE ìµœì €: {results_df['RMSE'].idxmin()} ({results_df['RMSE'].min():.2f})")
    print(f"  RÂ² ìµœê³ : {results_df['R2'].idxmax()} ({results_df['R2'].max():.4f})")
    print(f"  ì •í™•ë„ ìµœê³ : {results_df['Accuracy'].idxmax()} ({results_df['Accuracy'].max():.2%})")
    
    # CSV ì €ì¥
    results_df.to_csv('model_results_780k.csv')
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: model_results_780k.csv")
    
    print("\nâœ… 78ë§Œê°œ ë°ì´í„° í•™ìŠµ ì™„ë£Œ!")
    
    return results

if __name__ == "__main__":
    results = main()