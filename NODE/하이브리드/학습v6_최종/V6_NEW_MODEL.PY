"""
ğŸš€ Ultimate 1700+ Predictor v8.0 - Production Ready
==================================================
- Infinity/NaN ì˜¤ë¥˜ ì™„ì „ í•´ê²°
- ì „ì²´ êµ¬ê°„ë³„ í‰ê°€ (ì¼ë°˜, ì¤‘ê°„, 1700+)
- ì•ˆì •ì ì¸ ë°ì´í„° ì²˜ë¦¬
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import *
from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
import warnings
import logging
from datetime import datetime
import os
import json

warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# TensorFlow ì„¤ì •
tf.random.set_seed(42)
np.random.seed(42)

print("\n" + "="*80)
print("ğŸš€ Ultimate 1700+ Predictor v8.0")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print("="*80)

# ========================================
# ì•ˆì „í•œ ë°ì´í„° ì²˜ë¦¬
# ========================================

class SafeDataProcessor:
    """Infinity/NaN ì•ˆì „ ì²˜ë¦¬"""
    
    def __init__(self, seq_len=100, pred_len=10):
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.scaler = MinMaxScaler()  # RobustScaler ëŒ€ì‹  MinMaxScaler ì‚¬ìš©
        self.target_scaler = MinMaxScaler()
        
    def load_and_process(self, filepath):
        """ë°ì´í„° ë¡œë“œ - ì•ˆì „ ì²˜ë¦¬"""
        logger.info("ğŸ“‚ ë°ì´í„° ë¡œë”©...")
        
        # CSV ë¡œë“œ
        df = pd.read_csv(filepath)
        logger.info(f"âœ… ì›ë³¸ ë°ì´í„°: {df.shape}")
        
        # ì‹œê°„ ë³€í™˜
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'], format='%Y%m%d%H%M', errors='coerce')
        df['TIME'] = pd.to_datetime(df['TIME'], format='%Y%m%d%H%M', errors='coerce')
        
        # NaT ì œê±°
        df = df.dropna(subset=['CURRTIME', 'TIME'])
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # Infinityì™€ NaN ì²´í¬ ë° ì œê±°
        logger.info("ğŸ” ì´ìƒì¹˜ ì²´í¬...")
        for col in numeric_cols:
            # Infinityë¥¼ NaNìœ¼ë¡œ ë³€ê²½
            df[col] = df[col].replace([np.inf, -np.inf], np.nan)
            
            # NaNì„ ì¤‘ì•™ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
            if df[col].isna().sum() > 0:
                median_val = df[col].median()
                if pd.isna(median_val):
                    median_val = 0
                df[col] = df[col].fillna(median_val)
                logger.info(f"   {col}: {df[col].isna().sum()}ê°œ NaN â†’ {median_val:.2f}ë¡œ ëŒ€ì²´")
        
        # ê·¹ë‹¨ì  ì´ìƒì¹˜ ì œê±° (IQR ë°©ë²•)
        for col in ['TOTALCNT']:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower = Q1 - 3 * IQR
            upper = Q3 + 3 * IQR
            
            before = len(df)
            df = df[(df[col] >= lower) & (df[col] <= upper)]
            after = len(df)
            
            if before != after:
                logger.info(f"   {col}: {before-after}ê°œ ê·¹ë‹¨ ì´ìƒì¹˜ ì œê±°")
        
        # íƒ€ê²Ÿ ìƒì„±
        df['TARGET'] = df['TOTALCNT'].shift(-self.pred_len)
        df['IS_1700'] = (df['TARGET'] >= 1700).astype(int)
        
        # íŠ¹ì„± ìƒì„± (ì•ˆì „í•˜ê²Œ)
        df = self._create_safe_features(df)
        
        # ìµœì¢… NaN ì œê±°
        df = df.dropna()
        
        logger.info(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {df.shape}")
        logger.info(f"âœ… 1700+ ë¹„ìœ¨: {df['IS_1700'].mean():.2%} ({df['IS_1700'].sum()}ê°œ)")
        
        return df
    
    def _create_safe_features(self, df):
        """ì•ˆì „í•œ íŠ¹ì„± ìƒì„±"""
        logger.info("âš™ï¸ íŠ¹ì„± ìƒì„±...")
        
        # ì´ë™í‰ê·  (ì•ˆì „í•œ ë²”ìœ„)
        for window in [10, 30]:
            df[f'MA_{window}'] = df['TOTALCNT'].rolling(window, min_periods=1).mean()
            df[f'MA_{window}'].fillna(df['TOTALCNT'].mean(), inplace=True)
        
        # ë³€í™”ìœ¨ (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
        df['CHANGE'] = df['TOTALCNT'].diff().fillna(0)
        df['PCT_CHANGE'] = df['TOTALCNT'].pct_change().fillna(0)
        
        # ê·¹ë‹¨ê°’ ë°©ì§€
        df['PCT_CHANGE'] = df['PCT_CHANGE'].clip(-1, 1)
        
        # ë¹„ìœ¨ ê³„ì‚° (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
        if 'M14AM14B' in df.columns and 'M14AM10A' in df.columns:
            df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)  # +1ë¡œ 0 ë°©ì§€
            df['RATIO'] = df['RATIO'].clip(0, 20)  # ê·¹ë‹¨ê°’ ì œí•œ
        
        # ì‹œê°„ íŠ¹ì„±
        df['HOUR'] = df['CURRTIME'].dt.hour
        df['DAYOFWEEK'] = df['CURRTIME'].dt.dayofweek
        df['IS_DAWN'] = df['HOUR'].isin([4, 5, 6]).astype(int)
        
        # ì„ê³„ê°’ ì‹ í˜¸
        if 'M14AM14B' in df.columns:
            df['M14B_HIGH'] = (df['M14AM14B'] > df['M14AM14B'].quantile(0.9)).astype(int)
        
        # ëª¨ë“  ê°’ ì²´í¬
        for col in df.select_dtypes(include=[np.number]).columns:
            # Infinity ì²´í¬
            df[col] = df[col].replace([np.inf, -np.inf], df[col].median())
            # NaN ì²´í¬
            df[col] = df[col].fillna(df[col].median())
            
        return df
    
    def create_sequences(self, df):
        """ì•ˆì „í•œ ì‹œí€€ìŠ¤ ìƒì„±"""
        logger.info("ğŸ”„ ì‹œí€€ìŠ¤ ìƒì„±...")
        
        # íŠ¹ì„± ì„ íƒ (ë¬¸ì œ ë  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼ ì œì™¸)
        exclude_cols = ['CURRTIME', 'TIME', 'TARGET', 'IS_1700']
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        # ìˆ˜ì¹˜í˜•ë§Œ ì„ íƒ
        feature_cols = [col for col in feature_cols if df[col].dtype in [np.float64, np.int64, np.float32, np.int32]]
        
        logger.info(f"   ì‚¬ìš©í•  íŠ¹ì„±: {len(feature_cols)}ê°œ")
        
        # ë°ì´í„° ë³µì‚¬
        data = df[feature_cols].copy()
        targets = df['TARGET'].copy()
        is_1700 = df['IS_1700'].copy()
        
        # ìŠ¤ì¼€ì¼ë§ ì „ ì²´í¬
        logger.info("   ìŠ¤ì¼€ì¼ë§ ì „ ì²´í¬...")
        data = data.replace([np.inf, -np.inf], np.nan)
        data = data.fillna(data.median())
        
        # ìŠ¤ì¼€ì¼ë§
        try:
            data_scaled = self.scaler.fit_transform(data)
            targets_scaled = self.target_scaler.fit_transform(targets.values.reshape(-1, 1)).flatten()
        except Exception as e:
            logger.error(f"âŒ ìŠ¤ì¼€ì¼ë§ ì˜¤ë¥˜: {e}")
            # ëŒ€ì²´ ë°©ë²•: ìˆ˜ë™ ì •ê·œí™”
            data_scaled = (data - data.mean()) / (data.std() + 1e-7)
            targets_scaled = (targets - targets.mean()) / (targets.std() + 1e-7)
            data_scaled = data_scaled.fillna(0).values
            targets_scaled = targets_scaled.fillna(0).values
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y, y_binary = [], [], []
        
        for i in range(len(data_scaled) - self.seq_len - self.pred_len):
            seq = data_scaled[i:i+self.seq_len]
            
            # ì‹œí€€ìŠ¤ ì²´í¬
            if not np.any(np.isnan(seq)) and not np.any(np.isinf(seq)):
                X.append(seq)
                y.append(targets_scaled[i+self.seq_len])
                y_binary.append(is_1700.iloc[i+self.seq_len])
        
        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.float32)
        y_binary = np.array(y_binary, dtype=np.float32)
        
        # ìµœì¢… ì²´í¬
        logger.info("   ìµœì¢… ì²´í¬...")
        logger.info(f"   X: NaN={np.isnan(X).sum()}, Inf={np.isinf(X).sum()}")
        logger.info(f"   y: NaN={np.isnan(y).sum()}, Inf={np.isinf(y).sum()}")
        
        logger.info(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {X.shape}")
        
        return X, y, y_binary, feature_cols

# ========================================
# ì•ˆì •ì ì¸ ëª¨ë¸
# ========================================

class StableModels:
    """ëª¨ë“  ëª¨ë¸ êµ¬í˜„ - 15ê°œ"""
    
    def __init__(self, input_shape):
        self.input_shape = input_shape
        self.seq_len = input_shape[0]
        self.n_features = input_shape[1]
    
    # 1. PatchTST - Transformer ìµœê°•
    def build_patch_tst(self, patch_len=10, d_model=128, n_heads=4):
        """Patch Time Series Transformer - SOTA"""
        inputs = Input(shape=self.input_shape)
        
        # Normalize
        x = BatchNormalization()(inputs)
        
        # íŒ¨ì¹˜ ìƒì„± (100 -> 10 patches) - Reshape ë ˆì´ì–´ ì‚¬ìš©
        n_patches = self.seq_len // patch_len
        patch_size = patch_len * self.n_features
        
        # Reshapeë¥¼ Keras ë ˆì´ì–´ë¡œ ì²˜ë¦¬
        x = Reshape((n_patches, patch_size))(x)
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x = Dense(d_model)(x)
        x = LayerNormalization()(x)
        
        # Multi-Head Attention (2 layers)
        for _ in range(2):
            attn = MultiHeadAttention(
                num_heads=n_heads, 
                key_dim=d_model//n_heads,
                dropout=0.1
            )(x, x)
            x = Add()([x, attn])
            x = LayerNormalization()(x)
            
            # FFN
            ff = Dense(d_model * 2, activation='relu')(x)
            ff = Dropout(0.1)(ff)
            ff = Dense(d_model)(ff)
            x = Add()([x, ff])
            x = LayerNormalization()(x)
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.2)(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='PatchTST')
    
    # 2. Extreme Value Network - 1700+ í‚¬ëŸ¬
    def build_extreme_value_network(self):
        """ê·¹ë‹¨ê°’ ì´ë¡  ê¸°ë°˜ - 1700+ íŠ¹í™”"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # LSTM backbone
        x = LSTM(128, return_sequences=True)(x)
        x = Dropout(0.2)(x)
        x = LSTM(64)(x)
        x = Dropout(0.2)(x)
        
        # ê·¹ë‹¨ê°’ ë¸Œëœì¹˜
        extreme = Dense(32, activation='relu')(x)
        extreme = Dense(16, activation='relu')(extreme)
        xi = Dense(1, activation='tanh')(extreme)  # shape parameter
        sigma = Dense(1, activation='softplus')(extreme)  # scale parameter
        
        # ì¼ë°˜ ë¸Œëœì¹˜
        normal = Dense(32, activation='relu')(x)
        normal_pred = Dense(1, activation='linear')(normal)
        
        # GPD ê¸°ë°˜ ë³´ì •
        extreme_adjustment = Multiply()([sigma, xi])
        outputs = Add()([normal_pred, extreme_adjustment])
        outputs = Activation('sigmoid')(outputs)
        
        return Model(inputs, outputs, name='ExtremeValueNet')
    
    # 3. Enhanced Spike Detector - ê¸‰ë³€ ê°ì§€
    def build_spike_detector(self):
        """ê¸‰ë³€ ê°ì§€ íŠ¹í™”"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # CNN for pattern
        x = Conv1D(64, 3, padding='same', activation='relu')(x)
        x = BatchNormalization()(x)
        x = Conv1D(64, 3, padding='same', activation='relu')(x)
        x = MaxPooling1D(2)(x)
        
        # Bidirectional LSTM
        x = Bidirectional(LSTM(64, return_sequences=True))(x)
        x = Bidirectional(LSTM(32))(x)
        
        # Detection heads
        spike_prob = Dense(32, activation='relu')(x)
        spike_prob = Dense(1, activation='sigmoid')(spike_prob)
        
        spike_mag = Dense(32, activation='relu')(x)
        spike_mag = Dense(1, activation='linear')(spike_mag)
        
        outputs = Multiply()([spike_prob, spike_mag])
        outputs = Activation('sigmoid')(outputs)
        
        return Model(inputs, outputs, name='SpikeDetector')
    
    # 4. Mamba (State Space Model)
    def build_mamba_ssm(self):
        """State Space Model - íš¨ìœ¨ì """
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # SSM layers (GRUë¡œ ê·¼ì‚¬)
        x = GRU(128, return_sequences=True)(x)
        x = GRU(64, return_sequences=True)(x)
        x = GRU(32)(x)
        
        x = Dense(32, activation='relu')(x)
        x = Dropout(0.2)(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='MambaSSM')
    
    # 5. MTGNN (Graph Neural Network)
    def build_mtgnn(self):
        """Multivariate Time Graph NN - ì»¬ëŸ¼ ê´€ê³„"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # Graph construction
        node_emb = Dense(64, activation='relu')(x)
        
        # Temporal Conv
        x = Conv1D(64, 3, padding='causal', activation='relu')(x)
        x = Conv1D(64, 3, padding='causal', activation='relu')(x)
        
        # Graph Conv (simplified)
        x = Dense(128, activation='relu')(x)
        x = Dense(64, activation='relu')(x)
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='MTGNN')
    
    # 6. Neural SDE
    def build_neural_sde(self):
        """í™•ë¥ ì  ë¯¸ë¶„ë°©ì •ì‹"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # Drift (mu)
        drift = LSTM(64, return_sequences=True)(x)
        drift = LSTM(32)(drift)
        mu = Dense(16, activation='linear')(drift)
        
        # Diffusion (sigma)  
        diffusion = LSTM(64, return_sequences=True)(x)
        diffusion = LSTM(32)(diffusion)
        sigma = Dense(16, activation='softplus')(diffusion)
        
        # Stochastic integration - Lambda ë ˆì´ì–´ ì‚¬ìš©
        x = Add()([mu, Lambda(lambda s: s * np.random.normal(0, 0.1, (1,)))(sigma)])
        
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='NeuralSDE')
    
    # 7. Mixture of Experts
    def build_mixture_of_experts(self):
        """ì „ë¬¸ê°€ ì•™ìƒë¸”"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # Shared
        shared = LSTM(64, return_sequences=True)(x)
        shared = LSTM(32)(shared)
        
        # 3 Experts
        e1 = Dense(16, activation='relu')(shared)
        e1_out = Dense(1, activation='sigmoid')(e1)
        
        e2 = Dense(16, activation='relu')(shared)
        e2_out = Dense(1, activation='sigmoid')(e2)
        
        e3 = Dense(16, activation='relu')(shared)
        e3_out = Dense(1, activation='sigmoid')(e3)
        
        # Router
        gate = Dense(16, activation='relu')(shared)
        gate = Dense(3, activation='softmax')(gate)
        
        # Weighted sum - Keras ì—°ì‚°ìœ¼ë¡œ ì²˜ë¦¬
        e1_weighted = Multiply()([e1_out, Lambda(lambda g: g[:, 0:1])(gate)])
        e2_weighted = Multiply()([e2_out, Lambda(lambda g: g[:, 1:2])(gate)])
        e3_weighted = Multiply()([e3_out, Lambda(lambda g: g[:, 2:3])(gate)])
        
        outputs = Add()([e1_weighted, e2_weighted, e3_weighted])
        
        return Model(inputs, outputs, name='MixtureOfExperts')
    
    # 8. Diffusion Model
    def build_diffusion_model(self):
        """Denoising Diffusion"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # Encoder
        x = Conv1D(64, 3, padding='same', activation='relu')(x)
        x = Conv1D(128, 3, padding='same', activation='relu')(x)
        x = MaxPooling1D(2)(x)
        
        # Noising - Lambda ë ˆì´ì–´ ì‚¬ìš©
        x = LSTM(64, return_sequences=True)(x)
        x = Lambda(lambda t: t + np.random.normal(0, 0.05, (1,1,1)))(x)
        
        # Denoiser
        x = LSTM(64)(x)
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='DiffusionModel')
    
    # 9. Autoformer
    def build_autoformer(self):
        """Auto-Correlation Transformer"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # Decomposition
        trend = AveragePooling1D(10, strides=1, padding='same')(x)
        seasonal = x - trend
        
        # Process
        trend = Conv1D(64, 1, activation='relu')(trend)
        seasonal = Conv1D(64, 1, activation='relu')(seasonal)
        
        # Auto-Correlation
        x = Concatenate()([trend, seasonal])
        x = MultiHeadAttention(num_heads=4, key_dim=16)(x, x)
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='Autoformer')
    
    # 10. TCN (Temporal CNN)
    def build_tcn(self):
        """Temporal Convolutional Network"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # Dilated convolutions
        for dilation in [1, 2, 4, 8]:
            conv = Conv1D(64, 3, dilation_rate=dilation, 
                         padding='causal', activation='relu')(x)
            conv = Dropout(0.1)(conv)
            if x.shape[-1] != conv.shape[-1]:
                x = Conv1D(64, 1)(x)
            x = Add()([x, conv])
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='TCN')
    
    # 11. WaveNet
    def build_wavenet(self):
        """WaveNet Architecture"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        skip_connections = []
        
        for i in range(4):
            dilation = 2 ** i
            
            # Gated conv
            tanh_out = Conv1D(64, 2, dilation_rate=dilation,
                             padding='causal', activation='tanh')(x)
            sigm_out = Conv1D(64, 2, dilation_rate=dilation,
                             padding='causal', activation='sigmoid')(x)
            
            x = Multiply()([tanh_out, sigm_out])
            
            skip = Conv1D(64, 1)(x)
            skip_connections.append(skip)
            
            x = Conv1D(self.n_features, 1)(x)
            x = Add()([x, inputs])
        
        x = Add()(skip_connections)
        x = Activation('relu')(x)
        x = GlobalAveragePooling1D()(x)
        
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='WaveNet')
    
    # 12. Bi-LSTM with Attention
    def build_bilstm_attention(self):
        """ì–‘ë°©í–¥ LSTM + Attention"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # Bi-LSTM
        x = Bidirectional(LSTM(128, return_sequences=True))(x)
        x = Bidirectional(LSTM(64, return_sequences=True))(x)
        
        # Attention
        attention = Dense(1, activation='tanh')(x)
        attention = Flatten()(attention)
        attention = Activation('softmax')(attention)
        attention = RepeatVector(128)(attention)
        attention = Permute([2, 1])(attention)
        
        x = Multiply()([x, attention])
        x = GlobalAveragePooling1D()(x)
        
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.2)(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='BiLSTM_Attention')
    
    # 13. Informer
    def build_informer(self):
        """Informer - íš¨ìœ¨ì  Transformer"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # ProbSparse Attention (simplified)
        x = Dense(128)(x)
        x = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
        x = LayerNormalization()(x)
        
        x = Dense(64, activation='relu')(x)
        x = GlobalAveragePooling1D()(x)
        
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='Informer')
    
    # 14. TimesNet
    def build_timesnet(self):
        """TimesNet - 2D Transform"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # 1D to 2D transform - Keras ë ˆì´ì–´ ì‚¬ìš©
        x = Lambda(lambda t: tf.expand_dims(t, axis=-1))(x)
        x = Conv2D(64, (3, 1), padding='same', activation='relu')(x)
        x = Conv2D(32, (3, 1), padding='same', activation='relu')(x)
        x = Lambda(lambda t: tf.squeeze(t, axis=-1))(x)
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='TimesNet')
    
    # 15. Baseline Strong LSTM
    def build_baseline_lstm(self):
        """ê°•ë ¥í•œ LSTM ë² ì´ìŠ¤ë¼ì¸"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        x = LSTM(256, return_sequences=True)(x)
        x = Dropout(0.3)(x)
        x = LSTM(128, return_sequences=True)(x)
        x = Dropout(0.3)(x)
        x = LSTM(64)(x)
        x = Dropout(0.2)(x)
        
        x = Dense(32, activation='relu')(x)
        x = Dropout(0.1)(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='BaselineLSTM')

# ========================================
# í‰ê°€ ì‹œìŠ¤í…œ
# ========================================

class ComprehensiveEvaluator:
    """ì „ì²´ êµ¬ê°„ í‰ê°€"""
    
    def __init__(self, target_scaler):
        self.target_scaler = target_scaler
        self.results = {}
    
    def evaluate(self, model, X_test, y_test, y_binary, model_name):
        """ì¢…í•© í‰ê°€"""
        
        # ì˜ˆì¸¡
        y_pred = model.predict(X_test, batch_size=256, verbose=0).flatten()
        
        # Clip predictions to valid range
        y_pred = np.clip(y_pred, 0, 1)
        
        # ì—­ë³€í™˜
        y_test_real = self.target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
        y_pred_real = self.target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()
        
        # ===== ì „ì²´ ì„±ëŠ¥ =====
        mae_total = np.mean(np.abs(y_test_real - y_pred_real))
        rmse_total = np.sqrt(np.mean((y_test_real - y_pred_real) ** 2))
        
        # ===== êµ¬ê°„ë³„ ë¶„ì„ =====
        metrics = {
            'total': {'mae': mae_total, 'rmse': rmse_total}
        }
        
        # êµ¬ê°„ ì •ì˜
        ranges = [
            ('low', 0, 1200),
            ('normal', 1200, 1400),
            ('medium', 1400, 1600),
            ('high', 1600, 1700),
            ('extreme', 1700, 9999)
        ]
        
        for name, low, high in ranges:
            mask = (y_test_real >= low) & (y_test_real < high)
            if mask.sum() > 0:
                mae = np.mean(np.abs(y_test_real[mask] - y_pred_real[mask]))
                rmse = np.sqrt(np.mean((y_test_real[mask] - y_pred_real[mask]) ** 2))
                count = mask.sum()
                metrics[name] = {
                    'mae': mae,
                    'rmse': rmse,
                    'count': count,
                    'percentage': (count / len(y_test_real)) * 100
                }
        
        # ===== 1700+ íŠ¹ë³„ ë¶„ì„ =====
        actual_1700 = y_test_real >= 1700
        pred_1700 = y_pred_real >= 1700
        
        tp = np.sum(pred_1700 & actual_1700)
        fp = np.sum(pred_1700 & ~actual_1700)
        fn = np.sum(~pred_1700 & actual_1700)
        tn = np.sum(~pred_1700 & ~actual_1700)
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        # ===== ì •ìƒ êµ¬ê°„ ì˜¤íƒ =====
        normal_mask = (y_test_real >= 1200) & (y_test_real < 1400)
        if normal_mask.sum() > 0:
            false_alarm = np.sum(y_pred_real[normal_mask] >= 1700)
            false_alarm_rate = false_alarm / normal_mask.sum()
        else:
            false_alarm_rate = 0
        
        # ê²°ê³¼ ì €ì¥
        result = {
            'metrics': metrics,
            'precision_1700': precision,
            'recall_1700': recall,
            'f1_1700': f1,
            'false_alarm_rate': false_alarm_rate,
            'confusion_matrix': {'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn}
        }
        
        self.results[model_name] = result
        
        # ì¶œë ¥
        self._print_results(model_name, result)
        
        return result
    
    def _print_results(self, model_name, result):
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š {model_name} í‰ê°€ ê²°ê³¼")
        print(f"{'='*60}")
        
        print("\n[ì „ì²´ ì„±ëŠ¥]")
        print(f"  MAE: {result['metrics']['total']['mae']:.2f}")
        print(f"  RMSE: {result['metrics']['total']['rmse']:.2f}")
        
        print("\n[êµ¬ê°„ë³„ ì„±ëŠ¥]")
        for range_name in ['low', 'normal', 'medium', 'high', 'extreme']:
            if range_name in result['metrics']:
                m = result['metrics'][range_name]
                print(f"  {range_name:8s}: MAE={m['mae']:6.2f}, "
                      f"Count={m['count']:4d} ({m['percentage']:5.2f}%)")
        
        print("\n[1700+ ì˜ˆì¸¡ ì„±ëŠ¥]")
        print(f"  Precision: {result['precision_1700']:.2%}")
        print(f"  Recall: {result['recall_1700']:.2%}")
        print(f"  F1 Score: {result['f1_1700']:.2%}")
        
        print(f"\n[ì •ìƒâ†’1700+ ì˜¤íƒë¥ ]")
        print(f"  False Alarm Rate: {result['false_alarm_rate']:.2%}")
        
        # ì¢…í•© ì ìˆ˜
        balanced_score = (
            result['f1_1700'] * 0.4 +  # 1700+ ì˜ˆì¸¡
            (1 - result['false_alarm_rate']) * 0.3 +  # ì˜¤íƒ ë°©ì§€
            (1 - result['metrics']['normal']['mae']/100) * 0.3  # ì •ìƒ êµ¬ê°„ ì •í™•ë„
        )
        print(f"\n[ì¢…í•© ì ìˆ˜]")
        print(f"  Balanced Score: {balanced_score:.2%}")

# ========================================
# ë©”ì¸ ì‹¤í–‰
# ========================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("\nğŸš€ ì‹œì‘...")
    
    # 1. ë°ì´í„° ì²˜ë¦¬
    processor = SafeDataProcessor(seq_len=100, pred_len=10)
    
    # ë°ì´í„° ê²½ë¡œ í™•ì¸
    data_paths = [
        'data/20240201_TO_202507281705.csv',
        '/mnt/user-data/uploads/gs.CSV',
        '/mnt/user-data/uploads/20250807_DATA.CSV'
    ]
    
    data_path = None
    for path in data_paths:
        if os.path.exists(path):
            data_path = path
            break
    
    if data_path is None:
        logger.error("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    df = processor.load_and_process(data_path)
    X, y, y_binary, feature_cols = processor.create_sequences(df)
    
    # 2. ë°ì´í„° ë¶„í• 
    train_size = int(0.7 * len(X))
    val_size = int(0.15 * len(X))
    
    X_train, y_train = X[:train_size], y[:train_size]
    X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]
    X_test = X[train_size+val_size:]
    y_test = y[train_size+val_size:]
    y_test_binary = y_binary[train_size+val_size:]
    
    print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
    print(f"  Train: {X_train.shape}")
    print(f"  Val: {X_val.shape}")
    print(f"  Test: {X_test.shape}")
    
    # 3. ëª¨ë¸ ìƒì„±
    model_builder = StableModels(input_shape=(100, X.shape[2]))
    
    # ëª¨ë“  15ê°œ ëª¨ë¸!
    models = {
        '1_PatchTST': model_builder.build_patch_tst(),
        '2_ExtremeValueNet': model_builder.build_extreme_value_network(),
        '3_SpikeDetector': model_builder.build_spike_detector(),
        '4_MambaSSM': model_builder.build_mamba_ssm(),
        '5_MTGNN': model_builder.build_mtgnn(),
        '6_NeuralSDE': model_builder.build_neural_sde(),
        '7_MixtureOfExperts': model_builder.build_mixture_of_experts(),
        '8_DiffusionModel': model_builder.build_diffusion_model(),
        '9_Autoformer': model_builder.build_autoformer(),
        '10_TCN': model_builder.build_tcn(),
        '11_WaveNet': model_builder.build_wavenet(),
        '12_BiLSTM_Attention': model_builder.build_bilstm_attention(),
        '13_Informer': model_builder.build_informer(),
        '14_TimesNet': model_builder.build_timesnet(),
        '15_BaselineLSTM': model_builder.build_baseline_lstm()
    }
    
    print(f"\nâœ… {len(models)}ê°œ ëª¨ë¸ ìƒì„± ì™„ë£Œ!")
    print("="*60)
    for name in models.keys():
        print(f"  âœ“ {name}")
    print("="*60)
    
    # 4. í‰ê°€ì ìƒì„±
    evaluator = ComprehensiveEvaluator(processor.target_scaler)
    
    # 5. ê° ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    for name, model in models.items():
        print(f"\n{'='*60}")
        print(f"ğŸ¯ {name} í•™ìŠµ")
        print(f"{'='*60}")
        
        # ì»´íŒŒì¼
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',  # ì•ˆì •ì ì¸ MSE ì‚¬ìš©
            metrics=['mae']
        )
        
        # í•™ìŠµ
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=20,
            batch_size=256,
            callbacks=[
                EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)
            ],
            verbose=1
        )
        
        # í‰ê°€
        evaluator.evaluate(model, X_test, y_test, y_test_binary, name)
    
    # 6. ìµœì¢… ìˆœìœ„
    print("\n" + "="*80)
    print("ğŸ† ìµœì¢… ìˆœìœ„")
    print("="*80)
    
    # F1 ê¸°ì¤€ ì •ë ¬
    sorted_models = sorted(
        evaluator.results.items(),
        key=lambda x: x[1]['f1_1700'],
        reverse=True
    )
    
    for i, (name, result) in enumerate(sorted_models, 1):
        print(f"{i}. {name:15s} | F1: {result['f1_1700']:.2%} | "
              f"ì •ìƒMAE: {result['metrics']['normal']['mae']:.2f} | "
              f"ì˜¤íƒë¥ : {result['false_alarm_rate']:.2%}")
    
    print("\nâœ… ì™„ë£Œ!")
    
    return evaluator.results

if __name__ == "__main__":
    results = main()