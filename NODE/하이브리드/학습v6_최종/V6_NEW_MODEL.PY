"""
pip install tensorflow-probability[tf]==0.24.0
ğŸš€ Ultimate 1700+ Predictor Suite v7.1
=====================================
TensorFlow 2.16.1 + tf-keras í˜¸í™˜ ë²„ì „
Python 3.11 ì§€ì›
"""

import numpy as np
import pandas as pd
import tensorflow as tf

# TensorFlow 2.16.1 keras í˜¸í™˜ì„± ì²˜ë¦¬
try:
    from tensorflow import keras
    from tensorflow.keras import layers, Model, Input
    from tensorflow.keras.layers import *
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import *
    print("Using tensorflow.keras")
except:
    try:
        import tf_keras as keras
        from tf_keras import layers, Model, Input
        from tf_keras.layers import *
        from tf_keras.optimizers import Adam
        from tf_keras.callbacks import *
        print("Using tf_keras")
    except:
        print("ERROR: Please install tf-keras: pip install tf-keras")
        exit(1)

# tensorflow_probabilityëŠ” ì„ íƒì‚¬í•­
try:
    import tensorflow_probability as tfp
    HAS_TFP = True
except ImportError:
    print("Warning: tensorflow_probability not found. Some models may be limited.")
    HAS_TFP = False

from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.model_selection import train_test_split
from scipy import stats
import warnings
import logging
from datetime import datetime
import os
import json
import pickle
import time

warnings.filterwarnings('ignore')

# ========================================
# í™˜ê²½ ì„¤ì •
# ========================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# GPU ì²´í¬
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    logger.info(f"ğŸ® GPU ì‚¬ìš© ê°€ëŠ¥: {len(gpus)}ê°œ")
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except:
        pass
else:
    logger.info("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")

# ëœë¤ ì‹œë“œ
SEED = 42
tf.random.set_seed(SEED)
np.random.seed(SEED)

print("\n" + "="*60)
print("âœ… TensorFlow 2.16.1 Ultimate 1700+ Predictor")
print(f"âœ… TensorFlow Version: {tf.__version__}")
print(f"âœ… Keras Backend: {keras.__name__}")
if HAS_TFP:
    print(f"âœ… TF-Probability: Available")
print("="*60 + "\n")

# ========================================
# ë°ì´í„° ì „ì²˜ë¦¬
# ========================================

class DataProcessor:
    """ë°ì´í„° ì „ì²˜ë¦¬"""
    
    def __init__(self, seq_len=100, pred_len=10):
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.scaler = RobustScaler()
        
    def load_and_process(self, filepath):
        """ë°ì´í„° ë¡œë“œ"""
        logger.info("ğŸ“‚ ë°ì´í„° ë¡œë”©...")
        
        # CSV ë¡œë“œ
        df = pd.read_csv(filepath)
        logger.info(f"âœ… ì›ë³¸ shape: {df.shape}")
        
        # ì‹œê°„ ë³€í™˜
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'], format='%Y%m%d%H%M')
        df['TIME'] = pd.to_datetime(df['TIME'], format='%Y%m%d%H%M')
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        # íƒ€ê²Ÿ ìƒì„±
        df['TARGET'] = df['TOTALCNT'].shift(-self.pred_len)
        df['IS_HIGH'] = (df['TARGET'] >= 1700).astype(int)
        
        # íŠ¹ì„± ìƒì„±
        df = self._create_features(df)
        df = df.dropna()
        
        logger.info(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {df.shape}")
        logger.info(f"âœ… 1700+ ë¹„ìœ¨: {df['IS_HIGH'].mean():.2%} ({df['IS_HIGH'].sum()}ê°œ)")
        
        return df
    
    def _create_features(self, df):
        """íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§"""
        # ì´ë™í‰ê· 
        for window in [10, 30, 60]:
            df[f'MA_{window}'] = df['TOTALCNT'].rolling(window, min_periods=1).mean()
            df[f'STD_{window}'] = df['TOTALCNT'].rolling(window, min_periods=1).std()
        
        # ë³€í™”ìœ¨
        df['CHANGE_10'] = df['TOTALCNT'].diff(10)
        df['PCT_CHANGE'] = df['TOTALCNT'].pct_change(10)
        
        # í•µì‹¬ ë¹„ìœ¨ (1700+ ì˜ˆì¸¡ì˜ í•µì‹¬!)
        df['RATIO_M14B_M10A'] = df['M14AM14B'] / (df['M14AM10A'] + 1e-6)
        df['RATIO_HIGH'] = (df['RATIO_M14B_M10A'] > 7).astype(int)
        
        # ì‹œê°„ íŠ¹ì„±
        df['HOUR'] = df['CURRTIME'].dt.hour
        df['IS_DAWN'] = df['HOUR'].isin([4, 5, 6]).astype(int)
        
        # ì„ê³„ê°’ ì‹ í˜¸
        df['M14B_HIGH'] = (df['M14AM14B'] > 350).astype(int)
        df['SPIKE_SIGNAL'] = ((df['M14B_HIGH'] == 1) & (df['IS_DAWN'] == 1)).astype(int)
        
        return df
    
    def create_sequences(self, df):
        """ì‹œí€€ìŠ¤ ìƒì„±"""
        logger.info("ğŸ”„ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        
        # íŠ¹ì„± ì„ íƒ
        feature_cols = [col for col in df.columns 
                       if col not in ['CURRTIME', 'TIME', 'TARGET', 'IS_HIGH']]
        
        # ìŠ¤ì¼€ì¼ë§
        df_scaled = df.copy()
        df_scaled[feature_cols] = self.scaler.fit_transform(df[feature_cols])
        df_scaled['TARGET_SCALED'] = self.scaler.fit_transform(df[['TARGET']])
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y, y_binary = [], [], []
        
        for i in range(len(df_scaled) - self.seq_len - self.pred_len):
            X.append(df_scaled[feature_cols].iloc[i:i+self.seq_len].values)
            y.append(df_scaled['TARGET_SCALED'].iloc[i+self.seq_len])
            y_binary.append(df['IS_HIGH'].iloc[i+self.seq_len])
        
        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.float32)
        y_binary = np.array(y_binary, dtype=np.float32)
        
        logger.info(f"âœ… X shape: {X.shape}, y shape: {y.shape}")
        
        return X, y, y_binary, feature_cols

# ========================================
# ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜
# ========================================

def weighted_mse_loss(y_true, y_pred, weight_high=100.0):
    """1700+ ê°€ì¤‘ MSE"""
    # ìŠ¤ì¼€ì¼ëœ 1700ì€ ì•½ 0.5
    weights = tf.where(y_true > 0.5, weight_high, 1.0)
    return tf.reduce_mean(weights * tf.square(y_true - y_pred))

def focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):
    """Focal Loss"""
    diff = tf.abs(y_true - y_pred)
    focal_weight = tf.pow(diff, gamma)
    return tf.reduce_mean(alpha * focal_weight * tf.square(diff))

# ========================================
# ëª¨ë¸ êµ¬í˜„
# ========================================

class ModelBuilder:
    """ëª¨ë¸ ë¹Œë”"""
    
    def __init__(self, input_shape):
        self.input_shape = input_shape
        self.seq_len = input_shape[0]
        self.n_features = input_shape[1]
    
    # 1. PatchTST - Transformer ê¸°ë°˜
    def build_patch_tst(self, patch_len=10, d_model=128, n_heads=4):
        """Patch Time Series Transformer"""
        inputs = Input(shape=self.input_shape)
        
        # íŒ¨ì¹˜ ìƒì„± (100 -> 10 patches of 10)
        n_patches = self.seq_len // patch_len
        x = tf.reshape(inputs, [-1, n_patches, patch_len * self.n_features])
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x = Dense(d_model)(x)
        x = LayerNormalization()(x)
        
        # Multi-Head Attention
        for _ in range(2):
            attn_out = MultiHeadAttention(num_heads=n_heads, key_dim=d_model//n_heads)(x, x)
            x = Add()([x, attn_out])
            x = LayerNormalization()(x)
            
            # FFN
            ff = Dense(d_model * 2, activation='relu')(x)
            ff = Dense(d_model)(ff)
            x = Add()([x, ff])
            x = LayerNormalization()(x)
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.2)(x)
        outputs = Dense(1)(x)
        
        return Model(inputs, outputs, name='PatchTST')
    
    # 2. Extreme Value Network
    def build_extreme_value_network(self):
        """ê·¹ë‹¨ê°’ íŠ¹í™” ë„¤íŠ¸ì›Œí¬"""
        inputs = Input(shape=self.input_shape)
        
        # LSTM backbone
        x = LSTM(128, return_sequences=True)(inputs)
        x = LSTM(64)(x)
        
        # ë‘ ê°œ ë¸Œëœì¹˜
        # 1. ì¼ë°˜ ì˜ˆì¸¡
        normal_branch = Dense(32, activation='relu')(x)
        normal_pred = Dense(1, name='normal')(normal_branch)
        
        # 2. ê·¹ë‹¨ê°’ ë³´ì •
        extreme_branch = Dense(32, activation='relu')(x)
        extreme_branch = Dense(16, activation='relu')(extreme_branch)
        extreme_factor = Dense(1, activation='sigmoid', name='extreme')(extreme_branch)
        
        # ê²°í•©: ê·¹ë‹¨ê°’ì¼ ë•Œ ì¦í­
        outputs = Add()([normal_pred, Multiply()([normal_pred, extreme_factor])])
        
        return Model(inputs, outputs, name='ExtremeValueNet')
    
    # 3. Enhanced Spike Detector
    def build_spike_detector(self):
        """ê¸‰ë³€ ê°ì§€ ëª¨ë¸"""
        inputs = Input(shape=self.input_shape)
        
        # CNN for pattern
        x = Conv1D(64, 3, padding='same', activation='relu')(inputs)
        x = Conv1D(64, 3, padding='same', activation='relu')(x)
        x = MaxPooling1D(2)(x)
        
        # Bi-LSTM
        x = Bidirectional(LSTM(64, return_sequences=True))(x)
        x = Bidirectional(LSTM(32))(x)
        
        # Spike detection
        x = Dense(64, activation='relu')(x)
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1)(x)
        
        return Model(inputs, outputs, name='SpikeDetector')
    
    # 4. Mamba-inspired SSM
    def build_mamba_ssm(self):
        """State Space Model"""
        inputs = Input(shape=self.input_shape)
        
        # Simplified SSM using LSTM/GRU
        x = GRU(128, return_sequences=True)(inputs)
        x = GRU(64, return_sequences=True)(x)
        x = GRU(32)(x)
        
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1)(x)
        
        return Model(inputs, outputs, name='MambaSSM')
    
    # 5. Graph Neural Network
    def build_graph_nn(self):
        """ì»¬ëŸ¼ ê°„ ê´€ê³„ í•™ìŠµ"""
        inputs = Input(shape=self.input_shape)
        
        # Feature interaction
        x = Dense(128, activation='relu')(inputs)
        x = Dense(64, activation='relu')(x)
        
        # Temporal
        x = LSTM(64, return_sequences=True)(x)
        x = LSTM(32)(x)
        
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1)(x)
        
        return Model(inputs, outputs, name='GraphNN')
    
    # 6. Mixture of Experts
    def build_mixture_of_experts(self):
        """ì „ë¬¸ê°€ ì•™ìƒë¸”"""
        inputs = Input(shape=self.input_shape)
        
        # Shared feature extraction
        shared = LSTM(64, return_sequences=True)(inputs)
        shared = LSTM(32)(shared)
        
        # Expert 1: Low range (< 1400)
        e1 = Dense(16, activation='relu')(shared)
        e1_out = Dense(1)(e1)
        
        # Expert 2: Mid range (1400-1600)
        e2 = Dense(16, activation='relu')(shared)
        e2_out = Dense(1)(e2)
        
        # Expert 3: High range (1700+)
        e3 = Dense(16, activation='relu')(shared)
        e3_out = Dense(1)(e3)
        
        # Gating network
        gate = Dense(16, activation='relu')(shared)
        gate = Dense(3, activation='softmax')(gate)
        
        # Weighted sum
        experts = tf.stack([e1_out, e2_out, e3_out], axis=1)
        outputs = tf.reduce_sum(experts * gate[:, :, None], axis=1)
        
        return Model(inputs, outputs, name='MixtureOfExperts')
    
    # 7. TCN (Temporal CNN)
    def build_tcn(self):
        """Temporal Convolutional Network"""
        inputs = Input(shape=self.input_shape)
        
        x = inputs
        for dilation in [1, 2, 4, 8]:
            conv = Conv1D(64, 3, dilation_rate=dilation, padding='causal', activation='relu')(x)
            conv = Dropout(0.1)(conv)
            if x.shape[-1] != conv.shape[-1]:
                x = Conv1D(64, 1)(x)
            x = Add()([x, conv])
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1)(x)
        
        return Model(inputs, outputs, name='TCN')
    
    # 8. Bi-LSTM with Attention
    def build_bilstm_attention(self):
        """ì–‘ë°©í–¥ LSTM + Attention"""
        inputs = Input(shape=self.input_shape)
        
        # Bi-LSTM
        x = Bidirectional(LSTM(128, return_sequences=True))(inputs)
        x = Bidirectional(LSTM(64, return_sequences=True))(x)
        
        # Attention
        attention = Dense(1, activation='tanh')(x)
        attention = Flatten()(attention)
        attention = Activation('softmax')(attention)
        attention = RepeatVector(128)(attention)
        attention = Permute([2, 1])(attention)
        
        x = Multiply()([x, attention])
        x = GlobalAveragePooling1D()(x)
        
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.2)(x)
        outputs = Dense(1)(x)
        
        return Model(inputs, outputs, name='BiLSTM_Attention')
    
    # 9. Autoformer
    def build_autoformer(self):
        """Auto-correlation Transformer"""
        inputs = Input(shape=self.input_shape)
        
        # Decomposition
        trend = AveragePooling1D(10, strides=1, padding='same')(inputs)
        seasonal = inputs - trend
        
        # Process
        trend_out = Conv1D(64, 1, activation='relu')(trend)
        trend_out = GlobalAveragePooling1D()(trend_out)
        
        seasonal_out = Conv1D(64, 1, activation='relu')(seasonal)
        seasonal_out = GlobalAveragePooling1D()(seasonal_out)
        
        # Combine
        x = Concatenate()([trend_out, seasonal_out])
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1)(x)
        
        return Model(inputs, outputs, name='Autoformer')
    
    # 10. Simple but Strong Baseline
    def build_baseline_lstm(self):
        """ê°•ë ¥í•œ LSTM ë² ì´ìŠ¤ë¼ì¸"""
        inputs = Input(shape=self.input_shape)
        
        x = LSTM(256, return_sequences=True)(inputs)
        x = Dropout(0.2)(x)
        x = LSTM(128, return_sequences=True)(x)
        x = Dropout(0.2)(x)
        x = LSTM(64)(x)
        x = Dropout(0.2)(x)
        
        x = Dense(32, activation='relu')(x)
        x = Dropout(0.1)(x)
        outputs = Dense(1)(x)
        
        return Model(inputs, outputs, name='Baseline_LSTM')

# ========================================
# í•™ìŠµ ë° í‰ê°€
# ========================================

class Trainer:
    """ëª¨ë¸ í•™ìŠµ"""
    
    def __init__(self, models_dict):
        self.models_dict = models_dict
        self.results = {}
    
    def train_model(self, model, X_train, y_train, X_val, y_val, 
                   model_name, epochs=30, batch_size=256):
        """ê°œë³„ ëª¨ë¸ í•™ìŠµ"""
        
        logger.info(f"\n{'='*50}")
        logger.info(f"ğŸš€ Training {model_name}")
        logger.info(f"{'='*50}")
        
        # ì»´íŒŒì¼
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss=weighted_mse_loss,
            metrics=['mae']
        )
        
        # ì½œë°±
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
        ]
        
        # í•™ìŠµ
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def evaluate_model(self, model, X_test, y_test, y_test_binary, model_name):
        """ëª¨ë¸ í‰ê°€"""
        
        # ì˜ˆì¸¡
        y_pred = model.predict(X_test, verbose=0).flatten()
        
        # MAE, RMSE
        mae = np.mean(np.abs(y_test - y_pred))
        rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
        
        # 1700+ ì„±ëŠ¥
        high_mask = y_test_binary == 1
        if high_mask.sum() > 0:
            mae_high = np.mean(np.abs(y_test[high_mask] - y_pred[high_mask]))
            
            # Precision/Recall
            threshold = 0.5  # ìŠ¤ì¼€ì¼ëœ 1700
            pred_high = (y_pred > threshold).astype(int)
            
            tp = np.sum((pred_high == 1) & (high_mask == 1))
            fp = np.sum((pred_high == 1) & (high_mask == 0)) 
            fn = np.sum((pred_high == 0) & (high_mask == 1))
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        else:
            mae_high = precision = recall = f1 = 0
        
        results = {
            'mae': mae,
            'rmse': rmse,
            'mae_1700+': mae_high,
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
        
        self.results[model_name] = results
        
        logger.info(f"\nğŸ“Š {model_name} Results:")
        logger.info(f"   MAE: {mae:.4f}")
        logger.info(f"   RMSE: {rmse:.4f}")
        logger.info(f"   1700+ MAE: {mae_high:.4f}")
        logger.info(f"   Precision: {precision:.2%}")
        logger.info(f"   Recall: {recall:.2%}")
        logger.info(f"   F1: {f1:.2%}")
        
        return results

# ========================================
# ë©”ì¸ ì‹¤í–‰
# ========================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("\n" + "="*80)
    print("ğŸš€ Ultimate 1700+ Predictor Suite v7.1")
    print("="*80 + "\n")
    
    # 1. ë°ì´í„° ë¡œë“œ
    processor = DataProcessor(seq_len=100, pred_len=10)
    
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ í™•ì¸
    data_path = 'data/20240201_TO_202507281705.csv'
    if not os.path.exists(data_path):
        # ì—…ë¡œë“œëœ íŒŒì¼ ì‚¬ìš©
        data_path = '/mnt/user-data/uploads/20250807_DATA.CSV'
        if not os.path.exists(data_path):
            data_path = '/mnt/user-data/uploads/gs.CSV'
    
    df = processor.load_and_process(data_path)
    
    # 2. ì‹œí€€ìŠ¤ ìƒì„±
    X, y, y_binary, feature_cols = processor.create_sequences(df)
    
    # 3. ë°ì´í„° ë¶„í• 
    train_size = int(0.7 * len(X))
    val_size = int(0.15 * len(X))
    
    X_train = X[:train_size]
    y_train = y[:train_size]
    
    X_val = X[train_size:train_size+val_size]
    y_val = y[train_size:train_size+val_size]
    
    X_test = X[train_size+val_size:]
    y_test = y[train_size+val_size:]
    y_test_binary = y_binary[train_size+val_size:]
    
    logger.info(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
    logger.info(f"   Train: {X_train.shape}")
    logger.info(f"   Val: {X_val.shape}")
    logger.info(f"   Test: {X_test.shape}")
    logger.info(f"   1700+ in test: {y_test_binary.sum()}ê°œ")
    
    # 4. ëª¨ë¸ ìƒì„±
    builder = ModelBuilder(input_shape=(100, X.shape[2]))
    
    models = {
        'PatchTST': builder.build_patch_tst(),
        'ExtremeValueNet': builder.build_extreme_value_network(),
        'SpikeDetector': builder.build_spike_detector(),
        'MambaSSM': builder.build_mamba_ssm(),
        'GraphNN': builder.build_graph_nn(),
        'MixtureOfExperts': builder.build_mixture_of_experts(),
        'TCN': builder.build_tcn(),
        'BiLSTM_Attention': builder.build_bilstm_attention(),
        'Autoformer': builder.build_autoformer(),
        'Baseline_LSTM': builder.build_baseline_lstm()
    }
    
    logger.info(f"\nâœ… {len(models)}ê°œ ëª¨ë¸ ìƒì„± ì™„ë£Œ\n")
    
    # 5. í•™ìŠµ ë° í‰ê°€
    trainer = Trainer(models)
    
    for name, model in models.items():
        try:
            # í•™ìŠµ
            history = trainer.train_model(
                model, X_train, y_train, X_val, y_val, 
                name, epochs=20, batch_size=256
            )
            
            # í‰ê°€
            trainer.evaluate_model(
                model, X_test, y_test, y_test_binary, name
            )
            
        except Exception as e:
            logger.error(f"âŒ {name} ì‹¤íŒ¨: {str(e)}")
            continue
    
    # 6. ê²°ê³¼ ìš”ì•½
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    # F1 ê¸°ì¤€ ì •ë ¬
    sorted_results = sorted(trainer.results.items(), 
                           key=lambda x: x[1]['f1'], 
                           reverse=True)
    
    print("\nğŸ† ëª¨ë¸ ìˆœìœ„ (F1 Score ê¸°ì¤€):")
    print("-"*60)
    for i, (name, metrics) in enumerate(sorted_results, 1):
        print(f"{i:2d}. {name:20s} | F1: {metrics['f1']:.2%} | "
              f"MAE: {metrics['mae']:.4f} | "
              f"Recall: {metrics['recall']:.2%}")
    
    # ìµœê³  ëª¨ë¸
    best_model = sorted_results[0][0]
    best_metrics = sorted_results[0][1]
    
    print("\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸:", best_model)
    print(f"   - F1 Score: {best_metrics['f1']:.2%}")
    print(f"   - Precision: {best_metrics['precision']:.2%}")
    print(f"   - Recall: {best_metrics['recall']:.2%}")
    print(f"   - 1700+ MAE: {best_metrics['mae_1700+']:.4f}")
    
    # ê²°ê³¼ ì €ì¥
    with open('results_v7.json', 'w') as f:
        json.dump({
            'results': trainer.results,
            'best_model': best_model,
            'timestamp': datetime.now().isoformat()
        }, f, indent=4)
    
    print("\nâœ… ì™„ë£Œ! ê²°ê³¼ ì €ì¥: results_v7.json")
    
    return trainer.results

if __name__ == "__main__":
    results = main()