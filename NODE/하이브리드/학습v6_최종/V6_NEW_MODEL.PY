"""
ğŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ - 6ê°œ ëª¨ë¸ ìµœì¢… ê°œì„ íŒ
=========================================
PatchTST, EnsembleBase ë¬¸ì œ í•´ê²°
ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ë° ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì§€ì›
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import *
from tensorflow.keras.losses import Huber
from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score
import warnings
import os
import gc
import pickle
import json

warnings.filterwarnings('ignore')
tf.random.set_seed(42)
np.random.seed(42)

# GPU ì„¤ì •
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    print(f"ğŸ”§ GPU {len(gpus)}ê°œ ë°œê²¬")
    for i, gpu in enumerate(gpus):
        tf.config.experimental.set_memory_growth(gpu, True)
        print(f"  GPU:{i} í™œì„±í™”")
    strategy = tf.distribute.MirroredStrategy()
    device = f'GPU x {len(gpus)}ê°œ'
else:
    print("âš ï¸ GPU ì—†ìŒ, CPU ì‚¬ìš©")
    strategy = tf.distribute.get_strategy()
    device = 'CPU'

print("="*80)
print("ğŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ v2.0")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print(f"ğŸ”§ Device: {device}")
print("="*80)

# ========================================
# ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤
# ========================================
class DataProcessor:
    def __init__(self, seq_len=100, pred_len=10):
        self.seq_len = seq_len
        self.pred_len = pred_len
        # ì—¬ëŸ¬ ìŠ¤ì¼€ì¼ëŸ¬ í…ŒìŠ¤íŠ¸
        self.feature_scaler = StandardScaler()  # íŠ¹ì§•ìš©
        self.target_scaler = RobustScaler()     # íƒ€ê²Ÿìš© (ì´ìƒì¹˜ì— ê°•ê±´)
        self.feature_columns = None
        
    def save_scalers(self, path='scalers/'):
        """ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥"""
        os.makedirs(path, exist_ok=True)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        with open(f'{path}feature_scaler.pkl', 'wb') as f:
            pickle.dump(self.feature_scaler, f)
        with open(f'{path}target_scaler.pkl', 'wb') as f:
            pickle.dump(self.target_scaler, f)
            
        # ì„¤ì • ì €ì¥
        config = {
            'seq_len': self.seq_len,
            'pred_len': self.pred_len,
            'feature_columns': self.feature_columns
        }
        with open(f'{path}config.json', 'w') as f:
            json.dump(config, f)
            
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {path}")
    
    def load_scalers(self, path='scalers/'):
        """ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        with open(f'{path}feature_scaler.pkl', 'rb') as f:
            self.feature_scaler = pickle.load(f)
        with open(f'{path}target_scaler.pkl', 'rb') as f:
            self.target_scaler = pickle.load(f)
        with open(f'{path}config.json', 'r') as f:
            config = json.load(f)
            self.seq_len = config['seq_len']
            self.pred_len = config['pred_len']
            self.feature_columns = config['feature_columns']
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ: {path}")
        
    def load_and_process(self, filepath):
        print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {filepath}")
        df = pd.read_csv(filepath)
        print(f"âœ… ì›ë³¸: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")
        
        # 0ê°’ ì œê±°
        before = len(df)
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        print(f"âœ… 0ê°’ ì œê±°: {before - len(df):,}ê°œ")
        
        # ì‹œê°„ ì •ë ¬
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                       format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        # í†µê³„
        print(f"\nğŸ“Š TOTALCNT í†µê³„:")
        print(f"  í‰ê· : {df['TOTALCNT'].mean():,.0f}")
        print(f"  í‘œì¤€í¸ì°¨: {df['TOTALCNT'].std():,.0f}")
        print(f"  ìµœì†Œ: {df['TOTALCNT'].min():,}")
        print(f"  ìµœëŒ€: {df['TOTALCNT'].max():,}")
        
        # êµ¬ê°„ ë¶„í¬
        normal = (df['TOTALCNT'] < 1400).sum()
        warning = ((df['TOTALCNT'] >= 1400) & (df['TOTALCNT'] < 1700)).sum()
        critical = (df['TOTALCNT'] >= 1700).sum()
        
        print(f"\nğŸ“Š êµ¬ê°„ ë¶„í¬:")
        print(f"  ì •ìƒ: {normal:,}ê°œ ({normal/len(df)*100:.1f}%)")
        print(f"  ì£¼ì˜: {warning:,}ê°œ ({warning/len(df)*100:.1f}%)")
        print(f"  ì‹¬ê°: {critical:,}ê°œ ({critical/len(df)*100:.1f}%)")
        
        return df
    
    def create_features(self, df):
        print("\nâš™ï¸ íŠ¹ì„± ìƒì„±...")
        
        # ê¸°ë³¸ íŠ¹ì„±
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        
        # ì‹œê°„
        df['HOUR'] = df['CURRTIME'].dt.hour
        df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
        df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
        
        # ì´ë™í‰ê· 
        for w in [10, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
        
        # ë³€í™”ìœ¨
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        
        # íƒ€ê²Ÿ
        df['TARGET'] = df['TOTALCNT'].shift(-self.pred_len)
        df['LEVEL'] = 0
        df.loc[df['TARGET'] >= 1400, 'LEVEL'] = 1
        df.loc[df['TARGET'] >= 1700, 'LEVEL'] = 2
        
        df = df.dropna()
        print(f"âœ… ìœ íš¨ ë°ì´í„°: {len(df)}í–‰")
        
        return df
    
    def create_sequences(self, df):
        print("\nğŸ”„ ì‹œí€€ìŠ¤ ìƒì„±...")
        
        # íŠ¹ì„± ì„ íƒ
        self.feature_columns = [
            'TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M14AM10A', 'M14AM16',
            'RATIO', 'GOLDEN',
            'HOUR_SIN', 'HOUR_COS',
            'MA_10', 'MA_30', 'STD_10', 'STD_30',
            'CHANGE_1', 'CHANGE_10'
        ]
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ
        self.feature_columns = [f for f in self.feature_columns if f in df.columns]
        print(f"  íŠ¹ì„±: {len(self.feature_columns)}ê°œ")
        
        X_data = df[self.feature_columns].values.astype(np.float32)
        y_reg = df['TARGET'].values.astype(np.float32)
        y_cls = df['LEVEL'].values.astype(np.int32)
        
        # ë¶„í• 
        total_samples = len(X_data) - self.seq_len
        train_end = int(total_samples * 0.7)
        val_end = int(total_samples * 0.85)
        
        # ìŠ¤ì¼€ì¼ë§ (í›ˆë ¨ ë°ì´í„°ë¡œë§Œ)
        print("  ìŠ¤ì¼€ì¼ë§...")
        self.feature_scaler.fit(X_data[:train_end + self.seq_len])
        self.target_scaler.fit(y_reg[:train_end].reshape(-1, 1))
        
        # ë³€í™˜
        X_scaled = self.feature_scaler.transform(X_data)
        y_reg_scaled = self.target_scaler.transform(y_reg.reshape(-1, 1)).flatten()
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y_r, y_c = [], [], []
        for i in range(total_samples):
            X.append(X_scaled[i:i+self.seq_len])
            y_r.append(y_reg_scaled[i+self.seq_len-1])
            y_c.append(y_cls[i+self.seq_len-1])
        
        X = np.array(X, dtype=np.float32)
        y_r = np.array(y_r, dtype=np.float32)
        y_c = np.array(y_c, dtype=np.int32)
        
        # ì‹œê³„ì—´ ìˆœì„œ ë¶„í• 
        X_train = X[:train_end]
        X_val = X[train_end:val_end]
        X_test = X[val_end:]
        
        y_train = (y_r[:train_end], y_c[:train_end])
        y_val = (y_r[train_end:val_end], y_c[train_end:val_end])
        y_test = (y_r[val_end:], y_c[val_end:])
        
        print(f"  Train: {X_train.shape}")
        print(f"  Val: {X_val.shape}")
        print(f"  Test: {X_test.shape}")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        self.save_scalers()
        
        return (X_train, y_train), (X_val, y_val), (X_test, y_test)

# ========================================
# 6ê°œ ëª¨ë¸ (ê°œì„ ë¨)
# ========================================

def build_stable_lstm(input_shape):
    """1. StableLSTM"""
    inputs = Input(shape=input_shape)
    x = LSTM(64, return_sequences=True)(inputs)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    x = LSTM(32)(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    x = Dense(32, activation='relu')(x)
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    return Model(inputs, [out_reg, out_cls], name='StableLSTM')

def build_patch_tst(input_shape):
    """2. PatchTST - ì™„ì „ ì¬ì„¤ê³„"""
    inputs = Input(shape=input_shape)
    
    # GRUë¡œ ì‹œì‘ (ì‹œê³„ì—´ì— ê°•í•¨)
    x = GRU(64, return_sequences=True)(inputs)
    x = BatchNormalization()(x)
    
    # ê°€ë²¼ìš´ Attention
    attn = MultiHeadAttention(num_heads=2, key_dim=16)(x, x)
    x = Add()([x, attn])
    x = LayerNormalization()(x)
    
    # ìµœì¢… GRU
    x = GRU(32)(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    
    return Model(inputs, [out_reg, out_cls], name='PatchTST')

def build_spike_detector(input_shape):
    """3. SpikeDetector"""
    inputs = Input(shape=input_shape)
    recent = Lambda(lambda x: x[:, -20:, :])(inputs)
    x = Conv1D(32, 3, activation='relu', padding='same')(recent)
    x = MaxPooling1D(2)(x)
    x = GRU(32)(x)
    x = Dropout(0.2)(x)
    x = Dense(32, activation='relu')(x)
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    return Model(inputs, [out_reg, out_cls], name='SpikeDetector')

def build_extreme_net(input_shape):
    """4. ExtremeNet"""
    inputs = Input(shape=input_shape)
    lstm = LSTM(48, return_sequences=True)(inputs)
    lstm = LSTM(24)(lstm)
    attn = MultiHeadAttention(num_heads=4, key_dim=12)(inputs, inputs)
    attn = GlobalAveragePooling1D()(attn)
    x = Concatenate()([lstm, attn])
    x = BatchNormalization()(x)
    x = Dense(48, activation='relu')(x)
    x = Dropout(0.2)(x)
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    return Model(inputs, [out_reg, out_cls], name='ExtremeNet')

def build_ensemble_base(input_shape):
    """5. EnsembleBase - ì¬ì„¤ê³„"""
    inputs = Input(shape=input_shape)
    
    # Simple LSTM branch
    lstm_branch = LSTM(32)(inputs)
    
    # Simple Conv branch  
    conv_branch = Conv1D(32, 5, activation='relu')(inputs)
    conv_branch = GlobalAveragePooling1D()(conv_branch)
    
    # Statistics
    mean_pool = Lambda(lambda x: tf.reduce_mean(x, axis=1))(inputs)
    
    # Concatenate
    x = Concatenate()([lstm_branch, conv_branch, mean_pool])
    x = BatchNormalization()(x)
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    
    return Model(inputs, [out_reg, out_cls], name='EnsembleBase')

def build_golden_rule(input_shape):
    """6. GoldenRule"""
    inputs = Input(shape=input_shape)
    last = Lambda(lambda x: x[:, -10:, :])(inputs)
    x = Conv1D(32, 3, activation='relu')(last)
    x = GlobalAveragePooling1D()(x)
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.2)(x)
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    return Model(inputs, [out_reg, out_cls], name='GoldenRule')

# ========================================
# í•™ìŠµ í•¨ìˆ˜
# ========================================
def train_and_evaluate(model, train_data, val_data, test_data, processor):
    X_train, y_train = train_data
    X_val, y_val = val_data
    X_test, y_test = test_data
    
    with strategy.scope():
        # ëª¨ë¸ ì¬ìƒì„±
        input_shape = model.input_shape[1:]
        
        if model.name == 'StableLSTM':
            model = build_stable_lstm(input_shape)
        elif model.name == 'PatchTST':
            model = build_patch_tst(input_shape)
        elif model.name == 'SpikeDetector':
            model = build_spike_detector(input_shape)
        elif model.name == 'ExtremeNet':
            model = build_extreme_net(input_shape)
        elif model.name == 'EnsembleBase':
            model = build_ensemble_base(input_shape)
        elif model.name == 'GoldenRule':
            model = build_golden_rule(input_shape)
        
        model.compile(
            optimizer=Adam(0.001),
            loss=['mae', 'sparse_categorical_crossentropy'],
            loss_weights=[0.7, 0.3],
            metrics={'regression': 'mae', 'classification': 'accuracy'}
        )
    
    print(f"\n{'='*60}")
    print(f"ğŸ¯ {model.name} í•™ìŠµ")
    print(f"{'='*60}")
    
    callbacks = [
        EarlyStopping(patience=10, restore_best_weights=True, verbose=0),
        ReduceLROnPlateau(factor=0.5, patience=5, verbose=0)
    ]
    
    batch_size = 64 * strategy.num_replicas_in_sync if gpus else 32
    
    history = model.fit(
        X_train,
        {'regression': y_train[0], 'classification': y_train[1]},
        validation_data=(X_val, {'regression': y_val[0], 'classification': y_val[1]}),
        epochs=30,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=1
    )
    
    # í‰ê°€
    print(f"\nğŸ“Š {model.name} í‰ê°€:")
    
    preds = model.predict(X_test, verbose=0)
    y_reg_pred = preds[0].flatten()
    y_cls_pred = np.argmax(preds[1], axis=1)
    
    # ì—­ë³€í™˜
    y_reg_true_orig = processor.target_scaler.inverse_transform(
        y_test[0].reshape(-1, 1)).flatten()
    y_reg_pred_orig = processor.target_scaler.inverse_transform(
        y_reg_pred.reshape(-1, 1)).flatten()
    
    mae = mean_absolute_error(y_reg_true_orig, y_reg_pred_orig)
    rmse = np.sqrt(mean_squared_error(y_reg_true_orig, y_reg_pred_orig))
    r2 = r2_score(y_reg_true_orig, y_reg_pred_orig)
    acc = accuracy_score(y_test[1], y_cls_pred)
    
    print(f"  MAE: {mae:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  RÂ²: {r2:.4f}")
    print(f"  ì •í™•ë„: {acc:.2%}")
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs('models', exist_ok=True)
    model.save(f'models/{model.name}.keras')
    print(f"ğŸ’¾ ëª¨ë¸: models/{model.name}.keras")
    
    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'Accuracy': acc}

# ========================================
# ì‹¤ì‹œê°„ ì˜ˆì¸¡ í´ë˜ìŠ¤
# ========================================
class RealtimePredictor:
    def __init__(self, model_path, scaler_path='scalers/'):
        """ì‹¤ì‹œê°„ ì˜ˆì¸¡ê¸° ì´ˆê¸°í™”"""
        self.model = tf.keras.models.load_model(model_path)
        self.processor = DataProcessor()
        self.processor.load_scalers(scaler_path)
        print(f"âœ… ì‹¤ì‹œê°„ ì˜ˆì¸¡ê¸° ì¤€ë¹„ ì™„ë£Œ")
        
    def predict(self, data_100min):
        """100ë¶„ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡"""
        # ë°ì´í„° ê²€ì¦
        if len(data_100min) != 100:
            raise ValueError(f"100ë¶„ ë°ì´í„° í•„ìš” (í˜„ì¬: {len(data_100min)})")
        
        # íŠ¹ì„± ì¶”ì¶œ
        X = data_100min[self.processor.feature_columns].values
        
        # ìŠ¤ì¼€ì¼ë§
        X_scaled = self.processor.feature_scaler.transform(X)
        X_seq = X_scaled.reshape(1, 100, -1)
        
        # ì˜ˆì¸¡
        preds = self.model.predict(X_seq, verbose=0)
        y_reg_scaled = preds[0][0, 0]
        y_cls = np.argmax(preds[1][0])
        
        # ì—­ë³€í™˜
        y_reg = self.processor.target_scaler.inverse_transform([[y_reg_scaled]])[0, 0]
        
        # ë ˆë²¨
        level = ['ì •ìƒ', 'ì£¼ì˜', 'ì‹¬ê°'][y_cls]
        
        return {
            'prediction': float(y_reg),
            'level': level,
            'confidence': float(np.max(preds[1][0]))
        }

# ========================================
# ë©”ì¸ ì‹¤í–‰
# ========================================
def main():
    # ë°ì´í„° ì°¾ê¸°
    data_paths = [
        'data/20240201_TO_202507281705.csv',
        'data/20250731_to20250806.csv',
        '/mnt/user-data/uploads/gs.CSV',
        'data.csv'
    ]
    
    data_path = None
    for path in data_paths:
        if os.path.exists(path):
            data_path = path
            print(f"âœ… ë°ì´í„°: {path}")
            break
    
    if not data_path:
        print("âŒ ë°ì´í„° ì—†ìŒ")
        return
    
    # ë°ì´í„° ì²˜ë¦¬
    processor = DataProcessor()
    df = processor.load_and_process(data_path)
    df = processor.create_features(df)
    train_data, val_data, test_data = processor.create_sequences(df)
    
    # 6ê°œ ëª¨ë¸
    input_shape = (train_data[0].shape[1], train_data[0].shape[2])
    
    models = {
        'StableLSTM': build_stable_lstm(input_shape),
        'PatchTST': build_patch_tst(input_shape),
        'SpikeDetector': build_spike_detector(input_shape),
        'ExtremeNet': build_extreme_net(input_shape),
        'EnsembleBase': build_ensemble_base(input_shape),
        'GoldenRule': build_golden_rule(input_shape)
    }
    
    print(f"\nâœ… 6ê°œ ëª¨ë¸ ì¤€ë¹„")
    
    # í•™ìŠµ
    results = {}
    for name, model in models.items():
        try:
            results[name] = train_and_evaluate(
                model, train_data, val_data, test_data, processor
            )
        except Exception as e:
            print(f"âŒ {name} ì‹¤íŒ¨: {e}")
    
    # ê²°ê³¼
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… ì„±ëŠ¥")
    print("="*80)
    
    results_df = pd.DataFrame(results).T
    results_df = results_df.sort_values('R2', ascending=False)
    print(results_df.round(3))
    
    print("\nğŸ† ìµœê³  ì„±ëŠ¥:")
    print(f"  MAE: {results_df['MAE'].idxmin()} ({results_df['MAE'].min():.2f})")
    print(f"  RÂ²: {results_df['R2'].idxmax()} ({results_df['R2'].max():.4f})")
    
    results_df.to_csv('final_results.csv')
    print("\nğŸ’¾ ê²°ê³¼: final_results.csv")
    
    # ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    print("\n" + "="*80)
    print("ğŸ”® ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    best_model = results_df['R2'].idxmax()
    predictor = RealtimePredictor(f'models/{best_model}.keras')
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡
    test_sample = df.iloc[-100:].copy()
    result = predictor.predict(test_sample)
    
    print(f"\nì˜ˆì¸¡ ê²°ê³¼:")
    print(f"  10ë¶„ í›„ ì˜ˆì¸¡: {result['prediction']:.0f}")
    print(f"  ë ˆë²¨: {result['level']}")
    print(f"  ì‹ ë¢°ë„: {result['confidence']:.2%}")
    
    print("\nâœ… ì™„ë£Œ!")
    
    return results

if __name__ == "__main__":
    results = main()