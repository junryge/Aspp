"""
ğŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ TOTALCNT ì˜ˆì¸¡ ì‹œìŠ¤í…œ - Keras 3.0 í˜¸í™˜ ë²„ì „
==========================================================
TensorFlow 2.16 + Keras 3.0 ì™„ë²½ í˜¸í™˜

ì˜ˆì¸¡ êµ¬ê°„:
- ì •ìƒ: 0-1399 (56.53%)
- ì£¼ì˜: 1400-1699 (43.12%)  
- ì‹¬ê°: 1700+ (0.35%) â† í•µì‹¬ ëª©í‘œ

í‰ê°€ ì§€í‘œ:
- MAE, RMSE, RÂ² Score, ì •í™•ë„
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import *
from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)
import warnings
import os
import json
import pickle
from datetime import datetime

warnings.filterwarnings('ignore')

# ì„¤ì •
tf.random.set_seed(42)
np.random.seed(42)

# GPU ì„¤ì •
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"âœ… GPU ì‚¬ìš©: {len(gpus)}ê°œ")
    except RuntimeError as e:
        print(f"GPU ì„¤ì • ì˜¤ë¥˜: {e}")

print("="*80)
print("ğŸš€ ë°˜ë„ì²´ ë¬¼ë¥˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ v15.0 - Keras 3.0 Compatible")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print(f"ğŸ”§ Device: {'GPU' if gpus else 'CPU'}")
print(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*80)

# =========================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
# =========================================

class CheckpointManager:
    """í•™ìŠµ ì¤‘ë‹¨/ì¬ê°œ ê´€ë¦¬"""
    
    def __init__(self, checkpoint_dir='./checkpoints'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        
    def save_training_state(self, epoch, model_name, history, data_info):
        """í•™ìŠµ ìƒíƒœ ì €ì¥"""
        state = {
            'epoch': epoch,
            'model_name': model_name,
            'history': history,
            'data_info': data_info,
            'timestamp': datetime.now().isoformat()
        }
        
        filepath = os.path.join(self.checkpoint_dir, f'{model_name}_state.pkl')
        with open(filepath, 'wb') as f:
            pickle.dump(state, f)
        print(f"ğŸ’¾ í•™ìŠµ ìƒíƒœ ì €ì¥: {filepath}")
        
    def load_training_state(self, model_name):
        """í•™ìŠµ ìƒíƒœ ë¡œë“œ"""
        filepath = os.path.join(self.checkpoint_dir, f'{model_name}_state.pkl')
        
        if os.path.exists(filepath):
            with open(filepath, 'rb') as f:
                state = pickle.load(f)
            print(f"ğŸ“‚ í•™ìŠµ ìƒíƒœ ë¡œë“œ: {filepath}")
            return state
        return None
    
    def check_existing_model(self, model_name):
        """ê¸°ì¡´ ëª¨ë¸ ì²´í¬"""
        model_path = os.path.join(self.checkpoint_dir, f'{model_name}_model.keras')
        if os.path.exists(model_path):
            return model_path
        return None

# =========================================
# ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤
# =========================================

class DataProcessor:
    """í™©ê¸ˆ íŒ¨í„´ ê¸°ë°˜ ë°ì´í„° ì²˜ë¦¬"""
    
    def __init__(self, seq_len=100, pred_len=10):
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.feature_scaler = RobustScaler()
        self.target_scaler = MinMaxScaler()
        self.scaler_fitted = False
        
    def save_scalers(self, filepath='scalers.pkl'):
        """ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'feature_scaler': self.feature_scaler,
                'target_scaler': self.target_scaler,
                'scaler_fitted': self.scaler_fitted
            }, f)
        print(f"ğŸ’¾ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {filepath}")
    
    def load_scalers(self, filepath='scalers.pkl'):
        """ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        if os.path.exists(filepath):
            with open(filepath, 'rb') as f:
                scalers = pickle.load(f)
                self.feature_scaler = scalers['feature_scaler']
                self.target_scaler = scalers['target_scaler']
                self.scaler_fitted = scalers['scaler_fitted']
            print(f"ğŸ“‚ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ: {filepath}")
            return True
        return False
    
    def load_data(self, filepath):
        """ë°ì´í„° ë¡œë“œ"""
        print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {filepath}")
        
        df = pd.read_csv(filepath)
        print(f"âœ… ë°ì´í„° í¬ê¸°: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")
        
        # ì‹œê°„ ë³€í™˜
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                       format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        # í†µê³„
        print(f"\nğŸ“Š TOTALCNT ë¶„í¬:")
        print(f"  ìµœì†Œ: {df['TOTALCNT'].min():,}")
        print(f"  í‰ê· : {df['TOTALCNT'].mean():,.0f} Â± {df['TOTALCNT'].std():.0f}")
        print(f"  ìµœëŒ€: {df['TOTALCNT'].max():,}")
        
        # êµ¬ê°„ë³„ ë¶„í¬
        normal_cnt = (df['TOTALCNT'] < 1400).sum()
        warning_cnt = ((df['TOTALCNT'] >= 1400) & (df['TOTALCNT'] < 1700)).sum()
        critical_cnt = (df['TOTALCNT'] >= 1700).sum()
        
        print(f"\nğŸ“Š êµ¬ê°„ë³„ ë¶„í¬:")
        print(f"  ì •ìƒ(0-1399): {normal_cnt:,}ê°œ ({normal_cnt/len(df)*100:.2f}%)")
        print(f"  ì£¼ì˜(1400-1699): {warning_cnt:,}ê°œ ({warning_cnt/len(df)*100:.2f}%)")
        print(f"  ì‹¬ê°(1700+): {critical_cnt:,}ê°œ ({critical_cnt/len(df)*100:.2f}%)")
        
        return df
    
    def create_features(self, df):
        """íŠ¹ì„± ìƒì„±"""
        print("\nâš™ï¸ í™©ê¸ˆ íŒ¨í„´ íŠ¹ì„± ìƒì„±...")
        
        # 1. í•µì‹¬ ë¹„ìœ¨
        df['RATIO_M14B_M14A'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        
        # 2. í™©ê¸ˆ íŒ¨í„´
        df['GOLDEN_1'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(int)
        df['GOLDEN_2'] = (df['M14AM14BSUM'] > 500).astype(int)
        df['GOLDEN_3'] = (df['RATIO_M14B_M14A'] > 5).astype(int)
        
        # 3. M14AM14B êµ¬ê°„
        bins = [0, 200, 250, 300, 350, 400, 450, 999999]
        df['M14B_RANGE'] = pd.cut(df['M14AM14B'], bins=bins, labels=False)
        df['M14B_RANGE'] = df['M14B_RANGE'].fillna(0).astype(int)
        
        # 4. ì‹œê°„ íŠ¹ì„±
        df['HOUR'] = df['CURRTIME'].dt.hour
        df['DAYOFWEEK'] = df['CURRTIME'].dt.dayofweek
        df['IS_PEAK'] = df['HOUR'].isin([3, 4, 5, 8]).astype(int)
        df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
        df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
        
        # 5. ì´ë™ í†µê³„
        for w in [10, 30, 60]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
            
        # 6. ë³€í™”ìœ¨
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        df['PCT_CHANGE'] = df['TOTALCNT'].pct_change().fillna(0).clip(-1, 1)
        
        # 7. íƒ€ê²Ÿ ìƒì„±
        df['TARGET'] = df['TOTALCNT'].shift(-self.pred_len)
        
        # 8. êµ¬ê°„ ë ˆì´ë¸”
        def classify_level(x):
            if pd.isna(x):
                return np.nan
            elif x < 1400:
                return 0  # ì •ìƒ
            elif x < 1700:
                return 1  # ì£¼ì˜
            else:
                return 2  # ì‹¬ê°
        
        df['LEVEL'] = df['TARGET'].apply(classify_level)
        
        print(f"âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(df.columns)}ê°œ ì»¬ëŸ¼")
        
        return df
    
    def prepare_sequences(self, df):
        """ì‹œí€€ìŠ¤ ìƒì„±"""
        print("\nğŸ”„ ì‹œí€€ìŠ¤ ìƒì„±...")
        
        # NaN ì œê±°
        df = df.dropna()
        
        # íŠ¹ì„± ì„ íƒ
        feature_cols = [
            'TOTALCNT', 'M14AM14B', 'M14AM14BSUM', 'M14AM10A', 'M14AM16',
            'M14AM16SUM', 'M10AM14A', 'M14BM14A', 'M16M14A', 'M14AM10ASUM',
            'RATIO_M14B_M14A', 'GOLDEN_1', 'GOLDEN_2', 'GOLDEN_3',
            'M14B_RANGE', 'IS_PEAK', 'HOUR_SIN', 'HOUR_COS',
            'MA_10', 'MA_30', 'MA_60', 'STD_10', 'STD_30', 'STD_60',
            'CHANGE_1', 'CHANGE_10', 'PCT_CHANGE'
        ]
        
        available_cols = [col for col in feature_cols if col in df.columns]
        print(f"  ì‚¬ìš© íŠ¹ì„±: {len(available_cols)}ê°œ")
        
        # ë°ì´í„° ì¤€ë¹„
        X_data = df[available_cols].values
        y_reg = df['TARGET'].values
        y_cls = df['LEVEL'].values
        
        # ìŠ¤ì¼€ì¼ë§
        if not self.scaler_fitted:
            X_scaled = self.feature_scaler.fit_transform(X_data)
            y_reg_scaled = self.target_scaler.fit_transform(y_reg.reshape(-1, 1)).flatten()
            self.scaler_fitted = True
            self.save_scalers()
        else:
            X_scaled = self.feature_scaler.transform(X_data)
            y_reg_scaled = self.target_scaler.transform(y_reg.reshape(-1, 1)).flatten()
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y_regression, y_classification = [], [], []
        
        for i in range(len(X_scaled) - self.seq_len):
            X.append(X_scaled[i:i+self.seq_len])
            y_regression.append(y_reg_scaled[i+self.seq_len-1])
            y_classification.append(int(y_cls[i+self.seq_len-1]))
        
        X = np.array(X, dtype=np.float32)
        y_regression = np.array(y_regression, dtype=np.float32)
        y_classification = np.array(y_classification, dtype=np.int32)
        
        print(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {X.shape}")
        
        return X, y_regression, y_classification, available_cols

# =========================================
# ëª¨ë¸ ì •ì˜ (Keras 3.0 í˜¸í™˜)
# =========================================

class PatchEmbedding(Layer):
    """íŒ¨ì¹˜ ì„ë² ë”© ë ˆì´ì–´"""
    def __init__(self, n_patches, patch_len, n_features, d_model=128):
        super().__init__()
        self.n_patches = n_patches
        self.patch_len = patch_len
        self.n_features = n_features
        self.d_model = d_model
        self.linear = Dense(d_model)
        
    def call(self, x):
        # (batch, seq_len, features) -> (batch, n_patches, patch_len*features)
        batch_size = tf.shape(x)[0]
        x = tf.reshape(x, [batch_size, self.n_patches, self.patch_len * self.n_features])
        return self.linear(x)

def build_patch_tst(input_shape, patch_len=10):
    """PatchTST - Keras 3.0 í˜¸í™˜"""
    inputs = Input(shape=input_shape)
    seq_len, n_features = input_shape
    n_patches = seq_len // patch_len
    
    # íŒ¨ì¹˜ ì„ë² ë”©
    patch_embed = PatchEmbedding(n_patches, patch_len, n_features, 128)
    x = patch_embed(inputs)
    
    # Positional Encoding
    positions = Embedding(n_patches, 128)(tf.range(n_patches))
    x = x + positions[tf.newaxis, :, :]
    
    # Transformer Blocks
    for _ in range(3):
        # Multi-Head Attention
        attn_output = MultiHeadAttention(
            num_heads=8, 
            key_dim=16,
            dropout=0.1
        )(x, x)
        x = LayerNormalization()(x + attn_output)
        
        # Feed Forward
        ffn_output = Dense(512, activation='gelu')(x)
        ffn_output = Dropout(0.1)(ffn_output)
        ffn_output = Dense(128)(ffn_output)
        x = LayerNormalization()(x + ffn_output)
    
    # Global Pooling
    x = GlobalAveragePooling1D()(x)
    
    # Dense layers
    x = Dense(256, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    
    # ì¶œë ¥ì¸µ
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    
    model = Model(inputs, [out_reg, out_cls], name='PatchTST')
    return model

def build_stable_lstm(input_shape):
    """StableLSTM - ì–‘ë°©í–¥ LSTM"""
    inputs = Input(shape=input_shape)
    
    # Bidirectional LSTM
    x = Bidirectional(LSTM(128, return_sequences=True))(inputs)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    
    x = Bidirectional(LSTM(64, return_sequences=False))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    
    # Dense layers
    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    
    x = Dense(64, activation='relu')(x)
    x = BatchNormalization()(x)
    
    # ì¶œë ¥ì¸µ
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    
    model = Model(inputs, [out_reg, out_cls], name='StableLSTM')
    return model

def build_spike_detector(input_shape):
    """SpikeDetector - ê¸‰ë³€ ê°ì§€"""
    inputs = Input(shape=input_shape)
    
    # ìµœê·¼ 20ê°œ ì‹œì ì— ì§‘ì¤‘
    recent = Lambda(lambda x: x[:, -20:, :])(inputs)
    
    # 1D CNN
    x = Conv1D(64, kernel_size=3, activation='relu', padding='same')(recent)
    x = BatchNormalization()(x)
    x = MaxPooling1D(pool_size=2)(x)
    
    x = Conv1D(32, kernel_size=3, activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    
    # GRU
    x = GRU(64, return_sequences=False)(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    # Dense layers
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    x = Dense(32, activation='relu')(x)
    
    # ì¶œë ¥ì¸µ
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    
    model = Model(inputs, [out_reg, out_cls], name='SpikeDetector')
    return model

def build_extreme_net(input_shape):
    """ExtremeNet - ê·¹ë‹¨ê°’ ì „ë¬¸"""
    inputs = Input(shape=input_shape)
    
    # Branch 1: Normal pattern
    normal_branch = LSTM(64, return_sequences=True)(inputs)
    normal_branch = LSTM(32, return_sequences=False)(normal_branch)
    normal_branch = Dense(32, activation='relu')(normal_branch)
    
    # Branch 2: Extreme pattern
    extreme_branch = MultiHeadAttention(
        num_heads=4, 
        key_dim=16
    )(inputs, inputs)
    extreme_branch = GlobalAveragePooling1D()(extreme_branch)
    extreme_branch = Dense(32, activation='relu')(extreme_branch)
    
    # Merge
    combined = Concatenate()([normal_branch, extreme_branch])
    x = BatchNormalization()(combined)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.3)(x)
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    # ì¶œë ¥ì¸µ
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    
    model = Model(inputs, [out_reg, out_cls], name='ExtremeNet')
    return model

def build_ensemble_base(input_shape):
    """EnsembleBase - ì•™ìƒë¸” ê¸°ë³¸"""
    inputs = Input(shape=input_shape)
    
    # Multiple pooling
    avg_pool = GlobalAveragePooling1D()(inputs)
    max_pool = GlobalMaxPooling1D()(inputs)
    
    # Combine
    combined = Concatenate()([avg_pool, max_pool])
    x = BatchNormalization()(combined)
    
    # Dense layers
    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    x = Dense(64, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    
    # ì¶œë ¥ì¸µ
    out_reg = Dense(1, name='regression')(x)
    out_cls = Dense(3, activation='softmax', name='classification')(x)
    
    model = Model(inputs, [out_reg, out_cls], name='EnsembleBase')
    return model

# =========================================
# ëª¨ë¸ ë¹Œë”
# =========================================

def build_models(input_shape):
    """ëª¨ë“  ëª¨ë¸ ë¹Œë“œ"""
    models = {}
    
    print("\nğŸ“¦ ëª¨ë¸ ìƒì„± ì¤‘...")
    
    # 1. StableLSTM
    models['StableLSTM'] = build_stable_lstm(input_shape)
    print("  âœ… StableLSTM ìƒì„± ì™„ë£Œ")
    
    # 2. PatchTST
    models['PatchTST'] = build_patch_tst(input_shape)
    print("  âœ… PatchTST ìƒì„± ì™„ë£Œ")
    
    # 3. SpikeDetector
    models['SpikeDetector'] = build_spike_detector(input_shape)
    print("  âœ… SpikeDetector ìƒì„± ì™„ë£Œ")
    
    # 4. ExtremeNet
    models['ExtremeNet'] = build_extreme_net(input_shape)
    print("  âœ… ExtremeNet ìƒì„± ì™„ë£Œ")
    
    # 5. EnsembleBase
    models['EnsembleBase'] = build_ensemble_base(input_shape)
    print("  âœ… EnsembleBase ìƒì„± ì™„ë£Œ")
    
    return models

# =========================================
# í•™ìŠµ ë° í‰ê°€ í´ë˜ìŠ¤
# =========================================

class ModelTrainer:
    """ëª¨ë¸ í•™ìŠµ ë° í‰ê°€"""
    
    def __init__(self, models_dict, processor, checkpoint_manager):
        self.models = models_dict
        self.processor = processor
        self.checkpoint_manager = checkpoint_manager
        self.results = {}
        
    def train_model(self, model_name, X_train, y_train, X_val, y_val, 
                   epochs=30, initial_epoch=0):
        """ëª¨ë¸ í•™ìŠµ"""
        
        model = self.models[model_name]
        
        # ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ (í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬)
        def weighted_sparse_categorical_crossentropy(y_true, y_pred):
            # í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì ìš©
            weights = tf.constant([1.0, 5.0, 200.0])
            
            # sparse_categorical_crossentropy ê³„ì‚°
            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
            
            # ê°€ì¤‘ì¹˜ ì ìš©
            weight_map = tf.gather(weights, tf.cast(y_true, tf.int32))
            weighted_loss = loss * weight_map
            
            return tf.reduce_mean(weighted_loss)
        
        # ì»´íŒŒì¼
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss={
                'regression': 'mse',
                'classification': weighted_sparse_categorical_crossentropy
            },
            loss_weights={'regression': 0.3, 'classification': 0.7},
            metrics={
                'regression': ['mae'],
                'classification': ['accuracy']
            }
        )
        
        print(f"\n{'='*60}")
        print(f"ğŸ¯ {model_name} í•™ìŠµ ì‹œì‘")
        print(f"{'='*60}")
        
        # Callbacks
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=7,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=3,
                min_lr=1e-6,
                verbose=1
            ),
            ModelCheckpoint(
                f'./checkpoints/{model_name}_best.keras',
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            )
        ]
        
        # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ìƒì„± (class_weight ëŒ€ì²´)
        class_weight_dict = {0: 1.0, 1: 5.0, 2: 200.0}
        sample_weights = np.array([class_weight_dict[cls] for cls in y_train[1]])
        
        # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
        y_train_dict = {'regression': y_train[0], 'classification': y_train[1]}
        y_val_dict = {'regression': y_val[0], 'classification': y_val[1]}
        
        history = model.fit(
            X_train,
            y_train_dict,
            validation_data=(X_val, y_val_dict),
            epochs=epochs,
            initial_epoch=initial_epoch,
            batch_size=256,
            callbacks=callbacks,
            # sample_weight ì œê±° (ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬)
            verbose=1
        )
        
        # ìµœì¢… ì €ì¥
        os.makedirs('models', exist_ok=True)
        model.save(f'./models/{model_name}_final.keras')
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: ./models/{model_name}_final.keras")
        
        return history
    
    def evaluate_model(self, model_name, X_test, y_test):
        """ëª¨ë¸ í‰ê°€"""
        
        model = self.models[model_name]
        
        # ì˜ˆì¸¡
        predictions = model.predict(X_test, batch_size=256, verbose=0)
        y_reg_pred = predictions[0].flatten()
        y_cls_pred = np.argmax(predictions[1], axis=1)
        
        # ì‹¤ì œê°’
        y_reg_true = y_test[0]
        y_cls_true = y_test[1]
        
        # ì—­ë³€í™˜
        y_reg_true_orig = self.processor.target_scaler.inverse_transform(
            y_reg_true.reshape(-1, 1)
        ).flatten()
        y_reg_pred_orig = self.processor.target_scaler.inverse_transform(
            y_reg_pred.reshape(-1, 1)
        ).flatten()
        
        # íšŒê·€ ë©”íŠ¸ë¦­
        mae = mean_absolute_error(y_reg_true_orig, y_reg_pred_orig)
        rmse = np.sqrt(mean_squared_error(y_reg_true_orig, y_reg_pred_orig))
        r2 = r2_score(y_reg_true_orig, y_reg_pred_orig)
        
        # ë¶„ë¥˜ ë©”íŠ¸ë¦­
        accuracy = accuracy_score(y_cls_true, y_cls_pred)
        
        # êµ¬ê°„ë³„ ì •í™•ë„
        cm = confusion_matrix(y_cls_true, y_cls_pred)
        class_accuracies = []
        for i in range(len(cm)):
            if cm[i].sum() > 0:
                class_accuracies.append(cm[i, i] / cm[i].sum())
            else:
                class_accuracies.append(0)
        
        # 1700+ ì„±ëŠ¥
        if 2 in y_cls_true:
            precision_1700 = precision_score(y_cls_true, y_cls_pred, labels=[2], average='macro', zero_division=0)
            recall_1700 = recall_score(y_cls_true, y_cls_pred, labels=[2], average='macro', zero_division=0)
            f1_1700 = f1_score(y_cls_true, y_cls_pred, labels=[2], average='macro', zero_division=0)
        else:
            precision_1700 = recall_1700 = f1_1700 = 0
        
        # ê²°ê³¼ ì €ì¥
        results = {
            'MAE': mae,
            'RMSE': rmse,
            'R2': r2,
            'Accuracy': accuracy,
            'Accuracy_Normal': class_accuracies[0] if len(class_accuracies) > 0 else 0,
            'Accuracy_Warning': class_accuracies[1] if len(class_accuracies) > 1 else 0,
            'Accuracy_Critical': class_accuracies[2] if len(class_accuracies) > 2 else 0,
            'Precision_1700': precision_1700,
            'Recall_1700': recall_1700,
            'F1_1700': f1_1700
        }
        
        self.results[model_name] = results
        
        # ì¶œë ¥
        print(f"\n{'='*60}")
        print(f"ğŸ“Š {model_name} í‰ê°€ ê²°ê³¼")
        print(f"{'='*60}")
        print(f"\n[íšŒê·€ ì„±ëŠ¥]")
        print(f"  MAE:  {mae:.2f}")
        print(f"  RMSE: {rmse:.2f}")
        print(f"  RÂ²:   {r2:.4f}")
        
        print(f"\n[ë¶„ë¥˜ ì„±ëŠ¥]")
        print(f"  ì „ì²´ ì •í™•ë„: {accuracy:.2%}")
        print(f"  ì •ìƒ ì •í™•ë„: {results['Accuracy_Normal']:.2%}")
        print(f"  ì£¼ì˜ ì •í™•ë„: {results['Accuracy_Warning']:.2%}")
        print(f"  ì‹¬ê° ì •í™•ë„: {results['Accuracy_Critical']:.2%}")
        
        print(f"\n[1700+ ê°ì§€]")
        print(f"  Precision: {precision_1700:.2%}")
        print(f"  Recall:    {recall_1700:.2%}")
        print(f"  F1-Score:  {f1_1700:.2%}")
        
        return results

# =========================================
# ë©”ì¸ ì‹¤í–‰
# =========================================

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('checkpoints', exist_ok=True)
    os.makedirs('models', exist_ok=True)
    
    # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
    checkpoint_manager = CheckpointManager()
    
    # 1. ë°ì´í„° ì¤€ë¹„
    print("\nğŸ“ ë°ì´í„° ë¡œë“œ...")
    
    # ë°ì´í„° ê²½ë¡œ
    data_paths = [
        'data/20250731_to20250806.csv',  # ì‹¤ì œ ê²½ë¡œ
        '/mnt/user-data/uploads/gs.CSV',
        './data.csv'
    ]
    
    data_path = None
    for path in data_paths:
        if os.path.exists(path):
            data_path = path
            print(f"âœ… ë°ì´í„° ë°œê²¬: {path}")
            break
    
    if data_path is None:
        print("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # ë°ì´í„° ì²˜ë¦¬
    processor = DataProcessor(seq_len=100, pred_len=10)
    processor.load_scalers()
    
    df = processor.load_data(data_path)
    df = processor.create_features(df)
    X, y_reg, y_cls, feature_cols = processor.prepare_sequences(df)
    
    # 2. ë°ì´í„° ë¶„í• 
    n_train = int(0.7 * len(X))
    n_val = int(0.15 * len(X))
    
    X_train = X[:n_train]
    y_reg_train = y_reg[:n_train]
    y_cls_train = y_cls[:n_train]
    
    X_val = X[n_train:n_train+n_val]
    y_reg_val = y_reg[n_train:n_train+n_val]
    y_cls_val = y_cls[n_train:n_train+n_val]
    
    X_test = X[n_train+n_val:]
    y_reg_test = y_reg[n_train+n_val:]
    y_cls_test = y_cls[n_train+n_val:]
    
    print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
    print(f"  Train: {X_train.shape} (70%)")
    print(f"  Val:   {X_val.shape} (15%)")
    print(f"  Test:  {X_test.shape} (15%)")
    
    # 3. ëª¨ë¸ ìƒì„±
    input_shape = (X.shape[1], X.shape[2])
    models = build_models(input_shape)
    
    # 4. í•™ìŠµ ê´€ë¦¬ì
    trainer = ModelTrainer(models, processor, checkpoint_manager)
    
    # 5. ê° ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    for model_name in models.keys():
        # í•™ìŠµ
        trainer.train_model(
            model_name,
            X_train,
            (y_reg_train, y_cls_train),
            X_val,
            (y_reg_val, y_cls_val),
            epochs=30
        )
        
        # í‰ê°€
        trainer.evaluate_model(
            model_name,
            X_test,
            (y_reg_test, y_cls_test)
        )
    
    # 6. ìµœì¢… ê²°ê³¼
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… ì„±ëŠ¥ ë¹„êµ")
    print("="*80)
    
    results_df = pd.DataFrame(trainer.results).T
    print("\n[íšŒê·€ ì„±ëŠ¥]")
    print(results_df[['MAE', 'RMSE', 'R2']].round(3))
    
    print("\n[ë¶„ë¥˜ ì •í™•ë„]")
    print(results_df[['Accuracy', 'Accuracy_Normal', 'Accuracy_Warning', 'Accuracy_Critical']].round(3))
    
    print("\n[1700+ ê°ì§€]")
    print(results_df[['Precision_1700', 'Recall_1700', 'F1_1700']].round(3))
    
    # CSV ì €ì¥
    results_df.to_csv('model_results.csv')
    print("\nğŸ’¾ ê²°ê³¼ ì €ì¥: model_results.csv")
    
    print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
    
    return models, results_df

if __name__ == "__main__":
    models, results = main()