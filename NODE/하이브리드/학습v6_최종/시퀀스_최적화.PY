"""
optimize_csv_for_v6.py - V6 ì‹œí€€ìŠ¤ ìƒì„±ê¸°ìš© CSV ìµœì í™”
ì¤‘ìš” íŒ¨í„´ì€ ë³´ì¡´í•˜ë©´ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ë°ì´í„° ì¶•ì†Œ
"""

import pandas as pd
import numpy as np
from datetime import datetime
import gc

# ============================================
# ì„¤ì • (V6ì™€ ë™ì¼í•œ ìž„ê³„ê°’)
# ============================================
# ìž…ì¶œë ¥ íŒŒì¼
INPUT_FILE = './data/balanced_train_data.csv'  # ì›ë³¸ CSV
OUTPUT_FILE = './data/optimized_for_v6.csv'    # ìµœì í™”ëœ CSV

# V6 ì„¤ì •ê°’
LOOKBACK = 100
FORECAST = 10

# V6 ìž„ê³„ê°’ (ë™ì¼í•˜ê²Œ ìœ ì§€)
M14B_THRESHOLDS = {
    1400: 320,
    1500: 400,
    1600: 450,
    1700: 500
}

RATIO_THRESHOLDS = {
    1400: 4,
    1500: 5,
    1600: 6,
    1700: 7
}

# ìµœì í™” ì„¤ì •
OPTIMIZATION_CONFIG = {
    'target_size': 50000,        # ëª©í‘œ í¬ê¸° (5ë§Œí–‰)
    'min_segment_length': 200,   # ìµœì†Œ ì—°ì† êµ¬ê°„ (LOOKBACK + FORECAST + ì—¬ìœ )
    
    # ë³´ì¡´ ìš°ì„ ìˆœìœ„
    'preserve_priority': {
        'critical': 1.0,    # 100% ë³´ì¡´ - TOTALCNT >= 1500 ë˜ëŠ” M14B >= 400
        'important': 0.8,   # 80% ë³´ì¡´ - TOTALCNT >= 1400 ë˜ëŠ” M14B >= 350
        'medium': 0.3,      # 30% ë³´ì¡´ - TOTALCNT >= 1350
        'normal': 0.1       # 10% ë³´ì¡´ - ë‚˜ë¨¸ì§€
    }
}

# ============================================
# CSV ìµœì í™” í•¨ìˆ˜
# ============================================
def optimize_csv_for_sequences():
    """CSV ë°ì´í„° ìµœì í™”"""
    print("="*60)
    print("ðŸš€ V6 ì‹œí€€ìŠ¤ìš© CSV ìµœì í™”")
    print("="*60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print(f"\nðŸ“‚ ì›ë³¸ CSV ë¡œë”©: {INPUT_FILE}")
    df = pd.read_csv(INPUT_FILE)
    original_size = len(df)
    print(f"âœ… ì›ë³¸ í¬ê¸°: {original_size:,}í–‰")
    
    # 2. ì—°ì†ì„± í™•ì¸
    print("\nðŸ” ì—°ì† êµ¬ê°„ ë¶„ì„...")
    
    # ì‹œê°„ ì»¬ëŸ¼ í™•ì¸
    time_col = 'TIME' if 'TIME' in df.columns else 'CURRTIME'
    if time_col in df.columns:
        df['datetime'] = pd.to_datetime(df[time_col].astype(str), 
                                       format='%Y%m%d%H%M', 
                                       errors='coerce')
        df['time_diff'] = df['datetime'].diff().dt.total_seconds() / 60
        df['segment_id'] = (df['time_diff'] > 5).cumsum()
        
        segment_sizes = df.groupby('segment_id').size()
        valid_segments = segment_sizes[segment_sizes >= OPTIMIZATION_CONFIG['min_segment_length']]
        print(f"  â€¢ ìœ íš¨ ì„¸ê·¸ë¨¼íŠ¸: {len(valid_segments)}ê°œ")
    else:
        df['segment_id'] = 0
        valid_segments = pd.Series([len(df)], index=[0])
    
    # 3. ì¤‘ìš”ë„ ê³„ì‚°
    print("\nðŸ“Š ì¤‘ìš”ë„ ê¸°ë°˜ ìƒ˜í”Œë§...")
    
    # ì¤‘ìš”ë„ ë ˆë²¨ í• ë‹¹
    df['importance'] = 'normal'
    
    # Critical: TOTALCNT >= 1500 ë˜ëŠ” M14B >= 400
    critical_mask = (df['TOTALCNT'] >= 1500) | (df['M14AM14B'] >= M14B_THRESHOLDS[1500])
    df.loc[critical_mask, 'importance'] = 'critical'
    
    # Important: TOTALCNT >= 1400 ë˜ëŠ” M14B >= 350
    important_mask = ((df['TOTALCNT'] >= 1400) | (df['M14AM14B'] >= 350)) & ~critical_mask
    df.loc[important_mask, 'importance'] = 'important'
    
    # Medium: TOTALCNT >= 1350
    medium_mask = (df['TOTALCNT'] >= 1350) & ~critical_mask & ~important_mask
    df.loc[medium_mask, 'importance'] = 'medium'
    
    # í™©ê¸ˆ íŒ¨í„´ (M14B >= 300 & M10A < 80)ì€ ë¬´ì¡°ê±´ critical
    golden_mask = (df['M14AM14B'] >= 300) & (df['M14AM10A'] < 80)
    df.loc[golden_mask, 'importance'] = 'critical'
    
    # í†µê³„ ì¶œë ¥
    importance_counts = df['importance'].value_counts()
    print(f"  â€¢ Critical: {importance_counts.get('critical', 0):,}ê°œ")
    print(f"  â€¢ Important: {importance_counts.get('important', 0):,}ê°œ")
    print(f"  â€¢ Medium: {importance_counts.get('medium', 0):,}ê°œ")
    print(f"  â€¢ Normal: {importance_counts.get('normal', 0):,}ê°œ")
    
    # 4. ìƒ˜í”Œë§
    sampled_indices = []
    
    for importance_level, preserve_ratio in OPTIMIZATION_CONFIG['preserve_priority'].items():
        level_df = df[df['importance'] == importance_level]
        
        if len(level_df) > 0:
            if preserve_ratio == 1.0:
                # 100% ë³´ì¡´
                sampled_indices.extend(level_df.index.tolist())
            else:
                # ë¹„ìœ¨ë§Œí¼ ìƒ˜í”Œë§
                n_samples = int(len(level_df) * preserve_ratio)
                if n_samples > 0:
                    samples = level_df.sample(n=n_samples, random_state=42)
                    sampled_indices.extend(samples.index.tolist())
    
    # 5. ìµœì¢… ë°ì´í„° ìƒì„±
    print(f"\nðŸ“¦ ìµœì í™”ëœ ë°ì´í„° ìƒì„±...")
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    sampled_indices = sorted(list(set(sampled_indices)))
    
    # ëª©í‘œ í¬ê¸° ì¡°ì •
    if len(sampled_indices) > OPTIMIZATION_CONFIG['target_size']:
        # ì¤‘ìš”ë„ ë‚®ì€ ê²ƒë¶€í„° ì œê±°
        df_sampled = df.loc[sampled_indices]
        
        # importance ìˆœì„œëŒ€ë¡œ ì •ë ¬
        importance_order = {'critical': 0, 'important': 1, 'medium': 2, 'normal': 3}
        df_sampled['importance_order'] = df_sampled['importance'].map(importance_order)
        df_sampled = df_sampled.sort_values('importance_order')
        
        # ëª©í‘œ í¬ê¸°ë§Œí¼ë§Œ ìœ ì§€
        df_sampled = df_sampled.head(OPTIMIZATION_CONFIG['target_size'])
        df_optimized = df_sampled.sort_index()
    else:
        df_optimized = df.loc[sampled_indices]
    
    # ìž„ì‹œ ì»¬ëŸ¼ ì œê±°
    columns_to_drop = ['datetime', 'time_diff', 'segment_id', 'importance', 'importance_order']
    for col in columns_to_drop:
        if col in df_optimized.columns:
            df_optimized = df_optimized.drop(col, axis=1)
    
    # ì¸ë±ìŠ¤ ë¦¬ì…‹
    df_optimized = df_optimized.reset_index(drop=True)
    
    # 6. ì €ìž¥
    print(f"\nðŸ’¾ ìµœì í™”ëœ CSV ì €ìž¥: {OUTPUT_FILE}")
    df_optimized.to_csv(OUTPUT_FILE, index=False)
    
    # 7. ê²°ê³¼ í†µê³„
    print("\nâœ… ìµœì í™” ì™„ë£Œ!")
    print("="*60)
    print(f"ì›ë³¸: {original_size:,}í–‰ â†’ ìµœì í™”: {len(df_optimized):,}í–‰")
    print(f"ì¶•ì†Œìœ¨: {(1 - len(df_optimized)/original_size)*100:.1f}%")
    
    if 'TOTALCNT' in df_optimized.columns:
        print(f"\nTOTALCNT ë¶„í¬:")
        print(f"  â€¢ í‰ê· : {df_optimized['TOTALCNT'].mean():.0f}")
        print(f"  â€¢ 1400+: {(df_optimized['TOTALCNT'] >= 1400).sum():,}ê°œ")
        print(f"  â€¢ 1500+: {(df_optimized['TOTALCNT'] >= 1500).sum():,}ê°œ")
    
    if 'M14AM14B' in df_optimized.columns:
        print(f"\nM14AM14B ë¶„í¬:")
        print(f"  â€¢ í‰ê· : {df_optimized['M14AM14B'].mean():.0f}")
        print(f"  â€¢ 400+: {(df_optimized['M14AM14B'] >= 400).sum():,}ê°œ")
        
        # í™©ê¸ˆ íŒ¨í„´
        golden = (df_optimized['M14AM14B'] >= 300) & (df_optimized['M14AM10A'] < 80)
        print(f"  â€¢ í™©ê¸ˆ íŒ¨í„´: {golden.sum():,}ê°œ")
    
    # ì‹œí€€ìŠ¤ ìƒì„± ê°€ëŠ¥ ìˆ˜
    max_sequences = max(0, len(df_optimized) - LOOKBACK - FORECAST)
    print(f"\nìƒì„± ê°€ëŠ¥ ì‹œí€€ìŠ¤: {max_sequences:,}ê°œ")
    
    print("\n" + "="*60)
    print("ðŸ’¡ V6 ì‚¬ìš©ë²•:")
    print(f"DATA_FILE = '{OUTPUT_FILE}'")
    print("ë‚˜ë¨¸ì§€ëŠ” ê·¸ëŒ€ë¡œ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤!")
    print("="*60)
    
    return df_optimized

# ============================================
# ë©”ì¸ ì‹¤í–‰
# ============================================
if __name__ == '__main__':
    df_optimized = optimize_csv_for_sequences()