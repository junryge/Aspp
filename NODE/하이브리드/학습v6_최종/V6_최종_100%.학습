"""
V6 ì „ì²´ ë°ì´í„° í•™ìŠµ ë²„ì „
- ê¸°ì¡´ NPZ íŒŒì¼ í™œìš©
- ì „ì²´ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš© (ê²€ì¦ ë¶„í•  ì—†ìŒ)
- 100ë¶„ ë°ì´í„° ìœ ì§€
- 5ê°œ ëª¨ë¸ + ì•™ìƒë¸”
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import RobustScaler
import json
import os
import pickle
import warnings
import gc
warnings.filterwarnings('ignore')

print("="*60)
print("ğŸš€ V6 ì „ì²´ ë°ì´í„° í•™ìŠµ ë²„ì „")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print("="*60)

# ============================================
# GPU ì„¤ì •
# ============================================
def setup_gpu():
    """GPU ì„¤ì • ë° í™•ì¸"""
    print("\nğŸ® GPU í™˜ê²½ í™•ì¸...")
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"âœ… GPU ê°ì§€: {len(gpus)}ê°œ")
            return True
        except Exception as e:
            print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
            return False
    else:
        print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        return False

has_gpu = setup_gpu()

# ============================================
# ì„¤ì •
# ============================================
class Config:
    # ì‹œí€€ìŠ¤ íŒŒì¼ (ê¸°ì¡´ íŒŒì¼ í™œìš©)
    SEQUENCE_FILE = './sequences_v6_gpu.npz'
    SCALER_FILE = './scalers_v6_gpu.pkl'
    
    # ë°±ì—… ê²½ë¡œ (íŒŒì¼ì´ ì—†ì„ ê²½ìš°)
    DATA_FILE = '/mnt/user-data/uploads/20250807_DATA.CSV'
    
    # ì‹œí€€ìŠ¤ ì„¤ì •
    LOOKBACK = 100  # â­ ê³¼ê±° 100ë¶„ ë°ì´í„°
    FORECAST = 10   # â­ 10ë¶„ í›„ ì˜ˆì¸¡
    
    # í•™ìŠµ ì„¤ì • (ì „ì²´ ë°ì´í„°ìš©)
    BATCH_SIZE = 64
    EPOCHS = 200  # ì „ì²´ ë°ì´í„° í•™ìŠµì´ë¯€ë¡œ ì—í­ ì¦ê°€
    LEARNING_RATE = 0.001
    
    # ì €ì¥ ê²½ë¡œ
    MODEL_DIR = './models_v6_full_train/'
    
    # ìŠ¤ì¼€ì¼ ì‚¬ìš© ì—¬ë¶€
    USE_SCALED_Y = False  # ì›ë³¸ ê°’ ì‚¬ìš©

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(Config.MODEL_DIR, exist_ok=True)

# ============================================
# ì†ì‹¤ í•¨ìˆ˜
# ============================================
class WeightedLoss(tf.keras.losses.Loss):
    """ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜"""
    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        
        mae = tf.abs(y_true - y_pred)
        
        # ê°•í™”ëœ ê°€ì¤‘ì¹˜
        weights = tf.ones_like(y_true)
        if not Config.USE_SCALED_Y:
            weights = tf.where(y_true >= 1550, 30.0, weights)
            weights = tf.where((y_true >= 1500) & (y_true < 1550), 25.0, weights)
            weights = tf.where((y_true >= 1450) & (y_true < 1500), 20.0, weights)
            weights = tf.where((y_true >= 1400) & (y_true < 1450), 15.0, weights)
            weights = tf.where((y_true >= 1350) & (y_true < 1400), 10.0, weights)
        
        # í° ì˜¤ì°¨ í˜ë„í‹°
        large_error = tf.where(mae > 100, mae * 0.2, 0.0)
        
        return tf.reduce_mean(mae * weights + large_error)

# ============================================
# M14 ê·œì¹™ ë³´ì • ë ˆì´ì–´
# ============================================
class M14RuleCorrection(tf.keras.layers.Layer):
    """M14 ê·œì¹™ ê¸°ë°˜ ë³´ì •"""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    
    def call(self, inputs, training=None):
        pred, m14_features = inputs
        
        pred = tf.cast(pred, tf.float32)
        m14_features = tf.cast(m14_features, tf.float32)
        
        m14b = m14_features[:, 0:1]
        m10a = m14_features[:, 1:2]
        m16 = m14_features[:, 2:3]
        ratio = m14_features[:, 3:4]
        
        if not Config.USE_SCALED_Y:
            # ì„ê³„ê°’ ê·œì¹™
            pred = tf.where(m14b >= 420, tf.maximum(pred, 1550.0), pred)
            pred = tf.where(m14b >= 380, tf.maximum(pred, 1500.0), pred)
            pred = tf.where(m14b >= 350, tf.maximum(pred, 1450.0), pred)
            pred = tf.where(m14b >= 300, tf.maximum(pred, 1400.0), pred)
            
            # ë¹„ìœ¨ ë³´ì •
            pred = tf.where(ratio >= 5.5, pred * 1.15, pred)
            pred = tf.where((ratio >= 5.0) & (ratio < 5.5), pred * 1.10, pred)
            pred = tf.where((ratio >= 4.5) & (ratio < 5.0), pred * 1.08, pred)
            pred = tf.where((ratio >= 4.0) & (ratio < 4.5), pred * 1.05, pred)
            
            # í™©ê¸ˆ íŒ¨í„´
            golden = (m14b >= 350) & (m10a < 70)
            pred = tf.where(golden, pred * 1.2, pred)
            
            # ë²”ìœ„ ì œí•œ
            pred = tf.clip_by_value(pred, 1200.0, 2000.0)
        
        return pred

# ============================================
# ëª¨ë¸ ì •ì˜
# ============================================
class ModelsV6:
    
    @staticmethod
    def build_lstm_model(input_shape):
        """LSTM ëª¨ë¸"""
        model = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=input_shape),
            tf.keras.layers.LayerNormalization(),
            tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.2),
            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2),
            tf.keras.layers.LSTM(64, dropout=0.2),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dense(256, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dense(1)
        ], name='LSTM_Model')
        return model
    
    @staticmethod
    def build_gru_model(input_shape):
        """GRU ëª¨ë¸"""
        model = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=input_shape),
            tf.keras.layers.LayerNormalization(),
            tf.keras.layers.GRU(256, return_sequences=True, dropout=0.15),
            tf.keras.layers.GRU(128, return_sequences=True, dropout=0.15),
            tf.keras.layers.GRU(64, dropout=0.15),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dense(256, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dense(1)
        ], name='GRU_Model')
        return model
    
    @staticmethod
    def build_cnn_lstm(input_shape):
        """CNN-LSTM ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape)
        
        # ë©€í‹°ìŠ¤ì¼€ì¼ CNN
        convs = []
        for kernel_size in [3, 5, 7, 9]:
            conv = tf.keras.layers.Conv1D(128, kernel_size, activation='relu', padding='same')(inputs)
            conv = tf.keras.layers.BatchNormalization()(conv)
            convs.append(conv)
        
        concat = tf.keras.layers.Concatenate()(convs)
        
        lstm1 = tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.15)(concat)
        lstm2 = tf.keras.layers.LSTM(128, dropout=0.15)(lstm1)
        
        dense1 = tf.keras.layers.Dense(256, activation='relu')(lstm2)
        dropout1 = tf.keras.layers.Dropout(0.2)(dense1)
        dense2 = tf.keras.layers.Dense(128, activation='relu')(dropout1)
        dropout2 = tf.keras.layers.Dropout(0.15)(dense2)
        output = tf.keras.layers.Dense(1)(dropout2)
        
        return tf.keras.Model(inputs=inputs, outputs=output, name='CNN_LSTM_Model')
    
    @staticmethod
    def build_spike_detector(input_shape):
        """Spike Detector"""
        inputs = tf.keras.Input(shape=input_shape)
        
        # CNN
        convs = []
        for kernel_size in [3, 5, 7]:
            conv = tf.keras.layers.Conv1D(96, kernel_size, activation='relu', padding='same')(inputs)
            convs.append(conv)
        
        concat = tf.keras.layers.Concatenate()(convs)
        norm = tf.keras.layers.BatchNormalization()(concat)
        
        # Attention
        attention = tf.keras.layers.MultiHeadAttention(
            num_heads=8, key_dim=36, dropout=0.15
        )(norm, norm)
        
        # Bidirectional LSTM
        lstm = tf.keras.layers.Bidirectional(
            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.15)
        )(attention)
        
        # Pooling
        avg_pool = tf.keras.layers.GlobalAveragePooling1D()(lstm)
        max_pool = tf.keras.layers.GlobalMaxPooling1D()(lstm)
        pooled = tf.keras.layers.Concatenate()([avg_pool, max_pool])
        
        dense1 = tf.keras.layers.Dense(256, activation='relu')(pooled)
        dropout1 = tf.keras.layers.Dropout(0.2)(dense1)
        dense2 = tf.keras.layers.Dense(128, activation='relu')(dropout1)
        dropout2 = tf.keras.layers.Dropout(0.15)(dense2)
        
        output = tf.keras.layers.Dense(1, name='spike_value')(dropout2)
        
        return tf.keras.Model(inputs=inputs, outputs=output, name='Spike_Detector')
    
    @staticmethod
    def build_rule_based_model(input_shape, m14_shape):
        """Rule-Based ëª¨ë¸"""
        time_input = tf.keras.Input(shape=input_shape, name='time_input')
        m14_input = tf.keras.Input(shape=(m14_shape,), name='m14_input')
        
        lstm1 = tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.15)(time_input)
        lstm2 = tf.keras.layers.LSTM(32, dropout=0.15)(lstm1)
        
        m14_dense1 = tf.keras.layers.Dense(32, activation='relu')(m14_input)
        m14_dense2 = tf.keras.layers.Dense(16, activation='relu')(m14_dense1)
        
        combined = tf.keras.layers.Concatenate()([lstm2, m14_dense2])
        
        dense1 = tf.keras.layers.Dense(128, activation='relu')(combined)
        dropout1 = tf.keras.layers.Dropout(0.2)(dense1)
        dense2 = tf.keras.layers.Dense(64, activation='relu')(dropout1)
        dropout2 = tf.keras.layers.Dropout(0.15)(dense2)
        
        prediction = tf.keras.layers.Dense(1, name='rule_pred')(dropout2)
        corrected = M14RuleCorrection()([prediction, m14_input])
        
        return tf.keras.Model(
            inputs=[time_input, m14_input],
            outputs=corrected,
            name='Rule_Based_Model'
        )

# ============================================
# ë°ì´í„° ë¡œë“œ
# ============================================
print("\nğŸ“¦ ë°ì´í„° ë¡œë“œ ì¤‘...")

# ê¸°ì¡´ NPZ íŒŒì¼ ë¡œë“œ ì‹œë„
if os.path.exists(Config.SEQUENCE_FILE):
    print(f"âœ… ê¸°ì¡´ NPZ íŒŒì¼ ë¡œë“œ: {Config.SEQUENCE_FILE}")
    data = np.load(Config.SEQUENCE_FILE)
    X_train = data['X']
    y_train = data['y_original'] if 'y_original' in data else data['y']
    m14_train = data['m14_features']
    
    print(f"  X shape: {X_train.shape}")
    print(f"  y shape: {y_train.shape}")
    print(f"  m14 shape: {m14_train.shape}")
    print(f"  y ë²”ìœ„: {y_train.min():.0f} ~ {y_train.max():.0f}")
    
else:
    print("âš ï¸ NPZ íŒŒì¼ ì—†ìŒ - ìƒˆë¡œ ìƒì„± í•„ìš”")
    print("ê¸°ì¡´ V6 ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ NPZ íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.")
    exit(1)

# íƒ€ê²Ÿ ë¶„í¬ í™•ì¸
print("\nğŸ“Š íƒ€ê²Ÿ ë¶„í¬ (ì „ì²´ ë°ì´í„°):")
for level in [1300, 1400, 1450, 1500, 1550]:
    count = (y_train >= level).sum()
    ratio = count / len(y_train) * 100
    print(f"  {level}+: {count:,}ê°œ ({ratio:.1f}%)")

# ============================================
# ëª¨ë¸ í•™ìŠµ (ì „ì²´ ë°ì´í„°)
# ============================================
print("\n" + "="*60)
print("ğŸ‹ï¸ ì „ì²´ ë°ì´í„° í•™ìŠµ ì‹œì‘")
print("="*60)

models = {}
history = {}

# í•™ìŠµ ì½œë°±
def get_callbacks(model_name):
    return [
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='loss',
            patience=10,
            factor=0.5,
            min_lr=1e-7,
            verbose=1
        ),
        tf.keras.callbacks.ModelCheckpoint(
            f'{Config.MODEL_DIR}{model_name}_best.h5',
            save_best_only=True,
            monitor='loss'
        )
    ]

# 1. LSTM
print("\n1ï¸âƒ£ LSTM ëª¨ë¸ í•™ìŠµ...")
lstm_model = ModelsV6.build_lstm_model(X_train.shape[1:])
lstm_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
    loss=WeightedLoss(),
    metrics=['mae']
)
lstm_hist = lstm_model.fit(
    X_train, y_train,
    epochs=Config.EPOCHS,
    batch_size=Config.BATCH_SIZE,
    callbacks=get_callbacks('lstm'),
    verbose=1
)
models['lstm'] = lstm_model
history['lstm'] = lstm_hist
print(f"âœ… LSTM ì™„ë£Œ - ìµœì¢… Loss: {lstm_hist.history['loss'][-1]:.4f}")

# 2. GRU
print("\n2ï¸âƒ£ GRU ëª¨ë¸ í•™ìŠµ...")
gru_model = ModelsV6.build_gru_model(X_train.shape[1:])
gru_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
    loss=WeightedLoss(),
    metrics=['mae']
)
gru_hist = gru_model.fit(
    X_train, y_train,
    epochs=Config.EPOCHS,
    batch_size=Config.BATCH_SIZE,
    callbacks=get_callbacks('gru'),
    verbose=1
)
models['gru'] = gru_model
history['gru'] = gru_hist
print(f"âœ… GRU ì™„ë£Œ - ìµœì¢… Loss: {gru_hist.history['loss'][-1]:.4f}")

# 3. CNN-LSTM
print("\n3ï¸âƒ£ CNN-LSTM ëª¨ë¸ í•™ìŠµ...")
cnn_lstm_model = ModelsV6.build_cnn_lstm(X_train.shape[1:])
cnn_lstm_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
    loss=WeightedLoss(),
    metrics=['mae']
)
cnn_lstm_hist = cnn_lstm_model.fit(
    X_train, y_train,
    epochs=Config.EPOCHS,
    batch_size=Config.BATCH_SIZE,
    callbacks=get_callbacks('cnn_lstm'),
    verbose=1
)
models['cnn_lstm'] = cnn_lstm_model
history['cnn_lstm'] = cnn_lstm_hist
print(f"âœ… CNN-LSTM ì™„ë£Œ - ìµœì¢… Loss: {cnn_lstm_hist.history['loss'][-1]:.4f}")

# 4. Spike Detector
print("\n4ï¸âƒ£ Spike Detector ëª¨ë¸ í•™ìŠµ...")
spike_model = ModelsV6.build_spike_detector(X_train.shape[1:])
spike_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE),
    loss=WeightedLoss(),
    metrics=['mae']
)
spike_hist = spike_model.fit(
    X_train, y_train,
    epochs=Config.EPOCHS,
    batch_size=Config.BATCH_SIZE,
    callbacks=get_callbacks('spike'),
    verbose=1
)
models['spike'] = spike_model
history['spike'] = spike_hist
print(f"âœ… Spike ì™„ë£Œ - ìµœì¢… Loss: {spike_hist.history['loss'][-1]:.4f}")

# 5. Rule-Based
print("\n5ï¸âƒ£ Rule-Based ëª¨ë¸ í•™ìŠµ...")
rule_model = ModelsV6.build_rule_based_model(X_train.shape[1:], m14_train.shape[1])
rule_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * 0.5),
    loss=WeightedLoss(),
    metrics=['mae']
)
rule_hist = rule_model.fit(
    [X_train, m14_train], y_train,
    epochs=100,  # Ruleì€ ë¹¨ë¦¬ ìˆ˜ë ´
    batch_size=Config.BATCH_SIZE,
    callbacks=get_callbacks('rule'),
    verbose=1
)
models['rule'] = rule_model
history['rule'] = rule_hist
print(f"âœ… Rule ì™„ë£Œ - ìµœì¢… Loss: {rule_hist.history['loss'][-1]:.4f}")

# ============================================
# ì•™ìƒë¸” ëª¨ë¸
# ============================================
print("\n" + "="*60)
print("ğŸ¯ ì•™ìƒë¸” ëª¨ë¸ ìƒì„±")
print("="*60)

# ì•™ìƒë¸” ì…ë ¥
time_input = tf.keras.Input(shape=X_train.shape[1:], name='ensemble_time')
m14_input = tf.keras.Input(shape=(m14_train.shape[1],), name='ensemble_m14')

# ê° ëª¨ë¸ ì˜ˆì¸¡
lstm_pred = models['lstm'](time_input)
gru_pred = models['gru'](time_input)
cnn_lstm_pred = models['cnn_lstm'](time_input)
spike_pred = models['spike'](time_input)
rule_pred = models['rule']([time_input, m14_input])

# ê°€ì¤‘ í‰ê· 
ensemble_pred = tf.keras.layers.Lambda(
    lambda x: 0.25*x[0] + 0.20*x[1] + 0.25*x[2] + 0.15*x[3] + 0.15*x[4]
)([lstm_pred, gru_pred, cnn_lstm_pred, spike_pred, rule_pred])

# ìµœì¢… ë³´ì •
final_pred = M14RuleCorrection()([ensemble_pred, m14_input])

# ì•™ìƒë¸” ëª¨ë¸
ensemble_model = tf.keras.Model(
    inputs=[time_input, m14_input],
    outputs=final_pred,
    name='Ensemble_Model'
)

ensemble_model.compile(
    optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * 0.3),
    loss=WeightedLoss(),
    metrics=['mae']
)

print("ğŸ“Š ì•™ìƒë¸” íŒŒì¸íŠœë‹...")
ensemble_hist = ensemble_model.fit(
    [X_train, m14_train], y_train,
    epochs=50,
    batch_size=Config.BATCH_SIZE,
    callbacks=get_callbacks('ensemble'),
    verbose=1
)

models['ensemble'] = ensemble_model
history['ensemble'] = ensemble_hist
print(f"âœ… ì•™ìƒë¸” ì™„ë£Œ - ìµœì¢… Loss: {ensemble_hist.history['loss'][-1]:.4f}")

# ============================================
# í‰ê°€ (í•™ìŠµ ë°ì´í„°ë¡œ ìì²´ í‰ê°€)
# ============================================
print("\n" + "="*60)
print("ğŸ“Š ëª¨ë¸ í‰ê°€ (í•™ìŠµ ë°ì´í„°)")
print("="*60)

# ìƒ˜í”Œë§ í‰ê°€ (ì „ì²´ëŠ” ë„ˆë¬´ í¬ë¯€ë¡œ)
sample_size = min(10000, len(X_train))
sample_idx = np.random.choice(len(X_train), sample_size, replace=False)
X_sample = X_train[sample_idx]
y_sample = y_train[sample_idx]
m14_sample = m14_train[sample_idx]

evaluation_results = {}

for name, model in models.items():
    # ì˜ˆì¸¡
    if name == 'ensemble' or name == 'rule':
        pred = model.predict([X_sample, m14_sample], verbose=0).flatten()
    else:
        pred = model.predict(X_sample, verbose=0).flatten()
    
    # MAE
    mae = np.mean(np.abs(y_sample - pred))
    
    # êµ¬ê°„ë³„ ì„±ëŠ¥
    level_performance = {}
    for level in [1300, 1400, 1450, 1500]:
        mask = y_sample >= level
        if np.any(mask):
            recall = np.sum((pred >= level) & mask) / np.sum(mask)
            precision = np.sum((pred >= level) & mask) / max(np.sum(pred >= level), 1)
            f1 = 2 * (precision * recall) / max(precision + recall, 1e-7)
            level_mae = np.mean(np.abs(y_sample[mask] - pred[mask]))
            
            level_performance[level] = {
                'recall': recall,
                'precision': precision,
                'f1': f1,
                'mae': level_mae,
                'count': np.sum(mask)
            }
    
    evaluation_results[name] = {
        'overall_mae': mae,
        'levels': level_performance
    }
    
    # ì¶œë ¥
    print(f"\nğŸ¯ {name.upper()}:")
    print(f"  ì „ì²´ MAE: {mae:.2f}")
    for level, perf in level_performance.items():
        print(f"  {level}+: Recall={perf['recall']:.2%}, "
              f"Precision={perf['precision']:.2%}, "
              f"F1={perf['f1']:.2%}, MAE={perf['mae']:.1f}")

# ìµœê³  ëª¨ë¸
best_model = min(evaluation_results.keys(), 
                 key=lambda x: evaluation_results[x]['overall_mae'])
print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_model.upper()} "
      f"(MAE: {evaluation_results[best_model]['overall_mae']:.2f})")

# ============================================
# ëª¨ë¸ ì €ì¥
# ============================================
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")

for name, model in models.items():
    save_path = f"{Config.MODEL_DIR}{name}_final.h5"
    model.save(save_path)
    print(f"  âœ… {name} ì €ì¥: {save_path}")

# í‰ê°€ ê²°ê³¼ ì €ì¥
with open(f"{Config.MODEL_DIR}evaluation_results.json", 'w') as f:
    json.dump(evaluation_results, f, indent=2, default=str)

# í•™ìŠµ ê¸°ë¡ ì €ì¥
history_dict = {}
for name, hist in history.items():
    history_dict[name] = {
        'loss': hist.history['loss'][-10:],  # ë§ˆì§€ë§‰ 10ê°œë§Œ
        'mae': hist.history['mae'][-10:] if 'mae' in hist.history else []
    }
with open(f"{Config.MODEL_DIR}training_history.json", 'w') as f:
    json.dump(history_dict, f, indent=2)

print(f"  âœ… í‰ê°€ ê²°ê³¼ ë° í•™ìŠµ ê¸°ë¡ ì €ì¥")

# ============================================
# ìµœì¢… ì¶œë ¥
# ============================================
print("\n" + "="*60)
print("ğŸ‰ V6 ì „ì²´ ë°ì´í„° í•™ìŠµ ì™„ë£Œ!")
print("="*60)
print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {Config.MODEL_DIR}")
print(f"ğŸ† ìµœê³  ëª¨ë¸: {best_model.upper()}")
print(f"ğŸ“Š ì „ì²´ MAE: {evaluation_results[best_model]['overall_mae']:.2f}")
print("\nğŸ“Œ ì£¼ìš” ì„±ê³¼:")
print(f"  - í•™ìŠµ ë°ì´í„°: {len(y_train):,}ê°œ (100% í™œìš©)")
print(f"  - ì´ 6ê°œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (5ê°œ + ì•™ìƒë¸”)")
print(f"  - 100ë¶„ ì‹œí€€ìŠ¤ ë°ì´í„° ìœ ì§€")

# ë©”ëª¨ë¦¬ ì •ë¦¬
tf.keras.backend.clear_session()
gc.collect()

print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
print("="*60)