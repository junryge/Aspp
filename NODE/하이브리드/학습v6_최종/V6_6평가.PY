"""
ğŸ“Š V6.9 ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ê°œì„  - ì„±ëŠ¥ ë° êµ¬ì¡° ìµœì í™” ë²„ì „
============================================================
- ì£¼ìš” ê°œì„  ì‚¬í•­:
  1. Vectorization: ìˆœì°¨ ì²˜ë¦¬(for-loop)ë¥¼ ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë³€ê²½í•˜ì—¬ ì˜ˆì¸¡ ì†ë„ ëŒ€í­ í–¥ìƒ
  2. Modularization: ê¸°ëŠ¥ë³„ í•¨ìˆ˜ ë¶„ë¦¬ë¡œ ì½”ë“œ ê°€ë…ì„± ë° ìœ ì§€ë³´ìˆ˜ì„± ì¦ëŒ€
  3. Configuration: ë¶€ìŠ¤íŒ… ë° ì•™ìƒë¸” ê·œì¹™ì„ ì„¤ì •ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ìœ ì—°ì„± í™•ë³´
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, mean_squared_error
import pickle
import json
import os
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any

# ì´ˆê¸° ì„¤ì •
warnings.filterwarnings('ignore')
tf.keras.config.enable_unsafe_deserialization()


class OptimizedModelEvaluator:
    """
    ìµœì í™”ëœ ëª¨ë¸ í‰ê°€ê¸° í´ë˜ìŠ¤.
    ë²¡í„°í™” ì—°ì‚°ì„ í†µí•´ í‰ê°€ ì†ë„ë¥¼ ê·¹ëŒ€í™”í•˜ê³ , ì½”ë“œ êµ¬ì¡°ë¥¼ ê°œì„ í•˜ì—¬ ìœ ì§€ë³´ìˆ˜ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.
    """
    
    def __init__(self, scaler_path: str = 'scalers/', model_dir: str = 'models/'):
        """í‰ê°€ê¸° ì´ˆê¸°í™” ë° ë¦¬ì†ŒìŠ¤ ë¡œë“œ"""
        print("="*80)
        print("ğŸ”¥ V6.9 ê·¹ë‹¨ê°’ ì˜ˆì¸¡ í‰ê°€ (ì„±ëŠ¥/êµ¬ì¡° ìµœì í™” ë²„ì „)")
        print("="*80)
        
        self._load_scalers_and_config(scaler_path)
        self.models = self._load_all_models(model_dir)
        
        # ë¶€ìŠ¤íŒ… ë° ì•™ìƒë¸” ì „ëµ ì„¤ì •
        self.boosting_config = self._get_boosting_config()
        self.ensemble_weights = self._get_ensemble_weights()

    def _load_scalers_and_config(self, scaler_path: str):
        """ìŠ¤ì¼€ì¼ëŸ¬ì™€ ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        print("ğŸ”„ ìŠ¤ì¼€ì¼ëŸ¬ ë° ì„¤ì • ë¡œë”©...")
        with open(os.path.join(scaler_path, 'feature_scaler.pkl'), 'rb') as f:
            self.feature_scaler = pickle.load(f)
        with open(os.path.join(scaler_path, 'target_scaler.pkl'), 'rb') as f:
            self.target_scaler = pickle.load(f)
        with open(os.path.join(scaler_path, 'config.json'), 'r') as f:
            config = json.load(f)
            self.seq_len = config['seq_len']
            self.pred_len = config['pred_len']
            self.feature_columns = config['feature_columns']
        
        print(f"  âœ… ì„¤ì •: {self.seq_len}ë¶„ ì‹œí€€ìŠ¤ â†’ {self.pred_len}ë¶„ í›„ ì˜ˆì¸¡")

    def _load_all_models(self, model_dir: str) -> Dict[str, tf.keras.Model]:
        """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  Keras ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ“ ëª¨ë¸ ë¡œë”© (ê²½ë¡œ: {model_dir})")
        models = {}
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.keras')]
        
        for model_file in model_files:
            model_name = model_file.replace('.keras', '')
            model_path = os.path.join(model_dir, model_file)
            try:
                models[model_name] = tf.keras.models.load_model(model_path, safe_mode=False)
                print(f"  âœ… {model_name} ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"ì´ {len(models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return models

    def _get_boosting_config(self) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ… ê·œì¹™ ì„¤ì •"""
        return {
            'default': [
                {'threshold': 500, 'multiply': 1.15, 'min_value': 1750},
                {'threshold': 450, 'multiply': 1.10, 'min_value': 1700},
                {'threshold': 400, 'multiply': 1.05, 'min_value': 1650},
            ],
            'ExtremeNet': [
                {'threshold': 500, 'multiply': 1.30, 'min_value': 1750},
                {'threshold': 450, 'multiply': 1.20, 'min_value': 1700},
                {'threshold': 400, 'multiply': 1.10, 'min_value': 1650},
                {'threshold': 0,   'multiply': 1.05, 'min_value': 0},
            ],
            'SpikeDetector': [
                {'threshold': 500, 'multiply': 1.0, 'min_value': 1750},
                {'threshold': 450, 'multiply': 1.0, 'min_value': 1700},
                {'threshold': 400, 'multiply': 1.0, 'min_value': 1650},
            ],
            'GoldenRule': [ # SpikeDetectorì™€ ë™ì¼í•œ ê·œì¹™ ì ìš©
                {'threshold': 500, 'multiply': 1.0, 'min_value': 1750},
                {'threshold': 450, 'multiply': 1.0, 'min_value': 1700},
                {'threshold': 400, 'multiply': 1.0, 'min_value': 1650},
            ],
            'golden_pattern': {'m14b_thr': 300, 'm14a_thr': 80, 'multiply': 1.08}
        }

    def _get_ensemble_weights(self) -> Dict[str, Any]:
        """M14B ê°’ì— ë”°ë¥¸ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì„¤ì •"""
        return {
            'high': { # M14B > 500
                'weights': {'SpikeDetector': 0.35, 'GoldenRule': 0.30, 'PatchTST': 0.15, 'StableLSTM': 0.10, 'ExtremeNet': 0.10},
                'min_value': 1750
            },
            'medium': { # M14B > 450
                'weights': {'SpikeDetector': 0.30, 'GoldenRule': 0.25, 'PatchTST': 0.20, 'StableLSTM': 0.15, 'ExtremeNet': 0.10},
                'min_value': 1700
            },
            'low': { # M14B > 400
                'weights': {'PatchTST': 0.30, 'StableLSTM': 0.25, 'ExtremeNet': 0.20, 'SpikeDetector': 0.15, 'GoldenRule': 0.10},
                 'min_value': 1650
            },
            'default': { # ê·¸ ì™¸
                'weights': {'PatchTST': 0.30, 'StableLSTM': 0.25, 'ExtremeNet': 0.20, 'SpikeDetector': 0.15, 'GoldenRule': 0.10},
                'min_value': 0
            }
        }

    def _load_and_prepare_data(self, filepath: str) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬, íŠ¹ì„± ìƒì„±"""
        print(f"\nğŸ“Š ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ (íŒŒì¼: {filepath})")
        df = pd.read_csv(filepath)
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        # íŠ¹ì„± ìƒì„± (Vectorized)
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        df['HOUR'] = df['CURRTIME'].dt.hour
        df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
        df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
        
        for w in [10, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
        
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)

        print(f"  âœ… ë°ì´í„° {len(df)}í–‰ ì¤€ë¹„ ì™„ë£Œ")
        high_count = (df['TOTALCNT'] >= 1700).sum()
        print(f"  - ê³ ê°’(1700 ì´ìƒ) ìƒ˜í”Œ: {high_count}ê°œ ({high_count/len(df)*100:.1f}%)")
        
        return df

    def _create_sequences(self, df: pd.DataFrame) -> Tuple:
        """ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œë¶€í„° ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
        print("â³ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì¤‘...")
        
        # NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ì†ë„ í–¥ìƒ
        feature_array = df[self.feature_columns].values
        
        # íš¨ìœ¨ì ì¸ ì‹œí€€ìŠ¤ ìƒì„±ì„ ìœ„í•œ stride_tricks ì‚¬ìš©
        shape = (feature_array.shape[0] - self.seq_len - self.pred_len + 1, self.seq_len, feature_array.shape[1])
        strides = (feature_array.strides[0], feature_array.strides[0], feature_array.strides[1])
        sequences = np.lib.stride_tricks.as_strided(feature_array, shape=shape, strides=strides)

        # ì˜ˆì¸¡ ëŒ€ìƒì´ ë˜ëŠ” ë°ì´í„°(y) ë° ë©”íƒ€ ì •ë³´ ì¶”ì¶œ
        indices = np.arange(self.seq_len, len(df) - self.pred_len + 1)
        
        meta_df = df.iloc[indices - 1].copy() # ê° ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ í–‰ ì •ë³´
        meta_df['target_time'] = meta_df['CURRTIME'] + timedelta(minutes=self.pred_len)
        meta_df['actual'] = df['TOTALCNT'].iloc[indices + self.pred_len - 1].values
        
        print(f"  âœ… ì´ {len(sequences)}ê°œì˜ í‰ê°€ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ")
        return sequences, meta_df.reset_index(drop=True)

    def _predict_in_batches(self, sequences: np.ndarray, batch_size: int = 256) -> Dict[str, np.ndarray]:
        """ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰"""
        print(f"ğŸ”® ëª¨ë¸ ì˜ˆì¸¡ ì‹œì‘ (ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {batch_size})")
        
        scaled_sequences = self.feature_scaler.transform(sequences.reshape(-1, sequences.shape[-1]))
        scaled_sequences = scaled_sequences.reshape(sequences.shape)
        
        all_predictions = {}
        for model_name, model in self.models.items():
            print(f"  - {model_name} ì˜ˆì¸¡ ì¤‘...")
            raw_preds = model.predict(scaled_sequences, batch_size=batch_size, verbose=0)
            if isinstance(raw_preds, list): # Multi-output ëª¨ë¸ ì²˜ë¦¬
                raw_preds = raw_preds[0]
            
            # ì—­ìŠ¤ì¼€ì¼ë§
            unscaled_preds = self.target_scaler.inverse_transform(raw_preds)
            all_predictions[model_name] = unscaled_preds.flatten()
            
        print("  âœ… ëª¨ë“  ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ")
        return all_predictions

    def _apply_boosting_and_ensemble(self, predictions_df: pd.DataFrame) -> pd.DataFrame:
        """ë²¡í„°í™” ì—°ì‚°ìœ¼ë¡œ ë¶€ìŠ¤íŒ… ë° ì•™ìƒë¸” ì ìš©"""
        print("ğŸ› ï¸ ë¶€ìŠ¤íŒ… ë° ì•™ìƒë¸” ê³„ì‚° ì¤‘...")
        df = predictions_df.copy()
        
        # 1. ëª¨ë¸ë³„ ë¶€ìŠ¤íŒ… ì ìš©
        for model_name in self.models.keys():
            preds = df[f'{model_name}_ì›ë³¸'].values
            boosted_preds = preds.copy()
            
            rules = self.boosting_config.get(model_name, self.boosting_config['default'])
            
            for rule in rules:
                mask = (df['M14AM14B'] > rule['threshold'])
                boosted_preds[mask] = np.maximum(preds[mask] * rule['multiply'], rule['min_value'])
            
            # í™©ê¸ˆ íŒ¨í„´ ë¶€ìŠ¤íŒ…
            gp_rule = self.boosting_config['golden_pattern']
            golden_mask = (df['M14AM14B'] > gp_rule['m14b_thr']) & (df['M14AM10A'] < gp_rule['m14a_thr'])
            boosted_preds[golden_mask] *= gp_rule['multiply']
            
            df[f'{model_name}_ì˜ˆì¸¡'] = boosted_preds

        # 2. ê·¹ë‹¨ê°’ ì•™ìƒë¸” ê³„ì‚°
        ensemble_preds = np.zeros(len(df))
        
        # M14B ê°’ì— ë”°ë¼ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ ì ìš©
        conditions = {
            'high': df['M14AM14B'] > 500,
            'medium': (df['M14AM14B'] > 450) & (df['M14AM14B'] <= 500),
            'low': (df['M14AM14B'] > 400) & (df['M14AM14B'] <= 450)
        }
        
        # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì ìš© (default)
        weights = self.ensemble_weights['default']['weights']
        weighted_sum = sum(df[f'{name}_ì˜ˆì¸¡'] * w for name, w in weights.items() if f'{name}_ì˜ˆì¸¡' in df)
        total_weight = sum(weights.values())
        ensemble_preds = weighted_sum / total_weight

        # ì¡°ê±´ë³„ ê°€ì¤‘ì¹˜ ë° ìµœì†Œê°’ ì¬ê³„ì‚°
        for level, mask in conditions.items():
            config = self.ensemble_weights[level]
            weights = config['weights']
            
            weighted_sum = sum(df.loc[mask, f'{name}_ì˜ˆì¸¡'] * w for name, w in weights.items() if f'{name}_ì˜ˆì¸¡' in df)
            total_weight = sum(weights.values())
            
            level_ensemble = weighted_sum / total_weight
            level_ensemble = np.maximum(level_ensemble, config['min_value'])
            ensemble_preds[mask] = level_ensemble

        df['ê·¹ë‹¨ì•™ìƒë¸”_ì˜ˆì¸¡'] = ensemble_preds
        print("  âœ… ê³„ì‚° ì™„ë£Œ")
        return df

    def _analyze_and_save_results(self, final_df: pd.DataFrame):
        """ê²°ê³¼ ë¶„ì„, ì¶œë ¥ ë° CSV ì €ì¥"""
        print("\n" + "="*80)
        print("ğŸ“Š ìµœì¢… ì„±ëŠ¥ ë¶„ì„")
        print("="*80)

        models_to_eval = list(self.models.keys()) + ['ê·¹ë‹¨ì•™ìƒë¸”']
        actuals = final_df['actual'].values
        
        # ì „ì²´ ì„±ëŠ¥
        print(f"\n[ì „ì²´ ì„±ëŠ¥]")
        print(f"{'ëª¨ë¸':<15} {'MAE':>8} {'RMSE':>8} {'MAPE(%)':>8} {'ì •í™•ë„(%)':>10}")
        print("-" * 55)
        
        for model_name in models_to_eval:
            preds = final_df[f'{model_name}_ì˜ˆì¸¡'].values
            mae = mean_absolute_error(actuals, preds)
            rmse = np.sqrt(mean_squared_error(actuals, preds))
            mape = np.mean(np.abs((actuals - preds) / actuals)) * 100
            
            if model_name == 'ê·¹ë‹¨ì•™ìƒë¸”':
                print(f"{'ğŸ”¥ ' + model_name:<15} {mae:8.2f} {rmse:8.2f} {mape:8.2f} {100-mape:10.2f} â­")
            else:
                print(f"{model_name:<15} {mae:8.2f} {rmse:8.2f} {mape:8.2f} {100-mape:10.2f}")

        # ê³ ê°’ êµ¬ê°„ ë¶„ì„
        high_mask = final_df['actual'] >= 1700
        if high_mask.any():
            high_df = final_df[high_mask]
            print(f"\n[ê³ ê°’(1700+) êµ¬ê°„ ë¶„ì„] - {len(high_df)}ê°œ ìƒ˜í”Œ")
            print("-" * 75)
            
            for model_name in models_to_eval:
                preds = high_df[f'{model_name}_ì˜ˆì¸¡'].values
                hit_count = (preds >= 1700).sum()
                
                if model_name != 'ê·¹ë‹¨ì•™ìƒë¸”':
                    orig_preds = high_df[f'{model_name}_ì›ë³¸'].values
                    orig_hit_count = (orig_preds >= 1700).sum()
                    improvement = hit_count - orig_hit_count
                    print(f"{model_name:15}: ì›ë³¸ {orig_hit_count:2d} â†’ ë¶€ìŠ¤íŒ… {hit_count:2d}/{len(high_df)} "
                          f"({hit_count/len(high_df)*100:.1f}%) [ê°œì„  +{improvement}]")
                else:
                    print(f"{'ğŸ”¥ ' + model_name:15}: {hit_count:2d}/{len(high_df)} "
                          f"({hit_count/len(high_df)*100:.1f}%)")

            # ìƒì„¸ ìƒ˜í”Œ ì¶œë ¥
            print(f"\n[ê·¹ë‹¨ì•™ìƒë¸” ê³ ê°’ ì˜ˆì¸¡ ìƒì„¸ (ìƒìœ„ 15ê°œ)]")
            print(f"{'ì‹¤ì œ':>6} {'ì˜ˆì¸¡':>6} {'M14B':>5} {'M14A':>5} {'ì˜¤ì°¨':>6} {'ê²°ê³¼':>8}")
            print("-" * 45)
            
            for _, row in high_df.head(15).iterrows():
                actual, pred, m14b, m14a = row['actual'], row['ê·¹ë‹¨ì•™ìƒë¸”_ì˜ˆì¸¡'], row['M14AM14B'], row['M14AM10A']
                error = pred - actual
                result = "âœ… HIT" if pred >= 1700 else "âš ï¸ NEAR" if pred >= 1650 else "âŒ MISS"
                print(f"{actual:6.0f} {pred:6.0f} {m14b:5.0f} {m14a:5.0f} {error:+6.0f} {result:>8}")

        # CSV ì €ì¥
        output_file = f'v69_predictions_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        final_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")

    def run_evaluation(self, test_filepath: str):
        """ì „ì²´ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        if not self.models:
            print("âŒ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ì–´ í‰ê°€ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        df = self._load_and_prepare_data(test_filepath)
        
        # 2. ì‹œí€€ìŠ¤ ìƒì„±
        sequences, meta_df = self._create_sequences(df)
        if len(sequences) == 0:
            print("âŒ í‰ê°€í•  ì‹œí€€ìŠ¤ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return

        # 3. ëª¨ë¸ ì˜ˆì¸¡ (ë°°ì¹˜)
        predictions = self._predict_in_batches(sequences)
        
        # 4. ê²°ê³¼ í†µí•©
        # ì›ë³¸ ì˜ˆì¸¡ê°’ ì €ì¥
        preds_df = pd.DataFrame({f"{name}_ì›ë³¸": preds for name, preds in predictions.items()})
        # ë©”íƒ€ ì •ë³´ì™€ ê²°í•©
        results_df = pd.concat([meta_df, preds_df], axis=1)

        # 5. ë¶€ìŠ¤íŒ… ë° ì•™ìƒë¸” ì ìš©
        final_df = self._apply_boosting_and_ensemble(results_df)

        # 6. ìµœì¢… ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
        self._analyze_and_save_results(final_df)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\nğŸš€ V6.9 ê·¹ë‹¨ê°’ ê°œì„  í‰ê°€ ì‹œì‘")
    
    # í‰ê°€ê¸° ìƒì„±
    evaluator = OptimizedModelEvaluator(scaler_path='scalers/', model_dir='models/')
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ íƒìƒ‰
    test_files = [
        'data/20250731_to20250806.csv',
        'data/test_data.csv',
        '/mnt/user-data/uploads/test.csv'
    ]
    
    test_file_path = next((f for f in test_files if os.path.exists(f)), None)
    
    if not test_file_path:
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # í‰ê°€ ì‹¤í–‰
    evaluator.run_evaluation(test_file_path)
    
    print("\nâœ… ëª¨ë“  í‰ê°€ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
