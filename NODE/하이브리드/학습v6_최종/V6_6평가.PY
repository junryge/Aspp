"""
ğŸ“Š ì ì‘í˜• ë”¥ëŸ¬ë‹ ì•™ìƒë¸” í‰ê°€ ì‹œìŠ¤í…œ V3.0
- ëª¨ë“  ëª¨ë¸ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ Excel ì‹œíŠ¸ì— í†µí•©
- ì˜ˆì¸¡ì‹œì , ì˜ˆì¸¡ëŒ€ìƒì‹œê°„ í¬í•¨
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, RobustScaler
import pickle
import json
import os
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')
tf.keras.config.enable_unsafe_deserialization()

print("=" * 80)
print("ğŸš€ ì ì‘í˜• í‰ê°€ ì‹œìŠ¤í…œ V3.0")
print("  - í†µí•© Excel ì¶œë ¥ (ëª¨ë“  ëª¨ë¸ í•œ ì‹œíŠ¸)")
print("  - ì˜ˆì¸¡ì‹œì /ì˜ˆì¸¡ëŒ€ìƒì‹œê°„ í¬í•¨")
print("  - ì‹œí€€ìŠ¤ 100ê°œ â†’ 10ë¶„ í›„ ì˜ˆì¸¡")
print("=" * 80)
print(f"\nTensorFlow Version: {tf.__version__}")
print(f"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}")

# ====================== ì‹œí€€ìŠ¤ ê¸°ë°˜ ë°ì´í„° íƒ€ì… ê°ì§€ê¸° ======================
class SequenceBasedDetector:
    """100ê°œ ì‹œí€€ìŠ¤ë¡œ ë°ì´í„° íƒ€ì… íŒë³„"""
    
    def __init__(self):
        self.seq_len = 100
        self.pred_len = 10
        
    def analyze_sequence(self, sequence_data):
        """100ê°œ ì‹œí€€ìŠ¤ ë¶„ì„"""
        
        if len(sequence_data) < 100:
            print(f"âš ï¸ ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶€ì¡±: {len(sequence_data)}ê°œ")
            return None
            
        seq = sequence_data[-100:]
        
        analysis = {
            'mean': np.mean(seq['TOTALCNT']),
            'std': np.std(seq['TOTALCNT']),
            'max': np.max(seq['TOTALCNT']),
            'min': np.min(seq['TOTALCNT']),
            'noise_level': np.std(seq['TOTALCNT']) / np.mean(seq['TOTALCNT']) if np.mean(seq['TOTALCNT']) > 0 else 0,
        }
        
        if 'M14AM14B' in seq.columns:
            analysis['corr_m14b'] = seq['TOTALCNT'].corr(seq['M14AM14B'])
        
        if 'M14AM10A' in seq.columns:
            analysis['corr_m14a'] = seq['TOTALCNT'].corr(seq['M14AM10A'])
        
        return analysis
    
    def detect_data_type(self, sequence_df):
        """ë°ì´í„° íƒ€ì… íŒë³„"""
        
        analysis = self.analyze_sequence(sequence_df)
        if not analysis:
            return 'unknown', {}
        
        print("\nğŸ” ì‹œí€€ìŠ¤ ë¶„ì„ ê²°ê³¼:")
        print(f"  í‰ê· : {analysis['mean']:.0f}")
        print(f"  í‘œì¤€í¸ì°¨: {analysis['std']:.0f}")
        print(f"  ë…¸ì´ì¦ˆ: {analysis['noise_level']:.3f}")
        print(f"  M14B ìƒê´€: {analysis.get('corr_m14b', 0):.3f}")
        
        score_uu1 = 0
        score_uu2 = 0
        
        if analysis.get('corr_m14b', 0) > 0.8:
            score_uu1 += 3
        elif analysis.get('corr_m14b', 0) < 0.6:
            score_uu2 += 3
        
        if analysis['noise_level'] < 0.15:
            score_uu1 += 2
        elif analysis['noise_level'] > 0.18:
            score_uu2 += 2
        
        print(f"\nğŸ“Š íŒë³„ ì ìˆ˜: uu1={score_uu1}, uu2={score_uu2}")
        
        if score_uu1 > score_uu2:
            data_type = 'uu1'
            print("  âœ… íŒë³„: uu1 íƒ€ì… (ê³ ìƒê´€, ì €ë…¸ì´ì¦ˆ)")
        elif score_uu2 > score_uu1:
            data_type = 'uu2'
            print("  âœ… íŒë³„: uu2 íƒ€ì… (ì €ìƒê´€, ê³ ë…¸ì´ì¦ˆ)")
        else:
            data_type = 'balanced'
            print("  âœ… íŒë³„: ê· í˜• íƒ€ì…")
        
        return data_type, analysis

# ====================== ì ì‘í˜• ê·¹ë‹¨ê°’ ë¶€ìŠ¤í„° ======================
class AdaptiveExtremeBooster:
    """ë°ì´í„° íƒ€ì…ë³„ ì ì‘í˜• ë¶€ìŠ¤íŒ…"""
    
    def __init__(self, data_type='unknown'):
        self.data_type = data_type
        print(f"\nğŸ”¥ ë¶€ìŠ¤í„° ì´ˆê¸°í™” ({data_type} íƒ€ì…)")
        
        if data_type == 'uu1':
            self.spike_threshold_b = 300
            self.spike_threshold_a = 70
            self.spike_ratio = 1.3
            self.extreme_threshold = 1651
        elif data_type == 'uu2':
            self.spike_threshold_b = 350
            self.spike_threshold_a = 65
            self.spike_ratio = 1.4
            self.extreme_threshold = 1700
        else:
            self.spike_threshold_b = 325
            self.spike_threshold_a = 67
            self.spike_ratio = 1.35
            self.extreme_threshold = 1675
    
    def analyze_sequence_for_boost(self, sequence_data):
        """ë¶€ìŠ¤íŒ…ìš© ì‹œí€€ìŠ¤ ë¶„ì„"""
        
        seq_max = np.max(sequence_data)
        seq_min = np.min(sequence_data)
        seq_mean = np.mean(sequence_data[-30:]) if len(sequence_data) >= 30 else np.mean(sequence_data)
        
        is_extreme = seq_max >= self.extreme_threshold
        
        if len(sequence_data) >= 10:
            recent_10 = sequence_data[-10:]
            trend_change = recent_10[-1] - recent_10[0]
            
            if trend_change > 50:
                trend = 'strong_rising'
            elif trend_change > 20:
                trend = 'rising'
            elif trend_change < -50:
                trend = 'strong_falling'
            elif trend_change < -20:
                trend = 'falling'
            else:
                trend = 'stable'
        else:
            trend = 'stable'
        
        return {
            'max': seq_max,
            'min': seq_min,
            'mean': seq_mean,
            'trend': trend,
            'is_extreme': is_extreme
        }
    
    def boost_prediction(self, pred, m14b, m14a, model_name, seq_info):
        """ë°ì´í„° íƒ€ì…ë³„ ë¶€ìŠ¤íŒ…"""
        
        boosted = pred
        
        if model_name == 'ExtremeNet':
            if self.data_type == 'uu1':
                if seq_info['max'] >= 1651 and seq_info['trend'] in ['rising', 'strong_rising']:
                    if m14b > 300:
                        boosted = max(pred * 1.25, 1700)
                    else:
                        boosted = max(pred * 1.20, 1680)
                elif seq_info['is_extreme']:
                    boosted = max(pred * 1.15, seq_info['max'])
            else:
                if seq_info['is_extreme'] and seq_info['trend'] in ['rising', 'strong_rising']:
                    boosted = max(pred * 1.22, seq_info['max'] * 1.05)
        
        elif model_name == 'SpikeDetector':
            if m14b > self.spike_threshold_b and m14a < self.spike_threshold_a:
                boosted = max(pred * self.spike_ratio, seq_info['max'] * 1.05)
            elif seq_info['is_extreme']:
                boosted = max(pred * 1.10, seq_info['max'])
        
        elif model_name == 'GoldenRule':
            if self.data_type == 'uu1' and m14b > 300 and m14a < 70:
                boosted = max(pred * 1.30, seq_info['max'] * 1.05)
            elif self.data_type == 'uu2' and m14b > 350 and m14a < 65:
                boosted = max(pred * 1.40, seq_info['max'] * 1.08)
            elif seq_info['is_extreme']:
                boosted = max(pred * 1.08, seq_info['max'])
        
        return boosted

# ====================== í†µí•© í‰ê°€ ì‹œìŠ¤í…œ ======================
class UnifiedEvaluator:
    """ë‹¨ì¼ CSV íŒŒì¼ ì ì‘í˜• í‰ê°€ - í†µí•© ì¶œë ¥"""
    
    def __init__(self, scaler_path='scalers/', model_path='models/'):
        self.scaler_path = scaler_path
        self.model_path = model_path
        self.seq_len = 100
        self.pred_len = 10
        self.models = {}
        self.detector = SequenceBasedDetector()
        self.booster = None
        self.data_type = None
        self.feature_columns = None
        self.scaler_loaded = False
        
        print("\nğŸ“ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        self.load_scalers()
        self.load_models()
    
    def load_scalers(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        try:
            with open(f'{self.scaler_path}feature_scaler.pkl', 'rb') as f:
                self.feature_scaler = pickle.load(f)
            with open(f'{self.scaler_path}target_scaler.pkl', 'rb') as f:
                self.target_scaler = pickle.load(f)
            
            config_file = f'{self.scaler_path}config.json'
            if os.path.exists(config_file):
                with open(config_file, 'r') as f:
                    config = json.load(f)
                    self.feature_columns = config.get('feature_columns')
                    self.seq_len = config.get('seq_len', 100)
                    self.pred_len = config.get('pred_len', 10)
                    print(f"  âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
                    print(f"  âœ… Feature columns: {len(self.feature_columns)}ê°œ")
            else:
                print("  âš ï¸ config.json ì—†ìŒ")
                self.scaler_loaded = False
                return
            
            self.scaler_loaded = True
            
        except Exception as e:
            print(f"  âš ï¸ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨: {str(e)[:50]}")
            self.scaler_loaded = False
    
    def load_models(self):
        """ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ë§Œ ë¡œë“œ"""
        print("\nğŸ“‚ ëª¨ë¸ ë¡œë”©...")
        
        if not os.path.exists(self.model_path):
            print(f"  âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.model_path}")
            return
        
        model_files = [f for f in os.listdir(self.model_path) if f.endswith('.keras')]
        
        if not model_files:
            print("  âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        loaded_count = 0
        for model_file in model_files:
            model_name = model_file.replace('.keras', '')
            full_path = os.path.join(self.model_path, model_file)
            
            try:
                self.models[model_name] = tf.keras.models.load_model(
                    full_path, safe_mode=False
                )
                print(f"  âœ… {model_name} ë¡œë“œ")
                loaded_count += 1
            except Exception as e:
                print(f"  âŒ {model_name} ì‹¤íŒ¨: {str(e)[:50]}")
        
        print(f"\nì´ {loaded_count}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def create_features(self, df):
        """í•„ìˆ˜ íŠ¹ì§• ìƒì„±"""
        # ì‹œê°„ íŠ¹ì§•
        if 'CURRTIME' in df.columns:
            try:
                # ì´ë¯¸ datetimeì¸ì§€ í™•ì¸
                if not pd.api.types.is_datetime64_any_dtype(df['CURRTIME']):
                    df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                                   format='%Y%m%d%H%M', errors='coerce')
                df['HOUR'] = df['CURRTIME'].dt.hour
                df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
                df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
            except:
                df['HOUR'] = 12
                df['HOUR_SIN'] = np.sin(2 * np.pi * 12 / 24)
                df['HOUR_COS'] = np.cos(2 * np.pi * 12 / 24)
        else:
            df['HOUR'] = 12
            df['HOUR_SIN'] = np.sin(2 * np.pi * 12 / 24)
            df['HOUR_COS'] = np.cos(2 * np.pi * 12 / 24)
        
        # ë¹„ìœ¨ íŠ¹ì§•
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        
        # ì´ë™í‰ê· 
        for w in [10, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
        
        # ë³€í™”ëŸ‰
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        
        return df
    
    def load_and_prepare_data(self, csv_path):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print(f"\nğŸ“Š ë°ì´í„° ë¡œë“œ: {csv_path}")
        
        df = pd.read_csv(csv_path)
        print(f"  ì›ë³¸: {len(df)}í–‰")
        
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        
        # CURRTIME ì²˜ë¦¬
        if 'CURRTIME' in df.columns:
            df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                           format='%Y%m%d%H%M', errors='coerce')
            df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        # í•„ìˆ˜ íŠ¹ì§• ìƒì„±
        df = self.create_features(df)
        
        print(f"  ìœ íš¨: {len(df)}í–‰")
        print(f"  ì»¬ëŸ¼: {len(df.columns)}ê°œ")
        
        return df
    
    def evaluate(self, csv_path):
        """í†µí•© í‰ê°€ ì‹¤í–‰ - í•˜ë‚˜ì˜ Excelë¡œ ì¶œë ¥"""
        
        # 1. ë°ì´í„° ë¡œë“œ
        df = self.load_and_prepare_data(csv_path)
        
        if len(df) < self.seq_len + self.pred_len:
            print(f"âŒ ë°ì´í„° ë¶€ì¡±: ìµœì†Œ {self.seq_len + self.pred_len}í–‰ í•„ìš”")
            return None, None
        
        if not self.models:
            print("âŒ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            return None, None
        
        if not self.scaler_loaded:
            print("âŒ ìŠ¤ì¼€ì¼ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            return None, None
        
        # 2. ë°ì´í„° íƒ€ì… íŒë³„
        first_seq = df.iloc[:self.seq_len]
        self.data_type, seq_analysis = self.detector.detect_data_type(first_seq)
        
        # 3. ë¶€ìŠ¤í„° ì„¤ì •
        self.booster = AdaptiveExtremeBooster(self.data_type)
        
        # 4. ì˜ˆì¸¡ ì¤€ë¹„
        print(f"\nğŸ”® ì˜ˆì¸¡ ì‹œì‘...")
        print(f"  ì‹œí€€ìŠ¤: {self.seq_len}ê°œ â†’ ì˜ˆì¸¡: {self.pred_len}ë¶„ í›„")
        
        start_idx = self.seq_len
        end_idx = len(df) - self.pred_len
        total_predictions = min(end_idx - start_idx, 3000)  # ìµœëŒ€ 3000ê°œ
        
        print(f"  ì˜ˆì¸¡ ê°œìˆ˜: {total_predictions}ê°œ")
        
        # 5. ê¸°ë³¸ ë°ì´í„° ìˆ˜ì§‘
        print("\nğŸ“ˆ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        
        # í†µí•© ê²°ê³¼ DataFrame ìƒì„±
        results = pd.DataFrame()
        
        # ë‚ ì§œ/ì‹œê°„ ì •ë³´
        pred_times = []  # ì˜ˆì¸¡ ì‹œì 
        target_times = []  # ì˜ˆì¸¡ ëŒ€ìƒ ì‹œê°„ (10ë¶„ í›„)
        actuals = []
        m14b_values = []
        m14a_values = []
        seq_maxes = []
        seq_trends = []
        
        for i in range(start_idx, start_idx + total_predictions):
            if i + self.pred_len >= len(df):
                break
            
            # ì˜ˆì¸¡ ì‹œì  (í˜„ì¬)
            if 'CURRTIME' in df.columns:
                pred_time = df.iloc[i]['CURRTIME']
                target_time = pred_time + timedelta(minutes=self.pred_len)
                pred_times.append(pred_time)
                target_times.append(target_time)
            else:
                pred_times.append(i)
                target_times.append(i + self.pred_len)
            
            # ì‹¤ì œê°’ (10ë¶„ í›„)
            actuals.append(df.iloc[i + self.pred_len]['TOTALCNT'])
            m14b_values.append(df.iloc[i]['M14AM14B'])
            m14a_values.append(df.iloc[i]['M14AM10A'])
            
            # ì‹œí€€ìŠ¤ ë¶„ì„
            seq_data = df.iloc[i-self.seq_len:i]
            seq_info = self.booster.analyze_sequence_for_boost(seq_data['TOTALCNT'].values)
            seq_maxes.append(seq_info['max'])
            seq_trends.append(seq_info['trend'])
        
        # ê¸°ë³¸ ì •ë³´ ì €ì¥
        results['ì˜ˆì¸¡ì‹œì '] = pred_times
        results['ì˜ˆì¸¡ëŒ€ìƒì‹œê°„'] = target_times
        
        # ë‚ ì§œì™€ ì‹œê°„ ë¶„ë¦¬
        if 'CURRTIME' in df.columns:
            results['ì˜ˆì¸¡ì‹œì _ë‚ ì§œ'] = pd.to_datetime(results['ì˜ˆì¸¡ì‹œì ']).dt.strftime('%Y-%m-%d')
            results['ì˜ˆì¸¡ì‹œì _ì‹œê°„'] = pd.to_datetime(results['ì˜ˆì¸¡ì‹œì ']).dt.strftime('%H:%M')
            results['ì˜ˆì¸¡ëŒ€ìƒ_ë‚ ì§œ'] = pd.to_datetime(results['ì˜ˆì¸¡ëŒ€ìƒì‹œê°„']).dt.strftime('%Y-%m-%d')
            results['ì˜ˆì¸¡ëŒ€ìƒ_ì‹œê°„'] = pd.to_datetime(results['ì˜ˆì¸¡ëŒ€ìƒì‹œê°„']).dt.strftime('%H:%M')
        
        results['ì‹¤ì œê°’'] = actuals
        results['M14AM14B'] = m14b_values
        results['M14AM10A'] = m14a_values
        results['ì‹œí€€ìŠ¤_MAX'] = seq_maxes
        results['ì‹œí€€ìŠ¤_ì¶”ì„¸'] = seq_trends
        
        # 6. ëª¨ë¸ë³„ ì˜ˆì¸¡
        print("\nğŸ¯ ëª¨ë¸ë³„ ì˜ˆì¸¡...")
        model_predictions = {}
        
        for model_name, model in self.models.items():
            print(f"  {model_name} ì˜ˆì¸¡ ì¤‘...")
            predictions = []
            
            try:
                # ë°°ì¹˜ ì˜ˆì¸¡
                batch_size = 32
                for batch_start in range(0, len(actuals), batch_size):
                    batch_end = min(batch_start + batch_size, len(actuals))
                    batch_X = []
                    
                    for j in range(batch_start, batch_end):
                        idx = start_idx + j
                        
                        # ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
                        seq_data = df.iloc[idx-self.seq_len:idx][self.feature_columns].values
                        
                        # ìŠ¤ì¼€ì¼ë§
                        seq_scaled = self.feature_scaler.transform(seq_data)
                        batch_X.append(seq_scaled)
                    
                    if len(batch_X) == 0:
                        continue
                    
                    batch_X = np.array(batch_X)
                    
                    # ì˜ˆì¸¡
                    batch_pred = model.predict(batch_X, verbose=0)
                    
                    # ì—­ë³€í™˜
                    if isinstance(batch_pred, list):
                        batch_pred = batch_pred[0]
                    
                    batch_pred = batch_pred.reshape(-1, 1)
                    batch_pred_original = self.target_scaler.inverse_transform(batch_pred)
                    
                    predictions.extend(batch_pred_original.flatten())
                    
                    if len(predictions) % 500 == 0:
                        print(f"    {len(predictions)}/{len(actuals)} ì™„ë£Œ")
                
                # ë¶€ìŠ¤íŒ… ì ìš©
                boosted_preds = []
                for j in range(len(predictions)):
                    seq_info = {
                        'max': seq_maxes[j],
                        'trend': seq_trends[j],
                        'is_extreme': seq_maxes[j] >= self.booster.extreme_threshold
                    }
                    
                    boosted = self.booster.boost_prediction(
                        predictions[j],
                        m14b_values[j],
                        m14a_values[j],
                        model_name,
                        seq_info
                    )
                    boosted_preds.append(boosted)
                
                # ê²°ê³¼ ì €ì¥
                results[f'{model_name}_ì˜ˆì¸¡'] = [round(p) for p in boosted_preds]
                results[f'{model_name}_ì˜¤ì°¨'] = results[f'{model_name}_ì˜ˆì¸¡'] - results['ì‹¤ì œê°’']
                results[f'{model_name}_ì˜¤ì°¨ìœ¨(%)'] = round(abs(results[f'{model_name}_ì˜¤ì°¨']) / results['ì‹¤ì œê°’'] * 100, 2)
                
                # ExtremeNet ë²”ìœ„ê°’
                if model_name == 'ExtremeNet':
                    ranges = []
                    for j in range(len(predictions)):
                        ì›ë³¸ = predictions[j]
                        seq_max = seq_maxes[j]
                        
                        if ì›ë³¸ > 0:
                            ìµœì†Œê°’ = ì›ë³¸
                            ìµœëŒ€ê°’ = seq_max * 1.1 if seq_max > ì›ë³¸ else ì›ë³¸ * 1.1
                            ranges.append(f"{int(ìµœì†Œê°’)}~{int(ìµœëŒ€ê°’)}")
                        else:
                            ranges.append("0~0")
                    
                    results['ExtremeNet_ë²”ìœ„ê°’'] = ranges
                
                model_predictions[model_name] = boosted_preds
                print(f"    âœ… {model_name} ì™„ë£Œ")
                
            except Exception as e:
                print(f"    âŒ {model_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)[:100]}")
                continue
        
        # 7. ì•™ìƒë¸” ìƒì„±
        print("\nğŸ”¥ ì•™ìƒë¸” ìƒì„±...")
        
        available_models = list(model_predictions.keys())
        
        if len(available_models) > 0:
            # ë°ì´í„° íƒ€ì…ë³„ ê°€ì¤‘ì¹˜ ì„¤ì •
            if self.data_type == 'uu1':
                default_weights = {
                    'ExtremeNet': 0.25,
                    'SpikeDetector': 0.20,
                    'GoldenRule': 0.20,
                    'PatchTST': 0.20,
                    'StableLSTM': 0.15
                }
            else:
                default_weights = {
                    'ExtremeNet': 0.20,
                    'SpikeDetector': 0.18,
                    'GoldenRule': 0.18,
                    'PatchTST': 0.22,
                    'StableLSTM': 0.22
                }
            
            # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ì¬ê³„ì‚°
            total_weight = sum(default_weights.get(m, 0.2) for m in available_models)
            actual_weights = {m: default_weights.get(m, 0.2) / total_weight for m in available_models}
            
            ensemble_preds = []
            for i in range(len(actuals)):
                ensemble = 0
                for model_name in available_models:
                    ensemble += model_predictions[model_name][i] * actual_weights[model_name]
                ensemble_preds.append(ensemble)
            
            results['ì•™ìƒë¸”_ì˜ˆì¸¡'] = [round(p) for p in ensemble_preds]
            results['ì•™ìƒë¸”_ì˜¤ì°¨'] = results['ì•™ìƒë¸”_ì˜ˆì¸¡'] - results['ì‹¤ì œê°’']
            results['ì•™ìƒë¸”_ì˜¤ì°¨ìœ¨(%)'] = round(abs(results['ì•™ìƒë¸”_ì˜¤ì°¨']) / results['ì‹¤ì œê°’'] * 100, 2)
            
            print(f"  âœ… {len(available_models)}ê°œ ëª¨ë¸ ì•™ìƒë¸” ìƒì„±")
            print(f"     ê°€ì¤‘ì¹˜: {', '.join([f'{k}={v:.2f}' for k, v in actual_weights.items()])}")
        
        # 8. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        print("\nğŸ“Š ì„±ëŠ¥ í‰ê°€...")
        metrics = pd.DataFrame()
        
        for col in results.columns:
            if '_ì˜ˆì¸¡' in col and '_ë²”ìœ„ê°’' not in col:
                model_name = col.replace('_ì˜ˆì¸¡', '')
                mae = mean_absolute_error(results['ì‹¤ì œê°’'], results[col])
                rmse = np.sqrt(mean_squared_error(results['ì‹¤ì œê°’'], results[col]))
                mape = np.mean(abs((results['ì‹¤ì œê°’'] - results[col]) / results['ì‹¤ì œê°’'])) * 100
                
                metrics = pd.concat([metrics, pd.DataFrame({
                    'ëª¨ë¸': [model_name],
                    'MAE': [mae],
                    'RMSE': [rmse],
                    'MAPE(%)': [mape],
                    'ì •í™•ë„(%)': [100 - mape]
                })], ignore_index=True)
        
        # ì„±ëŠ¥ìˆœ ì •ë ¬
        metrics = metrics.sort_values('ì •í™•ë„(%)', ascending=False).reset_index(drop=True)
        
        # 9. Excel ì €ì¥ (í•˜ë‚˜ì˜ íŒŒì¼, ì—¬ëŸ¬ ì‹œíŠ¸)
        print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Excel Writer ìƒì„±
        output_file = f'í†µí•©í‰ê°€ê²°ê³¼_{self.data_type}_{timestamp}.xlsx'
        
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # 1. ë©”ì¸ ê²°ê³¼ ì‹œíŠ¸ (ëª¨ë“  ëª¨ë¸ ì˜ˆì¸¡ê°’)
            results.to_excel(writer, sheet_name='ì „ì²´_ì˜ˆì¸¡ê²°ê³¼', index=False)
            
            # 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì‹œíŠ¸
            metrics.to_excel(writer, sheet_name='ëª¨ë¸ë³„_ì„±ëŠ¥', index=False)
            
            # 3. ìš”ì•½ í†µê³„ ì‹œíŠ¸
            summary = pd.DataFrame({
                'í•­ëª©': ['ë°ì´í„° íƒ€ì…', 'í‰ê°€ ë°ì´í„° ìˆ˜', 'ì˜ˆì¸¡ ê¸°ê°„', 'ìµœê³  ì •í™•ë„ ëª¨ë¸', 'ìµœê³  ì •í™•ë„'],
                'ê°’': [
                    self.data_type,
                    len(results),
                    f"{self.seq_len}ë¶„ â†’ {self.pred_len}ë¶„ í›„",
                    metrics.iloc[0]['ëª¨ë¸'] if len(metrics) > 0 else 'N/A',
                    f"{metrics.iloc[0]['ì •í™•ë„(%)']:.2f}%" if len(metrics) > 0 else 'N/A'
                ]
            })
            summary.to_excel(writer, sheet_name='ìš”ì•½', index=False)
            
            # 4. ê³ ê°’ ì˜ˆì¸¡ ë¶„ì„ (1700 ì´ìƒ)
            if 'ì‹¤ì œê°’' in results.columns:
                high_value_mask = results['ì‹¤ì œê°’'] >= 1700
                if high_value_mask.sum() > 0:
                    high_value_analysis = results[high_value_mask].copy()
                    high_value_analysis.to_excel(writer, sheet_name='ê³ ê°’ì˜ˆì¸¡ë¶„ì„', index=False)
            
            # Excel ìŠ¤íƒ€ì¼ë§
            for sheet_name in writer.sheets:
                worksheet = writer.sheets[sheet_name]
                # í—¤ë” ìŠ¤íƒ€ì¼
                for cell in worksheet[1]:
                    cell.font = cell.font.copy(bold=True)
                # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 30)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
        
        print(f"  âœ… {output_file} ì €ì¥ ì™„ë£Œ")
        
        # ê°„ë‹¨í•œ CSVë„ ì €ì¥ (í˜¸í™˜ì„±)
        simple_output = f'í‰ê°€ê²°ê³¼_{self.data_type}_{timestamp}.csv'
        results.to_csv(simple_output, index=False, encoding='utf-8-sig')
        print(f"  âœ… {simple_output} ì €ì¥ ì™„ë£Œ (CSV ë°±ì—…)")
        
        # 10. ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ“Š í‰ê°€ ê²°ê³¼")
        print("="*80)
        print(f"ë°ì´í„° íƒ€ì…: {self.data_type}")
        print(f"í‰ê°€ ë°ì´í„°: {len(results)}ê°œ")
        print(f"ì˜ˆì¸¡ ê¸°ê°„: {self.seq_len}ë¶„ ì‹œí€€ìŠ¤ â†’ {self.pred_len}ë¶„ í›„ ì˜ˆì¸¡")
        
        if len(metrics) > 0:
            print(f"\n{'ëª¨ë¸':<15} {'MAE':>10} {'RMSE':>10} {'MAPE(%)':>10} {'ì •í™•ë„(%)':>10}")
            print("-"*60)
            
            for _, row in metrics.iterrows():
                print(f"{row['ëª¨ë¸']:<15} {row['MAE']:10.2f} {row['RMSE']:10.2f} "
                      f"{row['MAPE(%)']:10.2f} {row['ì •í™•ë„(%)']:10.2f}")
        
        print(f"\nğŸ“ ì €ì¥ íŒŒì¼:")
        print(f"  1. {output_file} (Excel - ëª¨ë“  ê²°ê³¼)")
        print(f"  2. {simple_output} (CSV ë°±ì—…)")
        
        return results, metrics

# ====================== ë©”ì¸ ì‹¤í–‰ ======================
def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    print("\n" + "="*80)
    print("ğŸš€ í†µí•© í‰ê°€ ì‹œìŠ¤í…œ ì‹œì‘!")
    print("="*80)
    
    evaluator = UnifiedEvaluator()
    
    test_files = [
        'data/M14_20250916_20250918.csv',
        'test_data.csv',
        'uu.csv',
        'uu2.csv'
    ]
    
    test_file = None
    for file in test_files:
        if os.path.exists(file):
            test_file = file
            print(f"âœ… í‰ê°€ íŒŒì¼: {file}")
            break
    
    if not test_file:
        print("âŒ í‰ê°€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    results, metrics = evaluator.evaluate(test_file)
    
    if results is not None:
        print("\nâœ… í‰ê°€ ì™„ë£Œ!")
        print("\nğŸ’¡ Excel íŒŒì¼ ì‹œíŠ¸ êµ¬ì„±:")
        print("  â€¢ ì „ì²´_ì˜ˆì¸¡ê²°ê³¼: ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ ì˜¤ì°¨")
        print("  â€¢ ëª¨ë¸ë³„_ì„±ëŠ¥: MAE, RMSE, MAPE, ì •í™•ë„")
        print("  â€¢ ìš”ì•½: í‰ê°€ ìš”ì•½ ì •ë³´")
        print("  â€¢ ê³ ê°’ì˜ˆì¸¡ë¶„ì„: 1700 ì´ìƒ êµ¬ê°„ ë¶„ì„")
    else:
        print("\nâŒ í‰ê°€ ì‹¤íŒ¨!")

if __name__ == "__main__":
    main()