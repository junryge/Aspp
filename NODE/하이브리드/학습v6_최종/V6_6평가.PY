"""
ğŸ“Š V6.8 ìµœì¢… ê°œì„ íŒ - UU1/UU2 íŒ¨í„´ ê°ì§€ + 1700+ ì˜ˆì¸¡ í†µí•©
========================================================
í•µì‹¬ ê°œì„ :
1. UU1/UU2 ìë™ íŒ¨í„´ ê°ì§€
2. 1700+ ê¸‰ì¦ ì˜ˆì¸¡ ë¡œì§
3. ì‹œí€€ìŠ¤ ì¶”ì„¸ ê¸°ë°˜ ë™ì  ì•™ìƒë¸”
4. M14B ê°’ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ì¡°ê±´ ìµœì í™”
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import json
import os
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')
tf.keras.config.enable_unsafe_deserialization()

# ====================== UU1/UU2 íŒ¨í„´ ê°ì§€ê¸° ======================
class DataPatternDetector:
    """ë°ì´í„° íŒ¨í„´ ìë™ ê°ì§€ (UU1 vs UU2)"""
    
    def detect_pattern(self, df):
        """
        UU1: ê¸‰ì¦ ì˜ˆì¸¡ í•„ìš” (M14B ë‚®ìŒ, 1700+ ê°€ëŠ¥)
        UU2: ê³ ê°’ ìœ ì§€ ìƒíƒœ (M14B ë†’ìŒ, ì´ë¯¸ í•œê³„)
        """
        
        # 1682+ ì¼€ì´ìŠ¤ ë¶„ì„
        high_cases = df[df['TOTALCNT'] >= 1682]
        
        if len(high_cases) == 0:
            # 1682ê°€ ì—†ìœ¼ë©´ M14B í‰ê· ìœ¼ë¡œ íŒë‹¨
            m14b_mean = df['M14AM14B'].mean()
            if m14b_mean > 250:
                print(f"  ğŸ”¥ UU2 íŒ¨í„´: M14B í‰ê·  {m14b_mean:.0f} (ê³ ê°’ ìƒíƒœ)")
                return "UU2"
            else:
                print(f"  ğŸ“ˆ UU1 íŒ¨í„´: M14B í‰ê·  {m14b_mean:.0f} (ê¸‰ì¦ ê°€ëŠ¥)")
                return "UU1"
        
        # 1682+ ì¼€ì´ìŠ¤ì˜ M14B ë¶„ì„
        high_m14b_mean = high_cases['M14AM14B'].mean()
        
        # M14B > 350 ë¹„ìœ¨ ê³„ì‚°
        m14b_high_ratio = (df['M14AM14B'] > 350).sum() / len(df)
        
        print(f"\nğŸ“Š íŒ¨í„´ ë¶„ì„:")
        print(f"  1682+ ì¼€ì´ìŠ¤: {len(high_cases)}ê°œ")
        print(f"  1682+ M14B í‰ê· : {high_m14b_mean:.0f}")
        print(f"  ì „ì²´ M14B>350 ë¹„ìœ¨: {m14b_high_ratio:.1%}")
        
        # íŒ¨í„´ íŒë³„
        if high_m14b_mean > 400:  # 1682+ì—ì„œ M14Bê°€ 400 ì´ìƒ
            print(f"  ğŸ”¥ UU2 íŒ¨í„´ ê°ì§€: ê³ ê°’ ìœ ì§€ ìƒíƒœ")
            return "UU2"
        elif m14b_high_ratio > 0.2:  # M14B>350ì´ 20% ì´ìƒ
            print(f"  ğŸ”¥ UU2 íŒ¨í„´ ê°ì§€: M14B ë†’ì€ ë¹„ìœ¨")
            return "UU2"
        else:
            print(f"  ğŸ“ˆ UU1 íŒ¨í„´ ê°ì§€: ê¸‰ì¦ ì˜ˆì¸¡ í•„ìš”")
            return "UU1"

# ====================== 1700+ ì˜ˆì¸¡ê¸° ======================
class SurgePredictor:
    """1700+ ê¸‰ì¦ ì˜ˆì¸¡ ì „ë¬¸ í´ë˜ìŠ¤"""
    
    def check_surge_conditions(self, seq_max, seq_trend, m14b, m14a, consecutive_rises):
        """
        1700+ ê¸‰ì¦ ê°€ëŠ¥ì„± ì²´í¬
        Returns: (surge_probable, confidence_level)
        """
        
        # í™©ê¸ˆ ì¡°ê±´ 1: ì‹œí€€ìŠ¤ MAX 1650-1682 & ìƒìŠ¹ ì¶”ì„¸
        if 1650 <= seq_max < 1682 and seq_trend == 'increasing':
            if m14b >= 200:  # UU1 íŒ¨í„´ì˜ ì ì • M14B
                return True, 'HIGH'
        
        # í™©ê¸ˆ ì¡°ê±´ 2: í˜„ì¬ê°’ 1680 ê·¼ì²˜ & M14B ìƒìŠ¹
        if 1670 <= seq_max <= 1690:
            if m14b >= 220 and consecutive_rises >= 5:
                return True, 'HIGH'
        
        # í™©ê¸ˆ ì¡°ê±´ 3: ì—°ì† ìƒìŠ¹ 10íšŒ ì´ìƒ
        if consecutive_rises >= 10 and seq_max >= 1600:
            return True, 'MEDIUM'
        
        # í™©ê¸ˆ íŒ¨í„´: M14B/M14A ë¹„ìœ¨
        if m14b > 200 and m14a < 80:
            ratio = m14b / (m14a + 1)
            if ratio > 3 and seq_max >= 1600:
                return True, 'MEDIUM'
        
        return False, 'LOW'

# ====================== ê°œì„ ëœ ê·¹ë‹¨ê°’ ë³´ì • í´ë˜ìŠ¤ ======================
class ImprovedExtremeValueBooster:
    """ì‹œí€€ìŠ¤ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ê·¹ë‹¨ê°’ ì˜ˆì¸¡"""
    
    def __init__(self):
        print("ğŸ”¥ V6.8 ê·¹ë‹¨ê°’ ë¶€ìŠ¤í„° ì´ˆê¸°í™”")
        print("  - UU1/UU2 íŒ¨í„´ë³„ ì°¨ë³„í™”")
        print("  - 1700+ ê¸‰ì¦ ì˜ˆì¸¡ ê°•í™”")
        print("  - M14B ê¸°ë°˜ ë™ì  ë¶€ìŠ¤íŒ…")
        self.data_pattern = "UU1"
        self.surge_predictor = SurgePredictor()
        
    def set_data_pattern(self, pattern):
        """ë°ì´í„° íŒ¨í„´ ì„¤ì •"""
        self.data_pattern = pattern
        
    def analyze_sequence(self, sequence_data):
        """ì‹œí€€ìŠ¤ ë¶„ì„: maxê°’, ì¶”ì„¸, ì—°ì† ìƒìŠ¹/í•˜ë½ ì •ë„ ê³„ì‚°"""
        if len(sequence_data) == 0:
            return None
        
        # ê¸°ë³¸ í†µê³„
        seq_max = np.max(sequence_data)
        seq_min = np.min(sequence_data)
        seq_mean = np.mean(sequence_data[-30:]) if len(sequence_data) >= 30 else np.mean(sequence_data)
        
        # ê³ í‰ì› ìƒíƒœ ì²´í¬
        is_high_plateau = seq_mean >= 1700
        
        # ì—°ì† ìƒìŠ¹ ì¹´ìš´íŠ¸
        consecutive_rises = 0
        for i in range(len(sequence_data)-1, 0, -1):
            if sequence_data[i] > sequence_data[i-1]:
                consecutive_rises += 1
            else:
                break
        
        # ì—°ì† í•˜ë½ ì¹´ìš´íŠ¸
        consecutive_falls = 0
        for i in range(len(sequence_data)-1, 0, -1):
            if sequence_data[i] < sequence_data[i-1]:
                consecutive_falls += 1
            else:
                break
        
        # ìƒìŠ¹/í•˜ë½ ê°•ë„
        rise_strength = 0
        fall_strength = 0
        if len(sequence_data) >= 10:
            recent_10 = sequence_data[-10:]
            change = recent_10[-1] - recent_10[0]
            if change > 0:
                rise_strength = change
            else:
                fall_strength = abs(change)
        
        # ì¶”ì„¸ ë¶„ì„
        if len(sequence_data) >= 30:
            recent = sequence_data[-30:]
            x = np.arange(len(recent))
            coeffs = np.polyfit(x, recent, 1)
            slope = coeffs[0]
            
            if is_high_plateau:
                if consecutive_rises >= 10 and rise_strength > 50:
                    trend = 'extreme_rising'
                elif consecutive_falls >= 10 and fall_strength > 50:
                    trend = 'extreme_falling'
                elif slope > 1 or consecutive_rises >= 5:
                    trend = 'high_increasing'
                elif slope < -1 or consecutive_falls >= 5:
                    trend = 'high_decreasing'
                else:
                    trend = 'high_stable'
            else:
                if consecutive_rises >= 10 and rise_strength > 50:
                    trend = 'strong_rising'
                elif consecutive_falls >= 10 and fall_strength > 50:
                    trend = 'strong_falling'
                elif consecutive_rises >= 7 and rise_strength > 30:
                    trend = 'rapid_increasing'
                elif consecutive_falls >= 7 and fall_strength > 30:
                    trend = 'rapid_decreasing'
                elif slope > 2:
                    trend = 'increasing'
                elif slope < -2:
                    trend = 'decreasing'
                else:
                    trend = 'stable'
        else:
            trend = 'stable'
        
        # ë³€ë™ì„±
        volatility = np.std(sequence_data[-10:]) if len(sequence_data) >= 10 else 0
        
        return {
            'max': seq_max,
            'min': seq_min,
            'mean': seq_mean,
            'trend': trend,
            'is_high_plateau': is_high_plateau,
            'consecutive_rises': consecutive_rises,
            'consecutive_falls': consecutive_falls,
            'rise_strength': rise_strength,
            'fall_strength': fall_strength,
            'volatility': volatility,
            'slope': slope if len(sequence_data) >= 30 else 0
        }
    
    def boost_prediction(self, pred, m14b_value, m14a_value=None, model_name=None, 
                        sequence_info=None):
        """ê°œì„ ëœ ë¶€ìŠ¤íŒ… ë¡œì§"""
        if not sequence_info:
            return pred
            
        seq_max = sequence_info.get('max', 0)
        seq_trend = sequence_info.get('trend', 'stable')
        consecutive_rises = sequence_info.get('consecutive_rises', 0)
        
        # 1700+ ê¸‰ì¦ ê°€ëŠ¥ì„± ì²´í¬
        surge_probable, confidence = self.surge_predictor.check_surge_conditions(
            seq_max, seq_trend, m14b_value, m14a_value, consecutive_rises
        )
        
        boosted = pred
        
        # ========== UU2 íŒ¨í„´ (ê³ ê°’ ìœ ì§€) ==========
        if self.data_pattern == "UU2":
            if model_name == 'ExtremeNet':
                # UU2ëŠ” ë³´ìˆ˜ì  ë¶€ìŠ¤íŒ…
                if seq_max >= 1680 and m14b_value > 450:
                    boosted = min(max(pred * 1.1, 1680), 1850)
                else:
                    boosted = pred
                    
            elif model_name in ['SpikeDetector', 'GoldenRule']:
                if m14b_value > 450:
                    boosted = max(pred * 1.03, 1650)
                else:
                    boosted = pred
            
            return boosted
        
        # ========== UU1 íŒ¨í„´ (ê¸‰ì¦ ì˜ˆì¸¡) ==========
        if model_name == 'ExtremeNet':
            
            # 1700+ ê¸‰ì¦ ì˜ˆì¸¡ ì¡°ê±´
            if surge_probable and confidence == 'HIGH':
                # ê°•ë ¥í•œ ë¶€ìŠ¤íŒ…
                if m14b_value > 250:
                    boosted = max(pred * 1.4, 1700)
                elif m14b_value > 220:
                    boosted = max(pred * 1.35, 1700)
                elif m14b_value > 200:
                    boosted = max(pred * 1.3, 1700)
                else:
                    boosted = max(pred * 1.25, 1680)
                print(f"    ğŸš€ ExtremeNet 1700+ ê¸‰ì¦ ë¶€ìŠ¤íŒ…! (confidence={confidence})")
            
            # ì¼ë°˜ ìƒìŠ¹ ì¡°ê±´ (1651+ & increasing)
            elif seq_max >= 1651 and seq_trend == 'increasing':
                if m14b_value > 200:
                    boosted = max(pred * 1.2, 1680)
                else:
                    boosted = max(pred * 1.15, 1650)
            
            # ì—°ì† ìƒìŠ¹ ë³´ë„ˆìŠ¤
            elif consecutive_rises >= 10:
                boosted = max(pred * 1.15, 1650)
            
            # ê·¸ ì™¸ëŠ” ì›ë³¸ ì‚¬ìš©
            else:
                boosted = pred
                
        # ========== SpikeDetector, GoldenRule ==========
        elif model_name in ['SpikeDetector', 'GoldenRule']:
            if surge_probable:
                boosted = max(pred * 1.15, 1700)
            elif seq_max >= 1651 and seq_trend == 'increasing':
                boosted = max(pred * 1.1, 1680)
            else:
                boosted = pred
                
        # ========== ê¸°íƒ€ ëª¨ë¸ ==========
        else:
            if surge_probable:
                boosted = max(pred * 1.1, 1680)
            elif seq_max >= 1651 and seq_trend == 'increasing':
                boosted = max(pred * 1.05, 1650)
            else:
                boosted = pred
        
        # ìƒí•œì„  ì ìš©
        if boosted > 2000:
            boosted = 2000
        
        return boosted

# ====================== ë©”ì¸ í‰ê°€ í´ë˜ìŠ¤ ======================
class ImprovedModelEvaluator:
    def __init__(self, scaler_path='scalers/'):
        """í‰ê°€ê¸° ì´ˆê¸°í™”"""
        print("="*80)
        print("ğŸ”¥ V6.8 ìµœì¢… ê°œì„ íŒ")
        print("  - UU1/UU2 ìë™ ê°ì§€")
        print("  - 1700+ ê¸‰ì¦ ì˜ˆì¸¡")
        print("  - ì¶”ì„¸ ê¸°ë°˜ ë™ì  ì•™ìƒë¸”")
        print("="*80)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        try:
            with open(f'{scaler_path}feature_scaler.pkl', 'rb') as f:
                self.feature_scaler = pickle.load(f)
            with open(f'{scaler_path}target_scaler.pkl', 'rb') as f:
                self.target_scaler = pickle.load(f)
            with open(f'{scaler_path}config.json', 'r') as f:
                config = json.load(f)
                self.seq_len = config['seq_len']
                self.pred_len = config['pred_len']
                self.feature_columns = config['feature_columns']
            print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
        except:
            print(f"âš ï¸ ìŠ¤ì¼€ì¼ëŸ¬ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
            self.seq_len = 100
            self.pred_len = 10
            self.feature_columns = ['TOTALCNT', 'M14AM14B', 'M14AM10A', 'M14AM14BSUM', 'M14AM16']
            self.feature_scaler = None
            self.target_scaler = None
        
        self.models = {}
        self.extreme_booster = ImprovedExtremeValueBooster()
        self.pattern_detector = DataPatternDetector()
        
    def load_all_models(self, model_dir='models/'):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        print(f"\nğŸ“ ëª¨ë¸ ë¡œë”©...")
        
        if not os.path.exists(model_dir):
            print(f"  âš ï¸ ëª¨ë¸ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {}
        
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.keras')]
        
        for model_file in model_files:
            model_name = model_file.replace('.keras', '')
            model_path = os.path.join(model_dir, model_file)
            
            try:
                self.models[model_name] = tf.keras.models.load_model(
                    model_path, safe_mode=False
                )
                print(f"  âœ… {model_name} ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"\nì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return self.models
    
    def load_test_data(self, filepath):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ + íŒ¨í„´ ê°ì§€"""
        print(f"\nğŸ“‚ í‰ê°€ ë°ì´í„° ë¡œë”©: {filepath}")
        df = pd.read_csv(filepath)
        print(f"  ì›ë³¸: {df.shape[0]:,}í–‰")
        
        # 0ê°’ ì œê±°
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        
        # ì‹œê°„ ë³€í™˜
        if 'CURRTIME' in df.columns:
            try:
                df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                               format='%Y%m%d%H%M', errors='coerce')
                df = df.sort_values('CURRTIME').reset_index(drop=True)
            except:
                pass
        
        print(f"  ìœ íš¨: {df.shape[0]:,}í–‰")
        
        # íŒ¨í„´ ìë™ ê°ì§€
        data_pattern = self.pattern_detector.detect_pattern(df)
        self.extreme_booster.set_data_pattern(data_pattern)
        
        # ê³ ê°’ í†µê³„
        high_count = (df['TOTALCNT'] >= 1700).sum()
        very_high_count = (df['TOTALCNT'] >= 1750).sum()
        extreme_count = (df['TOTALCNT'] >= 1800).sum()
        max_1682_count = (df['TOTALCNT'] >= 1682).sum()
        
        print(f"\nğŸ¯ ê³ ê°’ êµ¬ê°„ ë¶„í¬:")
        print(f"  1682+: {max_1682_count}ê°œ ({max_1682_count/len(df)*100:.1f}%)")
        print(f"  1700+: {high_count}ê°œ ({high_count/len(df)*100:.1f}%)")
        print(f"  1750+: {very_high_count}ê°œ ({very_high_count/len(df)*100:.1f}%)")
        print(f"  1800+: {extreme_count}ê°œ ({extreme_count/len(df)*100:.1f}%)")
        
        # M14B ë¶„í¬
        m14b_mean = df['M14AM14B'].mean()
        m14b_high = (df['M14AM14B'] > 350).sum()
        print(f"\nğŸ“Š M14AM14B ë¶„í¬:")
        print(f"  í‰ê· : {m14b_mean:.0f}")
        print(f"  350+: {m14b_high}ê°œ ({m14b_high/len(df)*100:.1f}%)")
        
        return df
    
    def create_features(self, df):
        """íŠ¹ì„± ìƒì„±"""
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 200) & (df['M14AM10A'] < 80)).astype(float)
        
        if 'CURRTIME' in df.columns:
            try:
                df['HOUR'] = df['CURRTIME'].dt.hour
                df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
                df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
            except:
                pass
        
        for w in [10, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
        
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        
        return df
    
    def evaluate_all_models(self, test_file):
        """ëª¨ë¸ í‰ê°€ ì‹¤í–‰"""
        
        # ë°ì´í„° ë¡œë“œ
        df = self.load_test_data(test_file)
        df = self.create_features(df)
        
        # ì˜ˆì¸¡ ê°€ëŠ¥ ë²”ìœ„
        start_idx = self.seq_len
        end_idx = len(df) - self.pred_len
        total = end_idx - start_idx
        
        if total <= 0:
            print("âŒ ì˜ˆì¸¡í•  ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            return None, None
        
        print(f"\nğŸ”® ì˜ˆì¸¡ ì‹œì‘...")
        print(f"  ì‹œí€€ìŠ¤: {self.seq_len}ë¶„ â†’ ì˜ˆì¸¡: {self.pred_len}ë¶„ í›„")
        print(f"  ì˜ˆì¸¡ ê°œìˆ˜: {total:,}ê°œ")
        print(f"  íŒ¨í„´ ëª¨ë“œ: {self.extreme_booster.data_pattern}")
        
        # ëª¨ë“  ì˜ˆì¸¡ì„ ì €ì¥í•  DataFrame ì¤€ë¹„
        all_predictions = pd.DataFrame()
        
        # ì‹œê°„ ë° íŠ¹ì§• ì •ë³´ ìˆ˜ì§‘
        timestamps_pred = []
        timestamps_target = []
        actuals = []
        m14b_values = []
        m14a_values = []
        sequence_maxes = []
        sequence_trends = []
        consecutive_rises_list = []
        consecutive_falls_list = []
        surge_predictions = []
        
        print("\nğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ë° ì‹œí€€ìŠ¤ ë¶„ì„ ì¤‘...")
        for i in range(start_idx, end_idx):
            if 'CURRTIME' in df.columns:
                pred_time = df.iloc[i]['CURRTIME']
                target_time = pred_time + timedelta(minutes=self.pred_len) if pd.notna(pred_time) else None
                timestamps_pred.append(pred_time)
                timestamps_target.append(target_time)
            
            actual_idx = i + self.pred_len
            if actual_idx < len(df):
                actuals.append(df.iloc[actual_idx]['TOTALCNT'])
                m14b_values.append(df.iloc[i]['M14AM14B'])
                m14a_values.append(df.iloc[i]['M14AM10A'])
                
                # ì‹œí€€ìŠ¤ ë¶„ì„
                seq_data = df.iloc[i-self.seq_len:i]['TOTALCNT'].values
                seq_info = self.extreme_booster.analyze_sequence(seq_data)
                
                if seq_info:
                    sequence_maxes.append(seq_info.get('max'))
                    sequence_trends.append(seq_info.get('trend'))
                    consecutive_rises_list.append(seq_info.get('consecutive_rises', 0))
                    consecutive_falls_list.append(seq_info.get('consecutive_falls', 0))
                    
                    # 1700+ ê¸‰ì¦ ì˜ˆì¸¡
                    surge_probable, confidence = self.extreme_booster.surge_predictor.check_surge_conditions(
                        seq_info.get('max'),
                        seq_info.get('trend'),
                        df.iloc[i]['M14AM14B'],
                        df.iloc[i]['M14AM10A'],
                        seq_info.get('consecutive_rises', 0)
                    )
                    surge_predictions.append(1 if surge_probable else 0)
                else:
                    sequence_maxes.append(0)
                    sequence_trends.append('stable')
                    consecutive_rises_list.append(0)
                    consecutive_falls_list.append(0)
                    surge_predictions.append(0)
        
        # ê¸°ë³¸ ì •ë³´ ì €ì¥
        if timestamps_pred and timestamps_pred[0] is not None:
            all_predictions['ì˜ˆì¸¡ì‹œì '] = [t.strftime('%Y-%m-%d %H:%M') if pd.notna(t) else '' for t in timestamps_pred]
            all_predictions['ì˜ˆì¸¡ëŒ€ìƒì‹œê°„'] = [t.strftime('%Y-%m-%d %H:%M') if pd.notna(t) else '' for t in timestamps_target]
        
        all_predictions['ì‹¤ì œê°’'] = actuals
        all_predictions['M14AM14B'] = m14b_values
        all_predictions['M14AM10A'] = m14a_values
        all_predictions['ì‹œí€€ìŠ¤_MAX'] = sequence_maxes
        all_predictions['ì‹œí€€ìŠ¤_ì¶”ì„¸'] = sequence_trends
        all_predictions['ì—°ì†ìƒìŠ¹'] = consecutive_rises_list
        all_predictions['ì—°ì†í•˜ë½'] = consecutive_falls_list
        all_predictions['ê¸‰ì¦ì˜ˆì¸¡'] = surge_predictions
        all_predictions['ë°ì´í„°íŒ¨í„´'] = self.extreme_booster.data_pattern
        
        print(f"  ì˜ˆì¸¡í•  ë°ì´í„°: {len(all_predictions)}ê°œ")
        
        # 1700+ ê¸‰ì¦ ì˜ˆì¸¡ í†µê³„
        surge_count = sum(surge_predictions)
        actual_1700 = (np.array(actuals) >= 1700).sum()
        
        print(f"\nğŸš€ 1700+ ê¸‰ì¦ ì˜ˆì¸¡:")
        print(f"  ê¸‰ì¦ ì˜ˆì¸¡: {surge_count}ê°œ ({surge_count/len(all_predictions)*100:.1f}%)")
        print(f"  ì‹¤ì œ 1700+: {actual_1700}ê°œ ({actual_1700/len(all_predictions)*100:.1f}%)")
        
        # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë”ë¯¸ ì˜ˆì¸¡
        if not self.models:
            print("\nâš ï¸ ì‹¤ì œ ëª¨ë¸ì´ ì—†ì–´ ë”ë¯¸ ì˜ˆì¸¡ì„ ì‹¤í–‰í•©ë‹ˆë‹¤")
            
            for model_name in ['ExtremeNet', 'SpikeDetector', 'GoldenRule', 'PatchTST', 'StableLSTM']:
                print(f"  {model_name} ë”ë¯¸ ì˜ˆì¸¡...")
                dummy_preds = []
                boosted_preds = []
                
                for i in range(len(all_predictions)):
                    dummy_pred = sequence_maxes[i] * 0.98 + np.random.randn() * 20
                    
                    seq_info = {
                        'max': sequence_maxes[i],
                        'trend': sequence_trends[i],
                        'consecutive_rises': consecutive_rises_list[i],
                        'consecutive_falls': consecutive_falls_list[i]
                    }
                    
                    boosted = self.extreme_booster.boost_prediction(
                        dummy_pred, 
                        m14b_values[i], 
                        m14a_values[i], 
                        model_name,
                        sequence_info=seq_info
                    )
                    
                    dummy_preds.append(dummy_pred)
                    boosted_preds.append(boosted)
                
                all_predictions[f'{model_name}_ì›ë³¸'] = [round(p) for p in dummy_preds]
                all_predictions[f'{model_name}_ì˜ˆì¸¡'] = [round(p) for p in boosted_preds]
                all_predictions[f'{model_name}_ì˜¤ì°¨'] = all_predictions[f'{model_name}_ì˜ˆì¸¡'] - all_predictions['ì‹¤ì œê°’']
                all_predictions[f'{model_name}_ì˜¤ì°¨ìœ¨(%)'] = round(
                    abs(all_predictions[f'{model_name}_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’'] * 100, 2
                )
        
        else:
            # ì‹¤ì œ ëª¨ë¸ë¡œ ì˜ˆì¸¡
            model_metrics = {}
            
            for model_name, model in self.models.items():
                print(f"\nğŸ¯ {model_name} ì˜ˆì¸¡ ì¤‘...")
                predictions = []
                
                # ë°°ì¹˜ ì˜ˆì¸¡
                batch_size = 500
                for i in range(start_idx, end_idx, batch_size):
                    batch_end = min(i + batch_size, end_idx)
                    
                    X_batch = []
                    for j in range(i, batch_end):
                        seq_data = df.iloc[j-self.seq_len:j][self.feature_columns].values
                        X_batch.append(seq_data)
                    
                    if len(X_batch) == 0:
                        continue
                    
                    X_batch = np.array(X_batch)
                    if self.feature_scaler:
                        X_batch_scaled = []
                        for seq in X_batch:
                            seq_scaled = self.feature_scaler.transform(seq)
                            X_batch_scaled.append(seq_scaled)
                        X_batch_scaled = np.array(X_batch_scaled)
                    else:
                        X_batch_scaled = X_batch
                    
                    preds = model.predict(X_batch_scaled, verbose=0)
                    
                    if isinstance(preds, list):
                        y_pred_scaled = preds[0].flatten()
                    else:
                        y_pred_scaled = preds.flatten()
                    
                    if self.target_scaler:
                        y_pred = self.target_scaler.inverse_transform(
                            y_pred_scaled.reshape(-1, 1)).flatten()
                    else:
                        y_pred = y_pred_scaled
                    
                    for k in range(len(y_pred)):
                        actual_idx = i - start_idx + k
                        if actual_idx < len(all_predictions):
                            predictions.append(y_pred[k])
                
                predictions = predictions[:len(all_predictions)]
                all_predictions[f'{model_name}_ì›ë³¸'] = [round(p) for p in predictions]
                
                # ë¶€ìŠ¤íŒ… ì ìš©
                print(f"  ğŸ”¥ {model_name} ë¶€ìŠ¤íŒ… ì ìš© ì¤‘...")
                boosted_predictions = []
                boost_applied_count = 0
                
                for i in range(len(predictions)):
                    seq_info = {
                        'max': all_predictions.iloc[i]['ì‹œí€€ìŠ¤_MAX'],
                        'trend': all_predictions.iloc[i]['ì‹œí€€ìŠ¤_ì¶”ì„¸'],
                        'consecutive_rises': all_predictions.iloc[i]['ì—°ì†ìƒìŠ¹'],
                        'consecutive_falls': all_predictions.iloc[i]['ì—°ì†í•˜ë½']
                    }
                    
                    boosted = self.extreme_booster.boost_prediction(
                        predictions[i],
                        all_predictions.iloc[i]['M14AM14B'],
                        all_predictions.iloc[i]['M14AM10A'],
                        model_name,
                        sequence_info=seq_info
                    )
                    
                    if model_name == 'ExtremeNet' and boosted != predictions[i]:
                        boost_applied_count += 1
                        
                    boosted_predictions.append(boosted)
                
                if model_name == 'ExtremeNet':
                    print(f"    ExtremeNet ë¶€ìŠ¤íŒ… ì ìš©: {boost_applied_count}ê°œ "
                          f"({boost_applied_count/len(predictions)*100:.1f}%)")
                
                all_predictions[f'{model_name}_ì˜ˆì¸¡'] = [round(p) for p in boosted_predictions]
                all_predictions[f'{model_name}_ì˜¤ì°¨'] = all_predictions[f'{model_name}_ì˜ˆì¸¡'] - all_predictions['ì‹¤ì œê°’']
                all_predictions[f'{model_name}_ì˜¤ì°¨ìœ¨(%)'] = round(
                    abs(all_predictions[f'{model_name}_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’'] * 100, 2
                )
                
                # ì„±ëŠ¥ ê³„ì‚°
                mae = mean_absolute_error(all_predictions['ì‹¤ì œê°’'], boosted_predictions)
                rmse = np.sqrt(mean_squared_error(all_predictions['ì‹¤ì œê°’'], boosted_predictions))
                r2 = r2_score(all_predictions['ì‹¤ì œê°’'], boosted_predictions)
                mape = np.mean(abs(all_predictions[f'{model_name}_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’']) * 100
                
                model_metrics[model_name] = {
                    'MAE': mae,
                    'RMSE': rmse,
                    'R2': r2,
                    'MAPE': mape,
                    'ì •í™•ë„(%)': 100 - mape
                }
                
                print(f"  âœ… {model_name} ì™„ë£Œ: MAE={mae:.2f}, RÂ²={r2:.4f}, ì •í™•ë„={100-mape:.2f}%")
        
        # ì¶”ì„¸ ê¸°ë°˜ ë™ì  ì•™ìƒë¸”
        print("\nğŸ”¥ ìµœì¢… ì•™ìƒë¸” ìƒì„±...")
        
        final_ensemble = []
        for i in range(len(all_predictions)):
            seq_trend = all_predictions.iloc[i]['ì‹œí€€ìŠ¤_ì¶”ì„¸']
            consecutive_rises = all_predictions.iloc[i]['ì—°ì†ìƒìŠ¹']
            surge_pred = all_predictions.iloc[i]['ê¸‰ì¦ì˜ˆì¸¡']
            m14b = all_predictions.iloc[i]['M14AM14B']
            
            # ê° ëª¨ë¸ ì˜ˆì¸¡ê°’
            preds = {}
            for model_name in ['ExtremeNet', 'SpikeDetector', 'GoldenRule', 'PatchTST', 'StableLSTM']:
                if f'{model_name}_ì˜ˆì¸¡' in all_predictions.columns:
                    preds[model_name] = all_predictions.iloc[i][f'{model_name}_ì˜ˆì¸¡']
            
            # ê¸‰ì¦ ì˜ˆì¸¡ ì‹œ ê°€ì¤‘ì¹˜
            if surge_pred == 1:
                weights = {
                    'ExtremeNet': 0.35,
                    'SpikeDetector': 0.30,
                    'GoldenRule': 0.20,
                    'PatchTST': 0.10,
                    'StableLSTM': 0.05
                }
            # ì—°ì† ìƒìŠ¹
            elif consecutive_rises >= 10:
                weights = {
                    'ExtremeNet': 0.30,
                    'SpikeDetector': 0.25,
                    'GoldenRule': 0.25,
                    'PatchTST': 0.10,
                    'StableLSTM': 0.10
                }
            # ì¼ë°˜ ìƒìŠ¹
            elif 'increasing' in seq_trend:
                weights = {
                    'ExtremeNet': 0.22,
                    'SpikeDetector': 0.22,
                    'GoldenRule': 0.22,
                    'PatchTST': 0.17,
                    'StableLSTM': 0.17
                }
            # í•˜ë½
            elif 'decreasing' in seq_trend or 'falling' in seq_trend:
                weights = {
                    'StableLSTM': 0.30,
                    'PatchTST': 0.25,
                    'GoldenRule': 0.20,
                    'SpikeDetector': 0.15,
                    'ExtremeNet': 0.10
                }
            # ì•ˆì •
            else:
                weights = {
                    'GoldenRule': 0.22,
                    'ExtremeNet': 0.22,
                    'SpikeDetector': 0.22,
                    'PatchTST': 0.17,
                    'StableLSTM': 0.17
                }
            
            # ê°€ì¤‘ í‰ê· 
            ensemble_pred = 0
            total_weight = 0
            for model_name, weight in weights.items():
                if model_name in preds:
                    ensemble_pred += preds[model_name] * weight
                    total_weight += weight
            
            if total_weight > 0:
                ensemble_pred = ensemble_pred / total_weight
            
            # ê¸‰ì¦ ì˜ˆì¸¡ ì‹œ ìµœì†Œê°’ ë³´ì¥
            if surge_pred == 1:
                ensemble_pred = max(ensemble_pred, 1700)
            
            final_ensemble.append(ensemble_pred)
        
        # ì•™ìƒë¸” ê²°ê³¼ ì¶”ê°€
        all_predictions['ìµœì¢…ì•™ìƒë¸”_ì˜ˆì¸¡'] = [round(p) for p in final_ensemble]
        all_predictions['ìµœì¢…ì•™ìƒë¸”_ì˜¤ì°¨'] = all_predictions['ìµœì¢…ì•™ìƒë¸”_ì˜ˆì¸¡'] - all_predictions['ì‹¤ì œê°’']
        all_predictions['ìµœì¢…ì•™ìƒë¸”_ì˜¤ì°¨ìœ¨(%)'] = round(
            abs(all_predictions['ìµœì¢…ì•™ìƒë¸”_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’'] * 100, 2
        )
        
        # ì•™ìƒë¸” ì„±ëŠ¥
        if self.models:
            ensemble_mae = mean_absolute_error(all_predictions['ì‹¤ì œê°’'], final_ensemble)
            ensemble_rmse = np.sqrt(mean_squared_error(all_predictions['ì‹¤ì œê°’'], final_ensemble))
            ensemble_r2 = r2_score(all_predictions['ì‹¤ì œê°’'], final_ensemble)
            ensemble_mape = np.mean(abs(all_predictions['ìµœì¢…ì•™ìƒë¸”_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’']) * 100
            
            model_metrics['ìµœì¢…ì•™ìƒë¸”'] = {
                'MAE': ensemble_mae,
                'RMSE': ensemble_rmse,
                'R2': ensemble_r2,
                'MAPE': ensemble_mape,
                'ì •í™•ë„(%)': 100 - ensemble_mape
            }
            
            print(f"âœ… ìµœì¢…ì•™ìƒë¸”: MAE={ensemble_mae:.2f}, RÂ²={ensemble_r2:.4f}, ì •í™•ë„={100-ensemble_mape:.2f}%")
        
        # CSV ì €ì¥
        pattern_suffix = self.extreme_booster.data_pattern
        output_file = f'v68_{pattern_suffix}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        all_predictions.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ì˜ˆì¸¡ê°’ ì €ì¥: {output_file}")
        
        # 1700+ ê¸‰ì¦ ì˜ˆì¸¡ ì •í™•ë„
        print("\n" + "="*80)
        print("ğŸš€ 1700+ ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥ ë¶„ì„")
        print("="*80)
        
        surge_mask = all_predictions['ê¸‰ì¦ì˜ˆì¸¡'] == 1
        if surge_mask.sum() > 0:
            surge_df = all_predictions[surge_mask]
            actual_1700_in_surge = (surge_df['ì‹¤ì œê°’'] >= 1700).sum()
            surge_accuracy = actual_1700_in_surge / len(surge_df) * 100
            
            print(f"ê¸‰ì¦ ì˜ˆì¸¡ ì¼€ì´ìŠ¤: {len(surge_df)}ê°œ")
            print(f"ì‹¤ì œ 1700+ ë‹¬ì„±: {actual_1700_in_surge}ê°œ")
            print(f"ê¸‰ì¦ ì˜ˆì¸¡ ì •í™•ë„: {surge_accuracy:.1f}%")
            
            # ì•™ìƒë¸” ì ì¤‘ë¥ 
            ensemble_1700_hit = (surge_df['ìµœì¢…ì•™ìƒë¸”_ì˜ˆì¸¡'] >= 1700).sum()
            print(f"ì•™ìƒë¸” 1700+ ì˜ˆì¸¡: {ensemble_1700_hit}ê°œ ({ensemble_1700_hit/len(surge_df)*100:.1f}%)")
        
        # ê³ ê°’ êµ¬ê°„ ë¶„ì„
        print("\n" + "="*80)
        print("ğŸ¯ ê³ ê°’ êµ¬ê°„ (1700+) ìƒì„¸ ë¶„ì„")
        print("="*80)
        
        high_mask = all_predictions['ì‹¤ì œê°’'] >= 1700
        if high_mask.any():
            high_df = all_predictions[high_mask]
            print(f"ì „ì²´ 1700+ ìƒ˜í”Œ: {len(high_df)}ê°œ")
            
            # íŒ¨í„´ë³„ ë¶„í¬
            if self.extreme_booster.data_pattern == "UU1":
                print("\n[UU1 íŒ¨í„´ - ê¸‰ì¦ ì˜ˆì¸¡]")
                print(f"  í‰ê·  M14B: {high_df['M14AM14B'].mean():.0f}")
                print(f"  ì‹œí€€ìŠ¤ MAX í‰ê· : {high_df['ì‹œí€€ìŠ¤_MAX'].mean():.0f}")
            else:
                print("\n[UU2 íŒ¨í„´ - ê³ ê°’ ìœ ì§€]")
                print(f"  í‰ê·  M14B: {high_df['M14AM14B'].mean():.0f}")
                print(f"  ì‹œí€€ìŠ¤ MAX í‰ê· : {high_df['ì‹œí€€ìŠ¤_MAX'].mean():.0f}")
            
            # ëª¨ë¸ë³„ ì ì¤‘ë¥ 
            print("\n[ëª¨ë¸ë³„ 1700+ ì ì¤‘ë¥ ]")
            for model_name in list(self.models.keys()) + ['ìµœì¢…ì•™ìƒë¸”']:
                if f'{model_name}_ì˜ˆì¸¡' in high_df.columns:
                    hit = (high_df[f'{model_name}_ì˜ˆì¸¡'] >= 1700).sum()
                    print(f"  {model_name}: {hit}/{len(high_df)} ({hit/len(high_df)*100:.1f}%)")
        
        # ì„±ëŠ¥ ìš”ì•½
        if self.models:
            print("\n" + "="*80)
            print("ğŸ“Š ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½")
            print("="*80)
            
            metrics_df = pd.DataFrame(model_metrics).T
            metrics_df = metrics_df.sort_values('R2', ascending=False)
            
            print(f"\n{'ëª¨ë¸':<15} {'MAE':>8} {'RMSE':>8} {'RÂ²':>8} {'MAPE(%)':>8} {'ì •í™•ë„(%)':>10}")
            print("-" * 65)
            
            for model_name, row in metrics_df.iterrows():
                if model_name == 'ìµœì¢…ì•™ìƒë¸”':
                    print(f"{'ğŸ”¥ ' + model_name:<15} {row['MAE']:8.2f} {row['RMSE']:8.2f} "
                          f"{row['R2']:8.4f} {row['MAPE']:8.2f} {row['ì •í™•ë„(%)']:10.2f} â­â­â­")
                else:
                    print(f"{model_name:<15} {row['MAE']:8.2f} {row['RMSE']:8.2f} "
                          f"{row['R2']:8.4f} {row['MAPE']:8.2f} {row['ì •í™•ë„(%)']:10.2f}")
            
            return all_predictions, metrics_df
        else:
            return all_predictions, None

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    print("\nğŸš€ V6.8 ìµœì¢… ê°œì„ íŒ ì‹œì‘!")
    print("í•µì‹¬ ê¸°ëŠ¥:")
    print("  1. UU1/UU2 ìë™ íŒ¨í„´ ê°ì§€")
    print("  2. 1700+ ê¸‰ì¦ ì˜ˆì¸¡ ê°•í™”")
    print("  3. ì¶”ì„¸ ê¸°ë°˜ ë™ì  ì•™ìƒë¸”")
    print("  4. M14B ê¸°ë°˜ ì°¨ë³„í™”ëœ ë¶€ìŠ¤íŒ…")
    
    # í‰ê°€ê¸° ìƒì„±
    evaluator = ImprovedModelEvaluator()
    
    # ëª¨ë“  ëª¨ë¸ ë¡œë“œ
    models = evaluator.load_all_models('models/')
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼
    test_files = [
        'uu.csv',
        'uu2.csv',
        'uu1.csv',
        'data/test_data.csv',
        '/mnt/user-data/uploads/uu.csv',
        '/mnt/user-data/uploads/uu2.csv'
    ]
    
    for file in test_files:
        if os.path.exists(file):
            print(f"\n{'='*80}")
            print(f"ğŸ“ í…ŒìŠ¤íŠ¸ íŒŒì¼: {file}")
            print('='*80)
            
            # í‰ê°€ ì‹¤í–‰
            all_predictions, metrics = evaluator.evaluate_all_models(file)
            
            if all_predictions is not None:
                print(f"\nâœ… {file} í‰ê°€ ì™„ë£Œ!")
    
    print("\n" + "="*80)
    print("ğŸ† V6.8 í‰ê°€ ì™„ë£Œ!")
    print("="*80)

if __name__ == "__main__":
    main()