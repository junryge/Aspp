"""

def boost_prediction(self, pred, m14b_value, m14a_value, sequence_trend, model_name=None):
    """íŠ¸ë Œë“œ ê¸°ë°˜ ê·¹ë‹¨ ë¶€ìŠ¤íŒ… - í˜„ì‹¤ì ìœ¼ë¡œ ìˆ˜ì •"""
    
    boosted = pred
    
    # 1ë‹¨ê³„: íŠ¸ë Œë“œ ê¸°ë°˜ ë¶€ìŠ¤íŒ… (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
    if sequence_trend == 'EXTREME_UP':
        # ìƒìŠ¹ íŠ¸ë Œë“œ + ê·¹ë‹¨ê°’ â†’ ì ì ˆí•œ ë¶€ìŠ¤íŒ…
        if m14b_value > 500:
            boosted = max(pred * 1.3, 1850)  # 2.0 â†’ 1.3
        elif m14b_value > 450:
            boosted = max(pred * 1.25, 1780)  # 1.8 â†’ 1.25
        elif m14b_value > 400:
            boosted = max(pred * 1.2, 1730)  # 1.6 â†’ 1.2
        else:
            boosted = max(pred * 1.15, 1700)  # 1.4 â†’ 1.15
    
    elif sequence_trend == 'UP':
        # ì¼ë°˜ ìƒìŠ¹ íŠ¸ë Œë“œ
        if m14b_value > 500:
            boosted = max(pred * 1.2, 1800)  # 1.6 â†’ 1.2
        elif m14b_value > 450:
            boosted = max(pred * 1.15, 1750)  # 1.5 â†’ 1.15
        elif m14b_value > 400:
            boosted = max(pred * 1.1, 1700)  # 1.4 â†’ 1.1
        else:
            boosted = pred * 1.05
    
    else:  # STABLE or DOWN
        # ì•ˆì •/í•˜ë½ â†’ ê¸°ë³¸ ë¶€ìŠ¤íŒ…
        if m14b_value > 500:
            boosted = max(pred * 1.1, 1750)  # 1.4 â†’ 1.1
        elif m14b_value > 450:
            boosted = max(pred * 1.05, 1700)  # 1.3 â†’ 1.05
        elif m14b_value > 400:
            boosted = max(pred, 1650)  # 1.2 â†’ 1.0
        else:
            boosted = pred
    
    # 2ë‹¨ê³„: í™©ê¸ˆ íŒ¨í„´ (ì ì ˆíˆ)
    if m14b_value > 300 and m14a_value and m14a_value < 80:
        boosted = boosted * 1.1  # 1.2 â†’ 1.1
    
    # 3ë‹¨ê³„: ìƒí•œì„  ì œí•œ (í˜„ì‹¤ì ìœ¼ë¡œ)
    boosted = min(boosted, 1950)  # ìµœëŒ€ 1950ìœ¼ë¡œ ì œí•œ
    
    return boosted


    # íŠ¸ë Œë“œ ê¸°ë°˜ ê·¹ë‹¨ê°’ ì•™ìƒë¸”
    print("\nğŸ”¥ íŠ¸ë Œë“œ ê¸°ë°˜ ê·¹ë‹¨ê°’ ì•™ìƒë¸” ìƒì„±...")
    
    extreme_ensemble = []
    for i in range(len(all_predictions)):
        m14b = all_predictions.iloc[i]['M14AM14B']
        m14a = all_predictions.iloc[i]['M14AM10A']
        trend = all_predictions.iloc[i]['ì‹œí€€ìŠ¤íŠ¸ë Œë“œ']
        
        # íŠ¸ë Œë“œë³„ ê°€ì¤‘ì¹˜ ì „ëµ
        if trend == 'EXTREME_UP':  # <-- ì—¬ê¸°ë¶€í„° ìˆ˜ì •
            # ê·¹ë‹¨ ìƒìŠ¹: ê°€ì¥ ë†’ì€ ì˜ˆì¸¡ê°’ ì„ íƒ + ì ì ˆí•œ ë¶€ìŠ¤íŒ…
            model_preds = []
            for model_name in self.models.keys():
                if f'{model_name}_ì˜ˆì¸¡' in all_predictions.columns:
                    model_preds.append(all_predictions.iloc[i][f'{model_name}_ì˜ˆì¸¡'])
            
            if model_preds:
                ensemble_pred = max(model_preds) * 1.1  # 1.2 â†’ 1.1ë¡œ ìˆ˜ì •
            else:
                ensemble_pred = 1800
            
            # ìµœì¢… ê°•ì œ (ì ì ˆí•˜ê²Œ ìˆ˜ì •)
            if m14b > 500:
                ensemble_pred = max(ensemble_pred, 1850)  # 1900 â†’ 1850
            elif m14b > 450:
                ensemble_pred = max(ensemble_pred, 1750)  # 1800 â†’ 1750
            else:
                ensemble_pred = max(ensemble_pred, 1700)  # 1750 â†’ 1700
        
        elif trend == 'UP':


V9.0 ì‹œí€€ìŠ¤ íŠ¸ë Œë“œ ê¸°ë°˜ ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
í•µì‹¬: 100ê°œ ì‹œí€€ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ â†’ ìƒìŠ¹ íŠ¸ë Œë“œë©´ EXTREME ë¶€ìŠ¤íŒ…
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import json
import os
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')
tf.keras.config.enable_unsafe_deserialization()

# ====================== íŠ¸ë Œë“œ ê¸°ë°˜ ê·¹ë‹¨ê°’ ë³´ì • í´ë˜ìŠ¤ ======================
class TrendBasedExtremeBooster:
    """ì‹œí€€ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ê¸°ë°˜ ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ…"""
    
    def __init__(self):
        print("ğŸ”¥ íŠ¸ë Œë“œ ê¸°ë°˜ ê·¹ë‹¨ê°’ ë¶€ìŠ¤í„° v9.0 ì´ˆê¸°í™”")
        
    def analyze_sequence_trend(self, sequence_100):
        """100ê°œ ì‹œí€€ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„"""
        if len(sequence_100) < 100:
            return 'UNKNOWN', {}
        
        # êµ¬ê°„ë³„ í‰ê· 
        first_20 = np.mean(sequence_100[:20])
        middle_20 = np.mean(sequence_100[40:60])
        last_20 = np.mean(sequence_100[-20:])
        max_val = np.max(sequence_100)
        
        # ì„ í˜• íšŒê·€ë¡œ ê¸°ìš¸ê¸° ê³„ì‚°
        x = np.arange(len(sequence_100))
        z = np.polyfit(x, sequence_100, 1)
        slope = z[0]
        
        # íŠ¸ë Œë“œ íŒë‹¨
        trend_info = {
            'first_20': first_20,
            'last_20': last_20,
            'max_val': max_val,
            'slope': slope,
            'ratio': last_20 / first_20 if first_20 > 0 else 1
        }
        
        # ìƒìŠ¹/í•˜ë½ íŒë‹¨
        if last_20 > first_20 * 1.05 and max_val >= 1650:
            return 'EXTREME_UP', trend_info  # ìƒìŠ¹ + ê·¹ë‹¨ê°’
        elif last_20 > first_20 * 1.02 and slope > 0:
            return 'UP', trend_info  # ìƒìŠ¹
        elif last_20 < first_20 * 0.98 and slope < 0:
            return 'DOWN', trend_info  # í•˜ë½
        else:
            return 'STABLE', trend_info  # ì•ˆì •
    
    def boost_prediction(self, pred, m14b_value, m14a_value, sequence_trend, model_name=None):
        """íŠ¸ë Œë“œ ê¸°ë°˜ ê·¹ë‹¨ ë¶€ìŠ¤íŒ…"""
        
        boosted = pred
        
        # 1ë‹¨ê³„: íŠ¸ë Œë“œ ê¸°ë°˜ ë¶€ìŠ¤íŒ…
        if sequence_trend == 'EXTREME_UP':
            # ìƒìŠ¹ íŠ¸ë Œë“œ + ê·¹ë‹¨ê°’ â†’ ì´ˆê°•ë ¥ ë¶€ìŠ¤íŒ…
            if m14b_value > 500:
                boosted = max(pred * 2.0, 1900)
            elif m14b_value > 450:
                boosted = max(pred * 1.8, 1800)
            elif m14b_value > 400:
                boosted = max(pred * 1.6, 1750)
            else:
                boosted = max(pred * 1.4, 1700)
        
        elif sequence_trend == 'UP':
            # ì¼ë°˜ ìƒìŠ¹ íŠ¸ë Œë“œ â†’ ì¤‘ê°„ ë¶€ìŠ¤íŒ…
            if m14b_value > 500:
                boosted = max(pred * 1.6, 1850)
            elif m14b_value > 450:
                boosted = max(pred * 1.5, 1750)
            elif m14b_value > 400:
                boosted = max(pred * 1.4, 1700)
            else:
                boosted = pred * 1.2
        
        else:  # STABLE or DOWN
            # ì•ˆì •/í•˜ë½ â†’ ê¸°ë³¸ ë¶€ìŠ¤íŒ…
            if m14b_value > 500:
                boosted = max(pred * 1.4, 1800)
            elif m14b_value > 450:
                boosted = max(pred * 1.3, 1700)
            elif m14b_value > 400:
                boosted = max(pred * 1.2, 1650)
            else:
                boosted = pred * 1.1
        
        # 2ë‹¨ê³„: í™©ê¸ˆ íŒ¨í„´ ì¶”ê°€ ë¶€ìŠ¤íŒ…
        if m14b_value > 300 and m14a_value and m14a_value < 80:
            boosted = boosted * 1.2
        
        # 3ë‹¨ê³„: ë¹„ìœ¨ ë¶€ìŠ¤íŒ…
        if m14a_value and m14a_value > 0:
            ratio = m14b_value / m14a_value
            if ratio > 8:
                boosted = boosted * 1.15
            elif ratio > 6:
                boosted = boosted * 1.1
        
        return boosted

class CompleteModelEvaluator:
    def __init__(self, scaler_path='scalers/'):
        """í‰ê°€ê¸° ì´ˆê¸°í™”"""
        print("="*80)
        print("ğŸ”¥ V9.0 ì‹œí€€ìŠ¤ íŠ¸ë Œë“œ ê¸°ë°˜ ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
        print("="*80)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        try:
            with open(f'{scaler_path}feature_scaler.pkl', 'rb') as f:
                self.feature_scaler = pickle.load(f)
            with open(f'{scaler_path}target_scaler.pkl', 'rb') as f:
                self.target_scaler = pickle.load(f)
            with open(f'{scaler_path}config.json', 'r') as f:
                config = json.load(f)
                self.seq_len = config['seq_len']
                self.pred_len = config['pred_len']
                self.feature_columns = config['feature_columns']
            print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
        except:
            print(f"âš ï¸ ìŠ¤ì¼€ì¼ëŸ¬ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
            self.seq_len = 100
            self.pred_len = 10
            self.feature_columns = ['M14AM14B', 'M14AM10A', 'M14AM16', 'M14AM14BSUM', 'TOTALCNT']
        
        self.models = {}
        self.trend_booster = TrendBasedExtremeBooster()
        
    def load_all_models(self, model_dir='models/'):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        print(f"\nğŸ“ ëª¨ë¸ ë¡œë”©...")
        
        if not os.path.exists(model_dir):
            print("  âš ï¸ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ")
            return self.models
        
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.keras')]
        
        for model_file in model_files:
            model_name = model_file.replace('.keras', '')
            model_path = os.path.join(model_dir, model_file)
            
            try:
                self.models[model_name] = tf.keras.models.load_model(
                    model_path, safe_mode=False
                )
                print(f"  âœ… {model_name} ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"\nì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return self.models
    
    def load_test_data(self, filepath):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        print(f"\nğŸ“‚ í‰ê°€ ë°ì´í„° ë¡œë”©: {filepath}")
        df = pd.read_csv(filepath)
        print(f"  ì›ë³¸: {df.shape[0]:,}í–‰")
        
        # 0ê°’ ì œê±°
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        
        # ì‹œê°„ ë³€í™˜
        if 'CURRTIME' in df.columns:
            df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                           format='%Y%m%d%H%M', errors='coerce')
            df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        print(f"  ìœ íš¨: {df.shape[0]:,}í–‰")
        
        # ê³ ê°’ í†µê³„ ì¶œë ¥
        high_count = (df['TOTALCNT'] >= 1700).sum()
        very_high_count = (df['TOTALCNT'] >= 1750).sum()
        extreme_count = (df['TOTALCNT'] >= 1800).sum()
        
        print(f"\nğŸ¯ ê³ ê°’ êµ¬ê°„ ë¶„í¬:")
        print(f"  1700+: {high_count}ê°œ ({high_count/len(df)*100:.1f}%)")
        print(f"  1750+: {very_high_count}ê°œ ({very_high_count/len(df)*100:.1f}%)")
        print(f"  1800+: {extreme_count}ê°œ ({extreme_count/len(df)*100:.1f}%)")
        
        return df
    
    def create_features(self, df):
        """íŠ¹ì„± ìƒì„±"""
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        
        if 'CURRTIME' in df.columns:
            df['HOUR'] = df['CURRTIME'].dt.hour
            df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
            df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
        
        for w in [10, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
        
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        
        return df
    
    def evaluate_all_models(self, test_file):
        """ëª¨ë“  ëª¨ë¸ í‰ê°€ - íŠ¸ë Œë“œ ê¸°ë°˜ ë¶€ìŠ¤íŒ… í¬í•¨"""
        
        # ë°ì´í„° ë¡œë“œ
        df = self.load_test_data(test_file)
        df = self.create_features(df)
        
        # ì˜ˆì¸¡ ê°€ëŠ¥ ë²”ìœ„
        start_idx = self.seq_len
        end_idx = len(df) - self.pred_len
        total = end_idx - start_idx
        
        print(f"\nğŸ”® ì˜ˆì¸¡ ì‹œì‘...")
        print(f"  ì‹œí€€ìŠ¤: {self.seq_len}ë¶„ â†’ ì˜ˆì¸¡: {self.pred_len}ë¶„ í›„")
        print(f"  ì˜ˆì¸¡ ê°œìˆ˜: {total:,}ê°œ")
        
        # ëª¨ë“  ì˜ˆì¸¡ì„ ì €ì¥í•  DataFrame ì¤€ë¹„
        all_predictions = pd.DataFrame()
        
        # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        timestamps_pred = []
        timestamps_target = []
        actuals = []
        m14b_values = []
        m14a_values = []
        sequence_trends = []
        trend_infos = []
        
        print("\nğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ë° íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
        for i in range(start_idx, end_idx):
            if 'CURRTIME' in df.columns:
                pred_time = df.iloc[i]['CURRTIME']
                target_time = pred_time + timedelta(minutes=self.pred_len)
                timestamps_pred.append(pred_time)
                timestamps_target.append(target_time)
            
            actual_idx = i + self.pred_len
            if actual_idx < len(df):
                actuals.append(df.iloc[actual_idx]['TOTALCNT'])
                m14b_values.append(df.iloc[i]['M14AM14B'])
                m14a_values.append(df.iloc[i]['M14AM10A'])
                
                # ì‹œí€€ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„
                sequence = df.iloc[i-self.seq_len:i]['TOTALCNT'].values
                trend, info = self.trend_booster.analyze_sequence_trend(sequence)
                sequence_trends.append(trend)
                trend_infos.append(info)
        
        # ê¸°ë³¸ ì •ë³´ ì €ì¥
        if timestamps_pred:
            all_predictions['ì˜ˆì¸¡ì‹œì '] = [t.strftime('%Y-%m-%d %H:%M') for t in timestamps_pred]
            all_predictions['ì˜ˆì¸¡ëŒ€ìƒì‹œê°„'] = [t.strftime('%Y-%m-%d %H:%M') for t in timestamps_target]
        
        all_predictions['ì‹¤ì œê°’'] = actuals
        all_predictions['M14AM14B'] = m14b_values
        all_predictions['M14AM10A'] = m14a_values
        all_predictions['ì‹œí€€ìŠ¤íŠ¸ë Œë“œ'] = sequence_trends
        
        print(f"  ì˜ˆì¸¡í•  ë°ì´í„°: {len(all_predictions)}ê°œ")
        
        # íŠ¸ë Œë“œ ë¶„í¬ ì¶œë ¥
        trend_counts = pd.Series(sequence_trends).value_counts()
        print(f"\nğŸ“ˆ ì‹œí€€ìŠ¤ íŠ¸ë Œë“œ ë¶„í¬:")
        for trend, count in trend_counts.items():
            print(f"  {trend}: {count}ê°œ ({count/len(sequence_trends)*100:.1f}%)")
        
        # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡
        model_metrics = {}
        model_predictions = {}
        
        for model_name, model in self.models.items():
            print(f"\nğŸ¯ {model_name} ì˜ˆì¸¡ ì¤‘...")
            predictions = []
            
            # ë°°ì¹˜ ì˜ˆì¸¡
            batch_size = 500
            for i in range(start_idx, end_idx, batch_size):
                batch_end = min(i + batch_size, end_idx)
                
                # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                X_batch = []
                for j in range(i, batch_end):
                    seq_data = df.iloc[j-self.seq_len:j][self.feature_columns].values
                    X_batch.append(seq_data)
                
                if len(X_batch) == 0:
                    continue
                
                # ìŠ¤ì¼€ì¼ë§
                X_batch = np.array(X_batch)
                X_batch_scaled = []
                for seq in X_batch:
                    if hasattr(self, 'feature_scaler'):
                        seq_scaled = self.feature_scaler.transform(seq)
                    else:
                        seq_scaled = seq
                    X_batch_scaled.append(seq_scaled)
                X_batch_scaled = np.array(X_batch_scaled)
                
                # ì˜ˆì¸¡
                preds = model.predict(X_batch_scaled, verbose=0)
                
                if isinstance(preds, list):
                    y_pred_scaled = preds[0].flatten()
                else:
                    y_pred_scaled = preds.flatten()
                
                # ì—­ë³€í™˜
                if hasattr(self, 'target_scaler'):
                    y_pred = self.target_scaler.inverse_transform(
                        y_pred_scaled.reshape(-1, 1)).flatten()
                else:
                    y_pred = y_pred_scaled
                
                # ìˆ˜ì§‘
                for k in range(len(y_pred)):
                    actual_idx = i - start_idx + k
                    if actual_idx < len(all_predictions):
                        predictions.append(y_pred[k])
            
            # ì˜ˆì¸¡ê°’ ì €ì¥
            predictions = predictions[:len(all_predictions)]
            model_predictions[model_name] = predictions
            
            # ì›ë³¸ ì˜ˆì¸¡ê°’ ì €ì¥
            all_predictions[f'{model_name}_ì›ë³¸'] = [round(p) for p in predictions]
            
            # íŠ¸ë Œë“œ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ì ìš©
            print(f"  ğŸ”¥ {model_name} íŠ¸ë Œë“œ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ì ìš© ì¤‘...")
            boosted_predictions = []
            
            for i in range(len(predictions)):
                m14b = all_predictions.iloc[i]['M14AM14B']
                m14a = all_predictions.iloc[i]['M14AM10A']
                trend = all_predictions.iloc[i]['ì‹œí€€ìŠ¤íŠ¸ë Œë“œ']
                original = predictions[i]
                
                # íŠ¸ë Œë“œ ê¸°ë°˜ ë¶€ìŠ¤íŒ…
                boosted = self.trend_booster.boost_prediction(
                    original, m14b, m14a, trend, model_name
                )
                boosted_predictions.append(boosted)
            
            all_predictions[f'{model_name}_ì˜ˆì¸¡'] = [round(p) for p in boosted_predictions]
            all_predictions[f'{model_name}_ì˜¤ì°¨'] = all_predictions[f'{model_name}_ì˜ˆì¸¡'] - all_predictions['ì‹¤ì œê°’']
            all_predictions[f'{model_name}_ì˜¤ì°¨ìœ¨(%)'] = round(
                abs(all_predictions[f'{model_name}_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’'] * 100, 2
            )
        
        # íŠ¸ë Œë“œ ê¸°ë°˜ ê·¹ë‹¨ê°’ ì•™ìƒë¸”
        print("\nğŸ”¥ íŠ¸ë Œë“œ ê¸°ë°˜ ê·¹ë‹¨ê°’ ì•™ìƒë¸” ìƒì„±...")
        
        extreme_ensemble = []
        for i in range(len(all_predictions)):
            m14b = all_predictions.iloc[i]['M14AM14B']
            m14a = all_predictions.iloc[i]['M14AM10A']
            trend = all_predictions.iloc[i]['ì‹œí€€ìŠ¤íŠ¸ë Œë“œ']
            
            # íŠ¸ë Œë“œë³„ ê°€ì¤‘ì¹˜ ì „ëµ
            if trend == 'EXTREME_UP':
                # ê·¹ë‹¨ ìƒìŠ¹: ê°€ì¥ ë†’ì€ ì˜ˆì¸¡ê°’ ì„ íƒ + ë¶€ìŠ¤íŒ…
                model_preds = []
                for model_name in self.models.keys():
                    if f'{model_name}_ì˜ˆì¸¡' in all_predictions.columns:
                        model_preds.append(all_predictions.iloc[i][f'{model_name}_ì˜ˆì¸¡'])
                
                if model_preds:
                    ensemble_pred = max(model_preds) * 1.2
                else:
                    ensemble_pred = 1800
                
                # ìµœì¢… ê°•ì œ
                if m14b > 500:
                    ensemble_pred = max(ensemble_pred, 1900)
                elif m14b > 450:
                    ensemble_pred = max(ensemble_pred, 1800)
                else:
                    ensemble_pred = max(ensemble_pred, 1750)
            
            elif trend == 'UP':
                # ìƒìŠ¹: ìƒìœ„ 75% ê°’ + ë¶€ìŠ¤íŒ…
                model_preds = []
                for model_name in self.models.keys():
                    if f'{model_name}_ì˜ˆì¸¡' in all_predictions.columns:
                        model_preds.append(all_predictions.iloc[i][f'{model_name}_ì˜ˆì¸¡'])
                
                if model_preds:
                    ensemble_pred = np.percentile(model_preds, 75) * 1.15
                else:
                    ensemble_pred = 1700
                
                if m14b > 450:
                    ensemble_pred = max(ensemble_pred, 1750)
            
            else:  # STABLE or DOWN
                # ì•ˆì •/í•˜ë½: í‰ê· ê°’ ì‚¬ìš©
                model_preds = []
                for model_name in self.models.keys():
                    if f'{model_name}_ì˜ˆì¸¡' in all_predictions.columns:
                        model_preds.append(all_predictions.iloc[i][f'{model_name}_ì˜ˆì¸¡'])
                
                if model_preds:
                    ensemble_pred = np.mean(model_preds)
                else:
                    ensemble_pred = 1600
                
                if m14b > 450:
                    ensemble_pred = max(ensemble_pred, 1700)
            
            # í™©ê¸ˆ íŒ¨í„´ ì¶”ê°€
            if m14b > 300 and m14a < 80:
                ensemble_pred = ensemble_pred * 1.1
            
            extreme_ensemble.append(ensemble_pred)
        
        # ê²°ê³¼ ì €ì¥
        all_predictions['íŠ¸ë Œë“œì•™ìƒë¸”_ì˜ˆì¸¡'] = [round(p) for p in extreme_ensemble]
        all_predictions['íŠ¸ë Œë“œì•™ìƒë¸”_ì˜¤ì°¨'] = all_predictions['íŠ¸ë Œë“œì•™ìƒë¸”_ì˜ˆì¸¡'] - all_predictions['ì‹¤ì œê°’']
        all_predictions['íŠ¸ë Œë“œì•™ìƒë¸”_ì˜¤ì°¨ìœ¨(%)'] = round(
            abs(all_predictions['íŠ¸ë Œë“œì•™ìƒë¸”_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’'] * 100, 2
        )
        
        # ì„±ëŠ¥ ê³„ì‚°
        extreme_mae = mean_absolute_error(all_predictions['ì‹¤ì œê°’'], extreme_ensemble)
        extreme_rmse = np.sqrt(mean_squared_error(all_predictions['ì‹¤ì œê°’'], extreme_ensemble))
        extreme_r2 = r2_score(all_predictions['ì‹¤ì œê°’'], extreme_ensemble)
        extreme_mape = np.mean(abs(all_predictions['íŠ¸ë Œë“œì•™ìƒë¸”_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’']) * 100
        
        model_metrics['íŠ¸ë Œë“œì•™ìƒë¸”'] = {
            'MAE': extreme_mae,
            'RMSE': extreme_rmse,
            'R2': extreme_r2,
            'MAPE': extreme_mape,
            'ì •í™•ë„(%)': 100 - extreme_mape
        }
        
        print(f"âœ… íŠ¸ë Œë“œì•™ìƒë¸”: MAE={extreme_mae:.2f}, RÂ²={extreme_r2:.4f}, ì •í™•ë„={100-extreme_mape:.2f}%")
        
        # CSV ì €ì¥
        output_file = f'v9_trend_predictions_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        all_predictions.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ì˜ˆì¸¡ê°’ ì €ì¥: {output_file}")
        
        # ê³ ê°’ êµ¬ê°„ ìƒì„¸ ë¶„ì„
        self.analyze_extreme_performance(all_predictions)
        
        return all_predictions, model_metrics
    
    def analyze_extreme_performance(self, df):
        """ê³ ê°’ êµ¬ê°„ ì„±ëŠ¥ ë¶„ì„"""
        
        print("\n" + "="*80)
        print("ğŸ¯ ê³ ê°’ êµ¬ê°„ (1700+) ìƒì„¸ ë¶„ì„")
        print("="*80)
        
        high_mask = df['ì‹¤ì œê°’'] >= 1700
        if high_mask.any():
            high_df = df[high_mask]
            print(f"\nğŸ“Š ì „ì²´ ê³ ê°’ ìƒ˜í”Œ: {high_mask.sum()}ê°œ")
            
            # íŠ¸ë Œë“œë³„ ë¶„ì„
            print(f"\n[íŠ¸ë Œë“œë³„ 1700+ ì ì¤‘ë¥ ]")
            print("-" * 70)
            
            for trend in high_df['ì‹œí€€ìŠ¤íŠ¸ë Œë“œ'].unique():
                trend_mask = high_df['ì‹œí€€ìŠ¤íŠ¸ë Œë“œ'] == trend
                if trend_mask.any():
                    trend_df = high_df[trend_mask]
                    hit_1700 = (trend_df['íŠ¸ë Œë“œì•™ìƒë¸”_ì˜ˆì¸¡'] >= 1700).sum()
                    print(f"  {trend:15s}: {hit_1700}/{len(trend_df)} "
                          f"({hit_1700/len(trend_df)*100:.1f}%)")
            
            # ì „ì²´ ì ì¤‘ë¥ 
            print(f"\n[ì „ì²´ ê³ ê°’ ì ì¤‘ë¥ ]")
            print("-" * 70)
            
            hit_1800 = (high_df['íŠ¸ë Œë“œì•™ìƒë¸”_ì˜ˆì¸¡'] >= 1800).sum()
            hit_1750 = (high_df['íŠ¸ë Œë“œì•™ìƒë¸”_ì˜ˆì¸¡'] >= 1750).sum()
            hit_1700 = (high_df['íŠ¸ë Œë“œì•™ìƒë¸”_ì˜ˆì¸¡'] >= 1700).sum()
            hit_1650 = (high_df['íŠ¸ë Œë“œì•™ìƒë¸”_ì˜ˆì¸¡'] >= 1650).sum()
            
            print(f"  1800+ ì˜ˆì¸¡: {hit_1800:2d}/{len(high_df):2d} ({hit_1800/len(high_df)*100:5.1f}%)")
            print(f"  1750+ ì˜ˆì¸¡: {hit_1750:2d}/{len(high_df):2d} ({hit_1750/len(high_df)*100:5.1f}%)")
            print(f"  1700+ ì˜ˆì¸¡: {hit_1700:2d}/{len(high_df):2d} ({hit_1700/len(high_df)*100:5.1f}%) ğŸ¯")
            print(f"  1650+ ì˜ˆì¸¡: {hit_1650:2d}/{len(high_df):2d} ({hit_1650/len(high_df)*100:5.1f}%)")
            
            # ìƒì„¸ ì¶œë ¥
            print(f"\n[ê³ ê°’ ì˜ˆì¸¡ ìƒì„¸] (ì²˜ìŒ 20ê°œ)")
            print("-" * 80)
            print(f"{'ì‹¤ì œê°’':>7} {'ì˜ˆì¸¡ê°’':>7} {'M14B':>6} {'M14A':>6} {'íŠ¸ë Œë“œ':>12} {'ì˜¤ì°¨':>7} {'ì ì¤‘':>6}")
            print("-" * 80)
            
            for idx in high_df.head(20).index:
                row = df.loc[idx]
                actual = row['ì‹¤ì œê°’']
                pred = row['íŠ¸ë Œë“œì•™ìƒë¸”_ì˜ˆì¸¡']
                m14b = row['M14AM14B']
                m14a = row['M14AM10A']
                trend = row['ì‹œí€€ìŠ¤íŠ¸ë Œë“œ']
                error = pred - actual
                
                if pred >= 1700:
                    hit = "âœ… HIT"
                elif pred >= 1650:
                    hit = "âš ï¸"
                else:
                    hit = "âŒ"
                
                print(f"{actual:7.0f} {pred:7.0f} {m14b:6.0f} {m14a:6.0f} {trend:>12} {error:+7.0f} {hit:>6}")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    print("\nğŸš€ V9.0 ì‹œí€€ìŠ¤ íŠ¸ë Œë“œ ê¸°ë°˜ ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œì‘!")
    print("í•µì‹¬: 100ê°œ ì‹œí€€ìŠ¤ ìƒìŠ¹ íŠ¸ë Œë“œ â†’ EXTREME ë¶€ìŠ¤íŒ…")
    
    # í‰ê°€ê¸° ìƒì„±
    evaluator = CompleteModelEvaluator()
    
    # ëª¨ë“  ëª¨ë¸ ë¡œë“œ
    models = evaluator.load_all_models('models/')
    
    if not models:
        print("âš ï¸ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤! ê¸°ë³¸ ì˜ˆì¸¡ë§Œ ìˆ˜í–‰")
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼
    test_files = [
        'data/M14_20250916_20250817.csv',
        'data/test_data.csv', 
        '/mnt/user-data/uploads/test.csv'
    ]
    
    test_file = None
    for file in test_files:
        if os.path.exists(file):
            test_file = file
            break
    
    if not test_file:
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # í‰ê°€ ì‹¤í–‰
    all_predictions, metrics = evaluator.evaluate_all_models(test_file)
    
    print("\n" + "="*80)
    print("ğŸ† V9.0 íŠ¸ë Œë“œ ê¸°ë°˜ ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì™„ë£Œ!")
    print("="*80)
    print("\nğŸ”¥ í•µì‹¬ ê°œì„ :")
    print("  âœ… 100ê°œ ì‹œí€€ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ (ìƒìŠ¹/í•˜ë½/ì•ˆì •)")
    print("  âœ… EXTREME_UP íŠ¸ë Œë“œ â†’ 2.0ë°° ë¶€ìŠ¤íŒ… + ìµœì†Œ 1900")
    print("  âœ… UP íŠ¸ë Œë“œ â†’ 1.6ë°° ë¶€ìŠ¤íŒ… + ìµœì†Œ 1750")
    print("  âœ… STABLE/DOWN â†’ ê¸°ë³¸ ë¶€ìŠ¤íŒ… + ìµœì†Œ 1700 (M14B>450)")
    print("  âœ… íŠ¸ë Œë“œë³„ ì°¨ë³„í™” ì•™ìƒë¸” ì „ëµ")
    print("="*80)

if __name__ == "__main__":
    main()