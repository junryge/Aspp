"""
ğŸ“Š V6.8 ì¶”ì„¸ ê°•í™” ê·¹ë‹¨ê°’ ì˜ˆì¸¡ í‰ê°€ ì‹œìŠ¤í…œ
========================================================
í•µì‹¬ ê°œì„ : ì‹œí€€ìŠ¤ ì¶”ì„¸ë³„ ì°¨ë³„í™”ëœ ì˜ˆì¸¡ ì „ëµ
- ê°•í•œ ìƒìŠ¹ â†’ ê³µê²©ì  ë¶€ìŠ¤íŒ…
- ê°•í•œ í•˜ë½ â†’ ë³´ìˆ˜ì  ì¡°ì •  
- ê³ í‰ì› ìœ ì§€ â†’ ì•ˆì •ì  ì˜ˆì¸¡
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import json
import os
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')
tf.keras.config.enable_unsafe_deserialization()

# ====================== ì¶”ì„¸ ê°•í™” ê·¹ë‹¨ê°’ ë³´ì • í´ë˜ìŠ¤ ======================
class TrendEnhancedExtremeBooster:
    """ì¶”ì„¸ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ê·¹ë‹¨ê°’ ì˜ˆì¸¡"""
    
    def __init__(self):
        print("ğŸ”¥ ì¶”ì„¸ ê°•í™” ê·¹ë‹¨ê°’ ë¶€ìŠ¤í„° ì´ˆê¸°í™”")
        print("  - ì—°ì† ìƒìŠ¹/í•˜ë½ ì¹´ìš´íŠ¸ ê¸°ë°˜ ì˜ˆì¸¡")
        print("  - ë³€í™”ìœ¨ ê¸°ë°˜ ëª¨ë©˜í…€ ë¶„ì„")
        print("  - ê³ í‰ì› íŠ¹ë³„ ì²˜ë¦¬")
        
    def analyze_sequence_advanced(self, sequence_data):
        """ê³ ê¸‰ ì‹œí€€ìŠ¤ ë¶„ì„"""
        if len(sequence_data) == 0:
            return None
        
        # ê¸°ë³¸ í†µê³„
        seq_max = np.max(sequence_data)
        seq_min = np.min(sequence_data)
        seq_mean = np.mean(sequence_data[-30:]) if len(sequence_data) >= 30 else np.mean(sequence_data)
        seq_std = np.std(sequence_data[-30:]) if len(sequence_data) >= 30 else np.std(sequence_data)
        
        # ìµœê·¼ê°’ê³¼ ë¹„êµ
        current_value = sequence_data[-1]
        recent_10_mean = np.mean(sequence_data[-10:]) if len(sequence_data) >= 10 else seq_mean
        recent_30_mean = np.mean(sequence_data[-30:]) if len(sequence_data) >= 30 else seq_mean
        
        # ëª¨ë©˜í…€ ì§€í‘œ
        momentum_10 = (current_value - sequence_data[-10]) / 10 if len(sequence_data) >= 10 else 0
        momentum_30 = (current_value - sequence_data[-30]) / 30 if len(sequence_data) >= 30 else 0
        
        # ì—°ì† ìƒìŠ¹/í•˜ë½ ì¹´ìš´íŠ¸
        consecutive_rises = 0
        consecutive_falls = 0
        max_consecutive_rises = 0
        max_consecutive_falls = 0
        
        temp_rises = 0
        temp_falls = 0
        
        for i in range(1, len(sequence_data)):
            if sequence_data[i] > sequence_data[i-1]:
                temp_rises += 1
                temp_falls = 0
                max_consecutive_rises = max(max_consecutive_rises, temp_rises)
            elif sequence_data[i] < sequence_data[i-1]:
                temp_falls += 1
                temp_rises = 0
                max_consecutive_falls = max(max_consecutive_falls, temp_falls)
            else:
                temp_rises = 0
                temp_falls = 0
        
        # ìµœê·¼ ì—°ì† ì¹´ìš´íŠ¸
        for i in range(len(sequence_data)-1, 0, -1):
            if sequence_data[i] > sequence_data[i-1]:
                consecutive_rises += 1
            else:
                break
        
        for i in range(len(sequence_data)-1, 0, -1):
            if sequence_data[i] < sequence_data[i-1]:
                consecutive_falls += 1
            else:
                break
        
        # ìƒìŠ¹/í•˜ë½ ê°•ë„
        rise_strength = 0
        fall_strength = 0
        if len(sequence_data) >= 10:
            recent_10 = sequence_data[-10:]
            for i in range(1, len(recent_10)):
                change = recent_10[i] - recent_10[i-1]
                if change > 0:
                    rise_strength += change
                else:
                    fall_strength += abs(change)
        
        # ì¶”ì„¸ íŒë‹¨ (ë” ì„¸ë°€í•˜ê²Œ)
        trend_score = 0
        trend = 'stable'
        
        # ì¶”ì„¸ ì ìˆ˜ ê³„ì‚°
        if consecutive_rises >= 15:
            trend_score += 5
        elif consecutive_rises >= 10:
            trend_score += 4
        elif consecutive_rises >= 7:
            trend_score += 3
        elif consecutive_rises >= 5:
            trend_score += 2
        elif consecutive_rises >= 3:
            trend_score += 1
            
        if consecutive_falls >= 15:
            trend_score -= 5
        elif consecutive_falls >= 10:
            trend_score -= 4
        elif consecutive_falls >= 7:
            trend_score -= 3
        elif consecutive_falls >= 5:
            trend_score -= 2
        elif consecutive_falls >= 3:
            trend_score -= 1
        
        # ëª¨ë©˜í…€ ê¸°ë°˜ ì¶”ê°€ ì ìˆ˜
        if momentum_10 > 10:
            trend_score += 2
        elif momentum_10 > 5:
            trend_score += 1
        elif momentum_10 < -10:
            trend_score -= 2
        elif momentum_10 < -5:
            trend_score -= 1
        
        # ê³ í‰ì› ì²´í¬
        is_high_plateau = seq_mean >= 1700
        is_extreme_high = seq_mean >= 1750
        
        # ìµœì¢… ì¶”ì„¸ ê²°ì •
        if is_extreme_high:
            if trend_score >= 3:
                trend = 'extreme_rising'
            elif trend_score <= -3:
                trend = 'extreme_falling'
            elif trend_score >= 1:
                trend = 'extreme_high_up'
            elif trend_score <= -1:
                trend = 'extreme_high_down'
            else:
                trend = 'extreme_plateau'
        elif is_high_plateau:
            if trend_score >= 4:
                trend = 'high_surging'
            elif trend_score >= 2:
                trend = 'high_increasing'
            elif trend_score <= -4:
                trend = 'high_collapsing'
            elif trend_score <= -2:
                trend = 'high_decreasing'
            else:
                trend = 'high_stable'
        else:
            if trend_score >= 5:
                trend = 'explosive_rising'
            elif trend_score >= 3:
                trend = 'strong_rising'
            elif trend_score >= 1:
                trend = 'increasing'
            elif trend_score <= -5:
                trend = 'sharp_falling'
            elif trend_score <= -3:
                trend = 'strong_falling'
            elif trend_score <= -1:
                trend = 'decreasing'
            else:
                trend = 'stable'
        
        # ê¸‰ë³€ ê°ì§€
        recent_spike = False
        if len(sequence_data) >= 5:
            recent_change = sequence_data[-1] - sequence_data[-5]
            if abs(recent_change) > seq_std * 2:
                recent_spike = True
        
        return {
            'max': seq_max,
            'min': seq_min,
            'mean': seq_mean,
            'std': seq_std,
            'current': current_value,
            'trend': trend,
            'trend_score': trend_score,
            'is_high_plateau': is_high_plateau,
            'is_extreme_high': is_extreme_high,
            'consecutive_rises': consecutive_rises,
            'consecutive_falls': consecutive_falls,
            'max_consecutive_rises': max_consecutive_rises,
            'max_consecutive_falls': max_consecutive_falls,
            'rise_strength': rise_strength,
            'fall_strength': fall_strength,
            'momentum_10': momentum_10,
            'momentum_30': momentum_30,
            'recent_10_mean': recent_10_mean,
            'recent_30_mean': recent_30_mean,
            'recent_spike': recent_spike
        }
    
    def trend_based_boosting(self, pred, m14b_value, m14a_value, model_name, seq_info):
        """ì¶”ì„¸ ê¸°ë°˜ ì°¨ë³„í™” ë¶€ìŠ¤íŒ…"""
        if not seq_info:
            return pred
            
        original = pred
        boosted = pred
        
        trend = seq_info.get('trend', 'stable')
        trend_score = seq_info.get('trend_score', 0)
        seq_max = seq_info.get('max', 0)
        seq_mean = seq_info.get('mean', 0)
        current_value = seq_info.get('current', 0)
        consecutive_rises = seq_info.get('consecutive_rises', 0)
        consecutive_falls = seq_info.get('consecutive_falls', 0)
        momentum_10 = seq_info.get('momentum_10', 0)
        recent_spike = seq_info.get('recent_spike', False)
        
        # ëª¨ë¸ë³„ ì¶”ì„¸ ë¯¼ê°ë„
        model_sensitivity = {
            'ExtremeNet': 1.5,      # ê·¹ë‹¨ê°’ì— ë§¤ìš° ë¯¼ê°
            'SpikeDetector': 1.3,   # ê¸‰ë³€ì— ë¯¼ê°
            'GoldenRule': 1.2,      # ê·œì¹™ ê¸°ë°˜
            'PatchTST': 1.0,        # íŒ¨í„´ ê¸°ë°˜
            'StableLSTM': 0.8       # ì•ˆì •ì 
        }
        
        sensitivity = model_sensitivity.get(model_name, 1.0)
        
        # ========== ê·¹ë‹¨ì  ìƒìŠ¹ ì¶”ì„¸ ==========
        if trend in ['extreme_rising', 'explosive_rising', 'high_surging']:
            boost_factor = 1.0
            
            # ê¸°ë³¸ ë¶€ìŠ¤íŒ…
            if consecutive_rises >= 15:
                boost_factor = 1.25
            elif consecutive_rises >= 10:
                boost_factor = 1.20
            elif consecutive_rises >= 7:
                boost_factor = 1.15
            elif consecutive_rises >= 5:
                boost_factor = 1.10
            else:
                boost_factor = 1.05
            
            # ëª¨ë©˜í…€ ì¶”ê°€ ë¶€ìŠ¤íŒ…
            if momentum_10 > 10:
                boost_factor *= 1.1
            elif momentum_10 > 5:
                boost_factor *= 1.05
            
            # ëª¨ë¸ ë¯¼ê°ë„ ì ìš©
            boost_factor = 1 + (boost_factor - 1) * sensitivity
            
            # ë¶€ìŠ¤íŒ… ì ìš©
            boosted = pred * boost_factor
            
            # ìµœì†Œê°’ ë³´ì¥
            if seq_max >= 1700:
                boosted = max(boosted, 1750)
            elif seq_max >= 1650:
                boosted = max(boosted, 1700)
            elif seq_max >= 1600:
                boosted = max(boosted, 1650)
            
            # í˜„ì¬ê°’ë³´ë‹¤ ë‚®ìœ¼ë©´ ë³´ì •
            if current_value >= 1700 and boosted < current_value:
                boosted = current_value * 1.05
        
        # ========== ê°•í•œ ìƒìŠ¹ ì¶”ì„¸ ==========
        elif trend in ['strong_rising', 'high_increasing']:
            boost_factor = 1.0
            
            if consecutive_rises >= 10:
                boost_factor = 1.15
            elif consecutive_rises >= 7:
                boost_factor = 1.12
            elif consecutive_rises >= 5:
                boost_factor = 1.08
            else:
                boost_factor = 1.05
            
            # ëª¨ë¸ ë¯¼ê°ë„ ì ìš©
            boost_factor = 1 + (boost_factor - 1) * sensitivity
            
            boosted = pred * boost_factor
            
            # ìµœì†Œê°’ ë³´ì¥
            if seq_max >= 1650:
                boosted = max(boosted, 1680)
            elif seq_max >= 1600:
                boosted = max(boosted, 1630)
        
        # ========== ì¼ë°˜ ìƒìŠ¹ ì¶”ì„¸ ==========
        elif trend == 'increasing':
            if consecutive_rises >= 5:
                boosted = pred * 1.05
            elif consecutive_rises >= 3:
                boosted = pred * 1.03
            else:
                boosted = pred * 1.01
            
            # ìµœì†Œê°’ ë³´ì¥
            if seq_max >= 1650 and model_name == 'ExtremeNet':
                boosted = max(boosted, 1650)
        
        # ========== ê·¹ë‹¨ì  í•˜ë½ ì¶”ì„¸ ==========
        elif trend in ['extreme_falling', 'high_collapsing', 'sharp_falling']:
            discount_factor = 1.0
            
            if consecutive_falls >= 15:
                discount_factor = 0.85
            elif consecutive_falls >= 10:
                discount_factor = 0.90
            elif consecutive_falls >= 7:
                discount_factor = 0.93
            elif consecutive_falls >= 5:
                discount_factor = 0.95
            else:
                discount_factor = 0.97
            
            # ëª¨ë¸ ë¯¼ê°ë„ ì ìš© (ë°˜ëŒ€ë¡œ)
            discount_factor = 1 - (1 - discount_factor) * (2 - sensitivity)
            
            boosted = pred * discount_factor
            
            # ê¸‰ë½ ë°©ì§€ (ë„ˆë¬´ ë‚®ê²Œ ì˜ˆì¸¡í•˜ì§€ ì•Šë„ë¡)
            if current_value >= 1700:
                boosted = max(boosted, 1650)
            elif current_value >= 1650:
                boosted = max(boosted, 1600)
        
        # ========== ì¼ë°˜ í•˜ë½ ì¶”ì„¸ ==========
        elif trend in ['strong_falling', 'high_decreasing', 'decreasing']:
            if consecutive_falls >= 10:
                boosted = pred * 0.92
            elif consecutive_falls >= 7:
                boosted = pred * 0.95
            elif consecutive_falls >= 5:
                boosted = pred * 0.97
            else:
                boosted = pred * 0.99
        
        # ========== ê³ í‰ì› ìœ ì§€ ==========
        elif trend in ['extreme_plateau', 'high_stable']:
            # ì•ˆì •ì  ì˜ˆì¸¡
            if seq_mean >= 1750:
                boosted = max(pred, 1730)
            elif seq_mean >= 1700:
                boosted = max(pred, 1680)
            
            # ë³€ë™ì„±ì´ ë‚®ìœ¼ë©´ í‰ê· ê°’ì— ê°€ê¹ê²Œ
            if seq_info.get('std', 0) < 30:
                boosted = boosted * 0.7 + seq_mean * 0.3
        
        # ========== ê¸‰ë³€ ì²˜ë¦¬ ==========
        if recent_spike and model_name == 'SpikeDetector':
            if momentum_10 > 0:
                boosted *= 1.1  # ê¸‰ìƒìŠ¹
            else:
                boosted *= 0.9  # ê¸‰í•˜ë½
        
        # M14B ê¸°ë°˜ ì¶”ê°€ ì¡°ì •
        if m14b_value > 450 and trend_score > 0:
            boosted *= 1.05
        elif m14b_value < 200 and trend_score < 0:
            boosted *= 0.95
        
        return boosted

class AdvancedModelEvaluator:
    def __init__(self, scaler_path='scalers/'):
        """ê³ ê¸‰ í‰ê°€ê¸° ì´ˆê¸°í™”"""
        print("="*80)
        print("ğŸ”¥ V6.8 ì¶”ì„¸ ê°•í™” ê·¹ë‹¨ê°’ í‰ê°€ ì‹œìŠ¤í…œ")
        print("  í•µì‹¬: ì¶”ì„¸ë³„ ì°¨ë³„í™”ëœ ì˜ˆì¸¡ ì „ëµ")
        print("="*80)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        with open(f'{scaler_path}feature_scaler.pkl', 'rb') as f:
            self.feature_scaler = pickle.load(f)
        with open(f'{scaler_path}target_scaler.pkl', 'rb') as f:
            self.target_scaler = pickle.load(f)
        with open(f'{scaler_path}config.json', 'r') as f:
            config = json.load(f)
            self.seq_len = config['seq_len']
            self.pred_len = config['pred_len']
            self.feature_columns = config['feature_columns']
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
        
        self.models = {}
        self.trend_booster = TrendEnhancedExtremeBooster()
        
    def load_all_models(self, model_dir='models/'):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        print(f"\nğŸ“ ëª¨ë¸ ë¡œë”©...")
        
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.keras')]
        
        for model_file in model_files:
            model_name = model_file.replace('.keras', '')
            model_path = os.path.join(model_dir, model_file)
            
            try:
                self.models[model_name] = tf.keras.models.load_model(
                    model_path, safe_mode=False
                )
                print(f"  âœ… {model_name} ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"\nì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return self.models
    
    def load_test_data(self, filepath):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        print(f"\nğŸ“‚ í‰ê°€ ë°ì´í„° ë¡œë”©: {filepath}")
        df = pd.read_csv(filepath)
        print(f"  ì›ë³¸: {df.shape[0]:,}í–‰")
        
        # 0ê°’ ì œê±°
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        
        # ì‹œê°„ ë³€í™˜
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                       format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        print(f"  ìœ íš¨: {df.shape[0]:,}í–‰")
        
        # ê³ ê°’ í†µê³„ ì¶œë ¥
        high_count = (df['TOTALCNT'] >= 1700).sum()
        very_high_count = (df['TOTALCNT'] >= 1750).sum()
        extreme_count = (df['TOTALCNT'] >= 1800).sum()
        
        print(f"\nğŸ¯ ê³ ê°’ êµ¬ê°„ ë¶„í¬:")
        print(f"  1700+: {high_count}ê°œ ({high_count/len(df)*100:.1f}%)")
        print(f"  1750+: {very_high_count}ê°œ ({very_high_count/len(df)*100:.1f}%)")
        print(f"  1800+: {extreme_count}ê°œ ({extreme_count/len(df)*100:.1f}%)")
        
        return df
    
    def create_features(self, df):
        """íŠ¹ì„± ìƒì„±"""
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        
        df['HOUR'] = df['CURRTIME'].dt.hour
        df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
        df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
        
        for w in [10, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
        
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        
        return df
    
    def create_trend_aware_ensemble(self, predictions_dict, seq_info, m14b, m14a):
        """ì¶”ì„¸ ì¸ì‹ ì•™ìƒë¸”"""
        
        trend = seq_info.get('trend', 'stable')
        trend_score = seq_info.get('trend_score', 0)
        seq_max = seq_info.get('max', 0)
        seq_mean = seq_info.get('mean', 0)
        consecutive_rises = seq_info.get('consecutive_rises', 0)
        consecutive_falls = seq_info.get('consecutive_falls', 0)
        momentum_10 = seq_info.get('momentum_10', 0)
        
        # ê¸°ë³¸ ê°€ì¤‘ì¹˜
        base_weights = {
            'ExtremeNet': 0.20,
            'SpikeDetector': 0.20,
            'GoldenRule': 0.20,
            'PatchTST': 0.20,
            'StableLSTM': 0.20
        }
        
        # ì¶”ì„¸ë³„ ê°€ì¤‘ì¹˜ ì¡°ì •
        if trend in ['extreme_rising', 'explosive_rising', 'high_surging']:
            # ê·¹ë‹¨ ìƒìŠ¹ â†’ ExtremeNet, SpikeDetector ê°•í™”
            weights = {
                'ExtremeNet': 0.35,
                'SpikeDetector': 0.30,
                'GoldenRule': 0.15,
                'PatchTST': 0.10,
                'StableLSTM': 0.10
            }
        elif trend in ['strong_rising', 'high_increasing']:
            # ê°•í•œ ìƒìŠ¹ â†’ SpikeDetector, GoldenRule ê°•í™”
            weights = {
                'ExtremeNet': 0.25,
                'SpikeDetector': 0.25,
                'GoldenRule': 0.25,
                'PatchTST': 0.15,
                'StableLSTM': 0.10
            }
        elif trend == 'increasing':
            # ì¼ë°˜ ìƒìŠ¹ â†’ ê· í˜•
            weights = {
                'ExtremeNet': 0.22,
                'SpikeDetector': 0.22,
                'GoldenRule': 0.22,
                'PatchTST': 0.17,
                'StableLSTM': 0.17
            }
        elif trend in ['extreme_falling', 'high_collapsing', 'sharp_falling']:
            # ê·¹ë‹¨ í•˜ë½ â†’ StableLSTM, PatchTST ê°•í™” (ë³´ìˆ˜ì )
            weights = {
                'ExtremeNet': 0.10,
                'SpikeDetector': 0.10,
                'GoldenRule': 0.15,
                'PatchTST': 0.30,
                'StableLSTM': 0.35
            }
        elif trend in ['strong_falling', 'high_decreasing', 'decreasing']:
            # ì¼ë°˜ í•˜ë½ â†’ ì•ˆì •ì  ëª¨ë¸ ê°•í™”
            weights = {
                'ExtremeNet': 0.15,
                'SpikeDetector': 0.15,
                'GoldenRule': 0.20,
                'PatchTST': 0.25,
                'StableLSTM': 0.25
            }
        elif trend in ['extreme_plateau', 'high_stable']:
            # ê³ í‰ì› ìœ ì§€ â†’ ê· ë“±
            weights = base_weights
        else:
            # ì•ˆì • â†’ PatchTST, StableLSTM ê°•í™”
            weights = {
                'ExtremeNet': 0.15,
                'SpikeDetector': 0.15,
                'GoldenRule': 0.20,
                'PatchTST': 0.25,
                'StableLSTM': 0.25
            }
        
        # M14B ê¸°ë°˜ ì¶”ê°€ ì¡°ì •
        if m14b > 450:
            weights['SpikeDetector'] *= 1.2
            weights['GoldenRule'] *= 1.1
            # ì •ê·œí™”
            total = sum(weights.values())
            weights = {k: v/total for k, v in weights.items()}
        
        # ì•™ìƒë¸” ê³„ì‚°
        ensemble_pred = 0
        for model_name, weight in weights.items():
            if model_name in predictions_dict:
                ensemble_pred += predictions_dict[model_name] * weight
        
        # ì¶”ì„¸ ê¸°ë°˜ ìµœì¢… ì¡°ì •
        if trend_score >= 5:  # ë§¤ìš° ê°•í•œ ìƒìŠ¹
            ensemble_pred *= 1.1
        elif trend_score >= 3:  # ê°•í•œ ìƒìŠ¹
            ensemble_pred *= 1.05
        elif trend_score <= -5:  # ë§¤ìš° ê°•í•œ í•˜ë½
            ensemble_pred *= 0.9
        elif trend_score <= -3:  # ê°•í•œ í•˜ë½
            ensemble_pred *= 0.95
        
        # ê·¹ë‹¨ê°’ ë³´í˜¸
        if seq_max >= 1700 and trend_score > 0:
            ensemble_pred = max(ensemble_pred, 1700)
        elif seq_max >= 1650 and trend_score > 0:
            ensemble_pred = max(ensemble_pred, 1650)
        
        return ensemble_pred, weights
    
    def evaluate_all_models(self, test_file):
        """ì¶”ì„¸ ê°•í™” ëª¨ë¸ í‰ê°€"""
        
        # ë°ì´í„° ë¡œë“œ
        df = self.load_test_data(test_file)
        df = self.create_features(df)
        
        # ì˜ˆì¸¡ ê°€ëŠ¥ ë²”ìœ„
        start_idx = self.seq_len
        end_idx = len(df) - self.pred_len
        total = end_idx - start_idx
        
        print(f"\nğŸ”® ì˜ˆì¸¡ ì‹œì‘...")
        print(f"  ì‹œí€€ìŠ¤: {self.seq_len}ë¶„ â†’ ì˜ˆì¸¡: {self.pred_len}ë¶„ í›„")
        print(f"  ì˜ˆì¸¡ ê°œìˆ˜: {total:,}ê°œ")
        
        # ëª¨ë“  ì˜ˆì¸¡ì„ ì €ì¥í•  DataFrame ì¤€ë¹„
        all_predictions = pd.DataFrame()
        
        # ì‹œê°„ ë° íŠ¹ì§• ì •ë³´ ìˆ˜ì§‘
        timestamps_pred = []
        timestamps_target = []
        actuals = []
        m14b_values = []
        m14a_values = []
        sequence_infos = []
        
        print("\nğŸ“Š ê³ ê¸‰ ì‹œí€€ìŠ¤ ë¶„ì„ ì¤‘...")
        for i in range(start_idx, end_idx):
            pred_time = df.iloc[i]['CURRTIME']
            target_time = pred_time + timedelta(minutes=self.pred_len)
            
            actual_idx = i + self.pred_len
            if actual_idx < len(df):
                # ê¸°ë³¸ ì •ë³´
                timestamps_pred.append(pred_time)
                timestamps_target.append(target_time)
                actuals.append(df.iloc[actual_idx]['TOTALCNT'])
                m14b_values.append(df.iloc[i]['M14AM14B'])
                m14a_values.append(df.iloc[i]['M14AM10A'])
                
                # ê³ ê¸‰ ì‹œí€€ìŠ¤ ë¶„ì„
                seq_data = df.iloc[i-self.seq_len:i]['TOTALCNT'].values
                seq_info = self.trend_booster.analyze_sequence_advanced(seq_data)
                sequence_infos.append(seq_info)
        
        # ê¸°ë³¸ ì •ë³´ ì €ì¥
        all_predictions['ì˜ˆì¸¡ì‹œì '] = [t.strftime('%Y-%m-%d %H:%M') for t in timestamps_pred]
        all_predictions['ì˜ˆì¸¡ëŒ€ìƒì‹œê°„'] = [t.strftime('%Y-%m-%d %H:%M') for t in timestamps_target]
        all_predictions['ì‹¤ì œê°’'] = actuals
        all_predictions['M14AM14B'] = m14b_values
        all_predictions['M14AM10A'] = m14a_values
        
        # ì‹œí€€ìŠ¤ ë¶„ì„ ì •ë³´ ì¶”ê°€
        all_predictions['ì‹œí€€ìŠ¤_MAX'] = [info['max'] for info in sequence_infos]
        all_predictions['ì‹œí€€ìŠ¤_í‰ê· '] = [info['mean'] for info in sequence_infos]
        all_predictions['í˜„ì¬ê°’'] = [info['current'] for info in sequence_infos]
        all_predictions['ì¶”ì„¸'] = [info['trend'] for info in sequence_infos]
        all_predictions['ì¶”ì„¸ì ìˆ˜'] = [info['trend_score'] for info in sequence_infos]
        all_predictions['ì—°ì†ìƒìŠ¹'] = [info['consecutive_rises'] for info in sequence_infos]
        all_predictions['ì—°ì†í•˜ë½'] = [info['consecutive_falls'] for info in sequence_infos]
        all_predictions['ëª¨ë©˜í…€10'] = [info['momentum_10'] for info in sequence_infos]
        
        print(f"  ì˜ˆì¸¡í•  ë°ì´í„°: {len(all_predictions)}ê°œ")
        
        # ì¶”ì„¸ ë¶„í¬ ë¶„ì„
        trend_counts = all_predictions['ì¶”ì„¸'].value_counts()
        print(f"\nğŸ“ˆ ì¶”ì„¸ ë¶„í¬:")
        for trend, count in trend_counts.items():
            print(f"  {trend}: {count}ê°œ ({count/len(all_predictions)*100:.1f}%)")
        
        # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡
        model_metrics = {}
        model_predictions = {}
        
        for model_name, model in self.models.items():
            print(f"\nğŸ¯ {model_name} ì˜ˆì¸¡ ì¤‘...")
            predictions = []
            
            # ë°°ì¹˜ ì˜ˆì¸¡
            batch_size = 500
            for i in range(start_idx, end_idx, batch_size):
                batch_end = min(i + batch_size, end_idx)
                
                # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                X_batch = []
                for j in range(i, batch_end):
                    seq_data = df.iloc[j-self.seq_len:j][self.feature_columns].values
                    X_batch.append(seq_data)
                
                if len(X_batch) == 0:
                    continue
                
                # ìŠ¤ì¼€ì¼ë§
                X_batch = np.array(X_batch)
                X_batch_scaled = []
                for seq in X_batch:
                    seq_scaled = self.feature_scaler.transform(seq)
                    X_batch_scaled.append(seq_scaled)
                X_batch_scaled = np.array(X_batch_scaled)
                
                # ì˜ˆì¸¡
                preds = model.predict(X_batch_scaled, verbose=0)
                
                if isinstance(preds, list):
                    y_pred_scaled = preds[0].flatten()
                else:
                    y_pred_scaled = preds.flatten()
                
                # ì—­ë³€í™˜
                y_pred = self.target_scaler.inverse_transform(
                    y_pred_scaled.reshape(-1, 1)).flatten()
                
                # ìˆ˜ì§‘
                predictions.extend(y_pred[:batch_end - i])
            
            # ì˜ˆì¸¡ê°’ ì €ì¥
            predictions = predictions[:len(all_predictions)]
            model_predictions[model_name] = predictions
            
            # ì›ë³¸ ì˜ˆì¸¡ê°’ ì €ì¥
            all_predictions[f'{model_name}_ì›ë³¸'] = [round(p) for p in predictions]
            
            # ì¶”ì„¸ ê¸°ë°˜ ë¶€ìŠ¤íŒ…
            print(f"  ğŸ”¥ {model_name} ì¶”ì„¸ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ì ìš© ì¤‘...")
            boosted_predictions = []
            
            for i in range(len(predictions)):
                m14b = all_predictions.iloc[i]['M14AM14B']
                m14a = all_predictions.iloc[i]['M14AM10A']
                seq_info = sequence_infos[i]
                original = predictions[i]
                
                # ì¶”ì„¸ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ì ìš©
                boosted = self.trend_booster.trend_based_boosting(
                    original, m14b, m14a, model_name, seq_info
                )
                
                boosted_predictions.append(boosted)
            
            all_predictions[f'{model_name}_ì˜ˆì¸¡'] = [round(p) for p in boosted_predictions]
            all_predictions[f'{model_name}_ì˜¤ì°¨'] = all_predictions[f'{model_name}_ì˜ˆì¸¡'] - all_predictions['ì‹¤ì œê°’']
            all_predictions[f'{model_name}_ì˜¤ì°¨ìœ¨(%)'] = round(
                abs(all_predictions[f'{model_name}_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’'] * 100, 2
            )
            
            # ì„±ëŠ¥ ê³„ì‚°
            mae = mean_absolute_error(all_predictions['ì‹¤ì œê°’'], boosted_predictions)
            rmse = np.sqrt(mean_squared_error(all_predictions['ì‹¤ì œê°’'], boosted_predictions))
            r2 = r2_score(all_predictions['ì‹¤ì œê°’'], boosted_predictions)
            mape = np.mean(abs(all_predictions[f'{model_name}_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’']) * 100
            
            model_metrics[model_name] = {
                'MAE': mae,
                'RMSE': rmse,
                'R2': r2,
                'MAPE': mape,
                'ì •í™•ë„(%)': 100 - mape
            }
            
            print(f"  âœ… {model_name} ì™„ë£Œ: MAE={mae:.2f}, RÂ²={r2:.4f}, ì •í™•ë„={100-mape:.2f}%")
        
        # ì¶”ì„¸ ì¸ì‹ ì•™ìƒë¸”
        print("\nğŸ”¥ ì¶”ì„¸ ì¸ì‹ ì•™ìƒë¸” ìƒì„±...")
        
        trend_ensemble = []
        ensemble_weights_list = []
        
        for i in range(len(all_predictions)):
            m14b = all_predictions.iloc[i]['M14AM14B']
            m14a = all_predictions.iloc[i]['M14AM10A']
            seq_info = sequence_infos[i]
            
            # ê° ëª¨ë¸ ì˜ˆì¸¡ê°’
            preds_dict = {}
            for model_name in self.models.keys():
                preds_dict[model_name] = all_predictions.iloc[i][f'{model_name}_ì˜ˆì¸¡']
            
            # ì¶”ì„¸ ì¸ì‹ ì•™ìƒë¸”
            ensemble_pred, weights = self.create_trend_aware_ensemble(
                preds_dict, seq_info, m14b, m14a
            )
            
            trend_ensemble.append(ensemble_pred)
            ensemble_weights_list.append(weights)
        
        # ì¶”ì„¸ ì•™ìƒë¸” ê²°ê³¼ ì¶”ê°€
        all_predictions['ì¶”ì„¸ì•™ìƒë¸”_ì˜ˆì¸¡'] = [round(p) for p in trend_ensemble]
        all_predictions['ì¶”ì„¸ì•™ìƒë¸”_ì˜¤ì°¨'] = all_predictions['ì¶”ì„¸ì•™ìƒë¸”_ì˜ˆì¸¡'] - all_predictions['ì‹¤ì œê°’']
        all_predictions['ì¶”ì„¸ì•™ìƒë¸”_ì˜¤ì°¨ìœ¨(%)'] = round(
            abs(all_predictions['ì¶”ì„¸ì•™ìƒë¸”_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’'] * 100, 2
        )
        
        # ì¶”ì„¸ ì•™ìƒë¸” ì„±ëŠ¥
        trend_mae = mean_absolute_error(all_predictions['ì‹¤ì œê°’'], trend_ensemble)
        trend_rmse = np.sqrt(mean_squared_error(all_predictions['ì‹¤ì œê°’'], trend_ensemble))
        trend_r2 = r2_score(all_predictions['ì‹¤ì œê°’'], trend_ensemble)
        trend_mape = np.mean(abs(all_predictions['ì¶”ì„¸ì•™ìƒë¸”_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’']) * 100
        
        model_metrics['ì¶”ì„¸ì•™ìƒë¸”'] = {
            'MAE': trend_mae,
            'RMSE': trend_rmse,
            'R2': trend_r2,
            'MAPE': trend_mape,
            'ì •í™•ë„(%)': 100 - trend_mape
        }
        
        print(f"âœ… ì¶”ì„¸ì•™ìƒë¸”: MAE={trend_mae:.2f}, RÂ²={trend_r2:.4f}, ì •í™•ë„={100-trend_mape:.2f}%")
        
        # CSV ì €ì¥
        output_file = f'v68_trend_predictions_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        all_predictions.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ì˜ˆì¸¡ê°’ ì €ì¥: {output_file}")
        
        # ==================== ì¶”ì„¸ë³„ ìƒì„¸ ë¶„ì„ ====================
        print("\n" + "="*80)
        print("ğŸ¯ğŸ¯ğŸ¯ ì¶”ì„¸ë³„ ìƒì„¸ ë¶„ì„ ğŸ¯ğŸ¯ğŸ¯")
        print("="*80)
        
        # ì¶”ì„¸ë³„ ì„±ëŠ¥ ë¶„ì„
        trend_performance = {}
        for trend in all_predictions['ì¶”ì„¸'].unique():
            trend_mask = all_predictions['ì¶”ì„¸'] == trend
            if trend_mask.sum() > 0:
                trend_df = all_predictions[trend_mask]
                
                # ê° ëª¨ë¸ì˜ ì¶”ì„¸ë³„ ì„±ëŠ¥
                trend_perf = {}
                for model_name in list(self.models.keys()) + ['ì¶”ì„¸ì•™ìƒë¸”']:
                    mae = mean_absolute_error(
                        trend_df['ì‹¤ì œê°’'], 
                        trend_df[f'{model_name}_ì˜ˆì¸¡']
                    )
                    mape = np.mean(abs(trend_df[f'{model_name}_ì˜¤ì°¨']) / trend_df['ì‹¤ì œê°’']) * 100
                    trend_perf[model_name] = {
                        'MAE': mae,
                        'MAPE': mape,
                        'ì •í™•ë„': 100 - mape
                    }
                
                trend_performance[trend] = {
                    'count': trend_mask.sum(),
                    'performance': trend_perf
                }
        
        # ì¶”ì„¸ë³„ ìµœì  ëª¨ë¸ ì¶œë ¥
        print("\nğŸ“Š ì¶”ì„¸ë³„ ìµœì  ëª¨ë¸:")
        print("-" * 80)
        print(f"{'ì¶”ì„¸':20} {'ìƒ˜í”Œìˆ˜':>8} {'ìµœì ëª¨ë¸':15} {'ì •í™•ë„':>10}")
        print("-" * 80)
        
        for trend, data in trend_performance.items():
            if data['count'] >= 10:  # ìƒ˜í”Œì´ 10ê°œ ì´ìƒì¸ ê²½ìš°ë§Œ
                best_model = min(data['performance'].items(), 
                                key=lambda x: x[1]['MAPE'])[0]
                best_accuracy = data['performance'][best_model]['ì •í™•ë„']
                
                trend_icon = "ğŸ“ˆ" if 'rising' in trend or 'increasing' in trend else \
                            "ğŸ“‰" if 'falling' in trend or 'decreasing' in trend else "â¡ï¸"
                
                print(f"{trend_icon} {trend:18} {data['count']:8} {best_model:15} {best_accuracy:10.2f}%")
        
        # ê³ ê°’ êµ¬ê°„ ì¶”ì„¸ ë¶„ì„
        print("\nğŸ¯ ê³ ê°’ êµ¬ê°„(1700+) ì¶”ì„¸ ë¶„ì„:")
        print("-" * 80)
        
        high_mask = all_predictions['ì‹¤ì œê°’'] >= 1700
        if high_mask.any():
            high_df = all_predictions[high_mask]
            
            # ì¶”ì„¸ë³„ ë¶„í¬
            high_trends = high_df['ì¶”ì„¸'].value_counts()
            print("\n[ê³ ê°’ êµ¬ê°„ ì¶”ì„¸ ë¶„í¬]")
            for trend, count in high_trends.items():
                print(f"  {trend}: {count}ê°œ ({count/len(high_df)*100:.1f}%)")
            
            # ì—°ì† ìƒìŠ¹/í•˜ë½ê³¼ ì •í™•ë„ ê´€ê³„
            print("\n[ì—°ì† ë³€í™”ì™€ ì˜ˆì¸¡ ì •í™•ë„]")
            
            # ì—°ì† ìƒìŠ¹ êµ¬ê°„ë³„
            for rises in [15, 10, 7, 5, 3]:
                rise_mask = high_df['ì—°ì†ìƒìŠ¹'] >= rises
                if rise_mask.sum() > 0:
                    rise_df = high_df[rise_mask]
                    accuracy = 100 - np.mean(abs(rise_df['ì¶”ì„¸ì•™ìƒë¸”_ì˜¤ì°¨']) / rise_df['ì‹¤ì œê°’']) * 100
                    hit_rate = (rise_df['ì¶”ì„¸ì•™ìƒë¸”_ì˜ˆì¸¡'] >= 1700).sum() / len(rise_df) * 100
                    print(f"  ì—°ì†ìƒìŠ¹ {rises}+: {rise_mask.sum()}ê°œ, "
                          f"ì •í™•ë„={accuracy:.1f}%, 1700+ì ì¤‘={hit_rate:.1f}%")
            
            # ìƒ˜í”Œ ì¶œë ¥
            print(f"\n[ê³ ê°’ ì˜ˆì¸¡ ìƒì„¸] (ì¶”ì„¸ë³„ ëŒ€í‘œ 10ê°œ)")
            print("-" * 100)
            print(f"{'ì¶”ì„¸':15} {'ì‹¤ì œ':>7} {'ì˜ˆì¸¡':>7} {'ì—°ì†â†‘':>6} {'ì—°ì†â†“':>6} {'ëª¨ë©˜í…€':>8} {'ì˜¤ì°¨':>7} {'ì ì¤‘':>6}")
            print("-" * 100)
            
            # ì£¼ìš” ì¶”ì„¸ë³„ë¡œ ìƒ˜í”Œ ì¶œë ¥
            for trend in ['extreme_rising', 'strong_rising', 'high_stable', 'strong_falling']:
                trend_mask = (high_df['ì¶”ì„¸'] == trend)
                if trend_mask.sum() > 0:
                    sample_df = high_df[trend_mask].head(10)
                    
                    for idx in sample_df.index:
                        row = all_predictions.loc[idx]
                        actual = row['ì‹¤ì œê°’']
                        pred = row['ì¶”ì„¸ì•™ìƒë¸”_ì˜ˆì¸¡']
                        c_rises = row['ì—°ì†ìƒìŠ¹']
                        c_falls = row['ì—°ì†í•˜ë½']
                        momentum = row['ëª¨ë©˜í…€10']
                        error = pred - actual
                        
                        # ì ì¤‘ ì—¬ë¶€
                        if pred >= 1700:
                            hit = "âœ…"
                        elif pred >= 1650:
                            hit = "âš ï¸"
                        else:
                            hit = "âŒ"
                        
                        print(f"{trend:15} {actual:7.0f} {pred:7.0f} {c_rises:6} "
                              f"{c_falls:6} {momentum:8.2f} {error:+7.0f} {hit:>6}")
        
        # ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸”
        print("\n" + "="*80)
        print("ğŸ“Š ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½")
        print("="*80)
        
        metrics_df = pd.DataFrame(model_metrics).T
        metrics_df = metrics_df.sort_values('R2', ascending=False)
        
        print(f"\n{'ëª¨ë¸':<15} {'MAE':>8} {'RMSE':>8} {'RÂ²':>8} {'MAPE(%)':>8} {'ì •í™•ë„(%)':>10}")
        print("-" * 65)
        
        for model_name, row in metrics_df.iterrows():
            if model_name == 'ì¶”ì„¸ì•™ìƒë¸”':
                print(f"{'ğŸ”¥ ' + model_name:<15} {row['MAE']:8.2f} {row['RMSE']:8.2f} "
                      f"{row['R2']:8.4f} {row['MAPE']:8.2f} {row['ì •í™•ë„(%)']:10.2f} â­â­â­")
            else:
                print(f"{model_name:<15} {row['MAE']:8.2f} {row['RMSE']:8.2f} "
                      f"{row['R2']:8.4f} {row['MAPE']:8.2f} {row['ì •í™•ë„(%)']:10.2f}")
        
        return all_predictions, metrics_df

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    print("\nğŸš€ V6.8 ì¶”ì„¸ ê°•í™” ê·¹ë‹¨ê°’ í‰ê°€ ì‹œì‘!")
    print("í•µì‹¬: ì¶”ì„¸ë³„ ì°¨ë³„í™”ëœ ì˜ˆì¸¡ ì „ëµ")
    print("  - ê·¹ë‹¨ ìƒìŠ¹ â†’ ê³µê²©ì  ë¶€ìŠ¤íŒ…")
    print("  - ê·¹ë‹¨ í•˜ë½ â†’ ë³´ìˆ˜ì  ì¡°ì •")
    print("  - ê³ í‰ì› â†’ ì•ˆì •ì  ì˜ˆì¸¡")
    
    # í‰ê°€ê¸° ìƒì„±
    evaluator = AdvancedModelEvaluator()
    
    # ëª¨ë“  ëª¨ë¸ ë¡œë“œ
    models = evaluator.load_all_models('models/')
    
    if not models:
        print("âŒ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼
    test_files = [
        'data/M14_20250916_20250817.csv',
        'data/test_data.csv',
        '/mnt/user-data/uploads/test.csv'
    ]
    
    test_file = None
    for file in test_files:
        if os.path.exists(file):
            test_file = file
            break
    
    if not test_file:
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # í‰ê°€ ì‹¤í–‰
    all_predictions, metrics = evaluator.evaluate_all_models(test_file)
    
    print("\n" + "="*80)
    print("ğŸ† V6.8 ì¶”ì„¸ ê°•í™” ê·¹ë‹¨ê°’ í‰ê°€ ì™„ë£Œ!")
    print("="*80)
    print("\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
    print(f"  1. v68_trend_predictions_YYYYMMDD.csv - ì¶”ì„¸ ê°•í™” ì˜ˆì¸¡")
    print("\nğŸ”¥ í•µì‹¬ ê°œì„ ì‚¬í•­:")
    print("  âœ… ì—°ì† ìƒìŠ¹/í•˜ë½ ì¹´ìš´íŠ¸ ê¸°ë°˜ ì˜ˆì¸¡")
    print("  âœ… ëª¨ë©˜í…€ ì§€í‘œ í™œìš©")
    print("  âœ… ì¶”ì„¸ë³„ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì •")
    print("  âœ… ê³ í‰ì› íŠ¹ë³„ ì²˜ë¦¬")
    print("  âœ… ê¸‰ë³€ ê°ì§€ ë° ëŒ€ì‘")
    print("="*80)

if __name__ == "__main__":
    main()