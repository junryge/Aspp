"""
ğŸ“Š V6.7 ê°œì„ ëœ ê·¹ë‹¨ê°’ ì˜ˆì¸¡ í‰ê°€ ì‹œìŠ¤í…œ
========================================================
í•µì‹¬ ê°œì„ : ì‹œí€€ìŠ¤ maxê°’ 1651+ & ì¦ê°€ ì¶”ì„¸ì¼ ë•Œë§Œ ExtremeNet ë¶€ìŠ¤íŒ…
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import json
import os
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')
tf.keras.config.enable_unsafe_deserialization()

# ====================== ê°œì„ ëœ ê·¹ë‹¨ê°’ ë³´ì • í´ë˜ìŠ¤ ======================
class ImprovedExtremeValueBooster:
    """ì‹œí€€ìŠ¤ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ê·¹ë‹¨ê°’ ì˜ˆì¸¡"""
    
    def __init__(self):
        print("ğŸ”¥ ê°œì„ ëœ ê·¹ë‹¨ê°’ ë¶€ìŠ¤í„° ì´ˆê¸°í™”")
        print("  - ì‹œí€€ìŠ¤ maxê°’ 1651 ì´ìƒ ì²´í¬")
        print("  - ì¦ê°€/í•˜ë½ ì¶”ì„¸ ë¶„ì„")
        print("  - ì¶”ì„¸ë³„ ì°¨ë³„í™”ëœ ë¶€ìŠ¤íŒ…")
        
    def analyze_sequence(self, sequence_data):
        """ì‹œí€€ìŠ¤ ë¶„ì„: maxê°’, ì¶”ì„¸, ì—°ì† ìƒìŠ¹/í•˜ë½ ì •ë„ ê³„ì‚°"""
        if len(sequence_data) == 0:
            return None, 'stable', False, 0, 0, 0, 0
        
        # 1. ì‹œí€€ìŠ¤ maxê°’ê³¼ í‰ê· ê°’
        seq_max = np.max(sequence_data)
        seq_min = np.min(sequence_data)
        seq_mean = np.mean(sequence_data[-30:]) if len(sequence_data) >= 30 else np.mean(sequence_data)
        
        # 2. ê³ í‰ì› ìƒíƒœ ì²´í¬ (ìµœê·¼ 30ê°œ í‰ê· ì´ 1700 ì´ìƒ)
        is_high_plateau = seq_mean >= 1700
        
        # 3. ì—°ì† ìƒìŠ¹ ì¹´ìš´íŠ¸ ê³„ì‚°
        consecutive_rises = 0
        for i in range(len(sequence_data)-1, 0, -1):
            if sequence_data[i] > sequence_data[i-1]:
                consecutive_rises += 1
            else:
                break
        
        # 4. ì—°ì† í•˜ë½ ì¹´ìš´íŠ¸ ê³„ì‚°
        consecutive_falls = 0
        for i in range(len(sequence_data)-1, 0, -1):
            if sequence_data[i] < sequence_data[i-1]:
                consecutive_falls += 1
            else:
                break
        
        # 5. ìƒìŠ¹/í•˜ë½ ê°•ë„ ê³„ì‚° (ìµœê·¼ 10ê°œ ë°ì´í„°)
        rise_strength = 0
        fall_strength = 0
        if len(sequence_data) >= 10:
            recent_10 = sequence_data[-10:]
            change = recent_10[-1] - recent_10[0]  # 10ë¶„ê°„ ì´ ë³€í™”ëŸ‰
            if change > 0:
                rise_strength = change
            else:
                fall_strength = abs(change)
        
        # 6. ì¶”ì„¸ ë¶„ì„ (ë§ˆì§€ë§‰ 30ê°œ ë°ì´í„°)
        if len(sequence_data) >= 30:
            recent = sequence_data[-30:]
            x = np.arange(len(recent))
            coeffs = np.polyfit(x, recent, 1)
            slope = coeffs[0]
            
            # ê³ í‰ì› ìƒíƒœì—ì„œì˜ ì¶”ì„¸ íŒë‹¨
            if is_high_plateau:
                # ê³ í‰ì›(1700+)ì—ì„œì˜ ê·¹ë‹¨ì  ë³€í™” ê°ì§€
                if consecutive_rises >= 10 and rise_strength > 50:
                    trend = 'extreme_rising'  # ê·¹ë‹¨ì  ìƒìŠ¹
                elif consecutive_falls >= 10 and fall_strength > 50:
                    trend = 'extreme_falling'  # ê·¹ë‹¨ì  í•˜ë½ (ê³ í‰ì›ì—ì„œ ê¸‰ë½)
                elif slope > 1 or consecutive_rises >= 5:
                    trend = 'high_increasing'  # ê³ í‰ì› ìƒìŠ¹
                elif slope < -1 or consecutive_falls >= 5:
                    trend = 'high_decreasing'  # ê³ í‰ì› í•˜ë½
                else:
                    trend = 'high_stable'  # ê³ í‰ì› ìœ ì§€
            else:
                # ì¼ë°˜ êµ¬ê°„ ì¶”ì„¸ íŒë‹¨
                if consecutive_rises >= 10 and rise_strength > 50:
                    trend = 'strong_rising'  # ê°•í•œ ì—°ì† ìƒìŠ¹
                elif consecutive_falls >= 10 and fall_strength > 50:
                    trend = 'strong_falling'  # ê°•í•œ ì—°ì† í•˜ë½
                elif consecutive_rises >= 7 and rise_strength > 30:
                    trend = 'rapid_increasing'  # ë¹ ë¥¸ ìƒìŠ¹
                elif consecutive_falls >= 7 and fall_strength > 30:
                    trend = 'rapid_decreasing'  # ë¹ ë¥¸ í•˜ë½
                elif slope > 2:
                    trend = 'increasing'
                elif slope < -2:
                    trend = 'decreasing'
                else:
                    trend = 'stable'
        else:
            trend = 'stable'
        
        # ë³€ë™ì„± ì§€í‘œ (ìµœê·¼ 10ê°œì˜ í‘œì¤€í¸ì°¨)
        volatility = np.std(sequence_data[-10:]) if len(sequence_data) >= 10 else 0
        
        return {
            'max': seq_max,
            'min': seq_min,
            'trend': trend,
            'is_high_plateau': is_high_plateau,
            'consecutive_rises': consecutive_rises,
            'consecutive_falls': consecutive_falls,
            'rise_strength': rise_strength,
            'fall_strength': fall_strength,
            'volatility': volatility
        }
    
    def boost_prediction(self, pred, m14b_value, m14a_value=None, model_name=None, 
                        sequence_info=None):
        """ì‹œí€€ìŠ¤ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë¶€ìŠ¤íŒ… (ìƒìŠ¹/í•˜ë½ ê°ë„ í¬í•¨)"""
        if not sequence_info:
            return pred
            
        original = pred
        boosted = pred
        seq_max = sequence_info.get('max', 0)
        seq_trend = sequence_info.get('trend', 'stable')
        consecutive_rises = sequence_info.get('consecutive_rises', 0)
        consecutive_falls = sequence_info.get('consecutive_falls', 0)
        rise_strength = sequence_info.get('rise_strength', 0)
        fall_strength = sequence_info.get('fall_strength', 0)
        
        # ========== ExtremeNet íŠ¹ë³„ ì²˜ë¦¬ ==========
        if model_name == 'ExtremeNet':
            
            # ğŸš€ ê·¹ë‹¨ì  ìƒìŠ¹ (10ë¶„+ ì—°ì† ìƒìŠ¹)
            if seq_trend == 'extreme_rising':
                print(f"    ğŸš€ğŸš€ ExtremeNet ê·¹ë‹¨ ìƒìŠ¹! (ì—°ì†{consecutive_rises}ë¶„â†‘, ê°•ë„{rise_strength:.0f})")
                # ìƒìŠ¹ ê°•ë„ì— ë¹„ë¡€í•œ ë¶€ìŠ¤íŒ…
                boost_factor = 1.5 + (rise_strength / 100) * 0.3  # ìµœëŒ€ 1.8ë°°
                boosted = pred * min(boost_factor, 1.8)
                boosted = max(boosted, 1900)
                
            # ğŸ’¥ ê·¹ë‹¨ì  í•˜ë½ (10ë¶„+ ì—°ì† í•˜ë½)
            elif seq_trend == 'extreme_falling':
                print(f"    ğŸ’¥ğŸ’¥ ExtremeNet ê·¹ë‹¨ í•˜ë½! (ì—°ì†{consecutive_falls}ë¶„â†“, ê°•ë„{fall_strength:.0f})")
                # í•˜ë½ ê°•ë„ì— ë¹„ë¡€í•œ ê°ì†Œ
                reduction_factor = 0.9 - (fall_strength / 100) * 0.1  # ìµœëŒ€ 20% ê°ì†Œ
                boosted = pred * max(reduction_factor, 0.8)
                
            # ğŸ“ˆ ê°•í•œ ìƒìŠ¹ ì¶”ì„¸
            elif seq_trend in ['strong_rising', 'increasing']:
                print(f"    ğŸ“ˆ ExtremeNet ìƒìŠ¹ (ì—°ì†{consecutive_rises}ë¶„â†‘)")
                boost_factor = 1.3 + (consecutive_rises / 20) * 0.2
                boosted = pred * min(boost_factor, 1.5)
                if seq_max >= 1651:
                    boosted = max(boosted, 1750)
                    
            # ğŸ“‰ ê°•í•œ í•˜ë½ ì¶”ì„¸
            elif seq_trend in ['strong_falling', 'rapid_decreasing']:
                print(f"    ğŸ“‰ ExtremeNet ê°•í•œ í•˜ë½ (ì—°ì†{consecutive_falls}ë¶„â†“)")
                reduction_factor = 0.95 - (consecutive_falls / 20) * 0.05
                boosted = pred * max(reduction_factor, 0.85)
                
            # ğŸ”¥ ê³ í‰ì› ìƒíƒœë³„ ì²˜ë¦¬
            elif seq_trend == 'high_increasing':
                print(f"    ğŸ“ˆ ExtremeNet ê³ í‰ì› ìƒìŠ¹! (max={seq_max:.0f})")
                if m14b_value > 500:
                    boosted = max(pred * 1.6, 1850)
                else:
                    boosted = max(pred * 1.4, 1750)
                    
            elif seq_trend == 'high_stable':
                print(f"    â¡ï¸ ExtremeNet ê³ í‰ì› ìœ ì§€ (max={seq_max:.0f})")
                boosted = max(pred, 1700)  # ìµœì†Œ 1700 ë³´ì¥
                
            elif seq_trend == 'high_decreasing':
                print(f"    ğŸ“‰ ExtremeNet ê³ í‰ì› í•˜ë½ (max={seq_max:.0f})")
                boosted = pred * 0.98
                
            # ì¼ë°˜ ìƒìŠ¹ (ê¸°ì¡´ ì¡°ê±´)
            elif seq_max >= 1651 and seq_trend == 'increasing':
                print(f"    ğŸ“ˆ ExtremeNet ë¶€ìŠ¤íŒ… í™œì„±í™”! (max={seq_max:.0f})")
                if m14b_value > 450:
                    boosted = max(pred * 1.4, 1700)
                else:
                    boosted = pred * 1.2
                    
            # ì¼ë°˜ í•˜ë½
            elif seq_trend == 'decreasing':
                if seq_max >= 1651:
                    boosted = pred * 0.95
                else:
                    boosted = pred
            
            # ì•ˆì •ì ì´ê±°ë‚˜ maxê°’ ë¯¸ë‹¬
            else:
                if seq_max < 1651:
                    print(f"    âšª ExtremeNet ì›ë³¸ ì‚¬ìš© (max={seq_max:.0f} < 1651)")
                boosted = pred
                
        # ========== SpikeDetector, GoldenRule ==========
        elif model_name in ['SpikeDetector', 'GoldenRule']:
            # sequence_infoì—ì„œ ì¶”ì„¸ ê°€ì ¸ì˜¤ê¸°
            seq_trend = sequence_info.get('trend', 'stable')
            
            # ê³ í‰ì› ìƒíƒœ ì²˜ë¦¬
            if seq_trend in ['high_increasing', 'high_stable', 'high_decreasing']:
                # ê³ í‰ì›ì—ì„œëŠ” ì´ë¯¸ ë†’ì€ ì˜ˆì¸¡ì„ ìœ ì§€
                if seq_trend == 'high_increasing':
                    # ê³ í‰ì› ìƒìŠ¹ â†’ ì¶”ê°€ ë¶€ìŠ¤íŒ…
                    if m14b_value > 500:
                        boosted = max(pred * 1.1, 1900)
                    else:
                        boosted = max(pred * 1.05, 1800)
                elif seq_trend == 'high_stable':
                    # ê³ í‰ì› ìœ ì§€ â†’ í˜„ì¬ ìˆ˜ì¤€
                    boosted = max(pred, 1700)
                else:  # high_decreasing
                    # ê³ í‰ì› í•˜ë½ â†’ ì•½ê°„ ë³´ìˆ˜ì 
                    boosted = pred * 0.98
            else:
                # ì¼ë°˜ ìƒíƒœ (ê¸°ì¡´ ë¡œì§)
                if m14b_value > 550:
                    boosted = max(pred, 1850)
                elif m14b_value > 500:
                    boosted = max(pred, 1750)
                elif m14b_value > 450:
                    boosted = max(pred, 1700)
                elif m14b_value > 400:
                    boosted = max(pred * 1.05, 1650)
            
            # í•˜ë½ ì¶”ì„¸ì—ì„œëŠ” ì¶”ê°€ ì¡°ì • ì—†ìŒ
                
        # ========== ê¸°íƒ€ ëª¨ë¸ (PatchTST, StableLSTM) ==========
        else:
            # sequence_infoì—ì„œ ì¶”ì„¸ ê°€ì ¸ì˜¤ê¸°
            seq_trend = sequence_info.get('trend', 'stable')
            
            # ê³ í‰ì› ìƒíƒœ ìš°ì„  ì²˜ë¦¬
            if seq_trend in ['high_increasing', 'high_stable', 'high_decreasing']:
                if seq_trend == 'high_increasing':
                    # ê³ í‰ì› ìƒìŠ¹ â†’ ê°•í•œ ë¶€ìŠ¤íŒ…
                    if m14b_value > 500:
                        boosted = max(pred * 1.5, 1850)
                    else:
                        boosted = max(pred * 1.3, 1750)
                elif seq_trend == 'high_stable':
                    # ê³ í‰ì› ìœ ì§€ â†’ ì•ˆì •ì  ì˜ˆì¸¡
                    boosted = max(pred, 1700)
                else:  # high_decreasing
                    # ê³ í‰ì› í•˜ë½ â†’ ë³´ìˆ˜ì 
                    boosted = pred * 0.97
            # ì¼ë°˜ ì¦ê°€ ì¶”ì„¸
            elif seq_trend == 'increasing':
                # ì¦ê°€ ì¶”ì„¸ â†’ ì ê·¹ì  ë¶€ìŠ¤íŒ…
                if m14b_value > 550:
                    boosted = max(pred * 1.45, 1850)
                elif m14b_value > 500:
                    boosted = max(pred * 1.35, 1750)
                elif m14b_value > 450:
                    boosted = max(pred * 1.25, 1700)
                elif m14b_value > 400:
                    boosted = max(pred * 1.15, 1650)
                elif m14b_value > 350:
                    boosted = max(pred * 1.08, 1550)
                    
            elif seq_trend == 'decreasing':
                # í•˜ë½ ì¶”ì„¸ â†’ ë³´ìˆ˜ì  ì ‘ê·¼
                if m14b_value > 450:
                    boosted = max(pred * 1.05, 1650)  # ì•½í•œ ë¶€ìŠ¤íŒ…
                else:
                    boosted = pred  # ì›ë³¸ ìœ ì§€
            else:
                # ì•ˆì •ì  â†’ ê¸°ë³¸ ë¶€ìŠ¤íŒ…
                if m14b_value > 450:
                    boosted = max(pred * 1.15, 1700)
                elif m14b_value > 400:
                    boosted = max(pred * 1.10, 1650)
        
        # í™©ê¸ˆ íŒ¨í„´ ì¶”ê°€ ë¶€ìŠ¤íŒ… (ëª¨ë“  ëª¨ë¸ ê³µí†µ)
        if m14b_value > 300 and m14a_value and m14a_value < 80:
            if seq_trend == 'increasing':
                boosted = boosted * 1.15  # ì¦ê°€ ì¶”ì„¸ë©´ ë” ê°•í•˜ê²Œ
            else:
                boosted = boosted * 1.08  # ì•„ë‹ˆë©´ ì•½í•˜ê²Œ
            
        return boosted

class ImprovedModelEvaluator:
    def __init__(self, scaler_path='scalers/'):
        """ê°œì„ ëœ í‰ê°€ê¸° ì´ˆê¸°í™”"""
        print("="*80)
        print("ğŸ”¥ V6.7 ê°œì„ ëœ ê·¹ë‹¨ê°’ í‰ê°€ ì‹œìŠ¤í…œ")
        print("  í•µì‹¬: ì‹œí€€ìŠ¤ max 1651+ & ì¦ê°€ ì¶”ì„¸ â†’ ExtremeNet ë¶€ìŠ¤íŒ…")
        print("="*80)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        with open(f'{scaler_path}feature_scaler.pkl', 'rb') as f:
            self.feature_scaler = pickle.load(f)
        with open(f'{scaler_path}target_scaler.pkl', 'rb') as f:
            self.target_scaler = pickle.load(f)
        with open(f'{scaler_path}config.json', 'r') as f:
            config = json.load(f)
            self.seq_len = config['seq_len']
            self.pred_len = config['pred_len']
            self.feature_columns = config['feature_columns']
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
        
        self.models = {}
        self.extreme_booster = ImprovedExtremeValueBooster()
        
    def load_all_models(self, model_dir='models/'):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        print(f"\nğŸ“ ëª¨ë¸ ë¡œë”©...")
        
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.keras')]
        
        for model_file in model_files:
            model_name = model_file.replace('.keras', '')
            model_path = os.path.join(model_dir, model_file)
            
            try:
                self.models[model_name] = tf.keras.models.load_model(
                    model_path, safe_mode=False
                )
                print(f"  âœ… {model_name} ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"\nì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return self.models
    
    def load_test_data(self, filepath):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        print(f"\nğŸ“‚ í‰ê°€ ë°ì´í„° ë¡œë”©: {filepath}")
        df = pd.read_csv(filepath)
        print(f"  ì›ë³¸: {df.shape[0]:,}í–‰")
        
        # 0ê°’ ì œê±°
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        
        # ì‹œê°„ ë³€í™˜
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                       format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        print(f"  ìœ íš¨: {df.shape[0]:,}í–‰")
        
        # ê³ ê°’ í†µê³„ ì¶œë ¥
        high_count = (df['TOTALCNT'] >= 1700).sum()
        very_high_count = (df['TOTALCNT'] >= 1750).sum()
        extreme_count = (df['TOTALCNT'] >= 1800).sum()
        max_1651_count = (df['TOTALCNT'] >= 1651).sum()
        
        print(f"\nğŸ¯ ê³ ê°’ êµ¬ê°„ ë¶„í¬:")
        print(f"  1651+: {max_1651_count}ê°œ ({max_1651_count/len(df)*100:.1f}%) â† ê¸°ì¤€ê°’")
        print(f"  1700+: {high_count}ê°œ ({high_count/len(df)*100:.1f}%)")
        print(f"  1750+: {very_high_count}ê°œ ({very_high_count/len(df)*100:.1f}%)")
        print(f"  1800+: {extreme_count}ê°œ ({extreme_count/len(df)*100:.1f}%)")
        
        # M14B ë¶„í¬
        m14b_high = (df['M14AM14B'] > 450).sum()
        print(f"\nğŸ“Š M14AM14B ë¶„í¬:")
        print(f"  450+: {m14b_high}ê°œ ({m14b_high/len(df)*100:.1f}%)")
        
        return df
    
    def create_features(self, df):
        """íŠ¹ì„± ìƒì„±"""
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        
        df['HOUR'] = df['CURRTIME'].dt.hour
        df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
        df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
        
        for w in [10, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
        
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        
        return df
    
    def evaluate_all_models(self, test_file):
        """ê°œì„ ëœ ëª¨ë¸ í‰ê°€ - ì‹œí€€ìŠ¤ ê¸°ë°˜ ë¶€ìŠ¤íŒ…"""
        
        # ë°ì´í„° ë¡œë“œ
        df = self.load_test_data(test_file)
        df = self.create_features(df)
        
        # ì˜ˆì¸¡ ê°€ëŠ¥ ë²”ìœ„
        start_idx = self.seq_len
        end_idx = len(df) - self.pred_len
        total = end_idx - start_idx
        
        print(f"\nğŸ”® ì˜ˆì¸¡ ì‹œì‘...")
        print(f"  ì‹œí€€ìŠ¤: {self.seq_len}ë¶„ â†’ ì˜ˆì¸¡: {self.pred_len}ë¶„ í›„")
        print(f"  ì˜ˆì¸¡ ê°œìˆ˜: {total:,}ê°œ")
        
        # ëª¨ë“  ì˜ˆì¸¡ì„ ì €ì¥í•  DataFrame ì¤€ë¹„
        all_predictions = pd.DataFrame()
        
        # ì‹œê°„ ë° íŠ¹ì§• ì •ë³´ ìˆ˜ì§‘
        timestamps_pred = []
        timestamps_target = []
        actuals = []
        m14b_values = []
        m14a_values = []
        sequence_maxes = []
        sequence_trends = []
        
        print("\nğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ë° ì‹œí€€ìŠ¤ ë¶„ì„ ì¤‘...")
        for i in range(start_idx, end_idx):
            pred_time = df.iloc[i]['CURRTIME']
            target_time = pred_time + timedelta(minutes=self.pred_len)
            
            actual_idx = i + self.pred_len
            if actual_idx < len(df):
                # ê¸°ë³¸ ì •ë³´
                timestamps_pred.append(pred_time)
                timestamps_target.append(target_time)
                actuals.append(df.iloc[actual_idx]['TOTALCNT'])
                m14b_values.append(df.iloc[i]['M14AM14B'])
                m14a_values.append(df.iloc[i]['M14AM10A'])
                
                # ì‹œí€€ìŠ¤ ë¶„ì„ (ê³¼ê±° 100ë¶„ TOTALCNT)
                seq_data = df.iloc[i-self.seq_len:i]['TOTALCNT'].values
                seq_info = self.extreme_booster.analyze_sequence(seq_data)
                sequence_maxes.append(seq_info.get('max'))
                sequence_trends.append(seq_info.get('trend'))
        
        # ê¸°ë³¸ ì •ë³´ ì €ì¥
        all_predictions['ì˜ˆì¸¡ì‹œì '] = [t.strftime('%Y-%m-%d %H:%M') for t in timestamps_pred]
        all_predictions['ì˜ˆì¸¡ëŒ€ìƒì‹œê°„'] = [t.strftime('%Y-%m-%d %H:%M') for t in timestamps_target]
        all_predictions['ì‹¤ì œê°’'] = actuals
        all_predictions['M14AM14B'] = m14b_values
        all_predictions['M14AM10A'] = m14a_values
        all_predictions['ì‹œí€€ìŠ¤_MAX'] = sequence_maxes
        all_predictions['ì‹œí€€ìŠ¤_ì¶”ì„¸'] = sequence_trends
        
        print(f"  ì˜ˆì¸¡í•  ë°ì´í„°: {len(all_predictions)}ê°œ")
        
        # ì‹œí€€ìŠ¤ ë¶„ì„ í†µê³„
        high_seq_count = (np.array(sequence_maxes) >= 1651).sum()
        inc_trend_count = (np.array(sequence_trends) == 'increasing').sum()
        dec_trend_count = (np.array(sequence_trends) == 'decreasing').sum()
        
        print(f"\nğŸ“ˆ ì‹œí€€ìŠ¤ ë¶„ì„ ê²°ê³¼:")
        print(f"  maxê°’ 1651+: {high_seq_count}ê°œ ({high_seq_count/len(all_predictions)*100:.1f}%)")
        print(f"  ì¦ê°€ ì¶”ì„¸: {inc_trend_count}ê°œ ({inc_trend_count/len(all_predictions)*100:.1f}%)")
        print(f"  í•˜ë½ ì¶”ì„¸: {dec_trend_count}ê°œ ({dec_trend_count/len(all_predictions)*100:.1f}%)")
        
        # ExtremeNet ë¶€ìŠ¤íŒ… ì¡°ê±´ ë§Œì¡± ê°œìˆ˜
        boost_condition = ((np.array(sequence_maxes) >= 1651) & 
                          (np.array(sequence_trends) == 'increasing')).sum()
        print(f"  ğŸ”¥ ExtremeNet ë¶€ìŠ¤íŒ… ì¡°ê±´ ì¶©ì¡±: {boost_condition}ê°œ ({boost_condition/len(all_predictions)*100:.1f}%)")
        
        # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡
        model_metrics = {}
        model_predictions = {}
        
        for model_name, model in self.models.items():
            print(f"\nğŸ¯ {model_name} ì˜ˆì¸¡ ì¤‘...")
            predictions = []
            
            # ë°°ì¹˜ ì˜ˆì¸¡
            batch_size = 500
            for i in range(start_idx, end_idx, batch_size):
                batch_end = min(i + batch_size, end_idx)
                
                # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                X_batch = []
                for j in range(i, batch_end):
                    seq_data = df.iloc[j-self.seq_len:j][self.feature_columns].values
                    X_batch.append(seq_data)
                
                if len(X_batch) == 0:
                    continue
                
                # ìŠ¤ì¼€ì¼ë§
                X_batch = np.array(X_batch)
                X_batch_scaled = []
                for seq in X_batch:
                    seq_scaled = self.feature_scaler.transform(seq)
                    X_batch_scaled.append(seq_scaled)
                X_batch_scaled = np.array(X_batch_scaled)
                
                # ì˜ˆì¸¡
                preds = model.predict(X_batch_scaled, verbose=0)
                
                if isinstance(preds, list):
                    y_pred_scaled = preds[0].flatten()
                else:
                    y_pred_scaled = preds.flatten()
                
                # ì—­ë³€í™˜
                y_pred = self.target_scaler.inverse_transform(
                    y_pred_scaled.reshape(-1, 1)).flatten()
                
                # ìˆ˜ì§‘
                for k in range(len(y_pred)):
                    actual_idx = i - start_idx + k
                    if actual_idx < len(all_predictions):
                        predictions.append(y_pred[k])
                
                if len(predictions) % 2000 == 0:
                    print(f"    {len(predictions):,}/{len(all_predictions):,} ì™„ë£Œ")
            
            # ì˜ˆì¸¡ê°’ ì €ì¥
            predictions = predictions[:len(all_predictions)]
            model_predictions[model_name] = predictions
            
            # ì›ë³¸ ì˜ˆì¸¡ê°’ ì €ì¥
            all_predictions[f'{model_name}_ì›ë³¸'] = [round(p) for p in predictions]
            
            # ì‹œí€€ìŠ¤ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë¶€ìŠ¤íŒ…
            print(f"  ğŸ”¥ {model_name} ì‹œí€€ìŠ¤ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ì ìš© ì¤‘...")
            boosted_predictions = []
            boost_applied_count = 0
            
            for i in range(len(predictions)):
                m14b = all_predictions.iloc[i]['M14AM14B']
                m14a = all_predictions.iloc[i]['M14AM10A']
                seq_max = all_predictions.iloc[i]['ì‹œí€€ìŠ¤_MAX']
                seq_trend = all_predictions.iloc[i]['ì‹œí€€ìŠ¤_ì¶”ì„¸']
                original = predictions[i]
                
                # ì‹œí€€ìŠ¤ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ìƒì„±
                seq_info = {
                    'max': seq_max,
                    'trend': seq_trend,
                    'consecutive_rises': 0,
                    'consecutive_falls': 0,
                    'rise_strength': 0,
                    'fall_strength': 0
                }
                
                # ì‹œí€€ìŠ¤ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ì ìš©
                boosted = self.extreme_booster.boost_prediction(
                    original, m14b, m14a, model_name,
                    sequence_info=seq_info
                )
                
                # ExtremeNet ë¶€ìŠ¤íŒ… í†µê³„
                if model_name == 'ExtremeNet' and boosted != original:
                    boost_applied_count += 1
                    
                boosted_predictions.append(boosted)
            
            if model_name == 'ExtremeNet':
                print(f"    ExtremeNet ë¶€ìŠ¤íŒ… ì ìš©: {boost_applied_count}ê°œ "
                      f"({boost_applied_count/len(predictions)*100:.1f}%)")
            
            all_predictions[f'{model_name}_ì˜ˆì¸¡'] = [round(p) for p in boosted_predictions]
            all_predictions[f'{model_name}_ì˜¤ì°¨'] = all_predictions[f'{model_name}_ì˜ˆì¸¡'] - all_predictions['ì‹¤ì œê°’']
            all_predictions[f'{model_name}_ì˜¤ì°¨ìœ¨(%)'] = round(
                abs(all_predictions[f'{model_name}_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’'] * 100, 2
            )
            
            # ì„±ëŠ¥ ê³„ì‚°
            mae = mean_absolute_error(all_predictions['ì‹¤ì œê°’'], boosted_predictions)
            rmse = np.sqrt(mean_squared_error(all_predictions['ì‹¤ì œê°’'], boosted_predictions))
            r2 = r2_score(all_predictions['ì‹¤ì œê°’'], boosted_predictions)
            mape = np.mean(abs(all_predictions[f'{model_name}_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’']) * 100
            
            model_metrics[model_name] = {
                'MAE': mae,
                'RMSE': rmse,
                'R2': r2,
                'MAPE': mape,
                'ì •í™•ë„(%)': 100 - mape
            }
            
            print(f"  âœ… {model_name} ì™„ë£Œ: MAE={mae:.2f}, RÂ²={r2:.4f}, ì •í™•ë„={100-mape:.2f}%")
        
        # ê°œì„ ëœ ê·¹ë‹¨ê°’ ì•™ìƒë¸”
        print("\nğŸ”¥ ê°œì„ ëœ ê·¹ë‹¨ê°’ ì•™ìƒë¸” ìƒì„±...")
        
        extreme_ensemble = []
        for i in range(len(all_predictions)):
            m14b = all_predictions.iloc[i]['M14AM14B']
            m14a = all_predictions.iloc[i]['M14AM10A']
            seq_max = all_predictions.iloc[i]['ì‹œí€€ìŠ¤_MAX']
            seq_trend = all_predictions.iloc[i]['ì‹œí€€ìŠ¤_ì¶”ì„¸']
            
            # ê° ëª¨ë¸ ì˜ˆì¸¡ê°’ í™•ì¸
            extreme_pred = all_predictions.iloc[i]['ExtremeNet_ì˜ˆì¸¡']
            spike_pred = all_predictions.iloc[i]['SpikeDetector_ì˜ˆì¸¡']
            golden_pred = all_predictions.iloc[i]['GoldenRule_ì˜ˆì¸¡']
            patch_pred = all_predictions.iloc[i]['PatchTST_ì˜ˆì¸¡']
            stable_pred = all_predictions.iloc[i]['StableLSTM_ì˜ˆì¸¡']
            
            # ğŸ”¥ í•µì‹¬: ì‹œí€€ìŠ¤ MAXê°€ 1650+ ì´ê³  increasingì´ë©´ ìµœì†Œê°’ ë³´ì¥
            if seq_max >= 1650 and seq_trend == 'increasing':
                # ê°€ì¥ ë†’ì€ ì˜ˆì¸¡ê°’ì„ ìš°ì„  ì‚¬ìš©
                max_pred = max(extreme_pred, spike_pred, golden_pred, patch_pred, stable_pred)
                
                # ìµœì†Œ 1700 ë³´ì¥
                ensemble_pred = max(max_pred, seq_max * 1.03, 1700)
                
            # ì‹œí€€ìŠ¤ MAXê°€ 1600+ ì´ë©´ ì¢€ ë” ë³´ìˆ˜ì 
            elif seq_max >= 1600:
                weights = {
                    'ExtremeNet': 0.25,
                    'SpikeDetector': 0.20,
                    'GoldenRule': 0.20,
                    'PatchTST': 0.20,
                    'StableLSTM': 0.15
                }
                
                ensemble_pred = (
                    extreme_pred * weights['ExtremeNet'] +
                    spike_pred * weights['SpikeDetector'] +
                    golden_pred * weights['GoldenRule'] +
                    patch_pred * weights['PatchTST'] +
                    stable_pred * weights['StableLSTM']
                )
                
                # ìµœì†Œê°’ ë³´ì¥
                ensemble_pred = max(ensemble_pred, seq_max, 1650)
                
            # ì¼ë°˜ ê²½ìš°
            else:
                # ë™ì  ê°€ì¤‘ì¹˜ (ê¸°ì¡´ ë¡œì§)
                if m14b > 200:  # ê¸°ì¤€ì„ ë‚®ì¶¤
                    weights = {
                        'SpikeDetector': 0.30,
                        'GoldenRule': 0.25,
                        'ExtremeNet': 0.20,
                        'PatchTST': 0.15,
                        'StableLSTM': 0.10
                    }
                else:
                    weights = {
                        'PatchTST': 0.30,
                        'StableLSTM': 0.25,
                        'ExtremeNet': 0.20,
                        'SpikeDetector': 0.15,
                        'GoldenRule': 0.10
                    }
                
                ensemble_pred = 0
                for model_name in self.models.keys():
                    if model_name in weights:
                        ensemble_pred += all_predictions.iloc[i][f'{model_name}_ì˜ˆì¸¡'] * weights[model_name]
            
            extreme_ensemble.append(ensemble_pred)
        
        # ê·¹ë‹¨ ì•™ìƒë¸” ê²°ê³¼ ì¶”ê°€
        all_predictions['ê°œì„ ì•™ìƒë¸”_ì˜ˆì¸¡'] = [round(p) for p in extreme_ensemble]
        all_predictions['ê°œì„ ì•™ìƒë¸”_ì˜¤ì°¨'] = all_predictions['ê°œì„ ì•™ìƒë¸”_ì˜ˆì¸¡'] - all_predictions['ì‹¤ì œê°’']
        all_predictions['ê°œì„ ì•™ìƒë¸”_ì˜¤ì°¨ìœ¨(%)'] = round(
            abs(all_predictions['ê°œì„ ì•™ìƒë¸”_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’'] * 100, 2
        )
        
        # ê°œì„  ì•™ìƒë¸” ì„±ëŠ¥
        extreme_mae = mean_absolute_error(all_predictions['ì‹¤ì œê°’'], extreme_ensemble)
        extreme_rmse = np.sqrt(mean_squared_error(all_predictions['ì‹¤ì œê°’'], extreme_ensemble))
        extreme_r2 = r2_score(all_predictions['ì‹¤ì œê°’'], extreme_ensemble)
        extreme_mape = np.mean(abs(all_predictions['ê°œì„ ì•™ìƒë¸”_ì˜¤ì°¨']) / all_predictions['ì‹¤ì œê°’']) * 100
        
        model_metrics['ê°œì„ ì•™ìƒë¸”'] = {
            'MAE': extreme_mae,
            'RMSE': extreme_rmse,
            'R2': extreme_r2,
            'MAPE': extreme_mape,
            'ì •í™•ë„(%)': 100 - extreme_mape
        }
        
        print(f"âœ… ê°œì„ ì•™ìƒë¸”: MAE={extreme_mae:.2f}, RÂ²={extreme_r2:.4f}, ì •í™•ë„={100-extreme_mape:.2f}%")
        
        # CSV ì €ì¥
        output_file = f'v67_improved_predictions_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        all_predictions.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ì˜ˆì¸¡ê°’ ì €ì¥: {output_file}")
        
        # ==================== ê³ ê°’ êµ¬ê°„ ìƒì„¸ ë¶„ì„ ====================
        print("\n" + "="*80)
        print("ğŸ¯ğŸ¯ğŸ¯ ê³ ê°’ êµ¬ê°„ (1700+) ìƒì„¸ ë¶„ì„ ğŸ¯ğŸ¯ğŸ¯")
        print("="*80)
        
        high_mask = all_predictions['ì‹¤ì œê°’'] >= 1700
        if high_mask.any():
            high_df = all_predictions[high_mask]
            print(f"\nğŸ“Š ì „ì²´ ê³ ê°’ ìƒ˜í”Œ: {high_mask.sum()}ê°œ")
            
            # ì¶”ì„¸ë³„ ë¶„í¬
            high_inc = (high_df['ì‹œí€€ìŠ¤_ì¶”ì„¸'] == 'increasing').sum()
            high_dec = (high_df['ì‹œí€€ìŠ¤_ì¶”ì„¸'] == 'decreasing').sum()
            high_stable = (high_df['ì‹œí€€ìŠ¤_ì¶”ì„¸'] == 'stable').sum()
            
            print(f"\n[ê³ ê°’ êµ¬ê°„ ì¶”ì„¸ ë¶„í¬]")
            print(f"  ì¦ê°€ ì¶”ì„¸: {high_inc}ê°œ ({high_inc/len(high_df)*100:.1f}%)")
            print(f"  í•˜ë½ ì¶”ì„¸: {high_dec}ê°œ ({high_dec/len(high_df)*100:.1f}%)")
            print(f"  ì•ˆì • ì¶”ì„¸: {high_stable}ê°œ ({high_stable/len(high_df)*100:.1f}%)")
            
            # ExtremeNet ì›ë³¸ vs ë¶€ìŠ¤íŒ… ë¹„êµ
            print(f"\n[ExtremeNet ë¶€ìŠ¤íŒ… íš¨ê³¼ ë¶„ì„]")
            extreme_orig = high_df['ExtremeNet_ì›ë³¸'].values
            extreme_boost = high_df['ExtremeNet_ì˜ˆì¸¡'].values
            
            orig_hit = (extreme_orig >= 1700).sum()
            boost_hit = (extreme_boost >= 1700).sum()
            
            print(f"  ì›ë³¸ 1700+ ì ì¤‘: {orig_hit}/{len(high_df)} ({orig_hit/len(high_df)*100:.1f}%)")
            print(f"  ë¶€ìŠ¤íŒ… 1700+ ì ì¤‘: {boost_hit}/{len(high_df)} ({boost_hit/len(high_df)*100:.1f}%)")
            print(f"  ê°œì„ : +{boost_hit - orig_hit}ê°œ")
            
            # ì‹œí€€ìŠ¤ ì¡°ê±´ë³„ ë¶„ì„
            seq_condition_mask = ((high_df['ì‹œí€€ìŠ¤_MAX'] >= 1651) & 
                                 (high_df['ì‹œí€€ìŠ¤_ì¶”ì„¸'] == 'increasing'))
            if seq_condition_mask.any():
                condition_df = high_df[seq_condition_mask]
                print(f"\n  ì¡°ê±´ ì¶©ì¡± ìƒ˜í”Œ (maxâ‰¥1651 & ì¦ê°€): {seq_condition_mask.sum()}ê°œ")
                cond_hit = (condition_df['ExtremeNet_ì˜ˆì¸¡'] >= 1700).sum()
                print(f"    â†’ ExtremeNet ì ì¤‘: {cond_hit}/{len(condition_df)} "
                      f"({cond_hit/len(condition_df)*100:.1f}%)")
            
            # ê° ëª¨ë¸ë³„ ê³ ê°’ ì ì¤‘ë¥  ê³„ì‚°
            print(f"\n[ëª¨ë¸ë³„ 1700+ ì ì¤‘ë¥ ]")
            print("-" * 70)
            
            for model_name in list(self.models.keys()) + ['ê°œì„ ì•™ìƒë¸”']:
                col_pred = f'{model_name}_ì˜ˆì¸¡'
                
                high_preds = high_df[col_pred].values
                hit_1700 = (high_preds >= 1700).sum()
                hit_1650 = (high_preds >= 1650).sum()
                
                print(f"  {model_name:15s}: "
                      f"{hit_1700:2d}/{len(high_df):2d} ({hit_1700/len(high_df)*100:5.1f}%)")
            
            # ê°œì„ ì•™ìƒë¸” ìƒì„¸ ê²°ê³¼ (ì¶”ì„¸ í¬í•¨)
            print(f"\n[ê°œì„ ì•™ìƒë¸” ê³ ê°’ ì˜ˆì¸¡ ìƒì„¸] (ì²˜ìŒ 20ê°œ)")
            print("-" * 90)
            print(f"{'ì‹¤ì œê°’':>7} {'ì˜ˆì¸¡ê°’':>7} {'M14B':>6} {'ì‹œí€€ìŠ¤MAX':>10} {'ì¶”ì„¸':>10} {'ì˜¤ì°¨':>7} {'ì ì¤‘':>6}")
            print("-" * 90)
            
            for idx in high_df.head(20).index:
                row = all_predictions.loc[idx]
                actual = row['ì‹¤ì œê°’']
                pred = row['ê°œì„ ì•™ìƒë¸”_ì˜ˆì¸¡']
                m14b = row['M14AM14B']
                seq_max = row['ì‹œí€€ìŠ¤_MAX']
                seq_trend = row['ì‹œí€€ìŠ¤_ì¶”ì„¸']
                error = pred - actual
                
                # ì ì¤‘ ì—¬ë¶€
                if pred >= 1700:
                    hit = "âœ… HIT"
                elif pred >= 1650:
                    hit = "âš ï¸ NEAR"
                else:
                    hit = "âŒ MISS"
                
                trend_icon = "ğŸ“ˆ" if seq_trend == 'increasing' else "ğŸ“‰" if seq_trend == 'decreasing' else "â¡ï¸"
                
                print(f"{actual:7.0f} {pred:7.0f} {m14b:6.0f} {seq_max:10.0f} "
                      f"{trend_icon:>2}{seq_trend:>8} {error:+7.0f} {hit:>6}")
        
        # ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸”
        print("\n" + "="*80)
        print("ğŸ“Š ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½")
        print("="*80)
        
        metrics_df = pd.DataFrame(model_metrics).T
        metrics_df = metrics_df.sort_values('R2', ascending=False)
        
        print(f"\n{'ëª¨ë¸':<15} {'MAE':>8} {'RMSE':>8} {'RÂ²':>8} {'MAPE(%)':>8} {'ì •í™•ë„(%)':>10}")
        print("-" * 65)
        
        for model_name, row in metrics_df.iterrows():
            if model_name == 'ê°œì„ ì•™ìƒë¸”':
                print(f"{'ğŸ”¥ ' + model_name:<15} {row['MAE']:8.2f} {row['RMSE']:8.2f} "
                      f"{row['R2']:8.4f} {row['MAPE']:8.2f} {row['ì •í™•ë„(%)']:10.2f} â­â­â­")
            else:
                print(f"{model_name:<15} {row['MAE']:8.2f} {row['RMSE']:8.2f} "
                      f"{row['R2']:8.4f} {row['MAPE']:8.2f} {row['ì •í™•ë„(%)']:10.2f}")
        
        return all_predictions, metrics_df

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    print("\nğŸš€ V6.7 ê°œì„ ëœ ê·¹ë‹¨ê°’ í‰ê°€ ì‹œì‘!")
    print("í•µì‹¬: ì‹œí€€ìŠ¤ max 1651+ & ì¦ê°€ ì¶”ì„¸ â†’ ExtremeNet ë¶€ìŠ¤íŒ…")
    print("      í•˜ë½ ì¶”ì„¸ â†’ ë³´ìˆ˜ì  ì˜ˆì¸¡")
    
    # í‰ê°€ê¸° ìƒì„±
    evaluator = ImprovedModelEvaluator()
    
    # ëª¨ë“  ëª¨ë¸ ë¡œë“œ
    models = evaluator.load_all_models('models/')
    
    if not models:
        print("âŒ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼
    test_files = [
        'data/M14_20250916_20250817.csv',
        'data/test_data.csv', 
        '/mnt/user-data/uploads/test.csv'
    ]
    
    test_file = None
    for file in test_files:
        if os.path.exists(file):
            test_file = file
            break
    
    if not test_file:
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # í‰ê°€ ì‹¤í–‰
    all_predictions, metrics = evaluator.evaluate_all_models(test_file)
    
    print("\n" + "="*80)
    print("ğŸ† V6.7 ê°œì„ ëœ ê·¹ë‹¨ê°’ í‰ê°€ ì™„ë£Œ!")
    print("="*80)
    print("\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
    print(f"  1. v67_improved_predictions_YYYYMMDD.csv - ê°œì„ ëœ ì˜ˆì¸¡")
    print("\nğŸ”¥ í•µì‹¬ ê°œì„ ì‚¬í•­:")
    print("  âœ… ì‹œí€€ìŠ¤ maxê°’ 1651 ì´ìƒ ì²´í¬")
    print("  âœ… ì¦ê°€ ì¶”ì„¸ì¼ ë•Œë§Œ ExtremeNet ë¶€ìŠ¤íŒ…")
    print("  âœ… í•˜ë½ ì¶”ì„¸ì—ì„œëŠ” ë³´ìˆ˜ì  ì˜ˆì¸¡")
    print("  âœ… ì¶”ì„¸ë³„ ì°¨ë³„í™”ëœ ì•™ìƒë¸” ê°€ì¤‘ì¹˜")
    print("="*80)

if __name__ == "__main__":
    main()