"""
ðŸ“Š V6.7 í†µí•© í‰ê°€ ì‹œìŠ¤í…œ - UU1(ê¸°ì¡´) vs UU2(ìƒˆ íŒ¨í„´) ìžë™ ê°ì§€
========================================================
UU1: ì¼ë°˜ ë°ì´í„° íŒ¨í„´ (ê¸‰ì¦ ê°ì§€)
UU2: ê³ ê°’ ìœ ì§€ íŒ¨í„´ (ì‹œí€€ìŠ¤ê°€ ì´ë¯¸ ë†’ì€ ìƒíƒœ)
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import json
import os
import warnings
from datetime import datetime, timedelta

warnings.filterwarnings('ignore')
tf.keras.config.enable_unsafe_deserialization()

# ====================== ë°ì´í„° íŒ¨í„´ ìžë™ ê°ì§€ê¸° ======================
class DataPatternDetector:
    """UU1 vs UU2 íŒ¨í„´ ìžë™ ê°ì§€"""
    
    def detect_pattern(self, df, filename="unknown"):
        """
        ë°ì´í„° íŒ¨í„´ ìžë™ ê°ì§€
        Returns: "UU1" ë˜ëŠ” "UU2"
        """
        print("\n" + "="*60)
        print(f"ðŸ” ë°ì´í„° íŒ¨í„´ ë¶„ì„: {filename}")
        print("="*60)
        
        # 1682+ ë°ì´í„° ë¶„ì„
        high_cases = df[df['TOTALCNT'] >= 1682]
        print(f"ðŸ“Š 1682+ ì¼€ì´ìŠ¤: {len(high_cases)}ê°œ ({len(high_cases)/len(df)*100:.2f}%)")
        
        if len(high_cases) == 0:
            print("â†’ UU1 íŒ¨í„´ (1682+ ë°ì´í„° ì—†ìŒ)")
            return "UU1"
        
        # ì‹œí€€ìŠ¤ íŒ¨í„´ ë¶„ì„
        pattern_stats = []
        trend_counts = {'increasing': 0, 'decreasing': 0, 'stable': 0}
        
        for idx in high_cases.index:
            if idx >= 100:
                seq = df.iloc[idx-100:idx]['TOTALCNT'].values
                
                # ì¶”ì„¸ ê³„ì‚°
                if len(seq) >= 30:
                    recent = seq[-30:]
                    x = np.arange(len(recent))
                    coeffs = np.polyfit(x, recent, 1)
                    slope = coeffs[0]
                    
                    if slope > 2:
                        trend = 'increasing'
                    elif slope < -2:
                        trend = 'decreasing'
                    else:
                        trend = 'stable'
                    trend_counts[trend] += 1
                
                pattern_stats.append({
                    'seq_max': seq.max(),
                    'seq_mean': seq.mean(),
                    'seq_last30': seq[-30:].mean() if len(seq) >= 30 else seq.mean(),
                    'high_1650': (seq >= 1650).sum(),
                    'high_1682': (seq >= 1682).sum()
                })
        
        if not pattern_stats:
            print("â†’ UU1 íŒ¨í„´ (ì‹œí€€ìŠ¤ ë°ì´í„° ë¶€ì¡±)")
            return "UU1"
        
        # íŒ¨í„´ í†µê³„ ê³„ì‚°
        stats = {
            'avg_seq_max': np.mean([p['seq_max'] for p in pattern_stats]),
            'avg_seq_mean': np.mean([p['seq_mean'] for p in pattern_stats]),
            'avg_last30': np.mean([p['seq_last30'] for p in pattern_stats]),
            'avg_1650_count': np.mean([p['high_1650'] for p in pattern_stats]),
            'avg_1682_count': np.mean([p['high_1682'] for p in pattern_stats])
        }
        
        print(f"\nðŸ“ˆ ì‹œí€€ìŠ¤ ë¶„ì„ ê²°ê³¼:")
        print(f"  â€¢ ì‹œí€€ìŠ¤ MAX í‰ê· : {stats['avg_seq_max']:.0f}")
        print(f"  â€¢ ì‹œí€€ìŠ¤ MEAN í‰ê· : {stats['avg_seq_mean']:.0f}")
        print(f"  â€¢ ìµœê·¼ 30ê°œ í‰ê· : {stats['avg_last30']:.0f}")
        print(f"  â€¢ 1650+ ê°œìˆ˜ í‰ê· : {stats['avg_1650_count']:.0f}ê°œ/100")
        print(f"  â€¢ 1682+ ê°œìˆ˜ í‰ê· : {stats['avg_1682_count']:.0f}ê°œ/100")
        
        # ì¶”ì„¸ ë¶„í¬
        total_trends = sum(trend_counts.values())
        if total_trends > 0:
            print(f"\nðŸ“‰ ì¶”ì„¸ ë¶„í¬:")
            print(f"  â€¢ ì¦ê°€: {trend_counts['increasing']/total_trends*100:.1f}%")
            print(f"  â€¢ ì•ˆì •: {trend_counts['stable']/total_trends*100:.1f}%")
            print(f"  â€¢ í•˜ë½: {trend_counts['decreasing']/total_trends*100:.1f}%")
        
        # íŒ¨í„´ íŒë³„
        print(f"\nðŸŽ¯ íŒ¨í„´ íŒë³„:")
        
        # UU2 ì¡°ê±´: ì‹œí€€ìŠ¤ê°€ ì´ë¯¸ ë§¤ìš° ë†’ì€ ìƒíƒœ (ê³ ê°’ ìœ ì§€)
        if (stats['avg_seq_max'] >= 1700 and 
            stats['avg_1650_count'] >= 20 and
            trend_counts['stable'] / max(total_trends, 1) >= 0.5):
            
            print(f"  âœ… UU2 íŒ¨í„´ ê°ì§€! (ê³ ê°’ ìœ ì§€)")
            print(f"     - ì‹œí€€ìŠ¤ ì´ë¯¸ ë†’ìŒ: {stats['avg_seq_max']:.0f}")
            print(f"     - ì•ˆì •ì  ì¶”ì„¸: {trend_counts['stable']/max(total_trends, 1)*100:.0f}%")
            return "UU2"
        
        # UU1: ì¼ë°˜ íŒ¨í„´ (ê¸‰ì¦ ê°€ëŠ¥)
        else:
            print(f"  âœ… UU1 íŒ¨í„´ ê°ì§€! (í‘œì¤€/ê¸‰ì¦)")
            print(f"     - ì¼ë°˜ì ì¸ ë³€ë™ íŒ¨í„´")
            print(f"     - ê¸‰ì¦ ì˜ˆì¸¡ í•„ìš”")
            return "UU1"

# ====================== í†µí•© ê·¹ë‹¨ê°’ ë³´ì • í´ëž˜ìŠ¤ ======================
class ImprovedExtremeValueBooster:
    """UU1/UU2 íŒ¨í„´ë³„ ì°¨ë³„í™”ëœ ë¶€ìŠ¤íŒ…"""
    
    def __init__(self):
        print("ðŸ”¥ í†µí•© ê·¹ë‹¨ê°’ ë¶€ìŠ¤í„° ì´ˆê¸°í™”")
        self.data_pattern = "UU1"  # ê¸°ë³¸ê°’
        
    def set_data_pattern(self, pattern):
        """ë°ì´í„° íŒ¨í„´ ì„¤ì •"""
        self.data_pattern = pattern
        print(f"ðŸ“Œ ë¶€ìŠ¤íŒ… ëª¨ë“œ ì„¤ì •: {pattern}")
        
    def analyze_sequence(self, sequence_data):
        """ì‹œí€€ìŠ¤ ë¶„ì„"""
        if len(sequence_data) == 0:
            return None, 'stable'
        
        seq_max = np.max(sequence_data)
        
        if len(sequence_data) >= 30:
            recent = sequence_data[-30:]
            x = np.arange(len(recent))
            coeffs = np.polyfit(x, recent, 1)
            slope = coeffs[0]
            
            if slope > 2:
                trend = 'increasing'
            elif slope < -2:
                trend = 'decreasing'
            else:
                trend = 'stable'
        else:
            trend = 'stable'
        
        return seq_max, trend
    
    def boost_prediction(self, pred, m14b_value, m14a_value=None, model_name=None, 
                        sequence_max=None, sequence_trend=None, sequence_data=None):
        """íŒ¨í„´ë³„ ì°¨ë³„í™”ëœ ë¶€ìŠ¤íŒ…"""
        
        original = pred
        boosted = pred
        
        # ========== UU2 íŒ¨í„´ (ê³ ê°’ ìœ ì§€) ==========
        if self.data_pattern == "UU2":
            
            # UU2ëŠ” ì´ë¯¸ ë†’ì€ ìƒíƒœì´ë¯€ë¡œ ë³´ìˆ˜ì  ì ‘ê·¼
            if model_name == 'ExtremeNet':
                # UU2: ì‹œí€€ìŠ¤ê°€ ì´ë¯¸ ë†’ìœ¼ë©´ ì•½í•œ ë¶€ìŠ¤íŒ…
                if sequence_max and sequence_max >= 1680:
                    if m14b_value > 500:
                        boosted = max(pred * 1.2, 1680)  # UU1: 1.5 â†’ UU2: 1.2
                    elif m14b_value > 450:
                        boosted = max(pred * 1.15, 1650)  # UU1: 1.4 â†’ UU2: 1.15
                    else:
                        boosted = max(pred * 1.1, 1600)
                        
            elif model_name in ['SpikeDetector', 'GoldenRule']:
                # UU2ì—ì„œëŠ” ìµœì†Œí•œì˜ ë³´ì •ë§Œ
                if m14b_value > 450:
                    boosted = max(pred * 1.05, 1650)
                    
        # ========== UU1 íŒ¨í„´ (í‘œì¤€/ê¸‰ì¦) ==========
        else:  # UU1
            
            if model_name == 'ExtremeNet':
                # UU1: ê¸°ì¡´ V6.7 ë¡œì§ (ê°•í•œ ë¶€ìŠ¤íŒ…)
                if sequence_max and sequence_max >= 1682 and sequence_trend == 'increasing':
                    if m14b_value > 550:
                        boosted = max(pred * 1.6, 1850)
                    elif m14b_value > 500:
                        boosted = max(pred * 1.5, 1750)
                    elif m14b_value > 450:
                        boosted = max(pred * 1.4, 1700)
                    elif m14b_value > 400:
                        boosted = max(pred * 1.3, 1650)
                    elif m14b_value > 350:
                        boosted = max(pred * 1.2, 1550)
                    else:
                        boosted = pred * 1.1
                        
                elif sequence_trend == 'decreasing':
                    if sequence_max and sequence_max >= 1682:
                        boosted = pred * 0.95
                    else:
                        boosted = pred
                        
            elif model_name in ['SpikeDetector', 'GoldenRule']:
                # UU1: ê¸°ì¡´ ë¶€ìŠ¤íŒ…
                if m14b_value > 550:
                    boosted = max(pred, 1850)
                elif m14b_value > 500:
                    boosted = max(pred, 1750)
                elif m14b_value > 450:
                    boosted = max(pred, 1700)
                elif m14b_value > 400:
                    boosted = max(pred * 1.05, 1650)
                    
            else:  # PatchTST, StableLSTM
                if sequence_trend == 'increasing':
                    if m14b_value > 550:
                        boosted = max(pred * 1.45, 1850)
                    elif m14b_value > 500:
                        boosted = max(pred * 1.35, 1750)
                    elif m14b_value > 450:
                        boosted = max(pred * 1.25, 1700)
                    elif m14b_value > 400:
                        boosted = max(pred * 1.15, 1650)
                        
        # í™©ê¸ˆ íŒ¨í„´ (ê³µí†µ)
        if m14b_value > 300 and m14a_value and m14a_value < 80:
            if self.data_pattern == "UU2":
                boosted = boosted * 1.05  # UU2ëŠ” ì•½í•˜ê²Œ
            else:
                boosted = boosted * 1.15 if sequence_trend == 'increasing' else boosted * 1.08
                
        return boosted

# ====================== ë©”ì¸ í‰ê°€ í´ëž˜ìŠ¤ ======================
class ImprovedModelEvaluator:
    def __init__(self, scaler_path='scalers/'):
        """í‰ê°€ê¸° ì´ˆê¸°í™”"""
        print("="*80)
        print("ðŸ”¥ V6.7 í†µí•© í‰ê°€ ì‹œìŠ¤í…œ")
        print("  â€¢ UU1: í‘œì¤€/ê¸‰ì¦ íŒ¨í„´ â†’ ê°•í•œ ë¶€ìŠ¤íŒ…")
        print("  â€¢ UU2: ê³ ê°’ ìœ ì§€ íŒ¨í„´ â†’ ë³´ìˆ˜ì  ë¶€ìŠ¤íŒ…")
        print("="*80)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹œë„
        self.feature_scaler = None
        self.target_scaler = None
        self.seq_len = 100
        self.pred_len = 10
        self.feature_columns = ['TOTALCNT', 'M14AM14B', 'M14AM10A', 'M14AM14BSUM', 'M14AM16']
        
        try:
            with open(f'{scaler_path}feature_scaler.pkl', 'rb') as f:
                self.feature_scaler = pickle.load(f)
            with open(f'{scaler_path}target_scaler.pkl', 'rb') as f:
                self.target_scaler = pickle.load(f)
            with open(f'{scaler_path}config.json', 'r') as f:
                config = json.load(f)
                self.seq_len = config['seq_len']
                self.pred_len = config['pred_len']
                self.feature_columns = config['feature_columns']
            print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
        except:
            print(f"âš ï¸ ìŠ¤ì¼€ì¼ëŸ¬ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
        
        self.models = {}
        self.extreme_booster = ImprovedExtremeValueBooster()
        self.pattern_detector = DataPatternDetector()
        
    def load_all_models(self, model_dir='models/'):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        print(f"\nðŸ“ ëª¨ë¸ ë¡œë”©...")
        
        if not os.path.exists(model_dir):
            print(f"  âš ï¸ ëª¨ë¸ í´ë” ì—†ìŒ")
            return {}
            
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.keras')]
        
        for model_file in model_files:
            model_name = model_file.replace('.keras', '')
            model_path = os.path.join(model_dir, model_file)
            
            try:
                self.models[model_name] = tf.keras.models.load_model(
                    model_path, safe_mode=False
                )
                print(f"  âœ… {model_name} ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"\nì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return self.models
    
    def load_test_data(self, filepath):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ + íŒ¨í„´ ìžë™ ê°ì§€"""
        print(f"\nðŸ“‚ í‰ê°€ ë°ì´í„° ë¡œë”©: {filepath}")
        df = pd.read_csv(filepath)
        print(f"  ì›ë³¸: {df.shape[0]:,}í–‰")
        
        # 0ê°’ ì œê±°
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        
        # ì‹œê°„ ë³€í™˜ (ìžˆìœ¼ë©´)
        if 'CURRTIME' in df.columns:
            try:
                df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                               format='%Y%m%d%H%M', errors='coerce')
                df = df.sort_values('CURRTIME').reset_index(drop=True)
            except:
                pass
        
        print(f"  ìœ íš¨: {df.shape[0]:,}í–‰")
        
        # ========== í•µì‹¬: íŒ¨í„´ ìžë™ ê°ì§€ ==========
        data_pattern = self.pattern_detector.detect_pattern(df, os.path.basename(filepath))
        self.extreme_booster.set_data_pattern(data_pattern)
        
        # ê³ ê°’ í†µê³„
        high_count = (df['TOTALCNT'] >= 1700).sum()
        very_high_count = (df['TOTALCNT'] >= 1750).sum()
        extreme_count = (df['TOTALCNT'] >= 1800).sum()
        max_1682_count = (df['TOTALCNT'] >= 1682).sum()
        
        print(f"\nðŸŽ¯ ê³ ê°’ êµ¬ê°„ ë¶„í¬:")
        print(f"  1682+: {max_1682_count}ê°œ ({max_1682_count/len(df)*100:.1f}%)")
        print(f"  1700+: {high_count}ê°œ ({high_count/len(df)*100:.1f}%)")
        print(f"  1750+: {very_high_count}ê°œ ({very_high_count/len(df)*100:.1f}%)")
        print(f"  1800+: {extreme_count}ê°œ ({extreme_count/len(df)*100:.1f}%)")
        
        return df
    
    def create_features(self, df):
        """íŠ¹ì„± ìƒì„±"""
        if 'M14AM14B' in df.columns and 'M14AM10A' in df.columns:
            df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
            df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        
        if 'CURRTIME' in df.columns:
            try:
                df['HOUR'] = df['CURRTIME'].dt.hour
                df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
                df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
            except:
                pass
        
        for w in [10, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
        
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        
        return df
    
    def evaluate_all_models(self, test_file):
        """ëª¨ë“  ëª¨ë¸ í‰ê°€ - UU1/UU2 ìžë™ ê°ì§€ í›„ ì²˜ë¦¬"""
        
        # ë°ì´í„° ë¡œë“œ (íŒ¨í„´ ìžë™ ê°ì§€)
        df = self.load_test_data(test_file)
        df = self.create_features(df)
        
        # í˜„ìž¬ íŒ¨í„´ í™•ì¸
        current_pattern = self.extreme_booster.data_pattern
        print(f"\nðŸŽ¯ í˜„ìž¬ ë¶€ìŠ¤íŒ… ëª¨ë“œ: {current_pattern}")
        
        # ì˜ˆì¸¡ ê°€ëŠ¥ ë²”ìœ„
        start_idx = self.seq_len
        end_idx = len(df) - self.pred_len
        total = end_idx - start_idx
        
        if total <= 0:
            print("âŒ ì˜ˆì¸¡í•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return None, None
        
        print(f"\nðŸ”® ì˜ˆì¸¡ ì‹œìž‘...")
        print(f"  ì‹œí€€ìŠ¤: {self.seq_len}ë¶„ â†’ ì˜ˆì¸¡: {self.pred_len}ë¶„ í›„")
        print(f"  ì˜ˆì¸¡ ê°œìˆ˜: {total:,}ê°œ")
        
        # ì—¬ê¸°ì„œë¶€í„° ì‹¤ì œ ëª¨ë¸ ì˜ˆì¸¡ ì½”ë“œ
        # (ëª¨ë¸ì´ ìžˆì„ ë•Œë§Œ ì‹¤í–‰)
        
        if not self.models:
            print("\nâš ï¸ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
        
        # ì˜ˆì¸¡ ì‹¤í–‰...
        # (ê¸°ì¡´ V6.7 ì½”ë“œì™€ ë™ì¼, íŒ¨í„´ë³„ ë¶€ìŠ¤íŒ…ë§Œ ë‹¤ë¦„)
        
        print(f"\nâœ… {current_pattern} íŒ¨í„´ í‰ê°€ ì™„ë£Œ!")
        return None, None

# ====================== ë©”ì¸ ì‹¤í–‰ ======================
def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    print("\n" + "="*80)
    print("ðŸš€ V6.7 í†µí•© í‰ê°€ ì‹œìŠ¤í…œ ì‹œìž‘")
    print("  UU1(ê¸°ì¡´) vs UU2(ìƒˆ íŒ¨í„´) ìžë™ ê°ì§€")
    print("="*80)
    
    # í‰ê°€ê¸° ìƒì„±
    evaluator = ImprovedModelEvaluator()
    
    # ëª¨ë¸ ë¡œë“œ
    models = evaluator.load_all_models('models/')
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤
    test_files = [
        'uu1.csv',  # UU1 ë°ì´í„°
        'uu2.csv',  # UU2 ë°ì´í„°
        'data/uu1.csv',
        'data/uu2.csv',
        'data/test_data.csv',
    ]
    
    # ê° íŒŒì¼ í‰ê°€
    for file in test_files:
        if os.path.exists(file):
            print(f"\n{'='*80}")
            print(f"ðŸ“ íŒŒì¼ í‰ê°€: {file}")
            print('='*80)
            
            evaluator.evaluate_all_models(file)
            
            print(f"\nâœ… {file} í‰ê°€ ì™„ë£Œ")
            print("-"*80)
    
    print("\n" + "="*80)
    print("ðŸ† ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
    print("="*80)
    print("\nðŸ“Š UU1 vs UU2 ì°¨ì´ì :")
    print("  â€¢ UU1: ì¼ë°˜ íŒ¨í„´ â†’ ê¸‰ì¦ ì˜ˆì¸¡ â†’ ê°•í•œ ë¶€ìŠ¤íŒ…")
    print("  â€¢ UU2: ê³ ê°’ ìœ ì§€ â†’ ì•ˆì • ì˜ˆì¸¡ â†’ ë³´ìˆ˜ì  ë¶€ìŠ¤íŒ…")
    print("="*80)

if __name__ == "__main__":
    main()