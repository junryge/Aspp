"""
ğŸ“Š V6.7 ì™„ì „í•œ í‰ê°€ ì‹œìŠ¤í…œ - UU1/UU2 ìë™ ê°ì§€ + CSV ì €ì¥
========================================================
ëª¨ë“  ëª¨ë¸ í‰ê°€ í›„ ê²°ê³¼ CSV íŒŒì¼ ìƒì„±
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import json
import os
import warnings
from datetime import datetime, timedelta

warnings.filterwarnings('ignore')
tf.keras.config.enable_unsafe_deserialization()

# ====================== ë°ì´í„° íŒ¨í„´ ìë™ ê°ì§€ê¸° ======================
class DataPatternDetector:
    """UU1 vs UU2 íŒ¨í„´ ìë™ ê°ì§€"""
    
    def detect_pattern(self, df, filename="unknown"):
        """
        ë°ì´í„° íŒ¨í„´ ìë™ ê°ì§€
        Returns: "UU1" ë˜ëŠ” "UU2"
        """
        print("\n" + "="*60)
        print(f"ğŸ” ë°ì´í„° íŒ¨í„´ ë¶„ì„: {filename}")
        print("="*60)
        
        high_cases = df[df['TOTALCNT'] >= 1682]
        print(f"ğŸ“Š 1682+ ì¼€ì´ìŠ¤: {len(high_cases)}ê°œ ({len(high_cases)/len(df)*100:.2f}%)")
        
        if len(high_cases) == 0:
            print("â†’ UU1 íŒ¨í„´ (1682+ ë°ì´í„° ì—†ìŒ)")
            return "UU1"
        
        pattern_stats = []
        trend_counts = {'increasing': 0, 'decreasing': 0, 'stable': 0}
        
        for idx in high_cases.index:
            if idx >= 100:
                seq = df.iloc[idx-100:idx]['TOTALCNT'].values
                
                if len(seq) >= 30:
                    recent = seq[-30:]
                    x = np.arange(len(recent))
                    coeffs = np.polyfit(x, recent, 1)
                    slope = coeffs[0]
                    
                    if slope > 2:
                        trend = 'increasing'
                    elif slope < -2:
                        trend = 'decreasing'
                    else:
                        trend = 'stable'
                    trend_counts[trend] += 1
                
                pattern_stats.append({
                    'seq_max': seq.max(),
                    'seq_mean': seq.mean(),
                    'high_1650': (seq >= 1650).sum(),
                })
        
        if not pattern_stats:
            print("â†’ UU1 íŒ¨í„´")
            return "UU1"
        
        stats = {
            'avg_seq_max': np.mean([p['seq_max'] for p in pattern_stats]),
            'avg_1650_count': np.mean([p['high_1650'] for p in pattern_stats]),
        }
        
        total_trends = sum(trend_counts.values())
        stable_ratio = trend_counts['stable'] / max(total_trends, 1)
        
        # UU2 íŒë³„
        if stats['avg_seq_max'] >= 1700 and stats['avg_1650_count'] >= 20 and stable_ratio >= 0.5:
            print(f"  âœ… UU2 íŒ¨í„´ ê°ì§€! (ê³ ê°’ ìœ ì§€)")
            return "UU2"
        else:
            print(f"  âœ… UU1 íŒ¨í„´ ê°ì§€! (í‘œì¤€/ê¸‰ì¦)")
            return "UU1"

# ====================== í†µí•© ê·¹ë‹¨ê°’ ë³´ì • í´ë˜ìŠ¤ ======================
class ImprovedExtremeValueBooster:
    """UU1/UU2 íŒ¨í„´ë³„ ì°¨ë³„í™”ëœ ë¶€ìŠ¤íŒ…"""
    
    def __init__(self):
        print("ğŸ”¥ í†µí•© ê·¹ë‹¨ê°’ ë¶€ìŠ¤í„° ì´ˆê¸°í™”")
        self.data_pattern = "UU1"
        
    def set_data_pattern(self, pattern):
        self.data_pattern = pattern
        print(f"ğŸ“Œ ë¶€ìŠ¤íŒ… ëª¨ë“œ: {pattern}")
        
    def analyze_sequence(self, sequence_data):
        if len(sequence_data) == 0:
            return None, 'stable'
        
        seq_max = np.max(sequence_data)
        
        if len(sequence_data) >= 30:
            recent = sequence_data[-30:]
            x = np.arange(len(recent))
            coeffs = np.polyfit(x, recent, 1)
            slope = coeffs[0]
            
            if slope > 2:
                trend = 'increasing'
            elif slope < -2:
                trend = 'decreasing'
            else:
                trend = 'stable'
        else:
            trend = 'stable'
        
        return seq_max, trend
    
    def boost_prediction(self, pred, m14b_value, m14a_value=None, model_name=None, 
                        sequence_max=None, sequence_trend=None, sequence_data=None):
        
        original = pred
        boosted = pred
        
        # UU2 íŒ¨í„´ (ë³´ìˆ˜ì )
        if self.data_pattern == "UU2":
            if model_name == 'ExtremeNet':
                if sequence_max and sequence_max >= 1680:
                    if m14b_value > 500:
                        boosted = max(pred * 1.2, 1680)
                    elif m14b_value > 450:
                        boosted = max(pred * 1.15, 1650)
                    else:
                        boosted = max(pred * 1.1, 1600)
                        
            elif model_name in ['SpikeDetector', 'GoldenRule']:
                if m14b_value > 450:
                    boosted = max(pred * 1.05, 1650)
                    
        # UU1 íŒ¨í„´ (ê°•í•œ ë¶€ìŠ¤íŒ…)
        else:
            if model_name == 'ExtremeNet':
                if sequence_max and sequence_max >= 1682 and sequence_trend == 'increasing':
                    if m14b_value > 550:
                        boosted = max(pred * 1.6, 1850)
                    elif m14b_value > 500:
                        boosted = max(pred * 1.5, 1750)
                    elif m14b_value > 450:
                        boosted = max(pred * 1.4, 1700)
                    elif m14b_value > 400:
                        boosted = max(pred * 1.3, 1650)
                    elif m14b_value > 350:
                        boosted = max(pred * 1.2, 1550)
                    else:
                        boosted = pred * 1.1
                        
            elif model_name in ['SpikeDetector', 'GoldenRule']:
                if m14b_value > 550:
                    boosted = max(pred, 1850)
                elif m14b_value > 500:
                    boosted = max(pred, 1750)
                elif m14b_value > 450:
                    boosted = max(pred, 1700)
                elif m14b_value > 400:
                    boosted = max(pred * 1.05, 1650)
                    
            else:  # PatchTST, StableLSTM
                if sequence_trend == 'increasing':
                    if m14b_value > 550:
                        boosted = max(pred * 1.45, 1850)
                    elif m14b_value > 500:
                        boosted = max(pred * 1.35, 1750)
                    elif m14b_value > 450:
                        boosted = max(pred * 1.25, 1700)
                    elif m14b_value > 400:
                        boosted = max(pred * 1.15, 1650)
        
        # í™©ê¸ˆ íŒ¨í„´
        if m14b_value > 300 and m14a_value and m14a_value < 80:
            if self.data_pattern == "UU2":
                boosted = boosted * 1.05
            else:
                boosted = boosted * 1.15 if sequence_trend == 'increasing' else boosted * 1.08
                
        return boosted

# ====================== ë©”ì¸ í‰ê°€ í´ë˜ìŠ¤ ======================
class ImprovedModelEvaluator:
    def __init__(self, scaler_path='scalers/'):
        print("="*80)
        print("ğŸ”¥ V6.7 ì™„ì „í•œ í‰ê°€ ì‹œìŠ¤í…œ")
        print("="*80)
        
        self.feature_scaler = None
        self.target_scaler = None
        self.seq_len = 100
        self.pred_len = 10
        self.feature_columns = ['TOTALCNT', 'M14AM14B', 'M14AM10A', 'M14AM14BSUM', 'M14AM16']
        
        try:
            with open(f'{scaler_path}feature_scaler.pkl', 'rb') as f:
                self.feature_scaler = pickle.load(f)
            with open(f'{scaler_path}target_scaler.pkl', 'rb') as f:
                self.target_scaler = pickle.load(f)
            with open(f'{scaler_path}config.json', 'r') as f:
                config = json.load(f)
                self.seq_len = config['seq_len']
                self.pred_len = config['pred_len']
                self.feature_columns = config['feature_columns']
            print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
        except:
            print(f"âš ï¸ ìŠ¤ì¼€ì¼ëŸ¬ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
        
        self.models = {}
        self.extreme_booster = ImprovedExtremeValueBooster()
        self.pattern_detector = DataPatternDetector()
        
    def load_all_models(self, model_dir='models/'):
        print(f"\nğŸ“ ëª¨ë¸ ë¡œë”©...")
        
        if not os.path.exists(model_dir):
            print(f"  âš ï¸ ëª¨ë¸ í´ë” ì—†ìŒ")
            return {}
            
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.keras')]
        
        for model_file in model_files:
            model_name = model_file.replace('.keras', '')
            model_path = os.path.join(model_dir, model_file)
            
            try:
                self.models[model_name] = tf.keras.models.load_model(
                    model_path, safe_mode=False
                )
                print(f"  âœ… {model_name} ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"\nì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ")
        return self.models
    
    def load_test_data(self, filepath):
        print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”©: {filepath}")
        df = pd.read_csv(filepath)
        print(f"  ì›ë³¸: {df.shape[0]:,}í–‰")
        
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        
        if 'CURRTIME' in df.columns:
            try:
                df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                               format='%Y%m%d%H%M', errors='coerce')
                df = df.sort_values('CURRTIME').reset_index(drop=True)
            except:
                pass
        
        print(f"  ìœ íš¨: {df.shape[0]:,}í–‰")
        
        # íŒ¨í„´ ìë™ ê°ì§€
        data_pattern = self.pattern_detector.detect_pattern(df, os.path.basename(filepath))
        self.extreme_booster.set_data_pattern(data_pattern)
        
        return df
    
    def create_features(self, df):
        if 'M14AM14B' in df.columns and 'M14AM10A' in df.columns:
            df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
            df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        
        if 'CURRTIME' in df.columns:
            try:
                df['HOUR'] = df['CURRTIME'].dt.hour
                df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
                df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
            except:
                pass
        
        for w in [10, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
        
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        
        return df
    
    def evaluate_all_models(self, test_file):
        """ëª¨ë“  ëª¨ë¸ í‰ê°€ + CSV ì €ì¥"""
        
        # ë°ì´í„° ë¡œë“œ
        df = self.load_test_data(test_file)
        df = self.create_features(df)
        
        current_pattern = self.extreme_booster.data_pattern
        
        # ì˜ˆì¸¡ ë²”ìœ„
        start_idx = self.seq_len
        end_idx = len(df) - self.pred_len
        total = end_idx - start_idx
        
        if total <= 0:
            print("âŒ ë°ì´í„° ë¶€ì¡±")
            return None, None
        
        print(f"\nğŸ”® ì˜ˆì¸¡ ì‹œì‘...")
        print(f"  ì˜ˆì¸¡ ê°œìˆ˜: {total:,}ê°œ")
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ìš©
        all_predictions = pd.DataFrame()
        
        # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        timestamps = []
        actuals = []
        m14b_values = []
        m14a_values = []
        sequence_maxes = []
        sequence_trends = []
        
        print("\nğŸ“Š ì‹œí€€ìŠ¤ ë¶„ì„ ì¤‘...")
        for i in range(start_idx, end_idx):
            actual_idx = i + self.pred_len
            if actual_idx < len(df):
                if 'CURRTIME' in df.columns:
                    timestamps.append(df.iloc[i]['CURRTIME'])
                else:
                    timestamps.append(i)
                    
                actuals.append(df.iloc[actual_idx]['TOTALCNT'])
                m14b_values.append(df.iloc[i]['M14AM14B'])
                m14a_values.append(df.iloc[i]['M14AM10A'])
                
                seq_data = df.iloc[i-self.seq_len:i]['TOTALCNT'].values
                seq_max, trend = self.extreme_booster.analyze_sequence(seq_data)
                sequence_maxes.append(seq_max)
                sequence_trends.append(trend)
        
        # ê¸°ë³¸ ì •ë³´ ì €ì¥
        all_predictions['ì‹œê°„'] = timestamps
        all_predictions['ì‹¤ì œê°’'] = actuals
        all_predictions['M14AM14B'] = m14b_values
        all_predictions['M14AM10A'] = m14a_values
        all_predictions['ì‹œí€€ìŠ¤_MAX'] = sequence_maxes
        all_predictions['ì‹œí€€ìŠ¤_ì¶”ì„¸'] = sequence_trends
        all_predictions['íŒ¨í„´'] = current_pattern
        
        # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë”ë¯¸ ì˜ˆì¸¡
        if not self.models:
            print("\nâš ï¸ ëª¨ë¸ ì—†ìŒ - ë”ë¯¸ ì˜ˆì¸¡")
            
            # ë”ë¯¸ ëª¨ë¸ ìƒì„±
            dummy_models = {
                'ExtremeNet': lambda x: x * 1.0,
                'SpikeDetector': lambda x: x * 1.02,
                'GoldenRule': lambda x: x * 1.01,
                'PatchTST': lambda x: x * 0.99,
                'StableLSTM': lambda x: x * 0.98
            }
            
            for model_name, dummy_func in dummy_models.items():
                print(f"  {model_name} ë”ë¯¸ ì˜ˆì¸¡...")
                dummy_preds = []
                boosted_preds = []
                
                for i in range(len(actuals)):
                    # ë”ë¯¸ ì˜ˆì¸¡ (ìµœê·¼ í‰ê·  + ë…¸ì´ì¦ˆ)
                    base_pred = sequence_maxes[i] * 0.95 + np.random.randn() * 10
                    dummy_pred = dummy_func(base_pred)
                    
                    # ë¶€ìŠ¤íŒ…
                    boosted = self.extreme_booster.boost_prediction(
                        dummy_pred, m14b_values[i], m14a_values[i], model_name,
                        sequence_max=sequence_maxes[i], 
                        sequence_trend=sequence_trends[i]
                    )
                    
                    dummy_preds.append(dummy_pred)
                    boosted_preds.append(boosted)
                
                all_predictions[f'{model_name}_ì›ë³¸'] = [round(p) for p in dummy_preds]
                all_predictions[f'{model_name}_ì˜ˆì¸¡'] = [round(p) for p in boosted_preds]
                all_predictions[f'{model_name}_ì˜¤ì°¨'] = all_predictions[f'{model_name}_ì˜ˆì¸¡'] - all_predictions['ì‹¤ì œê°’']
        
        else:
            # ì‹¤ì œ ëª¨ë¸ ì˜ˆì¸¡
            for model_name, model in self.models.items():
                print(f"\nğŸ¯ {model_name} ì˜ˆì¸¡ ì¤‘...")
                predictions = []
                
                # ë°°ì¹˜ ì˜ˆì¸¡
                batch_size = 500
                for i in range(start_idx, end_idx, batch_size):
                    batch_end = min(i + batch_size, end_idx)
                    
                    X_batch = []
                    for j in range(i, batch_end):
                        seq_data = df.iloc[j-self.seq_len:j][self.feature_columns].values
                        X_batch.append(seq_data)
                    
                    if len(X_batch) == 0:
                        continue
                    
                    X_batch = np.array(X_batch)
                    X_batch_scaled = []
                    for seq in X_batch:
                        seq_scaled = self.feature_scaler.transform(seq)
                        X_batch_scaled.append(seq_scaled)
                    X_batch_scaled = np.array(X_batch_scaled)
                    
                    preds = model.predict(X_batch_scaled, verbose=0)
                    
                    if isinstance(preds, list):
                        y_pred_scaled = preds[0].flatten()
                    else:
                        y_pred_scaled = preds.flatten()
                    
                    y_pred = self.target_scaler.inverse_transform(
                        y_pred_scaled.reshape(-1, 1)).flatten()
                    
                    for k in range(len(y_pred)):
                        actual_idx = i - start_idx + k
                        if actual_idx < len(all_predictions):
                            predictions.append(y_pred[k])
                
                # ë¶€ìŠ¤íŒ…
                predictions = predictions[:len(all_predictions)]
                all_predictions[f'{model_name}_ì›ë³¸'] = [round(p) for p in predictions]
                
                boosted_predictions = []
                for i in range(len(predictions)):
                    boosted = self.extreme_booster.boost_prediction(
                        predictions[i], 
                        all_predictions.iloc[i]['M14AM14B'],
                        all_predictions.iloc[i]['M14AM10A'],
                        model_name,
                        sequence_max=all_predictions.iloc[i]['ì‹œí€€ìŠ¤_MAX'],
                        sequence_trend=all_predictions.iloc[i]['ì‹œí€€ìŠ¤_ì¶”ì„¸']
                    )
                    boosted_predictions.append(boosted)
                
                all_predictions[f'{model_name}_ì˜ˆì¸¡'] = [round(p) for p in boosted_predictions]
                all_predictions[f'{model_name}_ì˜¤ì°¨'] = all_predictions[f'{model_name}_ì˜ˆì¸¡'] - all_predictions['ì‹¤ì œê°’']
        
        # ì•™ìƒë¸” ì˜ˆì¸¡
        print("\nğŸ”¥ ì•™ìƒë¸” ìƒì„±...")
        ensemble_preds = []
        
        model_names = ['ExtremeNet', 'SpikeDetector', 'GoldenRule', 'PatchTST', 'StableLSTM']
        
        for i in range(len(all_predictions)):
            m14b = all_predictions.iloc[i]['M14AM14B']
            seq_trend = all_predictions.iloc[i]['ì‹œí€€ìŠ¤_ì¶”ì„¸']
            
            # ë™ì  ê°€ì¤‘ì¹˜
            if current_pattern == "UU2":
                weights = {
                    'SpikeDetector': 0.30,
                    'GoldenRule': 0.25,
                    'PatchTST': 0.20,
                    'StableLSTM': 0.15,
                    'ExtremeNet': 0.10
                }
            else:
                if seq_trend == 'increasing' and m14b > 450:
                    weights = {
                        'SpikeDetector': 0.35,
                        'GoldenRule': 0.25,
                        'ExtremeNet': 0.20,
                        'PatchTST': 0.10,
                        'StableLSTM': 0.10
                    }
                else:
                    weights = {
                        'PatchTST': 0.30,
                        'StableLSTM': 0.25,
                        'ExtremeNet': 0.20,
                        'SpikeDetector': 0.15,
                        'GoldenRule': 0.10
                    }
            
            ensemble_pred = 0
            total_weight = 0
            
            for model_name in model_names:
                if f'{model_name}_ì˜ˆì¸¡' in all_predictions.columns:
                    weight = weights.get(model_name, 0.2)
                    ensemble_pred += all_predictions.iloc[i][f'{model_name}_ì˜ˆì¸¡'] * weight
                    total_weight += weight
            
            if total_weight > 0:
                ensemble_pred = ensemble_pred / total_weight
                
            ensemble_preds.append(ensemble_pred)
        
        all_predictions['ì•™ìƒë¸”_ì˜ˆì¸¡'] = [round(p) for p in ensemble_preds]
        all_predictions['ì•™ìƒë¸”_ì˜¤ì°¨'] = all_predictions['ì•™ìƒë¸”_ì˜ˆì¸¡'] - all_predictions['ì‹¤ì œê°’']
        
        # CSV ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f'predictions_{current_pattern}_{timestamp}.csv'
        all_predictions.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ê²°ê³¼ CSV ì €ì¥: {output_file}")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        print("\nğŸ“Š ì„±ëŠ¥ í‰ê°€:")
        print("-"*60)
        
        metrics = {}
        for model_name in model_names + ['ì•™ìƒë¸”']:
            pred_col = f'{model_name}_ì˜ˆì¸¡'
            if pred_col in all_predictions.columns:
                mae = mean_absolute_error(all_predictions['ì‹¤ì œê°’'], all_predictions[pred_col])
                mape = np.mean(abs(all_predictions[pred_col] - all_predictions['ì‹¤ì œê°’']) / all_predictions['ì‹¤ì œê°’']) * 100
                accuracy = 100 - mape
                
                metrics[model_name] = {
                    'MAE': mae,
                    'MAPE': mape,
                    'ì •í™•ë„': accuracy
                }
                
                print(f"{model_name:15} MAE: {mae:7.2f}  ì •í™•ë„: {accuracy:6.2f}%")
        
        # 1682+ ë¶„ì„
        high_mask = all_predictions['ì‹¤ì œê°’'] >= 1682
        if high_mask.any():
            print(f"\nğŸ¯ 1682+ ì¼€ì´ìŠ¤ ë¶„ì„:")
            high_df = all_predictions[high_mask]
            
            for model_name in model_names + ['ì•™ìƒë¸”']:
                pred_col = f'{model_name}_ì˜ˆì¸¡'
                if pred_col in all_predictions.columns:
                    hit_count = (high_df[pred_col] >= 1682).sum()
                    hit_rate = hit_count / len(high_df) * 100
                    print(f"  {model_name:15} ì ì¤‘: {hit_count}/{len(high_df)} ({hit_rate:.1f}%)")
        
        return all_predictions, metrics

# ====================== ë©”ì¸ ì‹¤í–‰ ======================
def main():
    print("\n" + "="*80)
    print("ğŸš€ V6.7 ì™„ì „í•œ í‰ê°€ ì‹œìŠ¤í…œ")
    print("="*80)
    
    evaluator = ImprovedModelEvaluator()
    models = evaluator.load_all_models('models/')
    
    test_files = [
        'uu1.csv',
        'uu2.csv',
        'data/test_data.csv',
    ]
    
    for file in test_files:
        if os.path.exists(file):
            print(f"\n{'='*80}")
            print(f"ğŸ“ íŒŒì¼: {file}")
            print('='*80)
            
            all_predictions, metrics = evaluator.evaluate_all_models(file)
            
            if all_predictions is not None:
                print(f"\nâœ… {file} í‰ê°€ ì™„ë£Œ!")
                print(f"   ê²°ê³¼ íŒŒì¼: predictions_*.csv")
    
    print("\n" + "="*80)
    print("ğŸ† ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
    print("="*80)

if __name__ == "__main__":
    main()