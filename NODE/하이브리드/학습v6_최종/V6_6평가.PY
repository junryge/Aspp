"""
ğŸ“Š ëª¨ë“  ëª¨ë¸ ë¹„êµ í‰ê°€ ì‹œìŠ¤í…œ
============================
6ê°œ ëª¨ë¸ ì „ì²´ë¥¼ í‰ê°€í•˜ê³  ë¹„êµ
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import json
import os
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class MultiModelEvaluator:
    def __init__(self, scaler_path='scalers/'):
        """ë‹¤ì¤‘ ëª¨ë¸ í‰ê°€ê¸° ì´ˆê¸°í™”"""
        print("="*80)
        print("ğŸ“Š ë‹¤ì¤‘ ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ")
        print("="*80)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        with open(f'{scaler_path}feature_scaler.pkl', 'rb') as f:
            self.feature_scaler = pickle.load(f)
        with open(f'{scaler_path}target_scaler.pkl', 'rb') as f:
            self.target_scaler = pickle.load(f)
        with open(f'{scaler_path}config.json', 'r') as f:
            config = json.load(f)
            self.seq_len = config['seq_len']
            self.pred_len = config['pred_len']
            self.feature_columns = config['feature_columns']
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
        
        self.models = {}
        self.results = {}
        
    def load_all_models(self, model_dir='models/'):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        print(f"\nğŸ“ ëª¨ë¸ ë¡œë”©...")
        
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.keras')]
        
        for model_file in model_files:
            model_name = model_file.replace('.keras', '')
            model_path = os.path.join(model_dir, model_file)
            
            try:
                self.models[model_name] = tf.keras.models.load_model(model_path)
                print(f"  âœ… {model_name} ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"\nì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return self.models
    
    def load_test_data(self, filepath):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        print(f"\nğŸ“‚ í‰ê°€ ë°ì´í„° ë¡œë”©: {filepath}")
        df = pd.read_csv(filepath)
        print(f"  ì›ë³¸: {df.shape[0]:,}í–‰")
        
        # 0ê°’ ì œê±°
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        
        # ì‹œê°„ ë³€í™˜
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                       format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        print(f"  ìœ íš¨: {df.shape[0]:,}í–‰")
        return df
    
    def create_features(self, df):
        """íŠ¹ì„± ìƒì„±"""
        # ê¸°ë³¸ íŠ¹ì„±
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        
        # ì‹œê°„
        df['HOUR'] = df['CURRTIME'].dt.hour
        df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
        df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
        
        # ì´ë™í‰ê· 
        for w in [10, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
        
        # ë³€í™”ìœ¨
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        
        return df
    
    def evaluate_single_model(self, model_name, model, df, save_details=False):
        """ë‹¨ì¼ ëª¨ë¸ í‰ê°€"""
        print(f"\nğŸ¯ {model_name} í‰ê°€ ì¤‘...")
        
        # ì˜ˆì¸¡ ê°€ëŠ¥ ë²”ìœ„
        start_idx = self.seq_len
        end_idx = len(df) - self.pred_len
        
        predictions = []
        actuals = []
        timestamps = []
        detailed_results = []
        
        # ë°°ì¹˜ ì˜ˆì¸¡
        batch_size = 500
        for i in range(start_idx, end_idx, batch_size):
            batch_end = min(i + batch_size, end_idx)
            
            # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
            X_batch = []
            for j in range(i, batch_end):
                seq_data = df.iloc[j-self.seq_len:j][self.feature_columns].values
                X_batch.append(seq_data)
            
            if len(X_batch) == 0:
                continue
                
            # ìŠ¤ì¼€ì¼ë§
            X_batch = np.array(X_batch)
            X_batch_scaled = []
            for seq in X_batch:
                seq_scaled = self.feature_scaler.transform(seq)
                X_batch_scaled.append(seq_scaled)
            X_batch_scaled = np.array(X_batch_scaled)
            
            # ì˜ˆì¸¡
            preds = model.predict(X_batch_scaled, verbose=0)
            
            # íšŒê·€ ì˜ˆì¸¡ê°’ ì¶”ì¶œ
            if isinstance(preds, list):
                y_pred_scaled = preds[0].flatten()
            else:
                y_pred_scaled = preds.flatten()
            
            # ì—­ë³€í™˜
            y_pred = self.target_scaler.inverse_transform(
                y_pred_scaled.reshape(-1, 1)).flatten()
            
            # ì‹¤ì œê°’ ìˆ˜ì§‘
            for k, j in enumerate(range(i, batch_end)):
                actual_time = df.iloc[j]['CURRTIME']
                pred_time = actual_time + timedelta(minutes=self.pred_len)
                
                actual_idx = j + self.pred_len
                if actual_idx < len(df):
                    actual_value = df.iloc[actual_idx]['TOTALCNT']
                    pred_value = y_pred[k]
                    
                    predictions.append(pred_value)
                    actuals.append(actual_value)
                    timestamps.append(pred_time)
                    
                    if save_details:
                        detailed_results.append({
                            'ëª¨ë¸': model_name,
                            'ì˜ˆì¸¡ì‹œì ': actual_time.strftime('%Y-%m-%d %H:%M'),
                            'ì˜ˆì¸¡ëŒ€ìƒì‹œê°„': pred_time.strftime('%Y-%m-%d %H:%M'),
                            'ì‹¤ì œê°’': actual_value,
                            'ì˜ˆì¸¡ê°’': round(pred_value),
                            'ì˜¤ì°¨': round(pred_value - actual_value),
                            'ì˜¤ì°¨ìœ¨(%)': round(abs(pred_value - actual_value) / actual_value * 100, 2)
                        })
        
        # ì„±ëŠ¥ ê³„ì‚°
        predictions = np.array(predictions)
        actuals = np.array(actuals)
        
        mae = mean_absolute_error(actuals, predictions)
        rmse = np.sqrt(mean_squared_error(actuals, predictions))
        r2 = r2_score(actuals, predictions)
        mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100
        
        results = {
            'MAE': mae,
            'RMSE': rmse,
            'R2': r2,
            'MAPE': mape,
            'ì •í™•ë„(%)': 100 - mape,
            'ì˜ˆì¸¡ê°œìˆ˜': len(predictions)
        }
        
        print(f"  MAE: {mae:.2f}, RMSE: {rmse:.2f}, RÂ²: {r2:.4f}, MAPE: {mape:.2f}%")
        
        if save_details:
            return results, detailed_results
        return results
    
    def evaluate_all_models(self, test_file):
        """ëª¨ë“  ëª¨ë¸ í‰ê°€"""
        # ë°ì´í„° ë¡œë“œ
        df = self.load_test_data(test_file)
        df = self.create_features(df)
        
        print("\n" + "="*80)
        print("ğŸ“Š ëª¨ë¸ë³„ í‰ê°€ ì‹œì‘")
        print("="*80)
        
        all_results = {}
        all_details = []
        
        # ê° ëª¨ë¸ í‰ê°€
        for model_name, model in self.models.items():
            results = self.evaluate_single_model(model_name, model, df, save_details=False)
            all_results[model_name] = results
        
        # ê²°ê³¼ DataFrame ìƒì„±
        results_df = pd.DataFrame(all_results).T
        results_df = results_df.sort_values('R2', ascending=False)
        
        print("\n" + "="*80)
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
        print("="*80)
        
        # ì„±ëŠ¥ í…Œì´ë¸” ì¶œë ¥
        print(f"\n{'ëª¨ë¸':<15} {'MAE':>8} {'RMSE':>8} {'RÂ²':>8} {'MAPE(%)':>8} {'ì •í™•ë„(%)':>8}")
        print("-" * 60)
        
        for model_name, row in results_df.iterrows():
            print(f"{model_name:<15} {row['MAE']:8.2f} {row['RMSE']:8.2f} "
                  f"{row['R2']:8.4f} {row['MAPE']:8.2f} {row['ì •í™•ë„(%)']:8.2f}")
        
        # ìµœê³  ì„±ëŠ¥
        print("\nğŸ† ìµœê³  ì„±ëŠ¥:")
        print(f"  MAE ìµœì €: {results_df['MAE'].idxmin()} ({results_df['MAE'].min():.2f})")
        print(f"  RMSE ìµœì €: {results_df['RMSE'].idxmin()} ({results_df['RMSE'].min():.2f})")
        print(f"  RÂ² ìµœê³ : {results_df['R2'].idxmax()} ({results_df['R2'].max():.4f})")
        print(f"  MAPE ìµœì €: {results_df['MAPE'].idxmin()} ({results_df['MAPE'].min():.2f}%)")
        print(f"  ì •í™•ë„ ìµœê³ : {results_df['ì •í™•ë„(%)'].idxmax()} ({results_df['ì •í™•ë„(%)'].max():.2f}%)")
        
        # CSV ì €ì¥
        output_file = f'model_comparison_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        results_df.to_csv(output_file, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥: {output_file}")
        
        # ì‹œê°í™”
        self.visualize_comparison(results_df)
        
        # ìµœê³  ëª¨ë¸ë¡œ ìƒì„¸ í‰ê°€
        best_model_name = results_df['R2'].idxmax()
        print(f"\nğŸ“ ìµœê³  ëª¨ë¸({best_model_name}) ìƒì„¸ í‰ê°€...")
        
        _, detailed = self.evaluate_single_model(
            best_model_name, 
            self.models[best_model_name], 
            df, 
            save_details=True
        )
        
        # ìƒì„¸ ê²°ê³¼ ì €ì¥
        if detailed:
            detailed_df = pd.DataFrame(detailed)
            detail_file = f'best_model_details_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
            
            # ì²˜ìŒ 100ê°œì™€ ë§ˆì§€ë§‰ 100ê°œë§Œ ì €ì¥
            sample_df = pd.concat([detailed_df.head(100), detailed_df.tail(100)])
            sample_df.to_csv(detail_file, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ ìƒì„¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {detail_file}")
            
            # ìƒ˜í”Œ ì¶œë ¥
            print(f"\nğŸ“ {best_model_name} ì˜ˆì¸¡ ìƒ˜í”Œ (ìµœê·¼ 10ê°œ):")
            print("="*100)
            print(f"{'ì˜ˆì¸¡ì‹œì ':^19} | {'ëŒ€ìƒì‹œê°„':^19} | {'ì‹¤ì œê°’':>8} | {'ì˜ˆì¸¡ê°’':>8} | {'ì˜¤ì°¨':>8} | {'ì˜¤ì°¨ìœ¨':>8}")
            print("-"*100)
            
            for _, row in sample_df.tail(10).iterrows():
                print(f"{row['ì˜ˆì¸¡ì‹œì ']:^19} | {row['ì˜ˆì¸¡ëŒ€ìƒì‹œê°„']:^19} | "
                      f"{row['ì‹¤ì œê°’']:8.0f} | {row['ì˜ˆì¸¡ê°’']:8.0f} | "
                      f"{row['ì˜¤ì°¨']:8.0f} | {row['ì˜¤ì°¨ìœ¨(%)']:7.1f}%")
        
        return results_df
    
    def visualize_comparison(self, results_df):
        """ëª¨ë¸ ë¹„êµ ì‹œê°í™”"""
        print("\nğŸ“ˆ ë¹„êµ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. MAE ë¹„êµ
        axes[0, 0].bar(results_df.index, results_df['MAE'], color='skyblue')
        axes[0, 0].set_title('MAE Comparison')
        axes[0, 0].set_ylabel('MAE')
        axes[0, 0].set_xticklabels(results_df.index, rotation=45)
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. RÂ² ë¹„êµ
        axes[0, 1].bar(results_df.index, results_df['R2'], color='lightgreen')
        axes[0, 1].set_title('RÂ² Score Comparison')
        axes[0, 1].set_ylabel('RÂ²')
        axes[0, 1].set_xticklabels(results_df.index, rotation=45)
        axes[0, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5)
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. MAPE ë¹„êµ
        axes[1, 0].bar(results_df.index, results_df['MAPE'], color='salmon')
        axes[1, 0].set_title('MAPE Comparison (%)')
        axes[1, 0].set_ylabel('MAPE (%)')
        axes[1, 0].set_xticklabels(results_df.index, rotation=45)
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. ì •í™•ë„ ë¹„êµ
        axes[1, 1].bar(results_df.index, results_df['ì •í™•ë„(%)'], color='gold')
        axes[1, 1].set_title('Accuracy Comparison (%)')
        axes[1, 1].set_ylabel('Accuracy (%)')
        axes[1, 1].set_xticklabels(results_df.index, rotation=45)
        axes[1, 1].axhline(y=95, color='g', linestyle='--', alpha=0.5, label='95% line')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_file = f'model_comparison_plot_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png'
        plt.savefig(plot_file, dpi=100, bbox_inches='tight')
        print(f"ğŸ“Š ë¹„êµ ê·¸ë˜í”„ ì €ì¥: {plot_file}")
        plt.show()

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    # í‰ê°€ê¸° ìƒì„±
    evaluator = MultiModelEvaluator()
    
    # ëª¨ë“  ëª¨ë¸ ë¡œë“œ
    models = evaluator.load_all_models('models/')
    
    if not models:
        print("âŒ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì°¾ê¸°
    test_files = [
        'data/20250731_to20250806.csv',
        'data/test_data.csv',
        '/mnt/user-data/uploads/test.csv'
    ]
    
    test_file = None
    for file in test_files:
        if os.path.exists(file):
            test_file = file
            break
    
    if not test_file:
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # ëª¨ë“  ëª¨ë¸ í‰ê°€
    results = evaluator.evaluate_all_models(test_file)
    
    print("\nâœ… ëª¨ë“  ëª¨ë¸ í‰ê°€ ì™„ë£Œ!")
    print("="*80)

if __name__ == "__main__":
    main()