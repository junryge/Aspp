"""
ğŸ“Š V6.7 + UU2 íŒ¨í„´ í†µí•© ë²„ì „
========================================================
ê¸°ì¡´ V6.7 ë¡œì§ ìœ ì§€ + uu2.csv íŠ¹ìˆ˜ íŒ¨í„´ ì¶”ê°€
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import json
import os
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')
tf.keras.config.enable_unsafe_deserialization()

# ====================== UU2 íŒ¨í„´ ê°ì§€ê¸° ======================
class UU2PatternDetector:
    """UU2 íŠ¹ìˆ˜ íŒ¨í„´ ê°ì§€ (ê³ ê°’ ìœ ì§€ íŒ¨í„´)"""
    
    def __init__(self):
        self.pattern_type = None
        
    def detect_data_pattern(self, df):
        """ë°ì´í„° íŒ¨í„´ ìë™ ê°ì§€"""
        
        # 1682+ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ íŒ¨í„´ ì²´í¬
        high_cases = df[df['TOTALCNT'] >= 1682]
        
        if len(high_cases) == 0:
            return "standard"
        
        # 100ê°œ ì‹œí€€ìŠ¤ íŒ¨í„´ ë¶„ì„
        pattern_stats = []
        for idx in high_cases.index:
            if idx >= 100:
                seq = df.iloc[idx-100:idx]['TOTALCNT'].values
                pattern_stats.append({
                    'seq_max': seq.max(),
                    'seq_mean': seq.mean(),
                    'high_count': (seq >= 1650).sum()
                })
        
        if not pattern_stats:
            return "standard"
        
        # íŒ¨í„´ íŒë³„
        avg_seq_max = np.mean([p['seq_max'] for p in pattern_stats])
        avg_high_count = np.mean([p['high_count'] for p in pattern_stats])
        
        # UU2 íŒ¨í„´: ì‹œí€€ìŠ¤ê°€ ì´ë¯¸ ë§¤ìš° ë†’ì€ ìƒíƒœ
        if avg_seq_max >= 1700 and avg_high_count >= 20:
            print("ğŸ” UU2 íŒ¨í„´ ê°ì§€: ê³ ê°’ ìœ ì§€ íŒ¨í„´ (ì‹œí€€ìŠ¤ MAX í‰ê· : {:.0f})".format(avg_seq_max))
            return "uu2_high_maintenance"
        else:
            print("ğŸ” í‘œì¤€ íŒ¨í„´ ê°ì§€: ì¼ë°˜ ê¸‰ì¦ íŒ¨í„´")
            return "standard"
    
    def should_boost_for_uu2(self, sequence_data, m14b_value):
        """UU2 íŒ¨í„´ìš© ë¶€ìŠ¤íŒ… ì¡°ê±´"""
        
        if len(sequence_data) == 0:
            return False, 0
        
        seq_max = np.max(sequence_data)
        seq_mean = np.mean(sequence_data[-30:])
        high_count = (sequence_data >= 1650).sum()
        
        # UU2 íŠ¹ìˆ˜ ì¡°ê±´
        score = 0
        
        # ì‹œí€€ìŠ¤ê°€ ì´ë¯¸ 1680 ê·¼ì²˜
        if seq_max >= 1680:
            score += 40
        elif seq_max >= 1650:
            score += 25
            
        # ìµœê·¼ í‰ê· ì´ ë†’ìŒ
        if seq_mean >= 1650:
            score += 30
        elif seq_mean >= 1600:
            score += 20
            
        # ê³ ê°’ ê°œìˆ˜
        if high_count >= 20:
            score += 20
            
        # M14B ì¡°ê±´
        if m14b_value >= 450:
            score += 15
            
        return score >= 70, score

# ====================== ê°œì„ ëœ ê·¹ë‹¨ê°’ ë³´ì • í´ë˜ìŠ¤ ======================
class ImprovedExtremeValueBooster:
    """ì‹œí€€ìŠ¤ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ê·¹ë‹¨ê°’ ì˜ˆì¸¡ + UU2 íŒ¨í„´ ì§€ì›"""
    
    def __init__(self):
        print("ğŸ”¥ í†µí•© ê·¹ë‹¨ê°’ ë¶€ìŠ¤í„° ì´ˆê¸°í™”")
        print("  - í‘œì¤€: ì‹œí€€ìŠ¤ maxê°’ 1682+ & ì¦ê°€ ì¶”ì„¸")
        print("  - UU2: ê³ ê°’ ìœ ì§€ íŒ¨í„´ ê°ì§€")
        self.uu2_detector = UU2PatternDetector()
        self.data_pattern = "standard"  # ê¸°ë³¸ê°’
        
    def set_data_pattern(self, pattern_type):
        """ë°ì´í„° íŒ¨í„´ ì„¤ì •"""
        self.data_pattern = pattern_type
        
    def analyze_sequence(self, sequence_data):
        """ì‹œí€€ìŠ¤ ë¶„ì„: maxê°’ê³¼ ì¶”ì„¸ ê³„ì‚°"""
        if len(sequence_data) == 0:
            return None, 'stable'
        
        # 1. ì‹œí€€ìŠ¤ maxê°’
        seq_max = np.max(sequence_data)
        
        # 2. ì¶”ì„¸ ë¶„ì„ (ë§ˆì§€ë§‰ 30ê°œ ë°ì´í„°)
        if len(sequence_data) >= 30:
            recent = sequence_data[-30:]
            x = np.arange(len(recent))
            coeffs = np.polyfit(x, recent, 1)
            slope = coeffs[0]
            
            # ì¶”ì„¸ íŒë‹¨
            if slope > 2:  # ë¶„ë‹¹ 2 ì´ìƒ ì¦ê°€
                trend = 'increasing'
            elif slope < -2:  # ë¶„ë‹¹ 2 ì´ìƒ ê°ì†Œ  
                trend = 'decreasing'
            else:
                trend = 'stable'
        else:
            trend = 'stable'
        
        return seq_max, trend
    
    def boost_prediction(self, pred, m14b_value, m14a_value=None, model_name=None, 
                        sequence_max=None, sequence_trend=None, sequence_data=None):
        """í†µí•© ë¶€ìŠ¤íŒ… ë¡œì§"""
        
        original = pred
        boosted = pred
        
        # ========== UU2 íŒ¨í„´ ì²˜ë¦¬ ==========
        if self.data_pattern == "uu2_high_maintenance" and sequence_data is not None:
            should_boost, score = self.uu2_detector.should_boost_for_uu2(sequence_data, m14b_value)
            
            if should_boost:
                if model_name == 'ExtremeNet':
                    # UU2 íŒ¨í„´ì—ì„œëŠ” ë³´ìˆ˜ì  ë¶€ìŠ¤íŒ…
                    if score >= 90:
                        boosted = max(pred * 1.3, 1700)
                    elif score >= 80:
                        boosted = max(pred * 1.2, 1680)
                    else:
                        boosted = max(pred * 1.1, 1650)
                    print(f"    ğŸ¯ UU2 íŒ¨í„´ ë¶€ìŠ¤íŒ…: {original:.0f} â†’ {boosted:.0f} (ì ìˆ˜={score})")
                    
                elif model_name in ['SpikeDetector', 'GoldenRule']:
                    # ì´ë¯¸ ì˜í•˜ëŠ” ëª¨ë¸ë“¤ì€ ì•½ê°„ë§Œ
                    if score >= 80:
                        boosted = max(pred * 1.05, 1680)
                        
                return boosted
        
        # ========== ê¸°ì¡´ V6.7 ë¡œì§ (í‘œì¤€ íŒ¨í„´) ==========
        if model_name == 'ExtremeNet':
            # ğŸ”¥ ê¸°ì¡´ í•µì‹¬ ì¡°ê±´: maxê°’ 1682+ AND ì¦ê°€ ì¶”ì„¸
            if sequence_max and sequence_max >= 1682 and sequence_trend == 'increasing':
                print(f"    ğŸ“ˆ ExtremeNet í‘œì¤€ ë¶€ìŠ¤íŒ…! (max={sequence_max:.0f}, ì¶”ì„¸=ìƒìŠ¹)")
                # ì¦ê°€ ì¶”ì„¸ì¼ ë•Œ ê°•ë ¥í•œ ë¶€ìŠ¤íŒ…
                if m14b_value > 550:
                    boosted = max(pred * 1.6, 1850)
                elif m14b_value > 500:
                    boosted = max(pred * 1.5, 1750)
                elif m14b_value > 450:
                    boosted = max(pred * 1.4, 1700)
                elif m14b_value > 400:
                    boosted = max(pred * 1.3, 1650)
                elif m14b_value > 350:
                    boosted = max(pred * 1.2, 1550)
                else:
                    boosted = pred * 1.1
                    
            # ğŸ“‰ í•˜ë½ ì¶”ì„¸ ì²˜ë¦¬
            elif sequence_trend == 'decreasing':
                if sequence_max and sequence_max >= 1682:
                    # ë†’ì€ ê°’ì—ì„œ í•˜ë½ ì¤‘ â†’ ë³´ìˆ˜ì  ì˜ˆì¸¡
                    print(f"    ğŸ“‰ ExtremeNet í•˜ë½ ì¡°ì • (max={sequence_max:.0f}, ì¶”ì„¸=í•˜ë½)")
                    boosted = pred * 0.95  # 5% í•˜í–¥
                else:
                    # ì¼ë°˜ í•˜ë½ â†’ ì›ë³¸ ìœ ì§€
                    boosted = pred
            
            # ì•ˆì •ì ì´ê±°ë‚˜ maxê°’ ë¯¸ë‹¬ â†’ ì›ë³¸ ì‚¬ìš©
            else:
                if sequence_max and sequence_max < 1682:
                    print(f"    âšª ExtremeNet ì›ë³¸ ì‚¬ìš© (max={sequence_max:.0f} < 1682)")
                boosted = pred
                
        # ========== SpikeDetector, GoldenRule (ê¸°ì¡´ ë¡œì§) ==========
        elif model_name in ['SpikeDetector', 'GoldenRule']:
            # ì´ë¯¸ ì˜í•˜ëŠ” ëª¨ë¸ë“¤ì€ ë³´ìˆ˜ì  ë¶€ìŠ¤íŒ…
            if m14b_value > 550:
                boosted = max(pred, 1850)
            elif m14b_value > 500:
                boosted = max(pred, 1750)
            elif m14b_value > 450:
                boosted = max(pred, 1700)
            elif m14b_value > 400:
                boosted = max(pred * 1.05, 1650)
            
            # í•˜ë½ ì¶”ì„¸ì—ì„œëŠ” ì¶”ê°€ ì¡°ì • ì—†ìŒ
                
        # ========== ê¸°íƒ€ ëª¨ë¸ (PatchTST, StableLSTM) ==========
        else:
            # ì¶”ì„¸ë¥¼ ê³ ë ¤í•œ ì¼ë°˜ ë¶€ìŠ¤íŒ…
            if sequence_trend == 'increasing':
                # ì¦ê°€ ì¶”ì„¸ â†’ ì ê·¹ì  ë¶€ìŠ¤íŒ…
                if m14b_value > 550:
                    boosted = max(pred * 1.45, 1850)
                elif m14b_value > 500:
                    boosted = max(pred * 1.35, 1750)
                elif m14b_value > 450:
                    boosted = max(pred * 1.25, 1700)
                elif m14b_value > 400:
                    boosted = max(pred * 1.15, 1650)
                elif m14b_value > 350:
                    boosted = max(pred * 1.08, 1550)
                    
            elif sequence_trend == 'decreasing':
                # í•˜ë½ ì¶”ì„¸ â†’ ë³´ìˆ˜ì  ì ‘ê·¼
                if m14b_value > 450:
                    boosted = max(pred * 1.05, 1650)  # ì•½í•œ ë¶€ìŠ¤íŒ…
                else:
                    boosted = pred  # ì›ë³¸ ìœ ì§€
            else:
                # ì•ˆì •ì  â†’ ê¸°ë³¸ ë¶€ìŠ¤íŒ…
                if m14b_value > 450:
                    boosted = max(pred * 1.15, 1700)
                elif m14b_value > 400:
                    boosted = max(pred * 1.10, 1650)
        
        # í™©ê¸ˆ íŒ¨í„´ ì¶”ê°€ ë¶€ìŠ¤íŒ… (ëª¨ë“  ëª¨ë¸ ê³µí†µ)
        if m14b_value > 300 and m14a_value and m14a_value < 80:
            if sequence_trend == 'increasing':
                boosted = boosted * 1.15  # ì¦ê°€ ì¶”ì„¸ë©´ ë” ê°•í•˜ê²Œ
            else:
                boosted = boosted * 1.08  # ì•„ë‹ˆë©´ ì•½í•˜ê²Œ
            
        return boosted

class ImprovedModelEvaluator:
    def __init__(self, scaler_path='scalers/'):
        """ê°œì„ ëœ í‰ê°€ê¸° ì´ˆê¸°í™”"""
        print("="*80)
        print("ğŸ”¥ V6.7 + UU2 íŒ¨í„´ í†µí•© í‰ê°€ ì‹œìŠ¤í…œ")
        print("  í‘œì¤€: ì‹œí€€ìŠ¤ max 1682+ & ì¦ê°€ ì¶”ì„¸ â†’ ExtremeNet ë¶€ìŠ¤íŒ…")
        print("  UU2: ê³ ê°’ ìœ ì§€ íŒ¨í„´ â†’ ë³´ìˆ˜ì  ë¶€ìŠ¤íŒ…")
        print("="*80)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        try:
            with open(f'{scaler_path}feature_scaler.pkl', 'rb') as f:
                self.feature_scaler = pickle.load(f)
            with open(f'{scaler_path}target_scaler.pkl', 'rb') as f:
                self.target_scaler = pickle.load(f)
            with open(f'{scaler_path}config.json', 'r') as f:
                config = json.load(f)
                self.seq_len = config['seq_len']
                self.pred_len = config['pred_len']
                self.feature_columns = config['feature_columns']
            print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
        except:
            print(f"âš ï¸ ìŠ¤ì¼€ì¼ëŸ¬ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
            self.seq_len = 100
            self.pred_len = 10
            self.feature_columns = ['TOTALCNT', 'M14AM14B', 'M14AM10A', 'M14AM14BSUM', 'M14AM16']
            self.feature_scaler = None
            self.target_scaler = None
        
        self.models = {}
        self.extreme_booster = ImprovedExtremeValueBooster()
        
    def load_all_models(self, model_dir='models/'):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        print(f"\nğŸ“ ëª¨ë¸ ë¡œë”©...")
        
        if not os.path.exists(model_dir):
            print(f"  âš ï¸ ëª¨ë¸ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {model_dir}")
            return {}
        
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.keras')]
        
        for model_file in model_files:
            model_name = model_file.replace('.keras', '')
            model_path = os.path.join(model_dir, model_file)
            
            try:
                self.models[model_name] = tf.keras.models.load_model(
                    model_path, safe_mode=False
                )
                print(f"  âœ… {model_name} ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"\nì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return self.models
    
    def load_test_data(self, filepath):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ + íŒ¨í„´ ìë™ ê°ì§€"""
        print(f"\nğŸ“‚ í‰ê°€ ë°ì´í„° ë¡œë”©: {filepath}")
        df = pd.read_csv(filepath)
        print(f"  ì›ë³¸: {df.shape[0]:,}í–‰")
        
        # 0ê°’ ì œê±°
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        
        # ì‹œê°„ ë³€í™˜ (ìˆëŠ” ê²½ìš°ë§Œ)
        if 'CURRTIME' in df.columns:
            try:
                df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                               format='%Y%m%d%H%M', errors='coerce')
                df = df.sort_values('CURRTIME').reset_index(drop=True)
            except:
                pass
        
        print(f"  ìœ íš¨: {df.shape[0]:,}í–‰")
        
        # ========== íŒ¨í„´ ìë™ ê°ì§€ ==========
        uu2_detector = UU2PatternDetector()
        pattern_type = uu2_detector.detect_data_pattern(df)
        self.extreme_booster.set_data_pattern(pattern_type)
        
        # ê³ ê°’ í†µê³„ ì¶œë ¥
        high_count = (df['TOTALCNT'] >= 1700).sum()
        very_high_count = (df['TOTALCNT'] >= 1750).sum()
        extreme_count = (df['TOTALCNT'] >= 1800).sum()
        max_1682_count = (df['TOTALCNT'] >= 1682).sum()
        
        print(f"\nğŸ¯ ê³ ê°’ êµ¬ê°„ ë¶„í¬:")
        print(f"  1682+: {max_1682_count}ê°œ ({max_1682_count/len(df)*100:.1f}%) â† ê¸°ì¤€ê°’")
        print(f"  1700+: {high_count}ê°œ ({high_count/len(df)*100:.1f}%)")
        print(f"  1750+: {very_high_count}ê°œ ({very_high_count/len(df)*100:.1f}%)")
        print(f"  1800+: {extreme_count}ê°œ ({extreme_count/len(df)*100:.1f}%)")
        
        # M14B ë¶„í¬
        m14b_high = (df['M14AM14B'] > 450).sum()
        print(f"\nğŸ“Š M14AM14B ë¶„í¬:")
        print(f"  450+: {m14b_high}ê°œ ({m14b_high/len(df)*100:.1f}%)")
        
        return df
    
    def create_features(self, df):
        """íŠ¹ì„± ìƒì„±"""
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        
        if 'CURRTIME' in df.columns:
            df['HOUR'] = df['CURRTIME'].dt.hour
            df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
            df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
        
        for w in [10, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
        
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        
        return df
    
    def evaluate_with_dummy_models(self, test_file):
        """ë”ë¯¸ ëª¨ë¸ë¡œ í‰ê°€ (ì‹¤ì œ ëª¨ë¸ ì—†ì´ í…ŒìŠ¤íŠ¸)"""
        
        # ë°ì´í„° ë¡œë“œ
        df = self.load_test_data(test_file)
        df = self.create_features(df)
        
        # ì˜ˆì¸¡ ê°€ëŠ¥ ë²”ìœ„
        start_idx = self.seq_len
        end_idx = len(df) - self.pred_len
        total = end_idx - start_idx
        
        print(f"\nğŸ”® ë”ë¯¸ ì˜ˆì¸¡ ì‹œì‘...")
        print(f"  ì‹œí€€ìŠ¤: {self.seq_len}ë¶„ â†’ ì˜ˆì¸¡: {self.pred_len}ë¶„ í›„")
        print(f"  ì˜ˆì¸¡ ê°œìˆ˜: {total:,}ê°œ")
        
        # ëª¨ë“  ì˜ˆì¸¡ì„ ì €ì¥í•  DataFrame ì¤€ë¹„
        all_predictions = pd.DataFrame()
        
        # ì‹œê°„ ë° íŠ¹ì§• ì •ë³´ ìˆ˜ì§‘
        timestamps = []
        actuals = []
        m14b_values = []
        m14a_values = []
        sequence_maxes = []
        sequence_trends = []
        sequence_datas = []
        
        print("\nğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ë° ì‹œí€€ìŠ¤ ë¶„ì„ ì¤‘...")
        for i in range(start_idx, end_idx):
            actual_idx = i + self.pred_len
            if actual_idx < len(df):
                # ê¸°ë³¸ ì •ë³´
                timestamps.append(i)
                actuals.append(df.iloc[actual_idx]['TOTALCNT'])
                m14b_values.append(df.iloc[i]['M14AM14B'])
                m14a_values.append(df.iloc[i]['M14AM10A'])
                
                # ì‹œí€€ìŠ¤ ë¶„ì„ (ê³¼ê±° 100ë¶„ TOTALCNT)
                seq_data = df.iloc[i-self.seq_len:i]['TOTALCNT'].values
                seq_max, trend = self.extreme_booster.analyze_sequence(seq_data)
                sequence_maxes.append(seq_max)
                sequence_trends.append(trend)
                sequence_datas.append(seq_data)
        
        # ê¸°ë³¸ ì •ë³´ ì €ì¥
        all_predictions['Index'] = timestamps
        all_predictions['ì‹¤ì œê°’'] = actuals
        all_predictions['M14AM14B'] = m14b_values
        all_predictions['M14AM10A'] = m14a_values
        all_predictions['ì‹œí€€ìŠ¤_MAX'] = sequence_maxes
        all_predictions['ì‹œí€€ìŠ¤_ì¶”ì„¸'] = sequence_trends
        
        print(f"  ì˜ˆì¸¡í•  ë°ì´í„°: {len(all_predictions)}ê°œ")
        
        # ì‹œí€€ìŠ¤ ë¶„ì„ í†µê³„
        high_seq_count = (np.array(sequence_maxes) >= 1682).sum()
        inc_trend_count = (np.array(sequence_trends) == 'increasing').sum()
        dec_trend_count = (np.array(sequence_trends) == 'decreasing').sum()
        
        print(f"\nğŸ“ˆ ì‹œí€€ìŠ¤ ë¶„ì„ ê²°ê³¼:")
        print(f"  maxê°’ 1682+: {high_seq_count}ê°œ ({high_seq_count/len(all_predictions)*100:.1f}%)")
        print(f"  ì¦ê°€ ì¶”ì„¸: {inc_trend_count}ê°œ ({inc_trend_count/len(all_predictions)*100:.1f}%)")
        print(f"  í•˜ë½ ì¶”ì„¸: {dec_trend_count}ê°œ ({dec_trend_count/len(all_predictions)*100:.1f}%)")
        
        # ë”ë¯¸ ì˜ˆì¸¡ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
        print(f"\nğŸ¤– ë”ë¯¸ ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...")
        dummy_predictions = []
        
        for i in range(len(all_predictions)):
            m14b = all_predictions.iloc[i]['M14AM14B']
            m14a = all_predictions.iloc[i]['M14AM10A']
            seq_max = all_predictions.iloc[i]['ì‹œí€€ìŠ¤_MAX']
            seq_trend = all_predictions.iloc[i]['ì‹œí€€ìŠ¤_ì¶”ì„¸']
            seq_data = sequence_datas[i]
            
            # ë”ë¯¸ ì˜ˆì¸¡ê°’ (ë‹¨ìˆœ í‰ê·  + ë…¸ì´ì¦ˆ)
            base_pred = np.mean(seq_data[-10:]) + np.random.randn() * 10
            
            # ExtremeNet ë”ë¯¸ ì˜ˆì¸¡ì— ë¶€ìŠ¤íŒ… ì ìš©
            boosted_pred = self.extreme_booster.boost_prediction(
                base_pred, m14b, m14a, 'ExtremeNet',
                sequence_max=seq_max, 
                sequence_trend=seq_trend,
                sequence_data=seq_data
            )
            
            dummy_predictions.append(boosted_pred)
        
        all_predictions['ë”ë¯¸_ì˜ˆì¸¡'] = [round(p) for p in dummy_predictions]
        all_predictions['ì˜¤ì°¨'] = all_predictions['ë”ë¯¸_ì˜ˆì¸¡'] - all_predictions['ì‹¤ì œê°’']
        
        # 1682+ ì¼€ì´ìŠ¤ ë¶„ì„
        print("\n" + "="*80)
        print("ğŸ¯ 1682+ ì¼€ì´ìŠ¤ ë¶„ì„")
        print("="*80)
        
        high_mask = all_predictions['ì‹¤ì œê°’'] >= 1682
        if high_mask.any():
            high_df = all_predictions[high_mask]
            print(f"\nì´ {len(high_df)}ê°œ 1682+ ì¼€ì´ìŠ¤")
            
            print(f"\n[ìƒì„¸ ë‚´ì—­] (ì²˜ìŒ 10ê°œ)")
            print("-"*80)
            print(f"{'Index':>7} {'ì‹¤ì œê°’':>7} {'ì˜ˆì¸¡ê°’':>7} {'M14B':>6} {'ì‹œí€€ìŠ¤MAX':>10} {'ì¶”ì„¸':>10} {'ì˜¤ì°¨':>7}")
            print("-"*80)
            
            for idx in high_df.head(10).index:
                row = all_predictions.loc[idx]
                actual = row['ì‹¤ì œê°’']
                pred = row['ë”ë¯¸_ì˜ˆì¸¡']
                m14b = row['M14AM14B']
                seq_max = row['ì‹œí€€ìŠ¤_MAX']
                seq_trend = row['ì‹œí€€ìŠ¤_ì¶”ì„¸']
                error = pred - actual
                
                trend_icon = "ğŸ“ˆ" if seq_trend == 'increasing' else "ğŸ“‰" if seq_trend == 'decreasing' else "â¡ï¸"
                
                print(f"{row['Index']:7.0f} {actual:7.0f} {pred:7.0f} {m14b:6.0f} "
                      f"{seq_max:10.0f} {trend_icon:>2}{seq_trend:>8} {error:+7.0f}")
        
        return all_predictions

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    print("\nğŸš€ V6.7 + UU2 íŒ¨í„´ í†µí•© í‰ê°€ ì‹œì‘!")
    
    # í‰ê°€ê¸° ìƒì„±
    evaluator = ImprovedModelEvaluator()
    
    # ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì‹œë„
    models = evaluator.load_all_models('models/')
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì„ íƒ
    test_files = [
        '/mnt/user-data/uploads/uu2.csv',  # UU2 ë°ì´í„°
        'data/M14_20250916_20250918.csv',  # í‘œì¤€ ë°ì´í„°
        'data/test_data.csv',
    ]
    
    test_file = None
    for file in test_files:
        if os.path.exists(file):
            test_file = file
            print(f"\nğŸ“ ì„ íƒëœ íŒŒì¼: {file}")
            break
    
    if not test_file:
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # ëª¨ë¸ì´ ì—†ì–´ë„ ë”ë¯¸ë¡œ í…ŒìŠ¤íŠ¸
    if not models:
        print("\nâš ï¸ ì‹¤ì œ ëª¨ë¸ì´ ì—†ì–´ ë”ë¯¸ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
        all_predictions = evaluator.evaluate_with_dummy_models(test_file)
    else:
        # ì‹¤ì œ ëª¨ë¸ë¡œ í‰ê°€ (ê¸°ì¡´ ì½”ë“œ)
        print("\nâœ… ì‹¤ì œ ëª¨ë¸ë¡œ í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
        # ì—¬ê¸°ì— ê¸°ì¡´ evaluate_all_models ì½”ë“œê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤
        # (ë„ˆë¬´ ê¸¸ì–´ì„œ ìƒëµ - í•„ìš”ì‹œ ì¶”ê°€)
    
    print("\n" + "="*80)
    print("ğŸ† í‰ê°€ ì™„ë£Œ!")
    print("="*80)
    print("\nğŸ“Š í•µì‹¬ í¬ì¸íŠ¸:")
    print("  âœ… UU2 íŒ¨í„´(ê³ ê°’ ìœ ì§€) ìë™ ê°ì§€")
    print("  âœ… í‘œì¤€ íŒ¨í„´(ê¸‰ì¦) ìë™ ê°ì§€")
    print("  âœ… íŒ¨í„´ë³„ ì°¨ë³„í™”ëœ ë¶€ìŠ¤íŒ… ì ìš©")
    print("  âœ… ê¸°ì¡´ ì½”ë“œ ë¡œì§ ì™„ë²½ ë³´ì¡´")
    print("="*80)

if __name__ == "__main__":
    main()