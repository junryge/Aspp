"""
ğŸ“Š V6.8 ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ê°œì„  - ë²„ê·¸ ìˆ˜ì • ë²„ì „
========================================================
ì›ë³¸ê³¼ ë¶€ìŠ¤íŒ… ì˜ˆì¸¡ê°’ì´ ì œëŒ€ë¡œ êµ¬ë¶„ë˜ë„ë¡ ìˆ˜ì •
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import json
import os
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')
tf.keras.config.enable_unsafe_deserialization()

class CompleteModelEvaluator:
    def __init__(self, scaler_path='scalers/'):
        """í‰ê°€ê¸° ì´ˆê¸°í™”"""
        print("="*80)
        print("ğŸ”¥ V6.8 ê·¹ë‹¨ê°’ ê°œì„  í‰ê°€ (ë²„ê·¸ ìˆ˜ì •)")
        print("="*80)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        with open(f'{scaler_path}feature_scaler.pkl', 'rb') as f:
            self.feature_scaler = pickle.load(f)
        with open(f'{scaler_path}target_scaler.pkl', 'rb') as f:
            self.target_scaler = pickle.load(f)
        with open(f'{scaler_path}config.json', 'r') as f:
            config = json.load(f)
            self.seq_len = config['seq_len']  # 100
            self.pred_len = config['pred_len']  # 10
            self.feature_columns = config['feature_columns']
        
        print(f"âœ… ì„¤ì •: {self.seq_len}ë¶„ ì‹œí€€ìŠ¤ â†’ {self.pred_len}ë¶„ í›„ ì˜ˆì¸¡")
        self.models = {}
        
    def load_all_models(self, model_dir='models/'):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        print(f"\nğŸ“ ëª¨ë¸ ë¡œë”©...")
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.keras')]
        
        for model_file in model_files:
            model_name = model_file.replace('.keras', '')
            model_path = os.path.join(model_dir, model_file)
            
            try:
                self.models[model_name] = tf.keras.models.load_model(
                    model_path, safe_mode=False
                )
                print(f"  âœ… {model_name} ë¡œë“œ")
            except Exception as e:
                print(f"  âŒ {model_name} ì‹¤íŒ¨: {e}")
        
        print(f"ì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ")
        return self.models
    
    def load_test_data(self, filepath):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        df = pd.read_csv(filepath)
        df = df[df['TOTALCNT'] > 0].reset_index(drop=True)
        
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'].astype(str), 
                                       format='%Y%m%d%H%M', errors='coerce')
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        print(f"\nğŸ“Š ë°ì´í„°: {len(df)}í–‰")
        high_count = (df['TOTALCNT'] >= 1700).sum()
        print(f"  ê³ ê°’(1700+): {high_count}ê°œ ({high_count/len(df)*100:.1f}%)")
        
        return df
    
    def create_features(self, df):
        """íŠ¹ì„± ìƒì„±"""
        df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)
        df['GOLDEN'] = ((df['M14AM14B'] > 300) & (df['M14AM10A'] < 80)).astype(float)
        
        df['HOUR'] = df['CURRTIME'].dt.hour
        df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)
        df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)
        
        for w in [10, 30]:
            df[f'MA_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).mean()
            df[f'STD_{w}'] = df['TOTALCNT'].rolling(w, min_periods=1).std().fillna(0)
        
        df['CHANGE_1'] = df['TOTALCNT'].diff(1).fillna(0)
        df['CHANGE_10'] = df['TOTALCNT'].diff(10).fillna(0)
        
        return df
    
    def apply_extreme_boosting(self, pred, m14b, m14a, model_name):
        """ê·¹ë‹¨ê°’ ë¶€ìŠ¤íŒ… ì ìš©"""
        original = pred
        
        # 1. ExtremeNet - ê³ ê°’ì—ì„œ ì•½í•˜ë¯€ë¡œ ê°•í™” í•„ìš”
        if model_name == 'ExtremeNet':
            if m14b > 500:
                boosted = max(pred * 1.3, 1750)
            elif m14b > 450:
                boosted = max(pred * 1.2, 1700)
            elif m14b > 400:
                boosted = max(pred * 1.1, 1650)
            else:
                boosted = pred * 1.05
                
        # 2. SpikeDetector, GoldenRule - ì´ë¯¸ ì˜í•˜ë¯€ë¡œ ìµœì†Œê°’ë§Œ
        elif model_name in ['SpikeDetector', 'GoldenRule']:
            if m14b > 500:
                boosted = max(pred, 1750)
            elif m14b > 450:
                boosted = max(pred, 1700)
            elif m14b > 400:
                boosted = max(pred, 1650)
            else:
                boosted = pred
                
        # 3. ë‚˜ë¨¸ì§€ ëª¨ë¸ë“¤
        else:
            if m14b > 500:
                boosted = max(pred * 1.15, 1750)
            elif m14b > 450:
                boosted = max(pred * 1.1, 1700)
            elif m14b > 400:
                boosted = max(pred * 1.05, 1650)
            else:
                boosted = pred
        
        # í™©ê¸ˆ íŒ¨í„´
        if m14b > 300 and m14a < 80:
            boosted *= 1.08
        
        return boosted
    
    def evaluate_all_models(self, test_file):
        """ëª¨ë“  ëª¨ë¸ í‰ê°€"""
        
        # ë°ì´í„° ì¤€ë¹„
        df = self.load_test_data(test_file)
        df = self.create_features(df)
        
        start_idx = self.seq_len
        end_idx = len(df) - self.pred_len
        total = end_idx - start_idx
        
        if total <= 0:
            print("âŒ ë°ì´í„° ë¶€ì¡±")
            return None, None
        
        print(f"\nğŸ”® ì˜ˆì¸¡ ì‹œì‘: {total}ê°œ")
        
        # ê²°ê³¼ DataFrame ì¤€ë¹„
        results = []
        
        # ê° ì‹œì ë³„ ì˜ˆì¸¡
        for idx in range(start_idx, end_idx):
            pred_time = df.iloc[idx]['CURRTIME']
            target_time = pred_time + timedelta(minutes=self.pred_len)
            
            actual_idx = idx + self.pred_len
            if actual_idx >= len(df):
                continue
            
            # í˜„ì¬ ì •ë³´
            row_data = {
                'ì˜ˆì¸¡ì‹œì ': pred_time.strftime('%Y-%m-%d %H:%M'),
                'ì˜ˆì¸¡ëŒ€ìƒì‹œê°„': target_time.strftime('%Y-%m-%d %H:%M'),
                'ì‹¤ì œê°’': df.iloc[actual_idx]['TOTALCNT'],
                'M14AM14B': df.iloc[idx]['M14AM14B'],
                'M14AM10A': df.iloc[idx]['M14AM10A']
            }
            
            # ì‹œí€€ìŠ¤ ë°ì´í„° ì¶”ì¶œ
            seq_data = df.iloc[idx-self.seq_len:idx][self.feature_columns].values
            if len(seq_data) != self.seq_len:
                continue
            
            seq_scaled = self.feature_scaler.transform(seq_data)
            seq_scaled = seq_scaled.reshape(1, self.seq_len, -1)
            
            # ê° ëª¨ë¸ ì˜ˆì¸¡
            model_preds = {}
            
            for model_name, model in self.models.items():
                # ì›ë³¸ ì˜ˆì¸¡
                pred = model.predict(seq_scaled, verbose=0)
                if isinstance(pred, list):
                    pred = pred[0]
                pred_value = self.target_scaler.inverse_transform(pred.reshape(-1, 1))[0, 0]
                
                # ì›ë³¸ ì €ì¥
                row_data[f'{model_name}_ì›ë³¸'] = round(pred_value)
                
                # ë¶€ìŠ¤íŒ… ì ìš©
                boosted_value = self.apply_extreme_boosting(
                    pred_value, 
                    row_data['M14AM14B'],
                    row_data['M14AM10A'],
                    model_name
                )
                
                # ë¶€ìŠ¤íŒ… ì €ì¥
                row_data[f'{model_name}_ì˜ˆì¸¡'] = round(boosted_value)
                row_data[f'{model_name}_ì˜¤ì°¨'] = round(boosted_value - row_data['ì‹¤ì œê°’'])
                row_data[f'{model_name}_ì˜¤ì°¨ìœ¨(%)'] = round(
                    abs(boosted_value - row_data['ì‹¤ì œê°’']) / row_data['ì‹¤ì œê°’'] * 100, 2
                )
                
                model_preds[model_name] = boosted_value
            
            # ê·¹ë‹¨ê°’ ì•™ìƒë¸”
            m14b = row_data['M14AM14B']
            
            # M14Bì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
            if m14b > 500:
                weights = {
                    'SpikeDetector': 0.35,
                    'GoldenRule': 0.30,
                    'PatchTST': 0.15,
                    'StableLSTM': 0.10,
                    'ExtremeNet': 0.10
                }
            elif m14b > 450:
                weights = {
                    'SpikeDetector': 0.30,
                    'GoldenRule': 0.25,
                    'PatchTST': 0.20,
                    'StableLSTM': 0.15,
                    'ExtremeNet': 0.10
                }
            else:
                weights = {
                    'PatchTST': 0.30,
                    'StableLSTM': 0.25,
                    'ExtremeNet': 0.20,
                    'SpikeDetector': 0.15,
                    'GoldenRule': 0.10
                }
            
            # ì•™ìƒë¸” ê³„ì‚°
            ensemble = 0
            total_weight = 0
            
            for model_name, pred in model_preds.items():
                weight = weights.get(model_name, 0.1)
                ensemble += pred * weight
                total_weight += weight
            
            if total_weight > 0:
                ensemble = ensemble / total_weight
            
            # ìµœì¢… ì¡°ì •
            if m14b > 500:
                ensemble = max(ensemble, 1750)
            elif m14b > 450:
                ensemble = max(ensemble, 1700)
            elif m14b > 400:
                ensemble = max(ensemble, 1650)
            
            row_data['ê·¹ë‹¨ì•™ìƒë¸”_ì˜ˆì¸¡'] = round(ensemble)
            row_data['ê·¹ë‹¨ì•™ìƒë¸”_ì˜¤ì°¨'] = round(ensemble - row_data['ì‹¤ì œê°’'])
            row_data['ê·¹ë‹¨ì•™ìƒë¸”_ì˜¤ì°¨ìœ¨(%)'] = round(
                abs(ensemble - row_data['ì‹¤ì œê°’']) / row_data['ì‹¤ì œê°’'] * 100, 2
            )
            
            results.append(row_data)
            
            # ì§„í–‰ ìƒí™©
            if len(results) % 100 == 0:
                print(f"  {len(results)}/{total} ì™„ë£Œ...")
        
        # DataFrame ë³€í™˜
        all_predictions = pd.DataFrame(results)
        
        # ì„±ëŠ¥ ë¶„ì„
        print("\n" + "="*80)
        print("ğŸ“Š ì„±ëŠ¥ ë¶„ì„")
        print("="*80)
        
        # ì „ì²´ ì„±ëŠ¥
        models_to_eval = list(self.models.keys()) + ['ê·¹ë‹¨ì•™ìƒë¸”']
        
        print(f"\n[ì „ì²´ ì„±ëŠ¥]")
        print(f"{'ëª¨ë¸':<15} {'MAE':>8} {'RMSE':>8} {'MAPE(%)':>8} {'ì •í™•ë„(%)':>10}")
        print("-" * 55)
        
        for model_name in models_to_eval:
            col_name = f'{model_name}_ì˜ˆì¸¡'
            if col_name in all_predictions.columns:
                preds = all_predictions[col_name].values
                actuals = all_predictions['ì‹¤ì œê°’'].values
                
                mae = mean_absolute_error(actuals, preds)
                rmse = np.sqrt(mean_squared_error(actuals, preds))
                mape = np.mean(abs(preds - actuals) / actuals) * 100
                
                if model_name == 'ê·¹ë‹¨ì•™ìƒë¸”':
                    print(f"{'ğŸ”¥ ' + model_name:<15} {mae:8.2f} {rmse:8.2f} {mape:8.2f} {100-mape:10.2f} â­")
                else:
                    print(f"{model_name:<15} {mae:8.2f} {rmse:8.2f} {mape:8.2f} {100-mape:10.2f}")
        
        # ê³ ê°’ êµ¬ê°„ ë¶„ì„
        high_mask = all_predictions['ì‹¤ì œê°’'] >= 1700
        if high_mask.any():
            high_df = all_predictions[high_mask]
            
            print(f"\n[ê³ ê°’(1700+) êµ¬ê°„ ë¶„ì„] - {len(high_df)}ê°œ ìƒ˜í”Œ")
            print("-" * 70)
            
            for model_name in models_to_eval:
                col_pred = f'{model_name}_ì˜ˆì¸¡'
                col_orig = f'{model_name}_ì›ë³¸'
                
                if col_pred in high_df.columns:
                    preds = high_df[col_pred].values
                    hit_1700 = (preds >= 1700).sum()
                    hit_1650 = (preds >= 1650).sum()
                    
                    # ì›ë³¸ê³¼ ë¹„êµ
                    if col_orig in high_df.columns:
                        orig_preds = high_df[col_orig].values
                        orig_hit = (orig_preds >= 1700).sum()
                        
                        print(f"{model_name:15}: ì›ë³¸ {orig_hit:2d} â†’ ë¶€ìŠ¤íŒ… {hit_1700:2d}/{len(high_df)} "
                              f"({hit_1700/len(high_df)*100:.1f}%) [ê°œì„  +{hit_1700-orig_hit}]")
                    else:
                        print(f"{'ğŸ”¥ ' + model_name:15}: {hit_1700:2d}/{len(high_df)} "
                              f"({hit_1700/len(high_df)*100:.1f}%)")
            
            # ìƒì„¸ ìƒ˜í”Œ
            print(f"\n[ê·¹ë‹¨ì•™ìƒë¸” ê³ ê°’ ì˜ˆì¸¡ ìƒì„¸]")
            print(f"{'ì‹¤ì œ':>6} {'ì˜ˆì¸¡':>6} {'M14B':>5} {'M14A':>5} {'ì˜¤ì°¨':>6} {'ê²°ê³¼':>8}")
            print("-" * 45)
            
            for idx in high_df.head(15).index:
                row = all_predictions.loc[idx]
                actual = row['ì‹¤ì œê°’']
                pred = row['ê·¹ë‹¨ì•™ìƒë¸”_ì˜ˆì¸¡']
                m14b = row['M14AM14B']
                m14a = row['M14AM10A']
                error = pred - actual
                
                if pred >= 1700:
                    result = "âœ… HIT"
                elif pred >= 1650:
                    result = "âš ï¸ NEAR"
                else:
                    result = "âŒ MISS"
                
                print(f"{actual:6.0f} {pred:6.0f} {m14b:5.0f} {m14a:5.0f} {error:+6.0f} {result:>8}")
        
        # CSV ì €ì¥
        output_file = f'v68_predictions_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        all_predictions.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ì €ì¥: {output_file}")
        
        return all_predictions, None

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    print("\nğŸš€ V6.8 ê·¹ë‹¨ê°’ ê°œì„  í‰ê°€ (ë²„ê·¸ ìˆ˜ì •)")
    
    # í‰ê°€ê¸° ìƒì„±
    evaluator = CompleteModelEvaluator()
    
    # ëª¨ë¸ ë¡œë“œ
    models = evaluator.load_all_models('models/')
    
    if not models:
        print("âŒ ëª¨ë¸ ì—†ìŒ")
        return
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼
    test_files = [
        'data/20250731_to20250806.csv',
        'data/test_data.csv',
        '/mnt/user-data/uploads/test.csv'
    ]
    
    test_file = None
    for file in test_files:
        if os.path.exists(file):
            test_file = file
            break
    
    if not test_file:
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì—†ìŒ")
        return
    
    # í‰ê°€ ì‹¤í–‰
    all_predictions, _ = evaluator.evaluate_all_models(test_file)
    
    print("\nâœ… ì™„ë£Œ!")
    print("\ní•µì‹¬:")
    print("  âœ… ì›ë³¸ê³¼ ë¶€ìŠ¤íŒ… ì˜ˆì¸¡ê°’ í™•ì‹¤íˆ êµ¬ë¶„")
    print("  âœ… ExtremeNet ì ì ˆí•œ ë¶€ìŠ¤íŒ… (1.1~1.3ë°°)")
    print("  âœ… M14B > 450 â†’ ìµœì†Œ 1700")
    print("  âœ… ê· í˜•ì¡íŒ ì•™ìƒë¸” ê°€ì¤‘ì¹˜")

if __name__ == "__main__":
    main()