"""
V6 GPU ìµœì í™” + ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ
- GPU ë©”ëª¨ë¦¬ ìµœëŒ€ í™œìš© (ë°°ì¹˜ 512)
- ì¤‘ë‹¨ ì‹œ ìë™ ì €ì¥ ë° ì¬ê°œ
- Mixed Precision Training
- í•™ìŠµ ì§„í–‰ ìƒí™© ì¶”ì 
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import RobustScaler
import json
import os
import pickle
import warnings
import gc
import time
from datetime import datetime
warnings.filterwarnings('ignore')

print("="*60)
print("ğŸš€ V6 GPU ìµœì í™” + ì²´í¬í¬ì¸íŠ¸ ë²„ì „")
print(f"ğŸ“¦ TensorFlow: {tf.__version__}")
print("="*60)

# ============================================
# GPU ìµœì í™” ì„¤ì •
# ============================================
def setup_gpu_optimized():
    """GPU ìµœì í™” ì„¤ì •"""
    print("\nğŸ® GPU ìµœì í™” ì„¤ì •...")
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            
            # Mixed Precision í™œì„±í™”
            policy = tf.keras.mixed_precision.Policy('mixed_float16')
            tf.keras.mixed_precision.set_global_policy(policy)
            print(f"âœ… Mixed Precision í™œì„±í™”: {policy.name}")
            
            print(f"âœ… GPU ê°ì§€: {len(gpus)}ê°œ")
            return True
        except Exception as e:
            print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
            return False
    else:
        print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        return False

has_gpu = setup_gpu_optimized()

# ============================================
# ìµœì í™”ëœ ì„¤ì •
# ============================================
class Config:
    # ì‹œí€€ìŠ¤ íŒŒì¼
    SEQUENCE_FILE = './sequences_v6_gpu.npz'
    SCALER_FILE = './scalers_v6_gpu.pkl'
    
    # ì‹œí€€ìŠ¤ ì„¤ì •
    LOOKBACK = 100
    FORECAST = 10
    
    # GPU ìµœì í™” í•™ìŠµ ì„¤ì •
    if has_gpu:
        BATCH_SIZE = 512  # GPU ë©”ëª¨ë¦¬ í™œìš©
        LEARNING_RATE = 0.003  # í° ë°°ì¹˜ì— ë§ê²Œ
    else:
        BATCH_SIZE = 64
        LEARNING_RATE = 0.001
    
    EPOCHS = 200
    
    # ì €ì¥ ê²½ë¡œ
    MODEL_DIR = './models_v6_checkpoint/'
    CHECKPOINT_DIR = './checkpoints_v6/'
    
    # ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
    SAVE_FREQ = 5  # 5 ì—í­ë§ˆë‹¤ ì €ì¥
    
    USE_SCALED_Y = False

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(Config.MODEL_DIR, exist_ok=True)
os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)

# ============================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
# ============================================
class CheckpointManager:
    """í•™ìŠµ ìƒíƒœ ì €ì¥ ë° ë³µì› ê´€ë¦¬"""
    
    def __init__(self):
        self.state_file = os.path.join(Config.CHECKPOINT_DIR, 'training_state.json')
        
    def save_state(self, model_name, epoch, history):
        """í˜„ì¬ í•™ìŠµ ìƒíƒœ ì €ì¥"""
        state = {
            'model_name': model_name,
            'last_epoch': epoch,
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'history': {
                'loss': history.history.get('loss', [])[-50:],  # ìµœê·¼ 50ê°œ
                'mae': history.history.get('mae', [])[-50:]
            }
        }
        
        # ìƒíƒœ íŒŒì¼ ì €ì¥
        state_path = os.path.join(Config.CHECKPOINT_DIR, f'{model_name}_state.json')
        with open(state_path, 'w') as f:
            json.dump(state, f, indent=2)
        
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {model_name} (Epoch {epoch})")
        
    def load_state(self, model_name):
        """ì €ì¥ëœ ìƒíƒœ ë¶ˆëŸ¬ì˜¤ê¸°"""
        state_path = os.path.join(Config.CHECKPOINT_DIR, f'{model_name}_state.json')
        
        if os.path.exists(state_path):
            with open(state_path, 'r') as f:
                state = json.load(f)
            print(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {model_name} (Epoch {state['last_epoch']})")
            return state
        return None
    
    def get_initial_epoch(self, model_name):
        """ì‹œì‘ ì—í­ í™•ì¸"""
        state = self.load_state(model_name)
        if state:
            return state['last_epoch']
        return 0

# ============================================
# ì†ì‹¤ í•¨ìˆ˜
# ============================================
class WeightedLoss(tf.keras.losses.Loss):
    """ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜"""
    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        
        mae = tf.abs(y_true - y_pred)
        
        # ê°•í™”ëœ ê°€ì¤‘ì¹˜
        weights = tf.ones_like(y_true)
        if not Config.USE_SCALED_Y:
            weights = tf.where(y_true >= 1550, 30.0, weights)
            weights = tf.where((y_true >= 1500) & (y_true < 1550), 25.0, weights)
            weights = tf.where((y_true >= 1450) & (y_true < 1500), 20.0, weights)
            weights = tf.where((y_true >= 1400) & (y_true < 1450), 15.0, weights)
            weights = tf.where((y_true >= 1350) & (y_true < 1400), 10.0, weights)
        
        # í° ì˜¤ì°¨ í˜ë„í‹°
        large_error = tf.where(mae > 100, mae * 0.2, 0.0)
        
        return tf.reduce_mean(mae * weights + large_error)

# ============================================
# M14 ê·œì¹™ ë³´ì • ë ˆì´ì–´
# ============================================
class M14RuleCorrection(tf.keras.layers.Layer):
    """M14 ê·œì¹™ ê¸°ë°˜ ë³´ì •"""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    
    def call(self, inputs, training=None):
        pred, m14_features = inputs
        
        pred = tf.cast(pred, tf.float32)
        m14_features = tf.cast(m14_features, tf.float32)
        
        m14b = m14_features[:, 0:1]
        m10a = m14_features[:, 1:2]
        m16 = m14_features[:, 2:3]
        ratio = m14_features[:, 3:4]
        
        if not Config.USE_SCALED_Y:
            # ì„ê³„ê°’ ê·œì¹™
            pred = tf.where(m14b >= 420, tf.maximum(pred, 1550.0), pred)
            pred = tf.where(m14b >= 380, tf.maximum(pred, 1500.0), pred)
            pred = tf.where(m14b >= 350, tf.maximum(pred, 1450.0), pred)
            pred = tf.where(m14b >= 300, tf.maximum(pred, 1400.0), pred)
            
            # ë¹„ìœ¨ ë³´ì •
            pred = tf.where(ratio >= 5.5, pred * 1.15, pred)
            pred = tf.where((ratio >= 5.0) & (ratio < 5.5), pred * 1.10, pred)
            pred = tf.where((ratio >= 4.5) & (ratio < 5.0), pred * 1.08, pred)
            pred = tf.where((ratio >= 4.0) & (ratio < 4.5), pred * 1.05, pred)
            
            # í™©ê¸ˆ íŒ¨í„´
            golden = (m14b >= 350) & (m10a < 70)
            pred = tf.where(golden, pred * 1.2, pred)
            
            # ë²”ìœ„ ì œí•œ
            pred = tf.clip_by_value(pred, 1200.0, 2000.0)
        
        return pred

# ============================================
# ëª¨ë¸ ì •ì˜
# ============================================
class ModelsV6:
    
    @staticmethod
    def build_lstm_model(input_shape):
        """LSTM ëª¨ë¸"""
        model = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=input_shape),
            tf.keras.layers.LayerNormalization(),
            tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.2),
            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2),
            tf.keras.layers.LSTM(64, dropout=0.2),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dense(256, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dense(1, dtype='float32')  # Mixed Precision ëŒ€ì‘
        ], name='LSTM_Model')
        return model
    
    @staticmethod
    def build_gru_model(input_shape):
        """GRU ëª¨ë¸"""
        model = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=input_shape),
            tf.keras.layers.LayerNormalization(),
            tf.keras.layers.GRU(256, return_sequences=True, dropout=0.15),
            tf.keras.layers.GRU(128, return_sequences=True, dropout=0.15),
            tf.keras.layers.GRU(64, dropout=0.15),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dense(256, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dense(1, dtype='float32')
        ], name='GRU_Model')
        return model
    
    @staticmethod
    def build_cnn_lstm(input_shape):
        """CNN-LSTM ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape)
        
        # ë©€í‹°ìŠ¤ì¼€ì¼ CNN
        convs = []
        for kernel_size in [3, 5, 7, 9]:
            conv = tf.keras.layers.Conv1D(128, kernel_size, activation='relu', padding='same')(inputs)
            conv = tf.keras.layers.BatchNormalization()(conv)
            convs.append(conv)
        
        concat = tf.keras.layers.Concatenate()(convs)
        
        lstm1 = tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.15)(concat)
        lstm2 = tf.keras.layers.LSTM(128, dropout=0.15)(lstm1)
        
        dense1 = tf.keras.layers.Dense(256, activation='relu')(lstm2)
        dropout1 = tf.keras.layers.Dropout(0.2)(dense1)
        dense2 = tf.keras.layers.Dense(128, activation='relu')(dropout1)
        dropout2 = tf.keras.layers.Dropout(0.15)(dense2)
        output = tf.keras.layers.Dense(1, dtype='float32')(dropout2)
        
        return tf.keras.Model(inputs=inputs, outputs=output, name='CNN_LSTM_Model')
    
    @staticmethod
    def build_spike_detector(input_shape):
        """Spike Detector"""
        inputs = tf.keras.Input(shape=input_shape)
        
        # CNN
        convs = []
        for kernel_size in [3, 5, 7]:
            conv = tf.keras.layers.Conv1D(96, kernel_size, activation='relu', padding='same')(inputs)
            convs.append(conv)
        
        concat = tf.keras.layers.Concatenate()(convs)
        norm = tf.keras.layers.BatchNormalization()(concat)
        
        # Attention
        attention = tf.keras.layers.MultiHeadAttention(
            num_heads=8, key_dim=36, dropout=0.15
        )(norm, norm)
        
        # Bidirectional LSTM
        lstm = tf.keras.layers.Bidirectional(
            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.15)
        )(attention)
        
        # Pooling
        avg_pool = tf.keras.layers.GlobalAveragePooling1D()(lstm)
        max_pool = tf.keras.layers.GlobalMaxPooling1D()(lstm)
        pooled = tf.keras.layers.Concatenate()([avg_pool, max_pool])
        
        dense1 = tf.keras.layers.Dense(256, activation='relu')(pooled)
        dropout1 = tf.keras.layers.Dropout(0.2)(dense1)
        dense2 = tf.keras.layers.Dense(128, activation='relu')(dropout1)
        dropout2 = tf.keras.layers.Dropout(0.15)(dense2)
        
        output = tf.keras.layers.Dense(1, name='spike_value', dtype='float32')(dropout2)
        
        return tf.keras.Model(inputs=inputs, outputs=output, name='Spike_Detector')
    
    @staticmethod
    def build_rule_based_model(input_shape, m14_shape):
        """Rule-Based ëª¨ë¸"""
        time_input = tf.keras.Input(shape=input_shape, name='time_input')
        m14_input = tf.keras.Input(shape=(m14_shape,), name='m14_input')
        
        lstm1 = tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.15)(time_input)
        lstm2 = tf.keras.layers.LSTM(32, dropout=0.15)(lstm1)
        
        m14_dense1 = tf.keras.layers.Dense(32, activation='relu')(m14_input)
        m14_dense2 = tf.keras.layers.Dense(16, activation='relu')(m14_dense1)
        
        combined = tf.keras.layers.Concatenate()([lstm2, m14_dense2])
        
        dense1 = tf.keras.layers.Dense(128, activation='relu')(combined)
        dropout1 = tf.keras.layers.Dropout(0.2)(dense1)
        dense2 = tf.keras.layers.Dense(64, activation='relu')(dropout1)
        dropout2 = tf.keras.layers.Dropout(0.15)(dense2)
        
        prediction = tf.keras.layers.Dense(1, name='rule_pred', dtype='float32')(dropout2)
        corrected = M14RuleCorrection()([prediction, m14_input])
        
        return tf.keras.Model(
            inputs=[time_input, m14_input],
            outputs=corrected,
            name='Rule_Based_Model'
        )

# ============================================
# ë°ì´í„° ë¡œë“œ
# ============================================
print("\nğŸ“¦ ë°ì´í„° ë¡œë“œ ì¤‘...")

# ê¸°ì¡´ NPZ íŒŒì¼ ë¡œë“œ
if os.path.exists(Config.SEQUENCE_FILE):
    print(f"âœ… ê¸°ì¡´ NPZ íŒŒì¼ ë¡œë“œ: {Config.SEQUENCE_FILE}")
    data = np.load(Config.SEQUENCE_FILE)
    X_train = data['X']
    y_train = data['y_original'] if 'y_original' in data else data['y']
    m14_train = data['m14_features']
    
    print(f"  X shape: {X_train.shape}")
    print(f"  y shape: {y_train.shape}")
    print(f"  m14 shape: {m14_train.shape}")
    print(f"  y ë²”ìœ„: {y_train.min():.0f} ~ {y_train.max():.0f}")
    
else:
    print("âš ï¸ NPZ íŒŒì¼ ì—†ìŒ - ìƒˆë¡œ ìƒì„± í•„ìš”")
    exit(1)

# íƒ€ê²Ÿ ë¶„í¬ í™•ì¸
print("\nğŸ“Š íƒ€ê²Ÿ ë¶„í¬ (ì „ì²´ ë°ì´í„°):")
for level in [1300, 1400, 1450, 1500, 1550]:
    count = (y_train >= level).sum()
    ratio = count / len(y_train) * 100
    print(f"  {level}+: {count:,}ê°œ ({ratio:.1f}%)")

# ============================================
# í•™ìŠµ í•¨ìˆ˜ (ì²´í¬í¬ì¸íŠ¸ í¬í•¨)
# ============================================
def train_model_with_checkpoint(model_name, model_builder, X_data, y_data, m14_data=None, epochs=200):
    """ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ì´ ìˆëŠ” í•™ìŠµ í•¨ìˆ˜"""
    
    print(f"\n{'='*60}")
    print(f"ğŸ¯ {model_name.upper()} ëª¨ë¸ í•™ìŠµ")
    print(f"{'='*60}")
    
    checkpoint_manager = CheckpointManager()
    
    # ì´ˆê¸° ì—í­ í™•ì¸
    initial_epoch = checkpoint_manager.get_initial_epoch(model_name)
    
    # ëª¨ë¸ ë¡œë“œ ë˜ëŠ” ìƒì„±
    model_path = os.path.join(Config.CHECKPOINT_DIR, f'{model_name}_model.keras')
    
    if os.path.exists(model_path) and initial_epoch > 0:
        print(f"ğŸ”„ ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ (Epoch {initial_epoch}ë¶€í„° ì¬ê°œ)")
        custom_objects = {'WeightedLoss': WeightedLoss}
        if model_name == 'rule':
            custom_objects['M14RuleCorrection'] = M14RuleCorrection
        model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)
    else:
        print("ğŸ†• ìƒˆ ëª¨ë¸ ìƒì„±")
        if model_name == 'rule':
            model = model_builder(X_data.shape[1:], m14_data.shape[1])
        else:
            model = model_builder(X_data.shape[1:])
        
        model.compile(
            optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * (0.5 if model_name == 'rule' else 1.0)),
            loss=WeightedLoss(),
            metrics=['mae']
        )
    
    # ì´ë¯¸ ì™„ë£Œëœ ê²½ìš°
    if initial_epoch >= epochs:
        print(f"âœ… ì´ë¯¸ í•™ìŠµ ì™„ë£Œ (Epoch {initial_epoch})")
        return model, None
    
    # ì½œë°± ì„¤ì •
    callbacks = [
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='loss',
            patience=10,
            factor=0.5,
            min_lr=1e-7,
            verbose=1
        ),
        tf.keras.callbacks.ModelCheckpoint(
            model_path,
            save_best_only=False,  # ë§¤ë²ˆ ì €ì¥
            save_freq='epoch',
            verbose=0
        ),
        tf.keras.callbacks.LambdaCallback(
            on_epoch_end=lambda epoch, logs: checkpoint_manager.save_state(
                model_name, epoch + 1, {'history': logs}
            ) if (epoch + 1) % Config.SAVE_FREQ == 0 else None
        )
    ]
    
    # í•™ìŠµ ì‹œì‘
    print(f"ğŸ“Š í•™ìŠµ ì‹œì‘ (Epoch {initial_epoch + 1}/{epochs})")
    print(f"   ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {Config.BATCH_SIZE}")
    print(f"   í•™ìŠµë¥ : {Config.LEARNING_RATE}")
    
    start_time = time.time()
    
    # ë°ì´í„° ì¤€ë¹„
    if model_name == 'rule':
        train_data = [X_data, m14_data]
    else:
        train_data = X_data
    
    # í•™ìŠµ ì‹¤í–‰
    history = model.fit(
        train_data, y_data,
        epochs=epochs,
        initial_epoch=initial_epoch,
        batch_size=Config.BATCH_SIZE,
        callbacks=callbacks,
        verbose=1
    )
    
    elapsed_time = time.time() - start_time
    print(f"âœ… {model_name} ì™„ë£Œ - ì‹œê°„: {elapsed_time/60:.1f}ë¶„")
    print(f"   ìµœì¢… Loss: {history.history['loss'][-1]:.4f}")
    
    # ìµœì¢… ì €ì¥
    final_path = os.path.join(Config.MODEL_DIR, f'{model_name}_final.keras')
    model.save(final_path)
    print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_path}")
    
    return model, history

# ============================================
# ëª¨ë“  ëª¨ë¸ í•™ìŠµ
# ============================================
print("\n" + "="*60)
print("ğŸ‹ï¸ ì „ì²´ ë°ì´í„° í•™ìŠµ ì‹œì‘ (ì²´í¬í¬ì¸íŠ¸ ì§€ì›)")
print("="*60)

models = {}
histories = {}

# ëª¨ë¸ ëª©ë¡
model_configs = [
    ('lstm', ModelsV6.build_lstm_model, 200),
    ('gru', ModelsV6.build_gru_model, 200),
    ('cnn_lstm', ModelsV6.build_cnn_lstm, 200),
    ('spike', ModelsV6.build_spike_detector, 200),
    ('rule', ModelsV6.build_rule_based_model, 100),
]

# ê° ëª¨ë¸ í•™ìŠµ
for model_name, model_builder, epochs in model_configs:
    if model_name == 'rule':
        model, history = train_model_with_checkpoint(
            model_name, model_builder, X_train, y_train, m14_train, epochs
        )
    else:
        model, history = train_model_with_checkpoint(
            model_name, model_builder, X_train, y_train, epochs=epochs
        )
    
    models[model_name] = model
    if history:
        histories[model_name] = history

# ============================================
# ì•™ìƒë¸” ëª¨ë¸
# ============================================
print("\n" + "="*60)
print("ğŸ¯ ì•™ìƒë¸” ëª¨ë¸ ìƒì„±")
print("="*60)

# ì•™ìƒë¸”ì€ ë³„ë„ ì²˜ë¦¬ (ê°œë³„ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ í›„)
ensemble_path = os.path.join(Config.MODEL_DIR, 'ensemble_final.keras')

if not os.path.exists(ensemble_path):
    # ì•™ìƒë¸” ì…ë ¥
    time_input = tf.keras.Input(shape=X_train.shape[1:], name='ensemble_time')
    m14_input = tf.keras.Input(shape=(m14_train.shape[1],), name='ensemble_m14')
    
    # ê° ëª¨ë¸ ì˜ˆì¸¡
    lstm_pred = models['lstm'](time_input)
    gru_pred = models['gru'](time_input)
    cnn_lstm_pred = models['cnn_lstm'](time_input)
    spike_pred = models['spike'](time_input)
    rule_pred = models['rule']([time_input, m14_input])
    
    # ê°€ì¤‘ í‰ê· 
    ensemble_pred = tf.keras.layers.Lambda(
        lambda x: 0.25*x[0] + 0.20*x[1] + 0.25*x[2] + 0.15*x[3] + 0.15*x[4]
    )([lstm_pred, gru_pred, cnn_lstm_pred, spike_pred, rule_pred])
    
    # ìµœì¢… ë³´ì •
    final_pred = M14RuleCorrection()([ensemble_pred, m14_input])
    
    # ì•™ìƒë¸” ëª¨ë¸
    ensemble_model = tf.keras.Model(
        inputs=[time_input, m14_input],
        outputs=final_pred,
        name='Ensemble_Model'
    )
    
    ensemble_model.compile(
        optimizer=tf.keras.optimizers.Adam(Config.LEARNING_RATE * 0.3),
        loss=WeightedLoss(),
        metrics=['mae']
    )
    
    print("ğŸ“Š ì•™ìƒë¸” íŒŒì¸íŠœë‹...")
    ensemble_hist = ensemble_model.fit(
        [X_train, m14_train], y_train,
        epochs=50,
        batch_size=Config.BATCH_SIZE,
        verbose=1
    )
    
    ensemble_model.save(ensemble_path)
    models['ensemble'] = ensemble_model
else:
    print("âœ… ì•™ìƒë¸” ëª¨ë¸ ì´ë¯¸ ì¡´ì¬")
    custom_objects = {'WeightedLoss': WeightedLoss, 'M14RuleCorrection': M14RuleCorrection}
    models['ensemble'] = tf.keras.models.load_model(ensemble_path, custom_objects=custom_objects)

# ============================================
# í‰ê°€
# ============================================
print("\n" + "="*60)
print("ğŸ“Š ëª¨ë¸ í‰ê°€")
print("="*60)

# ìƒ˜í”Œë§ í‰ê°€
sample_size = min(10000, len(X_train))
sample_idx = np.random.choice(len(X_train), sample_size, replace=False)
X_sample = X_train[sample_idx]
y_sample = y_train[sample_idx]
m14_sample = m14_train[sample_idx]

evaluation_results = {}

for name, model in models.items():
    # ì˜ˆì¸¡
    if name == 'ensemble' or name == 'rule':
        pred = model.predict([X_sample, m14_sample], verbose=0).flatten()
    else:
        pred = model.predict(X_sample, verbose=0).flatten()
    
    # MAE
    mae = np.mean(np.abs(y_sample - pred))
    
    # êµ¬ê°„ë³„ ì„±ëŠ¥
    level_performance = {}
    for level in [1300, 1400, 1450, 1500]:
        mask = y_sample >= level
        if np.any(mask):
            recall = np.sum((pred >= level) & mask) / np.sum(mask)
            precision = np.sum((pred >= level) & mask) / max(np.sum(pred >= level), 1)
            f1 = 2 * (precision * recall) / max(precision + recall, 1e-7)
            level_mae = np.mean(np.abs(y_sample[mask] - pred[mask]))
            
            level_performance[level] = {
                'recall': recall,
                'precision': precision,
                'f1': f1,
                'mae': level_mae
            }
    
    evaluation_results[name] = {
        'overall_mae': mae,
        'levels': level_performance
    }
    
    print(f"\nğŸ¯ {name.upper()}:")
    print(f"  ì „ì²´ MAE: {mae:.2f}")

# ìµœê³  ëª¨ë¸
best_model = min(evaluation_results.keys(), 
                 key=lambda x: evaluation_results[x]['overall_mae'])
print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_model.upper()}")

# ============================================
# ìµœì¢… ì €ì¥
# ============================================
print("\nğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥...")

with open(f"{Config.MODEL_DIR}evaluation_results.json", 'w') as f:
    json.dump(evaluation_results, f, indent=2, default=str)

print("\n" + "="*60)
print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
print("="*60)
print(f"ğŸ“ ëª¨ë¸ ìœ„ì¹˜: {Config.MODEL_DIR}")
print(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸: {Config.CHECKPOINT_DIR}")
print("\nğŸ’¡ ì¤‘ë‹¨ ì‹œ ìë™ìœ¼ë¡œ ì €ì¥ë˜ë©°,")
print("   ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤!")
print("="*60)

# ë©”ëª¨ë¦¬ ì •ë¦¬
tf.keras.backend.clear_session()
gc.collect()