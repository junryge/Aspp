"""
üöÄ Ultimate 1700+ Predictor v8.0 - Production Ready
==================================================
- Infinity/NaN Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞
- Ï†ÑÏ≤¥ Íµ¨Í∞ÑÎ≥Ñ ÌèâÍ∞Ä (ÏùºÎ∞ò, Ï§ëÍ∞Ñ, 1700+)
- ÏïàÏ†ïÏ†ÅÏù∏ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import *
from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
import warnings
import logging
from datetime import datetime
import os
import json

warnings.filterwarnings('ignore')

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# TensorFlow ÏÑ§Ï†ï
tf.random.set_seed(42)
np.random.seed(42)

print("\n" + "="*80)
print("üöÄ Ultimate 1700+ Predictor v8.0")
print(f"üì¶ TensorFlow: {tf.__version__}")
print("="*80)

# ========================================
# ÏïàÏ†ÑÌïú Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
# ========================================

class SafeDataProcessor:
    """Infinity/NaN ÏïàÏ†Ñ Ï≤òÎ¶¨"""
    
    def __init__(self, seq_len=100, pred_len=10):
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.scaler = MinMaxScaler()  # RobustScaler ÎåÄÏã† MinMaxScaler ÏÇ¨Ïö©
        self.target_scaler = MinMaxScaler()
        
    def load_and_process(self, filepath):
        """Îç∞Ïù¥ÌÑ∞ Î°úÎìú - ÏïàÏ†Ñ Ï≤òÎ¶¨"""
        logger.info("üìÇ Îç∞Ïù¥ÌÑ∞ Î°úÎî©...")
        
        # CSV Î°úÎìú
        df = pd.read_csv(filepath)
        logger.info(f"‚úÖ ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞: {df.shape}")
        
        # ÏãúÍ∞Ñ Î≥ÄÌôò
        df['CURRTIME'] = pd.to_datetime(df['CURRTIME'], format='%Y%m%d%H%M', errors='coerce')
        df['TIME'] = pd.to_datetime(df['TIME'], format='%Y%m%d%H%M', errors='coerce')
        
        # NaT Ï†úÍ±∞
        df = df.dropna(subset=['CURRTIME', 'TIME'])
        df = df.sort_values('CURRTIME').reset_index(drop=True)
        
        # ÏàòÏπòÌòï Ïª¨ÎüºÎßå ÏÑ†ÌÉù
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # InfinityÏôÄ NaN Ï≤¥ÌÅ¨ Î∞è Ï†úÍ±∞
        logger.info("üîç Ïù¥ÏÉÅÏπò Ï≤¥ÌÅ¨...")
        for col in numeric_cols:
            # InfinityÎ•º NaNÏúºÎ°ú Î≥ÄÍ≤Ω
            df[col] = df[col].replace([np.inf, -np.inf], np.nan)
            
            # NaNÏùÑ Ï§ëÏïôÍ∞íÏúºÎ°ú Ï±ÑÏö∞Í∏∞
            if df[col].isna().sum() > 0:
                median_val = df[col].median()
                if pd.isna(median_val):
                    median_val = 0
                df[col] = df[col].fillna(median_val)
                logger.info(f"   {col}: {df[col].isna().sum()}Í∞ú NaN ‚Üí {median_val:.2f}Î°ú ÎåÄÏ≤¥")
        
        # Í∑πÎã®Ï†Å Ïù¥ÏÉÅÏπò Ï†úÍ±∞ (IQR Î∞©Î≤ï)
        for col in ['TOTALCNT']:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower = Q1 - 3 * IQR
            upper = Q3 + 3 * IQR
            
            before = len(df)
            df = df[(df[col] >= lower) & (df[col] <= upper)]
            after = len(df)
            
            if before != after:
                logger.info(f"   {col}: {before-after}Í∞ú Í∑πÎã® Ïù¥ÏÉÅÏπò Ï†úÍ±∞")
        
        # ÌÉÄÍ≤ü ÏÉùÏÑ±
        df['TARGET'] = df['TOTALCNT'].shift(-self.pred_len)
        df['IS_1700'] = (df['TARGET'] >= 1700).astype(int)
        
        # ÌäπÏÑ± ÏÉùÏÑ± (ÏïàÏ†ÑÌïòÍ≤å)
        df = self._create_safe_features(df)
        
        # ÏµúÏ¢Ö NaN Ï†úÍ±∞
        df = df.dropna()
        
        logger.info(f"‚úÖ Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å: {df.shape}")
        logger.info(f"‚úÖ 1700+ ÎπÑÏú®: {df['IS_1700'].mean():.2%} ({df['IS_1700'].sum()}Í∞ú)")
        
        return df
    
    def _create_safe_features(self, df):
        """ÏïàÏ†ÑÌïú ÌäπÏÑ± ÏÉùÏÑ±"""
        logger.info("‚öôÔ∏è ÌäπÏÑ± ÏÉùÏÑ±...")
        
        # Ïù¥ÎèôÌèâÍ∑† (ÏïàÏ†ÑÌïú Î≤îÏúÑ)
        for window in [10, 30]:
            df[f'MA_{window}'] = df['TOTALCNT'].rolling(window, min_periods=1).mean()
            df[f'MA_{window}'].fillna(df['TOTALCNT'].mean(), inplace=True)
        
        # Î≥ÄÌôîÏú® (0ÏúºÎ°ú ÎÇòÎàÑÍ∏∞ Î∞©ÏßÄ)
        df['CHANGE'] = df['TOTALCNT'].diff().fillna(0)
        df['PCT_CHANGE'] = df['TOTALCNT'].pct_change().fillna(0)
        
        # Í∑πÎã®Í∞í Î∞©ÏßÄ
        df['PCT_CHANGE'] = df['PCT_CHANGE'].clip(-1, 1)
        
        # ÎπÑÏú® Í≥ÑÏÇ∞ (0ÏúºÎ°ú ÎÇòÎàÑÍ∏∞ Î∞©ÏßÄ)
        if 'M14AM14B' in df.columns and 'M14AM10A' in df.columns:
            df['RATIO'] = df['M14AM14B'] / (df['M14AM10A'] + 1)  # +1Î°ú 0 Î∞©ÏßÄ
            df['RATIO'] = df['RATIO'].clip(0, 20)  # Í∑πÎã®Í∞í Ï†úÌïú
        
        # ÏãúÍ∞Ñ ÌäπÏÑ±
        df['HOUR'] = df['CURRTIME'].dt.hour
        df['DAYOFWEEK'] = df['CURRTIME'].dt.dayofweek
        df['IS_DAWN'] = df['HOUR'].isin([4, 5, 6]).astype(int)
        
        # ÏûÑÍ≥ÑÍ∞í Ïã†Ìò∏
        if 'M14AM14B' in df.columns:
            df['M14B_HIGH'] = (df['M14AM14B'] > df['M14AM14B'].quantile(0.9)).astype(int)
        
        # Î™®Îì† Í∞í Ï≤¥ÌÅ¨
        for col in df.select_dtypes(include=[np.number]).columns:
            # Infinity Ï≤¥ÌÅ¨
            df[col] = df[col].replace([np.inf, -np.inf], df[col].median())
            # NaN Ï≤¥ÌÅ¨
            df[col] = df[col].fillna(df[col].median())
            
        return df
    
    def create_sequences(self, df):
        """ÏïàÏ†ÑÌïú ÏãúÌÄÄÏä§ ÏÉùÏÑ±"""
        logger.info("üîÑ ÏãúÌÄÄÏä§ ÏÉùÏÑ±...")
        
        # ÌäπÏÑ± ÏÑ†ÌÉù (Î¨∏Ï†ú Îê† Ïàò ÏûàÎäî Ïª¨Îüº Ï†úÏô∏)
        exclude_cols = ['CURRTIME', 'TIME', 'TARGET', 'IS_1700']
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        # ÏàòÏπòÌòïÎßå ÏÑ†ÌÉù
        feature_cols = [col for col in feature_cols if df[col].dtype in [np.float64, np.int64, np.float32, np.int32]]
        
        logger.info(f"   ÏÇ¨Ïö©Ìï† ÌäπÏÑ±: {len(feature_cols)}Í∞ú")
        
        # Îç∞Ïù¥ÌÑ∞ Î≥µÏÇ¨
        data = df[feature_cols].copy()
        targets = df['TARGET'].copy()
        is_1700 = df['IS_1700'].copy()
        
        # Ïä§ÏºÄÏùºÎßÅ Ï†Ñ Ï≤¥ÌÅ¨
        logger.info("   Ïä§ÏºÄÏùºÎßÅ Ï†Ñ Ï≤¥ÌÅ¨...")
        data = data.replace([np.inf, -np.inf], np.nan)
        data = data.fillna(data.median())
        
        # Ïä§ÏºÄÏùºÎßÅ
        try:
            data_scaled = self.scaler.fit_transform(data)
            targets_scaled = self.target_scaler.fit_transform(targets.values.reshape(-1, 1)).flatten()
        except Exception as e:
            logger.error(f"‚ùå Ïä§ÏºÄÏùºÎßÅ Ïò§Î•ò: {e}")
            # ÎåÄÏ≤¥ Î∞©Î≤ï: ÏàòÎèô Ï†ïÍ∑úÌôî
            data_scaled = (data - data.mean()) / (data.std() + 1e-7)
            targets_scaled = (targets - targets.mean()) / (targets.std() + 1e-7)
            data_scaled = data_scaled.fillna(0).values
            targets_scaled = targets_scaled.fillna(0).values
        
        # ÏãúÌÄÄÏä§ ÏÉùÏÑ±
        X, y, y_binary = [], [], []
        
        for i in range(len(data_scaled) - self.seq_len - self.pred_len):
            seq = data_scaled[i:i+self.seq_len]
            
            # ÏãúÌÄÄÏä§ Ï≤¥ÌÅ¨
            if not np.any(np.isnan(seq)) and not np.any(np.isinf(seq)):
                X.append(seq)
                y.append(targets_scaled[i+self.seq_len])
                y_binary.append(is_1700.iloc[i+self.seq_len])
        
        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.float32)
        y_binary = np.array(y_binary, dtype=np.float32)
        
        # ÏµúÏ¢Ö Ï≤¥ÌÅ¨
        logger.info("   ÏµúÏ¢Ö Ï≤¥ÌÅ¨...")
        logger.info(f"   X: NaN={np.isnan(X).sum()}, Inf={np.isinf(X).sum()}")
        logger.info(f"   y: NaN={np.isnan(y).sum()}, Inf={np.isinf(y).sum()}")
        
        logger.info(f"‚úÖ ÏãúÌÄÄÏä§ ÏÉùÏÑ± ÏôÑÎ£å: {X.shape}")
        
        return X, y, y_binary, feature_cols

# ========================================
# ÏïàÏ†ïÏ†ÅÏù∏ Î™®Îç∏
# ========================================

class StableModels:
    """Î™®Îì† Î™®Îç∏ Íµ¨ÌòÑ - 15Í∞ú"""
    
    def __init__(self, input_shape):
        self.input_shape = input_shape
        self.seq_len = input_shape[0]
        self.n_features = input_shape[1]
    
    # 1. PatchTST - Transformer ÏµúÍ∞ï
    def build_patch_tst(self, patch_len=10, d_model=128, n_heads=4):
        """Patch Time Series Transformer - SOTA"""
        inputs = Input(shape=self.input_shape)
        
        # Normalize
        x = BatchNormalization()(inputs)
        
        # Ìå®Ïπò ÏÉùÏÑ± (100 -> 10 patches) - Reshape Î†àÏù¥Ïñ¥ ÏÇ¨Ïö©
        n_patches = self.seq_len // patch_len
        patch_size = patch_len * self.n_features
        
        # ReshapeÎ•º Keras Î†àÏù¥Ïñ¥Î°ú Ï≤òÎ¶¨
        x = Reshape((n_patches, patch_size))(x)
        
        # Ìå®Ïπò ÏûÑÎ≤†Îî©
        x = Dense(d_model)(x)
        x = LayerNormalization()(x)
        
        # Multi-Head Attention (2 layers)
        for _ in range(2):
            attn = MultiHeadAttention(
                num_heads=n_heads, 
                key_dim=d_model//n_heads,
                dropout=0.1
            )(x, x)
            x = Add()([x, attn])
            x = LayerNormalization()(x)
            
            # FFN
            ff = Dense(d_model * 2, activation='relu')(x)
            ff = Dropout(0.1)(ff)
            ff = Dense(d_model)(ff)
            x = Add()([x, ff])
            x = LayerNormalization()(x)
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.2)(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='PatchTST')
    
    # 2. Extreme Value Network - 1700+ ÌÇ¨Îü¨
    def build_extreme_value_network(self):
        """Í∑πÎã®Í∞í Ïù¥Î°† Í∏∞Î∞ò - 1700+ ÌäπÌôî"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # LSTM backbone
        x = LSTM(128, return_sequences=True)(x)
        x = Dropout(0.2)(x)
        x = LSTM(64)(x)
        x = Dropout(0.2)(x)
        
        # Í∑πÎã®Í∞í Î∏åÎûúÏπò
        extreme = Dense(32, activation='relu')(x)
        extreme = Dense(16, activation='relu')(extreme)
        xi = Dense(1, activation='tanh')(extreme)  # shape parameter
        sigma = Dense(1, activation='softplus')(extreme)  # scale parameter
        
        # ÏùºÎ∞ò Î∏åÎûúÏπò
        normal = Dense(32, activation='relu')(x)
        normal_pred = Dense(1, activation='linear')(normal)
        
        # GPD Í∏∞Î∞ò Î≥¥Ï†ï
        extreme_adjustment = Multiply()([sigma, xi])
        outputs = Add()([normal_pred, extreme_adjustment])
        outputs = Activation('sigmoid')(outputs)
        
        return Model(inputs, outputs, name='ExtremeValueNet')
    
    # 3. Enhanced Spike Detector - Í∏âÎ≥Ä Í∞êÏßÄ
    def build_spike_detector(self):
        """Í∏âÎ≥Ä Í∞êÏßÄ ÌäπÌôî"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # CNN for pattern
        x = Conv1D(64, 3, padding='same', activation='relu')(x)
        x = BatchNormalization()(x)
        x = Conv1D(64, 3, padding='same', activation='relu')(x)
        x = MaxPooling1D(2)(x)
        
        # Bidirectional LSTM
        x = Bidirectional(LSTM(64, return_sequences=True))(x)
        x = Bidirectional(LSTM(32))(x)
        
        # Detection heads
        spike_prob = Dense(32, activation='relu')(x)
        spike_prob = Dense(1, activation='sigmoid')(spike_prob)
        
        spike_mag = Dense(32, activation='relu')(x)
        spike_mag = Dense(1, activation='linear')(spike_mag)
        
        outputs = Multiply()([spike_prob, spike_mag])
        outputs = Activation('sigmoid')(outputs)
        
        return Model(inputs, outputs, name='SpikeDetector')
    
    # 4. Mamba (State Space Model)
    def build_mamba_ssm(self):
        """State Space Model - Ìö®Ïú®Ï†Å"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # SSM layers (GRUÎ°ú Í∑ºÏÇ¨)
        x = GRU(128, return_sequences=True)(x)
        x = GRU(64, return_sequences=True)(x)
        x = GRU(32)(x)
        
        x = Dense(32, activation='relu')(x)
        x = Dropout(0.2)(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='MambaSSM')
    
    # 5. MTGNN (Graph Neural Network)
    def build_mtgnn(self):
        """Multivariate Time Graph NN - Ïª¨Îüº Í¥ÄÍ≥Ñ"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # Graph construction
        node_emb = Dense(64, activation='relu')(x)
        
        # Temporal Conv
        x = Conv1D(64, 3, padding='causal', activation='relu')(x)
        x = Conv1D(64, 3, padding='causal', activation='relu')(x)
        
        # Graph Conv (simplified)
        x = Dense(128, activation='relu')(x)
        x = Dense(64, activation='relu')(x)
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='MTGNN')
    
    # 6. Neural SDE
    def build_neural_sde(self):
        """ÌôïÎ•†Ï†Å ÎØ∏Î∂ÑÎ∞©Ï†ïÏãù"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # Drift (mu)
        drift = LSTM(64, return_sequences=True)(x)
        drift = LSTM(32)(drift)
        mu = Dense(16, activation='linear')(drift)
        
        # Diffusion (sigma)  
        diffusion = LSTM(64, return_sequences=True)(x)
        diffusion = LSTM(32)(diffusion)
        sigma = Dense(16, activation='softplus')(diffusion)
        
        # Stochastic integration - Lambda Î†àÏù¥Ïñ¥ ÏÇ¨Ïö©
        x = Add()([mu, Lambda(lambda s: s * np.random.normal(0, 0.1, (1,)))(sigma)])
        
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='NeuralSDE')
    
    # 7. Mixture of Experts
    def build_mixture_of_experts(self):
        """Ï†ÑÎ¨∏Í∞Ä ÏïôÏÉÅÎ∏î"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # Shared
        shared = LSTM(64, return_sequences=True)(x)
        shared = LSTM(32)(shared)
        
        # 3 Experts
        e1 = Dense(16, activation='relu')(shared)
        e1_out = Dense(1, activation='sigmoid')(e1)
        
        e2 = Dense(16, activation='relu')(shared)
        e2_out = Dense(1, activation='sigmoid')(e2)
        
        e3 = Dense(16, activation='relu')(shared)
        e3_out = Dense(1, activation='sigmoid')(e3)
        
        # Router
        gate = Dense(16, activation='relu')(shared)
        gate = Dense(3, activation='softmax')(gate)
        
        # Weighted sum - Keras Ïó∞ÏÇ∞ÏúºÎ°ú Ï≤òÎ¶¨
        e1_weighted = Multiply()([e1_out, Lambda(lambda g: g[:, 0:1])(gate)])
        e2_weighted = Multiply()([e2_out, Lambda(lambda g: g[:, 1:2])(gate)])
        e3_weighted = Multiply()([e3_out, Lambda(lambda g: g[:, 2:3])(gate)])
        
        outputs = Add()([e1_weighted, e2_weighted, e3_weighted])
        
        return Model(inputs, outputs, name='MixtureOfExperts')
    
    # 8. Diffusion Model
    def build_diffusion_model(self):
        """Denoising Diffusion"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # Encoder
        x = Conv1D(64, 3, padding='same', activation='relu')(x)
        x = Conv1D(128, 3, padding='same', activation='relu')(x)
        x = MaxPooling1D(2)(x)
        
        # Noising - Lambda Î†àÏù¥Ïñ¥ ÏÇ¨Ïö©
        x = LSTM(64, return_sequences=True)(x)
        x = Lambda(lambda t: t + np.random.normal(0, 0.05, (1,1,1)))(x)
        
        # Denoiser
        x = LSTM(64)(x)
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='DiffusionModel')
    
    # 9. Autoformer
    def build_autoformer(self):
        """Auto-Correlation Transformer"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # Decomposition
        trend = AveragePooling1D(10, strides=1, padding='same')(x)
        seasonal = x - trend
        
        # Process
        trend = Conv1D(64, 1, activation='relu')(trend)
        seasonal = Conv1D(64, 1, activation='relu')(seasonal)
        
        # Auto-Correlation
        x = Concatenate()([trend, seasonal])
        x = MultiHeadAttention(num_heads=4, key_dim=16)(x, x)
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='Autoformer')
    
    # 10. TCN (Temporal CNN)
    def build_tcn(self):
        """Temporal Convolutional Network"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # Dilated convolutions
        for dilation in [1, 2, 4, 8]:
            conv = Conv1D(64, 3, dilation_rate=dilation, 
                         padding='causal', activation='relu')(x)
            conv = Dropout(0.1)(conv)
            if x.shape[-1] != conv.shape[-1]:
                x = Conv1D(64, 1)(x)
            x = Add()([x, conv])
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='TCN')
    
    # 11. WaveNet
    def build_wavenet(self):
        """WaveNet Architecture"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        skip_connections = []
        
        for i in range(4):
            dilation = 2 ** i
            
            # Gated conv
            tanh_out = Conv1D(64, 2, dilation_rate=dilation,
                             padding='causal', activation='tanh')(x)
            sigm_out = Conv1D(64, 2, dilation_rate=dilation,
                             padding='causal', activation='sigmoid')(x)
            
            x = Multiply()([tanh_out, sigm_out])
            
            skip = Conv1D(64, 1)(x)
            skip_connections.append(skip)
            
            x = Conv1D(self.n_features, 1)(x)
            x = Add()([x, inputs])
        
        x = Add()(skip_connections)
        x = Activation('relu')(x)
        x = GlobalAveragePooling1D()(x)
        
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='WaveNet')
    
    # 12. Bi-LSTM with Attention
    def build_bilstm_attention(self):
        """ÏñëÎ∞©Ìñ• LSTM + Attention"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # Bi-LSTM
        x = Bidirectional(LSTM(128, return_sequences=True))(x)
        x = Bidirectional(LSTM(64, return_sequences=True))(x)
        
        # Attention
        attention = Dense(1, activation='tanh')(x)
        attention = Flatten()(attention)
        attention = Activation('softmax')(attention)
        attention = RepeatVector(128)(attention)
        attention = Permute([2, 1])(attention)
        
        x = Multiply()([x, attention])
        x = GlobalAveragePooling1D()(x)
        
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.2)(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='BiLSTM_Attention')
    
    # 13. Informer
    def build_informer(self):
        """Informer - Ìö®Ïú®Ï†Å Transformer"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # ProbSparse Attention (simplified)
        x = Dense(128)(x)
        x = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
        x = LayerNormalization()(x)
        
        x = Dense(64, activation='relu')(x)
        x = GlobalAveragePooling1D()(x)
        
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='Informer')
    
    # 14. TimesNet
    def build_timesnet(self):
        """TimesNet - 2D Transform"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        # 1D to 2D transform - Keras Î†àÏù¥Ïñ¥ ÏÇ¨Ïö©
        x = Lambda(lambda t: tf.expand_dims(t, axis=-1))(x)
        x = Conv2D(64, (3, 1), padding='same', activation='relu')(x)
        x = Conv2D(32, (3, 1), padding='same', activation='relu')(x)
        x = Lambda(lambda t: tf.squeeze(t, axis=-1))(x)
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(32, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='TimesNet')
    
    # 15. Baseline Strong LSTM
    def build_baseline_lstm(self):
        """Í∞ïÎ†•Ìïú LSTM Î≤†Ïù¥Ïä§ÎùºÏù∏"""
        inputs = Input(shape=self.input_shape)
        
        x = BatchNormalization()(inputs)
        
        x = LSTM(256, return_sequences=True)(x)
        x = Dropout(0.3)(x)
        x = LSTM(128, return_sequences=True)(x)
        x = Dropout(0.3)(x)
        x = LSTM(64)(x)
        x = Dropout(0.2)(x)
        
        x = Dense(32, activation='relu')(x)
        x = Dropout(0.1)(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        return Model(inputs, outputs, name='BaselineLSTM')

# ========================================
# ÌèâÍ∞Ä ÏãúÏä§ÌÖú
# ========================================

class ComprehensiveEvaluator:
    """Ï†ÑÏ≤¥ Íµ¨Í∞Ñ ÌèâÍ∞Ä"""
    
    def __init__(self, target_scaler):
        self.target_scaler = target_scaler
        self.results = {}
    
    def evaluate(self, model, X_test, y_test, y_binary, model_name):
        """Ï¢ÖÌï© ÌèâÍ∞Ä"""
        
        # ÏòàÏ∏°
        y_pred = model.predict(X_test, batch_size=256, verbose=0).flatten()
        
        # Clip predictions to valid range
        y_pred = np.clip(y_pred, 0, 1)
        
        # Ïó≠Î≥ÄÌôò
        y_test_real = self.target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
        y_pred_real = self.target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()
        
        # ===== Ï†ÑÏ≤¥ ÏÑ±Îä• =====
        mae_total = np.mean(np.abs(y_test_real - y_pred_real))
        rmse_total = np.sqrt(np.mean((y_test_real - y_pred_real) ** 2))
        
        # ===== Íµ¨Í∞ÑÎ≥Ñ Î∂ÑÏÑù =====
        metrics = {
            'total': {'mae': mae_total, 'rmse': rmse_total}
        }
        
        # Íµ¨Í∞Ñ Ï†ïÏùò
        ranges = [
            ('low', 0, 1200),
            ('normal', 1200, 1400),
            ('medium', 1400, 1600),
            ('high', 1600, 1700),
            ('extreme', 1700, 9999)
        ]
        
        for name, low, high in ranges:
            mask = (y_test_real >= low) & (y_test_real < high)
            if mask.sum() > 0:
                mae = np.mean(np.abs(y_test_real[mask] - y_pred_real[mask]))
                rmse = np.sqrt(np.mean((y_test_real[mask] - y_pred_real[mask]) ** 2))
                count = mask.sum()
                metrics[name] = {
                    'mae': mae,
                    'rmse': rmse,
                    'count': count,
                    'percentage': (count / len(y_test_real)) * 100
                }
        
        # ===== 1700+ ÌäπÎ≥Ñ Î∂ÑÏÑù =====
        actual_1700 = y_test_real >= 1700
        pred_1700 = y_pred_real >= 1700
        
        tp = np.sum(pred_1700 & actual_1700)
        fp = np.sum(pred_1700 & ~actual_1700)
        fn = np.sum(~pred_1700 & actual_1700)
        tn = np.sum(~pred_1700 & ~actual_1700)
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        # ===== Ï†ïÏÉÅ Íµ¨Í∞Ñ Ïò§ÌÉê =====
        normal_mask = (y_test_real >= 1200) & (y_test_real < 1400)
        if normal_mask.sum() > 0:
            false_alarm = np.sum(y_pred_real[normal_mask] >= 1700)
            false_alarm_rate = false_alarm / normal_mask.sum()
        else:
            false_alarm_rate = 0
        
        # Í≤∞Í≥º Ï†ÄÏû•
        result = {
            'metrics': metrics,
            'precision_1700': precision,
            'recall_1700': recall,
            'f1_1700': f1,
            'false_alarm_rate': false_alarm_rate,
            'confusion_matrix': {'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn}
        }
        
        self.results[model_name] = result
        
        # Ï∂úÎ†•
        self._print_results(model_name, result)
        
        return result
    
    def _print_results(self, model_name, result):
        """Í≤∞Í≥º Ï∂úÎ†•"""
        print(f"\n{'='*60}")
        print(f"üìä {model_name} ÌèâÍ∞Ä Í≤∞Í≥º")
        print(f"{'='*60}")
        
        print("\n[Ï†ÑÏ≤¥ ÏÑ±Îä•]")
        print(f"  MAE: {result['metrics']['total']['mae']:.2f}")
        print(f"  RMSE: {result['metrics']['total']['rmse']:.2f}")
        
        print("\n[Íµ¨Í∞ÑÎ≥Ñ ÏÑ±Îä•]")
        for range_name in ['low', 'normal', 'medium', 'high', 'extreme']:
            if range_name in result['metrics']:
                m = result['metrics'][range_name]
                print(f"  {range_name:8s}: MAE={m['mae']:6.2f}, "
                      f"Count={m['count']:4d} ({m['percentage']:5.2f}%)")
        
        print("\n[1700+ ÏòàÏ∏° ÏÑ±Îä•]")
        print(f"  Precision: {result['precision_1700']:.2%}")
        print(f"  Recall: {result['recall_1700']:.2%}")
        print(f"  F1 Score: {result['f1_1700']:.2%}")
        
        print(f"\n[Ï†ïÏÉÅ‚Üí1700+ Ïò§ÌÉêÎ•†]")
        print(f"  False Alarm Rate: {result['false_alarm_rate']:.2%}")
        
        # Ï¢ÖÌï© Ï†êÏàò
        balanced_score = (
            result['f1_1700'] * 0.4 +  # 1700+ ÏòàÏ∏°
            (1 - result['false_alarm_rate']) * 0.3 +  # Ïò§ÌÉê Î∞©ÏßÄ
            (1 - result['metrics']['normal']['mae']/100) * 0.3  # Ï†ïÏÉÅ Íµ¨Í∞Ñ Ï†ïÌôïÎèÑ
        )
        print(f"\n[Ï¢ÖÌï© Ï†êÏàò]")
        print(f"  Balanced Score: {balanced_score:.2%}")

# ========================================
# Î©îÏù∏ Ïã§Ìñâ
# ========================================

def main():
    """Î©îÏù∏ Ìï®Ïàò"""
    
    print("\nüöÄ ÏãúÏûë...")
    
    # 1. Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
    processor = SafeDataProcessor(seq_len=100, pred_len=10)
    
    # Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú ÌôïÏù∏
    data_paths = [
        'data/20240201_TO_202507281705.csv',
        '/mnt/user-data/uploads/gs.CSV',
        '/mnt/user-data/uploads/20250807_DATA.CSV'
    ]
    
    data_path = None
    for path in data_paths:
        if os.path.exists(path):
            data_path = path
            break
    
    if data_path is None:
        logger.error("‚ùå Îç∞Ïù¥ÌÑ∞ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§!")
        return
    
    df = processor.load_and_process(data_path)
    X, y, y_binary, feature_cols = processor.create_sequences(df)
    
    # 2. Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†
    train_size = int(0.7 * len(X))
    val_size = int(0.15 * len(X))
    
    X_train, y_train = X[:train_size], y[:train_size]
    X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]
    X_test = X[train_size+val_size:]
    y_test = y[train_size+val_size:]
    y_test_binary = y_binary[train_size+val_size:]
    
    print(f"\nüìä Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†:")
    print(f"  Train: {X_train.shape}")
    print(f"  Val: {X_val.shape}")
    print(f"  Test: {X_test.shape}")
    
    # 3. Î™®Îç∏ ÏÉùÏÑ±
    model_builder = StableModels(input_shape=(100, X.shape[2]))
    
    # Î™®Îì† 15Í∞ú Î™®Îç∏!
    models = {
        '1_PatchTST': model_builder.build_patch_tst(),
        '2_ExtremeValueNet': model_builder.build_extreme_value_network(),
        '3_SpikeDetector': model_builder.build_spike_detector(),
        '4_MambaSSM': model_builder.build_mamba_ssm(),
        '5_MTGNN': model_builder.build_mtgnn(),
        '6_NeuralSDE': model_builder.build_neural_sde(),
        '7_MixtureOfExperts': model_builder.build_mixture_of_experts(),
        '8_DiffusionModel': model_builder.build_diffusion_model(),
        '9_Autoformer': model_builder.build_autoformer(),
        '10_TCN': model_builder.build_tcn(),
        '11_WaveNet': model_builder.build_wavenet(),
        '12_BiLSTM_Attention': model_builder.build_bilstm_attention(),
        '13_Informer': model_builder.build_informer(),
        '14_TimesNet': model_builder.build_timesnet(),
        '15_BaselineLSTM': model_builder.build_baseline_lstm()
    }
    
    print(f"\n‚úÖ {len(models)}Í∞ú Î™®Îç∏ ÏÉùÏÑ± ÏôÑÎ£å!")
    print("="*60)
    for name in models.keys():
        print(f"  ‚úì {name}")
    print("="*60)
    
    # 4. ÌèâÍ∞ÄÏûê ÏÉùÏÑ±
    evaluator = ComprehensiveEvaluator(processor.target_scaler)
    
    # 5. Í∞Å Î™®Îç∏ ÌïôÏäµ Î∞è ÌèâÍ∞Ä
    for name, model in models.items():
        print(f"\n{'='*60}")
        print(f"üéØ {name} ÌïôÏäµ")
        print(f"{'='*60}")
        
        # Ïª¥ÌååÏùº
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',  # ÏïàÏ†ïÏ†ÅÏù∏ MSE ÏÇ¨Ïö©
            metrics=['mae']
        )
        
        # ÌïôÏäµ
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=20,
            batch_size=256,
            callbacks=[
                EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)
            ],
            verbose=1
        )
        
        # ÌèâÍ∞Ä
        evaluator.evaluate(model, X_test, y_test, y_test_binary, name)
    
    # 6. ÏµúÏ¢Ö ÏàúÏúÑ
    print("\n" + "="*80)
    print("üèÜ ÏµúÏ¢Ö ÏàúÏúÑ")
    print("="*80)
    
    # F1 Í∏∞Ï§Ä Ï†ïÎ†¨
    sorted_models = sorted(
        evaluator.results.items(),
        key=lambda x: x[1]['f1_1700'],
        reverse=True
    )
    
    for i, (name, result) in enumerate(sorted_models, 1):
        print(f"{i}. {name:15s} | F1: {result['f1_1700']:.2%} | "
              f"Ï†ïÏÉÅMAE: {result['metrics']['normal']['mae']:.2f} | "
              f"Ïò§ÌÉêÎ•†: {result['false_alarm_rate']:.2%}")
    
    print("\n‚úÖ ÏôÑÎ£å!")
    
    return evaluator.results

if __name__ == "__main__":
    results = main()