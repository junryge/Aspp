"""
ê¸‰ì¦ ì˜ˆì¸¡ í•˜ì´ë¸Œë¦¬ë“œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ v3.0
=======================================================
ì´ì¤‘ ì¶œë ¥ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸(LSTM, RNN, GRU, Bi-LSTM)ì˜ 
ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

ì£¼ìš” í‰ê°€ ì§€í‘œ:
1. ê¸‰ì¦ ì˜ˆì¸¡ ì •í™•ë„ (TOTALCNT > 1400)
2. Precision, Recall, F1-Score
3. ìˆ˜ì¹˜ ì˜ˆì¸¡ ì •í™•ë„ (MAPE, MAE)
4. ê¸‰ì¦ êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
5. ì˜¤íƒ/ë¯¸íƒ ë¶„ì„

ê°œë°œì¼: 2024ë…„
ë²„ì „: 3.0 (ê¸‰ì¦ ì˜ˆì¸¡ íŠ¹í™”)
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import load_model
from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score, 
                           classification_report, confusion_matrix, roc_auc_score, roc_curve)
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
import os
import platform
from datetime import datetime, timedelta
import joblib
import json
import warnings
import traceback

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')

# ===================================
# í•œê¸€ í°íŠ¸ ì„¤ì •
# ===================================
def set_korean_font():
    """ìš´ì˜ì²´ì œë³„ í•œê¸€ í°íŠ¸ ìë™ ì„¤ì •"""
    system = platform.system()
    
    if system == 'Windows':
        font_paths = [
            'C:/Windows/Fonts/malgun.ttf',
            'C:/Windows/Fonts/ngulim.ttf',
            'C:/Windows/Fonts/NanumGothic.ttf'
        ]
        font_family = 'Malgun Gothic'
    elif system == 'Darwin':
        font_paths = [
            '/System/Library/Fonts/Supplemental/AppleGothic.ttf',
            '/Library/Fonts/NanumGothic.ttf'
        ]
        font_family = 'AppleGothic'
    else:
        font_paths = [
            '/usr/share/fonts/truetype/nanum/NanumGothic.ttf',
            '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf'
        ]
        font_family = 'NanumGothic'
    
    font_set = False
    for font_path in font_paths:
        if os.path.exists(font_path):
            try:
                font_prop = fm.FontProperties(fname=font_path)
                plt.rcParams['font.family'] = font_prop.get_name()
                font_set = True
                print(f"âœ“ í•œê¸€ í°íŠ¸ ì„¤ì •: {font_prop.get_name()}")
                break
            except:
                continue
    
    if not font_set:
        try:
            plt.rcParams['font.family'] = font_family
            print(f"âœ“ í•œê¸€ í°íŠ¸ ì„¤ì •: {font_family}")
        except:
            print("âš  í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ë¬¸ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")
            return False
    
    plt.rcParams['axes.unicode_minus'] = False
    return True

# í•œê¸€ í°íŠ¸ ì„¤ì • ì‹¤í–‰
USE_KOREAN = set_korean_font()

# CPU ëª¨ë“œ ì„¤ì •
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
tf.config.set_visible_devices([], 'GPU')

# ëœë¤ ì‹œë“œ ê³ ì •
RANDOM_SEED = 2079936
tf.random.set_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

class SpikeModelEvaluator:
    """ê¸‰ì¦ ì˜ˆì¸¡ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.models = {}
        self.scaler = None
        self.config = None
        self.spike_threshold = 1400  # ê¸‰ì¦ ì„ê³„ê°’
        
    def load_models_and_config(self):
        """í•™ìŠµëœ ëª¨ë¸ê³¼ ì„¤ì • ë¡œë“œ"""
        print("="*70)
        print("í•™ìŠµëœ ê¸‰ì¦ ì˜ˆì¸¡ ëª¨ë¸ ë¡œë”© ì¤‘..." if USE_KOREAN else "Loading trained spike prediction models...")
        print("="*70)
        
        # v3 ì´ì¤‘ ì¶œë ¥ ëª¨ë¸ë“¤ ë¡œë“œ
        model_names = ['dual_lstm', 'dual_gru', 'dual_rnn', 'dual_bilstm']
        for model_name in model_names:
            try:
                model_path = f'model_v3/{model_name}_final.keras'
                if os.path.exists(model_path):
                    self.models[model_name] = load_model(model_path, compile=False)
                    print(f"âœ“ {model_name.upper()} {'ëª¨ë¸ ë¡œë“œ ì™„ë£Œ' if USE_KOREAN else 'model loaded'}")
                else:
                    # ê¸°ì¡´ ëª¨ë¸ ê²½ë¡œ ì‹œë„
                    alt_path = f'model/{model_name.replace("dual_", "")}_final_hybrid.keras'
                    if os.path.exists(alt_path):
                        print(f"âš  {model_name} v3 ëª¨ë¸ì´ ì—†ì–´ ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©")
            except Exception as e:
                print(f"âš  {model_name.upper()} {'ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨' if USE_KOREAN else 'model load failed'}: {str(e)}")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        try:
            scaler_paths = [
                'scaler_v3/scaler_v3.pkl',
                'scaler/standard_scaler_hybrid.pkl',
                'scaler/StdScaler_s30f10_0731_2079936.save'
            ]
            for scaler_path in scaler_paths:
                if os.path.exists(scaler_path):
                    self.scaler = joblib.load(scaler_path)
                    print(f"âœ“ {'ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ' if USE_KOREAN else 'Scaler loaded'}")
                    break
        except Exception as e:
            print(f"âš  {'ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨' if USE_KOREAN else 'Scaler load failed'}: {str(e)}")
        
        # ì„¤ì • ë¡œë“œ
        try:
            config_paths = [
                'results_v3/training_config.json',
                'results/training_config.json'
            ]
            for config_path in config_paths:
                if os.path.exists(config_path):
                    with open(config_path, 'r') as f:
                        self.config = json.load(f)
                    break
            
            if not self.config:
                self.config = {
                    'seq_length': 30,
                    'future_minutes': 10,
                    'spike_threshold': 1400
                }
            print(f"âœ“ {'ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ' if USE_KOREAN else 'Config file loaded'}")
        except:
            self.config = {'seq_length': 30, 'future_minutes': 10, 'spike_threshold': 1400}
    
    def calculate_spike_metrics(self, y_true, y_pred_prob, threshold=0.5):
        """ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        # ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜
        y_pred = (y_pred_prob > threshold).astype(int)
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # ê¸°ë³¸ ì§€í‘œ
        accuracy = (tp + tn) / (tp + tn + fp + fn) * 100 if (tp + tn + fp + fn) > 0 else 0
        precision = tp / (tp + fp) * 100 if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        # ì¶”ê°€ ì§€í‘œ
        specificity = tn / (tn + fp) * 100 if (tn + fp) > 0 else 0
        
        # AUC ê³„ì‚°
        try:
            auc = roc_auc_score(y_true, y_pred_prob) * 100
        except:
            auc = 0
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'specificity': specificity,
            'auc': auc,
            'confusion_matrix': cm,
            'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn
        }
    
    def comprehensive_spike_evaluation(self, y_true_reg, y_pred_reg, y_true_cls, y_pred_cls, model_name="Model"):
        """ê¸‰ì¦ ì˜ˆì¸¡ ì¤‘ì‹¬ì˜ ì¢…í•© í‰ê°€"""
        print(f"\n{'='*70}")
        print(f"{model_name} {'ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€' if USE_KOREAN else 'Spike Prediction Performance'}")
        print(f"{'='*70}")
        
        # 1. ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥
        spike_metrics = self.calculate_spike_metrics(y_true_cls, y_pred_cls)
        
        # 2. ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥
        thresholds = [0.3, 0.5, 0.7]
        threshold_metrics = {}
        for thr in thresholds:
            threshold_metrics[thr] = self.calculate_spike_metrics(y_true_cls, y_pred_cls, thr)
        
        # 3. ìˆ˜ì¹˜ ì˜ˆì¸¡ ì„±ëŠ¥ (ì›ë³¸ ìŠ¤ì¼€ì¼)
        y_true_original = self.inverse_scale(y_true_reg)
        y_pred_original = self.inverse_scale(y_pred_reg)
        
        mae = mean_absolute_error(y_true_original, y_pred_original)
        mape = np.mean(np.abs((y_true_original - y_pred_original) / (y_true_original + 1e-8))) * 100
        rmse = np.sqrt(mean_squared_error(y_true_original, y_pred_original))
        
        # 4. ê¸‰ì¦ êµ¬ê°„ì—ì„œì˜ ì˜ˆì¸¡ ì„±ëŠ¥
        spike_mask = y_true_cls == 1
        if np.any(spike_mask):
            spike_mae = mean_absolute_error(y_true_original[spike_mask], y_pred_original[spike_mask])
            spike_mape = np.mean(np.abs((y_true_original[spike_mask] - y_pred_original[spike_mask]) / 
                                       (y_true_original[spike_mask] + 1e-8))) * 100
        else:
            spike_mae = 0
            spike_mape = 0
        
        # 5. ì¢…í•© ì ìˆ˜ (ê¸‰ì¦ ì˜ˆì¸¡ ì¤‘ì‹¬)
        overall_score = (
            spike_metrics['recall'] * 0.35 +          # ì¬í˜„ìœ¨ ìµœìš°ì„ 
            spike_metrics['precision'] * 0.25 +       # ì •ë°€ë„
            spike_metrics['f1_score'] * 0.2 +         # F1 ì ìˆ˜
            (100 - min(spike_mape, 100)) * 0.1 +     # ê¸‰ì¦ êµ¬ê°„ ì˜ˆì¸¡ ì •í™•ë„
            spike_metrics['auc'] * 0.1                # AUC
        )
        
        # ê²°ê³¼ ì¶œë ¥
        if USE_KOREAN:
            print("\nğŸ¯ ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥ (ì„ê³„ê°’ 0.5):")
            print(f"  â€¢ ì •í™•ë„: {spike_metrics['accuracy']:.1f}%")
            print(f"  â€¢ ì •ë°€ë„: {spike_metrics['precision']:.1f}%")
            print(f"  â€¢ ì¬í˜„ìœ¨: {spike_metrics['recall']:.1f}% â­")
            print(f"  â€¢ F1-Score: {spike_metrics['f1_score']:.1f}%")
            print(f"  â€¢ íŠ¹ì´ë„: {spike_metrics['specificity']:.1f}%")
            print(f"  â€¢ AUC: {spike_metrics['auc']:.1f}%")
            
            print("\nğŸ“Š ì„ê³„ê°’ë³„ ì¬í˜„ìœ¨:")
            for thr, metrics in threshold_metrics.items():
                print(f"  â€¢ ì„ê³„ê°’ {thr}: {metrics['recall']:.1f}%")
            
            print("\nğŸ’¹ ê¸‰ì¦ êµ¬ê°„ ì˜ˆì¸¡ ì •í™•ë„:")
            print(f"  â€¢ ê¸‰ì¦ êµ¬ê°„ MAE: {spike_mae:.0f}")
            print(f"  â€¢ ê¸‰ì¦ êµ¬ê°„ MAPE: {spike_mape:.1f}%")
            
            print("\nğŸ”¢ ì „ì²´ ìˆ˜ì¹˜ ì˜ˆì¸¡ ì„±ëŠ¥:")
            print(f"  â€¢ MAE: {mae:.0f}")
            print(f"  â€¢ MAPE: {mape:.1f}%")
            print(f"  â€¢ RMSE: {rmse:.0f}")
            
            print("\nâš ï¸ ì˜¤íƒ/ë¯¸íƒ ë¶„ì„:")
            print(f"  â€¢ ë¯¸íƒ (False Negative): {spike_metrics['fn']}ê±´")
            print(f"  â€¢ ì˜¤íƒ (False Positive): {spike_metrics['fp']}ê±´")
            
            print("\nâ­ ì¢…í•© ì ìˆ˜: {:.1f}%".format(overall_score))
        else:
            print("\nğŸ¯ Spike Prediction Performance (threshold 0.5):")
            print(f"  â€¢ Accuracy: {spike_metrics['accuracy']:.1f}%")
            print(f"  â€¢ Precision: {spike_metrics['precision']:.1f}%")
            print(f"  â€¢ Recall: {spike_metrics['recall']:.1f}% â­")
            print(f"  â€¢ F1-Score: {spike_metrics['f1_score']:.1f}%")
            print(f"  â€¢ Specificity: {spike_metrics['specificity']:.1f}%")
            print(f"  â€¢ AUC: {spike_metrics['auc']:.1f}%")
            
            print("\nâ­ Overall Score: {:.1f}%".format(overall_score))
        
        # ì„±ëŠ¥ ë“±ê¸‰
        if spike_metrics['recall'] >= 70:
            grade = "A (ëª©í‘œ ë‹¬ì„±)" if USE_KOREAN else "A (Target Achieved)"
        elif spike_metrics['recall'] >= 60:
            grade = "B (ì–‘í˜¸)" if USE_KOREAN else "B (Good)"
        elif spike_metrics['recall'] >= 50:
            grade = "C (ë³´í†µ)" if USE_KOREAN else "C (Average)"
        else:
            grade = "D (ê°œì„  í•„ìš”)" if USE_KOREAN else "D (Needs Improvement)"
        
        print(f"ğŸ“Š {'ê¸‰ì¦ ì˜ˆì¸¡ ë“±ê¸‰' if USE_KOREAN else 'Spike Prediction Grade'}: {grade}")
        
        return {
            'spike_metrics': spike_metrics,
            'threshold_metrics': threshold_metrics,
            'mae': mae,
            'mape': mape,
            'rmse': rmse,
            'spike_mae': spike_mae,
            'spike_mape': spike_mape,
            'overall_score': overall_score,
            'grade': grade
        }
    
    def plot_spike_evaluation_results(self, evaluation_results):
        """ê¸‰ì¦ ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""
        fig = plt.figure(figsize=(20, 15))
        
        # í•œê¸€/ì˜ë¬¸ ë¼ë²¨
        if USE_KOREAN:
            labels = {
                'recall_comparison': 'ëª¨ë¸ë³„ ê¸‰ì¦ ì˜ˆì¸¡ ì¬í˜„ìœ¨',
                'confusion_matrix': 'í˜¼ë™ í–‰ë ¬',
                'threshold_analysis': 'ì„ê³„ê°’ë³„ ì„±ëŠ¥ ë¶„ì„',
                'spike_timeline': 'ê¸‰ì¦ ì˜ˆì¸¡ íƒ€ì„ë¼ì¸',
                'feature_importance': 'êµ¬ê°„ë³„ ê¸‰ì¦ ê¸°ì—¬ë„',
                'overall_performance': 'ì¢…í•© ì„±ëŠ¥ ë¹„êµ'
            }
        else:
            labels = {
                'recall_comparison': 'Spike Prediction Recall by Model',
                'confusion_matrix': 'Confusion Matrix',
                'threshold_analysis': 'Performance by Threshold',
                'spike_timeline': 'Spike Prediction Timeline',
                'feature_importance': 'Segment Contribution to Spikes',
                'overall_performance': 'Overall Performance Comparison'
            }
        
        # 1. ëª¨ë¸ë³„ ì¬í˜„ìœ¨ ë¹„êµ
        ax1 = plt.subplot(3, 3, 1)
        models = list(evaluation_results.keys())
        recalls = [evaluation_results[m]['spike_metrics']['recall'] for m in models]
        precisions = [evaluation_results[m]['spike_metrics']['precision'] for m in models]
        f1_scores = [evaluation_results[m]['spike_metrics']['f1_score'] for m in models]
        
        x = np.arange(len(models))
        width = 0.25
        
        ax1.bar(x - width, recalls, width, label='Recall', color='green')
        ax1.bar(x, precisions, width, label='Precision', color='blue')
        ax1.bar(x + width, f1_scores, width, label='F1-Score', color='orange')
        
        ax1.set_title(labels['recall_comparison'], fontsize=14)
        ax1.set_xticks(x)
        ax1.set_xticklabels([m.upper() for m in models], rotation=45)
        ax1.set_ylabel('Score (%)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # ëª©í‘œì„  í‘œì‹œ
        ax1.axhline(y=70, color='red', linestyle='--', alpha=0.5, label='Target (70%)')
        
        # 2. ìµœê³  ëª¨ë¸ì˜ í˜¼ë™ í–‰ë ¬
        ax2 = plt.subplot(3, 3, 2)
        best_model = max(evaluation_results.items(), 
                        key=lambda x: x[1]['spike_metrics']['recall'])
        cm = best_model[1]['spike_metrics']['confusion_matrix']
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['Normal', 'Spike'],
                   yticklabels=['Normal', 'Spike'],
                   ax=ax2)
        ax2.set_title(f"{labels['confusion_matrix']} - {best_model[0].upper()}", fontsize=14)
        
        # 3. ì„ê³„ê°’ ë¶„ì„
        ax3 = plt.subplot(3, 3, 3)
        thresholds = [0.3, 0.5, 0.7]
        for model_name, results in evaluation_results.items():
            recalls = [results['threshold_metrics'][t]['recall'] for t in thresholds]
            ax3.plot(thresholds, recalls, marker='o', label=model_name.upper())
        
        ax3.set_title(labels['threshold_analysis'], fontsize=14)
        ax3.set_xlabel('Threshold')
        ax3.set_ylabel('Recall (%)')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ëª¨ë¸ë³„ ì¢…í•© ì ìˆ˜
        ax4 = plt.subplot(3, 3, 4)
        overall_scores = [evaluation_results[m]['overall_score'] for m in models]
        bars = ax4.bar(models, overall_scores, color=['#2E86AB', '#A23B72', '#F18F01', '#C73E1D'])
        
        ax4.set_title(labels['overall_performance'], fontsize=14)
        ax4.set_ylabel('Overall Score (%)')
        ax4.set_ylim(0, 100)
        
        # ì ìˆ˜ í‘œì‹œ
        for bar, score in zip(bars, overall_scores):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{score:.1f}%', ha='center', va='bottom')
        
        # 5. ROC ê³¡ì„  (ê°€ëŠ¥í•œ ê²½ìš°)
        ax5 = plt.subplot(3, 3, 5)
        ax5.plot([0, 1], [0, 1], 'k--', alpha=0.5)
        ax5.set_xlim([0, 1])
        ax5.set_ylim([0, 1])
        ax5.set_xlabel('False Positive Rate')
        ax5.set_ylabel('True Positive Rate')
        ax5.set_title('ROC Curves', fontsize=14)
        ax5.grid(True, alpha=0.3)
        
        # 6. ê¸‰ì¦ ì˜ˆì¸¡ ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸
        ax6 = plt.subplot(3, 3, 6, projection='polar')
        
        # ìµœê³  ëª¨ë¸ì˜ ì„±ëŠ¥ ì§€í‘œ
        metrics = best_model[1]['spike_metrics']
        categories = ['Recall', 'Precision', 'F1-Score', 'Specificity', 'AUC']
        values = [
            metrics['recall'],
            metrics['precision'],
            metrics['f1_score'],
            metrics['specificity'],
            metrics['auc']
        ]
        
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        values += values[:1]
        angles += angles[:1]
        
        ax6.plot(angles, values, 'o-', linewidth=2, color='darkred')
        ax6.fill(angles, values, alpha=0.25, color='darkred')
        ax6.set_xticks(angles[:-1])
        ax6.set_xticklabels(categories)
        ax6.set_ylim(0, 100)
        ax6.set_title(f"Best Model Performance: {best_model[0].upper()}", fontsize=14)
        
        # 7. ê¸‰ì¦ êµ¬ê°„ MAE ë¹„êµ
        ax7 = plt.subplot(3, 3, 7)
        spike_maes = [evaluation_results[m]['spike_mae'] for m in models]
        bars = ax7.bar(models, spike_maes, color='coral')
        
        ax7.set_title('Spike Period MAE' if not USE_KOREAN else 'ê¸‰ì¦ êµ¬ê°„ MAE', fontsize=14)
        ax7.set_ylabel('MAE')
        
        for bar, mae in zip(bars, spike_maes):
            height = bar.get_height()
            ax7.text(bar.get_x() + bar.get_width()/2., height + 5,
                    f'{mae:.0f}', ha='center', va='bottom')
        
        # 8. ë¯¸íƒ/ì˜¤íƒ ë¶„ì„
        ax8 = plt.subplot(3, 3, 8)
        fn_counts = [evaluation_results[m]['spike_metrics']['fn'] for m in models]
        fp_counts = [evaluation_results[m]['spike_metrics']['fp'] for m in models]
        
        x = np.arange(len(models))
        width = 0.35
        
        ax8.bar(x - width/2, fn_counts, width, label='False Negative', color='red', alpha=0.7)
        ax8.bar(x + width/2, fp_counts, width, label='False Positive', color='orange', alpha=0.7)
        
        ax8.set_title('False Predictions Analysis' if not USE_KOREAN else 'ì˜¤íƒ/ë¯¸íƒ ë¶„ì„', fontsize=14)
        ax8.set_xticks(x)
        ax8.set_xticklabels([m.upper() for m in models])
        ax8.set_ylabel('Count')
        ax8.legend()
        
        # 9. ì„±ëŠ¥ ë“±ê¸‰ í‘œì‹œ
        ax9 = plt.subplot(3, 3, 9)
        ax9.axis('off')
        
        # ë“±ê¸‰ë³„ ìƒ‰ìƒ
        grade_colors = {
            'A': 'darkgreen',
            'B': 'green',
            'C': 'orange',
            'D': 'red'
        }
        
        y_pos = 0.9
        for model_name, results in evaluation_results.items():
            grade = results['grade']
            grade_letter = grade.split()[0]
            color = grade_colors.get(grade_letter, 'gray')
            
            ax9.text(0.1, y_pos, f"{model_name.upper()}:", fontsize=12, fontweight='bold')
            ax9.text(0.5, y_pos, grade, fontsize=12, color=color, fontweight='bold')
            ax9.text(0.8, y_pos, f"{results['spike_metrics']['recall']:.1f}%", fontsize=12)
            y_pos -= 0.15
        
        ax9.set_title('Model Grades' if not USE_KOREAN else 'ëª¨ë¸ ë“±ê¸‰', fontsize=14)
        ax9.text(0.1, 0.2, 'Target: Recall â‰¥ 70%' if not USE_KOREAN else 'ëª©í‘œ: ì¬í˜„ìœ¨ â‰¥ 70%', 
                fontsize=10, style='italic')
        
        plt.tight_layout()
        
        # ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        plt.savefig(f'spike_evaluation_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def prepare_test_data(self, data_path):
        """ê¸‰ì¦ ì˜ˆì¸¡ì„ ìœ„í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„"""
        try:
            # ë°ì´í„° ë¡œë“œ
            data = pd.read_csv(data_path)
            
            # ì‹œê°„ ì»¬ëŸ¼ ë³€í™˜
            data['CURRTIME'] = pd.to_datetime(data['CURRTIME'], format='%Y%m%d%H%M')
            data['TIME'] = pd.to_datetime(data['TIME'], format='%Y%m%d%H%M')
            
            # í•„ìš”í•œ ì»¬ëŸ¼ ì„ íƒ
            required_columns = ['CURRTIME', 'TOTALCNT', 'M14AM10A', 'M10AM14A', 
                              'M14AM14B', 'M14BM14A', 'M14AM16', 'M16M14A', 'TIME']
            available_columns = [col for col in required_columns if col in data.columns]
            data = data[available_columns]
            data.set_index('CURRTIME', inplace=True)
            
            # FUTURE ì»¬ëŸ¼ ìƒì„± (10ë¶„ í›„)
            data['FUTURE'] = pd.NA
            future_minutes = self.config.get('future_minutes', 10)
            
            for i in data.index:
                future_time = i + pd.Timedelta(minutes=future_minutes)
                if (future_time <= data.index.max()) & (future_time in data.index):
                    data.loc[i, 'FUTURE'] = data.loc[future_time, 'TOTALCNT']
            
            data.dropna(subset=['FUTURE'], inplace=True)
            
            # ê¸‰ì¦ ë¼ë²¨ ìƒì„±
            data['future_spike'] = (data['FUTURE'] > self.spike_threshold).astype(int)
            
            print(f"ê¸‰ì¦ ë¹„ìœ¨: {data['future_spike'].mean():.2%}")
            
            # ê°œë³„ êµ¬ê°„ íŠ¹ì§• ìƒì„±
            segment_columns = ['M14AM10A', 'M10AM14A', 'M14AM14B', 'M14BM14A', 'M14AM16', 'M16M14A']
            available_segments = [col for col in segment_columns if col in data.columns]
            
            for col in available_segments:
                # ë¹„ìœ¨
                data[f'{col}_ratio'] = data[col] / (data['TOTALCNT'] + 1e-6)
                # ë³€í™”ìœ¨
                data[f'{col}_change_10'] = data[col].pct_change(10).fillna(0)
                # ì´ë™í‰ê· 
                data[f'{col}_MA5'] = data[col].rolling(window=5, min_periods=1).mean()
            
            # ê¸‰ì¦ ì‹ í˜¸ íŠ¹ì§•
            if 'M14AM14B' in data.columns:
                data['M14AM14B_spike_signal'] = (data['M14AM14B_change_10'] > 0.5).astype(int)
            if 'M16M14A' in data.columns:
                data['M16M14A_spike_signal'] = (data['M16M14A_change_10'] > 0.5).astype(int)
            
            # ê¸°ë³¸ íŠ¹ì§•
            data['hour'] = data.index.hour
            data['dayofweek'] = data.index.dayofweek
            data['is_weekend'] = (data.index.dayofweek >= 5).astype(int)
            data['MA_5'] = data['TOTALCNT'].rolling(window=5, min_periods=1).mean()
            data['MA_10'] = data['TOTALCNT'].rolling(window=10, min_periods=1).mean()
            data['MA_30'] = data['TOTALCNT'].rolling(window=30, min_periods=1).mean()
            data['STD_5'] = data['TOTALCNT'].rolling(window=5, min_periods=1).std()
            data['STD_10'] = data['TOTALCNT'].rolling(window=10, min_periods=1).std()
            data['change_rate'] = data['TOTALCNT'].pct_change()
            data['change_rate_5'] = data['TOTALCNT'].pct_change(5)
            
            # ê²°ì¸¡ê°’ ì²˜ë¦¬
            data = data.ffill().fillna(0)
            
            # ìŠ¤ì¼€ì¼ë§
            scaling_columns = ['TOTALCNT', 'FUTURE'] + available_segments
            scaling_columns += [col for col in data.columns if 'MA' in col or 'STD' in col]
            scaling_columns += [f'{seg}_MA5' for seg in available_segments if f'{seg}_MA5' in data.columns]
            scaling_columns = list(set(scaling_columns))
            
            if hasattr(self.scaler, 'feature_names_in_'):
                expected_columns = list(self.scaler.feature_names_in_)
                scaling_columns = [col for col in expected_columns if col in data.columns]
            
            scaled_data = self.scaler.transform(data[scaling_columns])
            scaled_df = pd.DataFrame(scaled_data, columns=[f'scaled_{col}' for col in scaling_columns], 
                                   index=data.index)
            
            # ë¹„ìŠ¤ì¼€ì¼ íŠ¹ì§•
            non_scaled_features = [col for col in data.columns 
                                 if ('ratio' in col or 'change' in col or 'signal' in col or 
                                     col in ['hour', 'dayofweek', 'is_weekend', 'future_spike'])]
            
            # ìµœì¢… ë°ì´í„°
            final_data = pd.concat([data[non_scaled_features], scaled_df], axis=1)
            
            # ì‹œí€€ìŠ¤ ìƒì„±
            def split_data_by_continuity(data):
                time_diff = data.index.to_series().diff()
                split_points = time_diff > pd.Timedelta(minutes=1)
                segment_ids = split_points.cumsum()
                segments = []
                for segment_id in segment_ids.unique():
                    segment = data[segment_ids == segment_id].copy()
                    if len(segment) > 30:
                        segments.append(segment)
                return segments
            
            data_segments = split_data_by_continuity(final_data)
            
            # ì‹œí€€ìŠ¤ ìƒì„±
            def create_sequences(data, feature_cols, target_col_reg, target_col_cls, seq_length=30):
                X, y_reg, y_cls = [], [], []
                feature_data = data[feature_cols].values
                target_data_reg = data[target_col_reg].values
                target_data_cls = data[target_col_cls].values
                
                for i in range(len(data) - seq_length):
                    X.append(feature_data[i:i+seq_length])
                    y_reg.append(target_data_reg[i+seq_length])
                    y_cls.append(target_data_cls[i+seq_length])
                
                return np.array(X), np.array(y_reg), np.array(y_cls)
            
            seq_length = self.config.get('seq_length', 30)
            input_features = [col for col in final_data.columns 
                            if col not in ['scaled_FUTURE', 'future_spike']]
            
            all_X, all_y_reg, all_y_cls = [], [], []
            for segment in data_segments:
                X_seg, y_reg_seg, y_cls_seg = create_sequences(
                    segment, input_features, 'scaled_FUTURE', 'future_spike', seq_length
                )
                if len(X_seg) > 0:
                    all_X.append(X_seg)
                    all_y_reg.append(y_reg_seg)
                    all_y_cls.append(y_cls_seg)
            
            X = np.concatenate(all_X, axis=0)
            y_reg = np.concatenate(all_y_reg, axis=0)
            y_cls = np.concatenate(all_y_cls, axis=0)
            
            print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° shape: X={X.shape}, y_reg={y_reg.shape}, y_cls={y_cls.shape}")
            print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸‰ì¦ ë¹„ìœ¨: {y_cls.mean():.2%}")
            
            return X, y_reg, y_cls, data
            
        except Exception as e:
            print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {str(e)}")
            traceback.print_exc()
            return None, None, None, None
    
    def inverse_scale(self, scaled_data):
        """ì—­ìŠ¤ì¼€ì¼ë§"""
        try:
            if hasattr(self.scaler, 'feature_names_in_'):
                feature_names = list(self.scaler.feature_names_in_)
                n_features = len(feature_names)
                
                dummy = np.zeros((len(scaled_data), n_features))
                
                if 'FUTURE' in feature_names:
                    future_idx = feature_names.index('FUTURE')
                else:
                    future_idx = 0
                
                dummy[:, future_idx] = scaled_data
                return self.scaler.inverse_transform(dummy)[:, future_idx]
            else:
                # ê¸°ë³¸ ì—­ìŠ¤ì¼€ì¼ë§
                n_features = self.scaler.n_features_in_
                dummy = np.zeros((len(scaled_data), n_features))
                dummy[:, 0] = scaled_data
                return self.scaler.inverse_transform(dummy)[:, 0]
                
        except Exception as e:
            print(f"ì—­ìŠ¤ì¼€ì¼ë§ ì‹¤íŒ¨: {str(e)}")
            return scaled_data
    
    def enhanced_ensemble_predict(self, models, X_test, spike_weight_threshold=0.7):
        """ê¸‰ì¦ ì˜ˆì¸¡ì„ ê°•í™”í•œ ì•™ìƒë¸”"""
        regression_preds = {}
        spike_preds = {}
        
        # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡
        for model_name, model in models.items():
            pred = model.predict(X_test, verbose=0)
            
            # ì´ì¤‘ ì¶œë ¥ ëª¨ë¸ì¸ ê²½ìš°
            if isinstance(pred, list) and len(pred) == 2:
                regression_preds[model_name] = pred[0].flatten()
                spike_preds[model_name] = pred[1].flatten()
            else:
                # ë‹¨ì¼ ì¶œë ¥ ëª¨ë¸ì¸ ê²½ìš° (ê¸°ì¡´ ëª¨ë¸)
                regression_preds[model_name] = pred.flatten()
                spike_preds[model_name] = np.zeros_like(pred.flatten())
        
        # ê°€ì¤‘ í‰ê· 
        weights = self.config.get('model_weights', {
            'dual_lstm': 0.35,
            'dual_gru': 0.25,
            'dual_rnn': 0.15,
            'dual_bilstm': 0.25
        })
        
        ensemble_regression = np.zeros_like(list(regression_preds.values())[0])
        ensemble_spike = np.zeros_like(list(spike_preds.values())[0])
        total_weight = 0
        
        for model_name in regression_preds:
            weight = weights.get(model_name, 1/len(models))
            ensemble_regression += weight * regression_preds[model_name]
            ensemble_spike += weight * spike_preds[model_name]
            total_weight += weight
        
        ensemble_regression /= total_weight
        ensemble_spike /= total_weight
        
        # ê¸‰ì¦ í™•ë¥ ì´ ë†’ìœ¼ë©´ ì˜ˆì¸¡ê°’ ìƒí–¥ ì¡°ì •
        spike_mask = ensemble_spike > spike_weight_threshold
        ensemble_regression[spike_mask] *= 1.15
        
        return ensemble_regression, ensemble_spike, regression_preds, spike_preds
    
    def evaluate_all_models(self, test_data_path=None):
        """ëª¨ë“  ëª¨ë¸ í‰ê°€ ì‹¤í–‰"""
        # ë°ì´í„° ê²½ë¡œ ì„¤ì •
        if test_data_path is None:
            possible_paths = [
                'data/20250731_to_20250806.csv',
                'data/0730to31.csv',
                'data/20240201_TO_202507281705.csv'
            ]
            for path in possible_paths:
                if os.path.exists(path):
                    test_data_path = path
                    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ: {test_data_path}")
                    break
            
            if test_data_path is None:
                print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
        
        # ëª¨ë¸ ë¡œë“œ
        self.load_models_and_config()
        
        if not self.models:
            print("âŒ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"\në¡œë“œëœ ëª¨ë¸ ìˆ˜: {len(self.models)}ê°œ")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        print("\ní…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        X_test, y_test_reg, y_test_cls, test_data = self.prepare_test_data(test_data_path)
        
        if X_test is None:
            print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨")
            return None
        
        # ê° ëª¨ë¸ í‰ê°€
        evaluation_results = {}
        
        for model_name, model in self.models.items():
            print(f"\n{model_name.upper()} ëª¨ë¸ í‰ê°€ ì¤‘...")
            
            # ì˜ˆì¸¡
            pred = model.predict(X_test, verbose=0)
            
            if isinstance(pred, list) and len(pred) == 2:
                # ì´ì¤‘ ì¶œë ¥ ëª¨ë¸
                y_pred_reg = pred[0].flatten()
                y_pred_cls = pred[1].flatten()
            else:
                # ë‹¨ì¼ ì¶œë ¥ ëª¨ë¸ (ê¸‰ì¦ ì˜ˆì¸¡ ì—†ìŒ)
                y_pred_reg = pred.flatten()
                y_pred_cls = np.zeros_like(y_pred_reg)
            
            # í‰ê°€
            results = self.comprehensive_spike_evaluation(
                y_test_reg, y_pred_reg, y_test_cls, y_pred_cls, model_name.upper()
            )
            evaluation_results[model_name] = results
        
        # ì•™ìƒë¸” í‰ê°€
        if len(self.models) > 1:
            print("\nì•™ìƒë¸” ëª¨ë¸ í‰ê°€ ì¤‘...")
            ensemble_reg, ensemble_spike, _, _ = self.enhanced_ensemble_predict(self.models, X_test)
            
            results = self.comprehensive_spike_evaluation(
                y_test_reg, ensemble_reg, y_test_cls, ensemble_spike, "ENSEMBLE"
            )
            evaluation_results['ensemble'] = results
        
        # ì‹œê°í™”
        self.plot_spike_evaluation_results(evaluation_results)
        
        # ê²°ê³¼ ì €ì¥
        self.save_evaluation_results(evaluation_results)
        
        # ê¸‰ì¦ ì˜ˆì¸¡ ìƒ˜í”Œ ë¶„ì„
        self.analyze_spike_predictions(X_test, y_test_reg, y_test_cls, test_data, evaluation_results)
        
        # ìµœì¢… ìš”ì•½
        print("\n" + "="*70)
        print("í‰ê°€ ì™„ë£Œ - ìµœì¢… ìš”ì•½")
        print("="*70)
        
        # ëª©í‘œ ë‹¬ì„± í™•ì¸
        target_recall = 70
        achieved_models = [(name, res) for name, res in evaluation_results.items() 
                          if res['spike_metrics']['recall'] >= target_recall]
        
        if achieved_models:
            print(f"\nâœ… ëª©í‘œ ë‹¬ì„± ëª¨ë¸ (ì¬í˜„ìœ¨ â‰¥ {target_recall}%):")
            for name, res in achieved_models:
                print(f"   â€¢ {name.upper()}: {res['spike_metrics']['recall']:.1f}%")
        else:
            print(f"\nâŒ ëª©í‘œ ë¯¸ë‹¬ì„± (ì¬í˜„ìœ¨ < {target_recall}%)")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_model = max(evaluation_results.items(), 
                        key=lambda x: x[1]['spike_metrics']['recall'])
        print(f"\nğŸ† ìµœê³  ê¸‰ì¦ ì˜ˆì¸¡ ëª¨ë¸: {best_model[0].upper()}")
        print(f"   ì¬í˜„ìœ¨: {best_model[1]['spike_metrics']['recall']:.1f}%")
        print(f"   ì •ë°€ë„: {best_model[1]['spike_metrics']['precision']:.1f}%")
        print(f"   F1-Score: {best_model[1]['spike_metrics']['f1_score']:.1f}%")
        
        return evaluation_results
    
    def analyze_spike_predictions(self, X_test, y_test_reg, y_test_cls, test_data, evaluation_results):
        """ê¸‰ì¦ ì˜ˆì¸¡ ìƒì„¸ ë¶„ì„"""
        print("\n" + "="*70)
        print("ê¸‰ì¦ ì˜ˆì¸¡ ìƒì„¸ ë¶„ì„")
        print("="*70)
        
        # ìµœê³  ëª¨ë¸ ì„ íƒ
        best_model_name = max(evaluation_results.items(), 
                             key=lambda x: x[1]['spike_metrics']['recall'])[0]
        best_model = self.models[best_model_name]
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        pred = best_model.predict(X_test[:100], verbose=0)  # ìƒ˜í”Œ 100ê°œ
        if isinstance(pred, list) and len(pred) == 2:
            y_pred_cls = pred[1].flatten()
        else:
            return
        
        # ê¸‰ì¦ ì˜ˆì¸¡ ìƒ˜í”Œ ë¶„ì„
        spike_pred_indices = np.where(y_pred_cls > 0.7)[0]
        actual_spike_indices = np.where(y_test_cls[:100] == 1)[0]
        
        print(f"\nì˜ˆì¸¡ëœ ê¸‰ì¦ ìˆ˜: {len(spike_pred_indices)}")
        print(f"ì‹¤ì œ ê¸‰ì¦ ìˆ˜: {len(actual_spike_indices)}")
        
        # ì •í™•íˆ ì˜ˆì¸¡í•œ ê¸‰ì¦
        correct_spikes = np.intersect1d(spike_pred_indices, actual_spike_indices)
        print(f"ì •í™•íˆ ì˜ˆì¸¡í•œ ê¸‰ì¦: {len(correct_spikes)}ê±´")
        
        # ë¯¸íƒ ì‚¬ë¡€
        missed_spikes = np.setdiff1d(actual_spike_indices, spike_pred_indices)
        print(f"ë†“ì¹œ ê¸‰ì¦ (ë¯¸íƒ): {len(missed_spikes)}ê±´")
        
        # ì˜¤íƒ ì‚¬ë¡€
        false_alarms = np.setdiff1d(spike_pred_indices, actual_spike_indices)
        print(f"ì˜ëª»ëœ ê¸‰ì¦ ì˜ˆì¸¡ (ì˜¤íƒ): {len(false_alarms)}ê±´")
    
    def save_evaluation_results(self, results):
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # DataFrame ìƒì„±
        rows = []
        for model_name, metrics in results.items():
            row = {
                'Model': model_name.upper(),
                'Overall Score (%)': metrics['overall_score'],
                'Grade': metrics['grade'],
                'Spike Recall (%)': metrics['spike_metrics']['recall'],
                'Spike Precision (%)': metrics['spike_metrics']['precision'],
                'Spike F1 (%)': metrics['spike_metrics']['f1_score'],
                'Spike AUC (%)': metrics['spike_metrics']['auc'],
                'False Negatives': metrics['spike_metrics']['fn'],
                'False Positives': metrics['spike_metrics']['fp'],
                'MAE': metrics['mae'],
                'MAPE (%)': metrics['mape'],
                'Spike MAE': metrics['spike_mae'],
                'Spike MAPE (%)': metrics['spike_mape']
            }
            rows.append(row)
        
        df = pd.DataFrame(rows)
        df = df.sort_values('Spike Recall (%)', ascending=False)
        
        # CSV ì €ì¥
        csv_path = f'spike_evaluation_results_{timestamp}.csv'
        df.to_csv(csv_path, index=False)
        print(f"\nğŸ“ í‰ê°€ ê²°ê³¼ ì €ì¥: {csv_path}")

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main(test_data_path=None):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*70)
    print("ê¸‰ì¦ ì˜ˆì¸¡ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ v3.0")
    print("="*70)
    
    evaluator = SpikeModelEvaluator()
    
    # í‰ê°€ ì‹¤í–‰
    results = evaluator.evaluate_all_models(test_data_path)
    
    if results:
        print("\nâœ… ëª¨ë“  í‰ê°€ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return results
    else:
        print("\nâŒ í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        return None

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        test_path = sys.argv[1]
        print(f"ì‚¬ìš©ì ì§€ì • í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_path}")
        main(test_path)
    else:
        main()