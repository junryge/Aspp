"""
20250807_DATA.CSV ì•™ìƒë¸” ì˜ˆì¸¡ - êµ¬ê°„ë³„ í™•ë¥  ë¶„ì„ ë²„ì „
ê³¼ê±° 100ë¶„ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Input, LSTM, Dense, Dropout, BatchNormalization,
                                     GRU, Conv1D, MaxPooling1D, Bidirectional)
from tensorflow.keras.regularizers import l1_l2
from sklearn.preprocessing import RobustScaler
import joblib
import os
import warnings
warnings.filterwarnings('ignore')
tf.get_logger().setLevel('ERROR')

class Predictor20250807:
    """20250807 ë°ì´í„° ì˜ˆì¸¡"""
   
    def __init__(self):
        # ëª¨ë¸ ê²½ë¡œ (ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)
        self.model_dir = './models'
        self.data_path = './data/20250807_DATA.csv'
       
        self.sequence_length = 100  # ê³¼ê±° 100ë¶„
        self.prediction_horizon = 10  # 10ë¶„ í›„ ì˜ˆì¸¡
        self.spike_threshold = 1400
       
        self.models = {}
        self.scaler = None
       
    def build_improved_lstm(self, input_shape):
        """LSTM ëª¨ë¸"""
        model = Sequential([
            Input(shape=input_shape),
            LSTM(128, return_sequences=True, kernel_regularizer=l1_l2(l1=0.005, l2=0.005)),
            Dropout(0.4),
            BatchNormalization(),
            LSTM(64, return_sequences=True),
            Dropout(0.4),
            LSTM(32, return_sequences=False),
            Dropout(0.3),
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(1)
        ])
        return model

    def build_improved_gru(self, input_shape):
        """GRU ëª¨ë¸"""
        model = Sequential([
            Input(shape=input_shape),
            GRU(128, return_sequences=True, kernel_regularizer=l1_l2(l1=0.005, l2=0.005)),
            Dropout(0.4),
            GRU(64, return_sequences=True),
            Dropout(0.4),
            GRU(32, return_sequences=False),
            Dropout(0.3),
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(1)
        ])
        return model

    def build_improved_cnn_lstm(self, input_shape):
        """CNN-LSTM ëª¨ë¸"""
        model = Sequential([
            Input(shape=input_shape),
            Conv1D(64, 3, activation='relu', padding='same'),
            BatchNormalization(),
            Conv1D(64, 3, activation='relu', padding='same'),
            MaxPooling1D(2),
            Dropout(0.3),
            LSTM(64, return_sequences=True),
            Dropout(0.4),
            LSTM(32, return_sequences=False),
            Dropout(0.3),
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(1)
        ])
        return model

    def build_improved_spike_detector(self, input_shape):
        """Spike Detector"""
        model = Sequential([
            Input(shape=input_shape),
            Conv1D(64, 3, activation='relu', padding='same'),
            BatchNormalization(),
            Conv1D(64, 3, activation='relu', padding='same'),
            MaxPooling1D(2),
            Dropout(0.3),
            Bidirectional(LSTM(64, return_sequences=True)),
            Dropout(0.4),
            Bidirectional(LSTM(32, return_sequences=False)),
            Dropout(0.3),
            Dense(64, activation='relu'),
            BatchNormalization(),
            Dropout(0.3),
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(1, activation='sigmoid')
        ])
        return model
   
    def load_models(self):
        """ëª¨ë¸ ë¡œë“œ"""
        input_shape = (self.sequence_length, 12)
       
        model_configs = {
            'lstm': (self.build_improved_lstm, 'lstm_final.keras'),
            'gru': (self.build_improved_gru, 'gru_final.keras'),
            'cnn_lstm': (self.build_improved_cnn_lstm, 'cnn_lstm_final.keras'),
            'spike_detector': (self.build_improved_spike_detector, 'spike_detector_final.keras')
        }
       
        for name, (build_func, filename) in model_configs.items():
            filepath = os.path.join(self.model_dir, filename)
           
            if os.path.exists(filepath):
                try:
                    model = build_func(input_shape)
                    model.load_weights(filepath)
                    model.compile(
                        optimizer='adam',
                        loss='mae' if name != 'spike_detector' else 'binary_crossentropy',
                        metrics=['mae'] if name != 'spike_detector' else ['accuracy']
                    )
                    self.models[name] = model
                except Exception as e:
                    pass
       
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        scaler_path = os.path.join(self.model_dir, './scaler/scaler.pkl')
        if os.path.exists(scaler_path):
            try:
                self.scaler = joblib.load(scaler_path)
            except:
                self.scaler = RobustScaler()
        else:
            self.scaler = RobustScaler()
       
        return len(self.models) > 0
   
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        df = pd.read_csv(self.data_path, encoding='utf-8')
        df['datetime'] = pd.to_datetime(df['CURRTIME'], format='%Y%m%d%H%M')
        return df
   
    def create_features(self, df):
        """íŠ¹ì§• ìƒì„±"""
        df = df.copy()
       
        # ì‹œê°„ íŠ¹ì§•
        df['hour'] = df['datetime'].dt.hour
        df['dayofweek'] = df['datetime'].dt.dayofweek
        df['is_weekend'] = (df['datetime'].dt.dayofweek >= 5).astype(int)
       
        # ì´ë™í‰ê· 
        df['MA_10'] = df['TOTALCNT'].rolling(10, min_periods=1).mean()
        df['MA_30'] = df['TOTALCNT'].rolling(30, min_periods=1).mean()
        df['MA_60'] = df['TOTALCNT'].rolling(60, min_periods=1).mean()
       
        # í‘œì¤€í¸ì°¨
        df['STD_10'] = df['TOTALCNT'].rolling(10, min_periods=1).std().fillna(0)
        df['STD_30'] = df['TOTALCNT'].rolling(30, min_periods=1).std().fillna(0)
       
        # ë³€í™”ìœ¨
        df['change_rate'] = df['TOTALCNT'].pct_change().fillna(0)
        df['change_rate_10'] = df['TOTALCNT'].pct_change(10).fillna(0)
       
        # íŠ¸ë Œë“œ
        df['trend'] = df['MA_10'] - df['MA_30']
       
        # NaN ì²˜ë¦¬
        df = df.fillna(method='ffill').fillna(0)
       
        return df
   
    def prepare_sequences(self, df):
        """100ë¶„ ì‹œí€€ìŠ¤ ìƒì„±"""
        # ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš° ì²˜ë¦¬
        if len(df) < self.sequence_length + self.prediction_horizon:
            self.sequence_length = min(50, len(df) - self.prediction_horizon - 1)
       
        # íŠ¹ì§• ì»¬ëŸ¼
        feature_cols = ['TOTALCNT', 'MA_10', 'MA_30', 'MA_60',
                       'STD_10', 'STD_30',
                       'change_rate', 'change_rate_10',
                       'hour', 'dayofweek', 'is_weekend', 'trend']
       
        # ìŠ¤ì¼€ì¼ë§
        scaled_data = self.scaler.fit_transform(df[feature_cols])
       
        X = []
        timestamps = []
       
        # ì‹œí€€ìŠ¤ ìƒì„±
        for i in range(self.sequence_length, len(df) - self.prediction_horizon + 1):
            # ê³¼ê±° ë°ì´í„°
            seq = scaled_data[i-self.sequence_length:i]
           
            # 100ë¶„ ë§ì¶”ê¸° ìœ„í•´ íŒ¨ë”© (í•„ìš”ì‹œ)
            if seq.shape[0] < 100:
                padding = np.zeros((100 - seq.shape[0], seq.shape[1]))
                seq = np.vstack([padding, seq])
           
            X.append(seq)
           
            # ì‹œê°„ ì •ë³´
            timestamps.append({
                'current_time': df['datetime'].iloc[i-1],
                'predict_time': df['datetime'].iloc[min(i+self.prediction_horizon-1, len(df)-1)],
                'current_value': df['TOTALCNT'].iloc[i-1]
            })
       
        X = np.array(X) if X else np.zeros((0, 100, len(feature_cols)))
       
        return X, timestamps
   
    def predict_ensemble(self, X, timestamps):
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        if len(X) == 0:
            return np.array([]), {}
       
        predictions = {}
       
        # ê° ëª¨ë¸ ì˜ˆì¸¡
        for name, model in self.models.items():
            pred = model.predict(X, batch_size=256, verbose=0)
            predictions[name] = pred.flatten()
       
        # ì•™ìƒë¸” ê³„ì‚°
        ensemble_pred = np.zeros(len(X))
       
        for i in range(len(X)):
            # Spike detector ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜
            spike_prob = predictions.get('spike_detector', [0.5])[i]
           
            if spike_prob > 0.7:  # ë†’ì€ ìŠ¤íŒŒì´í¬ í™•ë¥ 
                weights = {
                    'lstm': 0.15,
                    'gru': 0.20,
                    'cnn_lstm': 0.50,  # CNN-LSTM ê°•ì¡°
                    'spike_detector': 0.15
                }
                boost = 1.10  # 10% ìƒí–¥
            elif spike_prob > 0.4:  # ì¤‘ê°„ í™•ë¥ 
                weights = {
                    'lstm': 0.25,
                    'gru': 0.35,
                    'cnn_lstm': 0.30,
                    'spike_detector': 0.10
                }
                boost = 1.05  # 5% ìƒí–¥
            else:  # ì •ìƒ ë²”ìœ„
                weights = {
                    'lstm': 0.30,
                    'gru': 0.40,  # GRU ì¤‘ì‹¬
                    'cnn_lstm': 0.25,
                    'spike_detector': 0.05
                }
                boost = 1.0
           
            # ê°€ì¤‘ í‰ê· 
            weighted_sum = 0
            weight_total = 0
           
            for name, weight in weights.items():
                if name in predictions:
                    if name == 'spike_detector':
                        continue
                    else:
                        value = predictions[name][i]
                   
                    weighted_sum += value * weight
                    weight_total += weight
           
            ensemble_pred[i] = (weighted_sum / weight_total) * boost
       
        # ì—­ë³€í™˜
        dummy = np.zeros((len(ensemble_pred), 12))
        dummy[:, 0] = ensemble_pred
        ensemble_original = self.scaler.inverse_transform(dummy)[:, 0]
       
        return ensemble_original, predictions

def ensemble_value():
    """ì•™ìƒë¸” êµ¬ê°„ë³„ í™•ë¥  ë¶„ì„"""
   
    predictor = Predictor20250807()
   
    # 1. ëª¨ë¸ ë¡œë“œ
    if not predictor.load_models():
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        return None
   
    # 2. ë°ì´í„° ë¡œë“œ
    df = predictor.load_data()
    if df is None:
        print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        return None
   
    # 3. íŠ¹ì§• ìƒì„±
    df = predictor.create_features(df)
   
    # 4. ì‹œí€€ìŠ¤ ìƒì„± (100ë¶„)
    X, timestamps = predictor.prepare_sequences(df)
   
    if len(X) == 0:
        print("âŒ ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨")
        return None
   
    # 5. ì•™ìƒë¸” ì˜ˆì¸¡
    ensemble_pred, all_predictions = predictor.predict_ensemble(X, timestamps)
   
    if len(ensemble_pred) == 0:
        print("âŒ ì˜ˆì¸¡ ì‹¤íŒ¨")
        return None
   
    # 6. êµ¬ê°„ë³„ ë¶„ë¥˜ ë° í™•ë¥  ê³„ì‚°
    total_count = len(ensemble_pred)
    
    # êµ¬ê°„ë³„ ê°œìˆ˜ ê³„ì‚°
    normal_count = np.sum((ensemble_pred >= 1200) & (ensemble_pred < 1400))  # ì–‘í˜¸
    good_count = np.sum((ensemble_pred >= 1400) & (ensemble_pred < 1600))    # ë³´í†µ
    warning_count = np.sum((ensemble_pred >= 1600) & (ensemble_pred < 1700)) # ê²½ê³ 
    danger_count = np.sum(ensemble_pred >= 1700)                             # ì£¼ì˜
    
    # ê¸°íƒ€ (1200 ë¯¸ë§Œ)
    below_1200_count = np.sum(ensemble_pred < 1200)
    
    # í™•ë¥  ê³„ì‚°
    normal_percent = (normal_count / total_count * 100) if total_count > 0 else 0
    good_percent = (good_count / total_count * 100) if total_count > 0 else 0
    warning_percent = (warning_count / total_count * 100) if total_count > 0 else 0
    danger_percent = (danger_count / total_count * 100) if total_count > 0 else 0
    below_percent = (below_1200_count / total_count * 100) if total_count > 0 else 0
    
    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    ensemble_value = {
        'ìµœì†Œê°’': int(round(ensemble_pred.min())),
        'ìµœëŒ€ê°’': int(round(ensemble_pred.max())),
        'í‰ê· ê°’': int(round(ensemble_pred.mean())),
        'í‘œì¤€í¸ì°¨': int(round(ensemble_pred.std())),
        'ì´ì˜ˆì¸¡ìˆ˜': total_count,
        
        # êµ¬ê°„ë³„ í™•ë¥ 
        '1200ë¯¸ë§Œ_ê°œìˆ˜': int(below_1200_count),
        '1200ë¯¸ë§Œ_í¼ì„¼íŠ¸': round(below_percent, 1),
        
        '1200-1400ì–‘í˜¸_ê°œìˆ˜': int(normal_count),
        '1200-1400ì–‘í˜¸_í¼ì„¼íŠ¸': round(normal_percent, 1),
        
        '1400-1600ë³´í†µ_ê°œìˆ˜': int(good_count),
        '1400-1600ë³´í†µ_í¼ì„¼íŠ¸': round(good_percent, 1),
        
        '1600ì´ìƒê²½ê³ _ê°œìˆ˜': int(warning_count),
        '1600ì´ìƒê²½ê³ _í¼ì„¼íŠ¸': round(warning_percent, 1),
        
        '1700ì£¼ì˜_ê°œìˆ˜': int(danger_count),
        '1700ì£¼ì˜_í¼ì„¼íŠ¸': round(danger_percent, 1)
    }
    
    # ìƒíƒœë³„ ë¶„ë¥˜ ì¶œë ¥
    print("="*60)
    print("ğŸ¯ V5 ì•™ìƒë¸” ëª¨ë¸ êµ¬ê°„ë³„ ì˜ˆì¸¡ í™•ë¥ ")
    print("="*60)
    print(f"ğŸ“Š ì´ ì˜ˆì¸¡ ìˆ˜: {total_count}ê°œ")
    print(f"ğŸ“ˆ ì˜ˆì¸¡ ë²”ìœ„: {ensemble_value['ìµœì†Œê°’']} ~ {ensemble_value['ìµœëŒ€ê°’']}")
    print(f"ğŸ“Š ì˜ˆì¸¡ í‰ê· : {ensemble_value['í‰ê· ê°’']}")
    print()
    print("ğŸ“‹ êµ¬ê°„ë³„ ë¶„í¬:")
    print(f"  ğŸ”µ 1200ë¯¸ë§Œ      : {ensemble_value['1200ë¯¸ë§Œ_ê°œìˆ˜']:3d}ê°œ ({ensemble_value['1200ë¯¸ë§Œ_í¼ì„¼íŠ¸']:5.1f}%)")
    print(f"  ğŸŸ¢ 1200-1400ì–‘í˜¸ : {ensemble_value['1200-1400ì–‘í˜¸_ê°œìˆ˜']:3d}ê°œ ({ensemble_value['1200-1400ì–‘í˜¸_í¼ì„¼íŠ¸']:5.1f}%)")
    print(f"  ğŸŸ¡ 1400-1600ë³´í†µ : {ensemble_value['1400-1600ë³´í†µ_ê°œìˆ˜']:3d}ê°œ ({ensemble_value['1400-1600ë³´í†µ_í¼ì„¼íŠ¸']:5.1f}%)")
    print(f"  ğŸŸ  1600ì´ìƒê²½ê³   : {ensemble_value['1600ì´ìƒê²½ê³ _ê°œìˆ˜']:3d}ê°œ ({ensemble_value['1600ì´ìƒê²½ê³ _í¼ì„¼íŠ¸']:5.1f}%)")
    print(f"  ğŸ”´ 1700ì£¼ì˜      : {ensemble_value['1700ì£¼ì˜_ê°œìˆ˜']:3d}ê°œ ({ensemble_value['1700ì£¼ì˜_í¼ì„¼íŠ¸']:5.1f}%)")
    print("="*60)
    
    # íŠ¹ì • ê°’ë§Œ ì¶œë ¥í•˜ëŠ” ì˜ˆì‹œë“¤
    print(f"\nğŸ“Š í‰ê· ê°’ ì¶œë ¥: {ensemble_value['í‰ê· ê°’']}")
    print(f"ğŸŸ¢ ì–‘í˜¸ êµ¬ê°„ í™•ë¥ : {ensemble_value['1200-1400ì–‘í˜¸_í¼ì„¼íŠ¸']}%")
    print(f"ğŸŸ  ê²½ê³  êµ¬ê°„ í™•ë¥ : {ensemble_value['1600ì´ìƒê²½ê³ _í¼ì„¼íŠ¸']}%")
    print(f"ğŸ”´ ì£¼ì˜ êµ¬ê°„ í™•ë¥ : {ensemble_value['1700ì£¼ì˜_í¼ì„¼íŠ¸']}%")
    
    return ensemble_value

# ì‹¤í–‰ ë° ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì „ì²´ ë¶„ì„ ì‹¤í–‰
    result = ensemble_value()
    
    if result:
        print("\n" + "="*60)
        print("ğŸ” ê°œë³„ ê°’ ì¶”ì¶œ ì˜ˆì‹œ:")
        print("="*60)
        
        # ê°œë³„ ê°’ ì ‘ê·¼ ë°©ë²•ë“¤
        print(f"í‰ê· ê°’ë§Œ:        {result['í‰ê· ê°’']}")
        print(f"ì–‘í˜¸ í¼ì„¼íŠ¸:     {result['1200-1400ì–‘í˜¸_í¼ì„¼íŠ¸']}%")  
        print(f"ê²½ê³  í¼ì„¼íŠ¸:     {result['1600ì´ìƒê²½ê³ _í¼ì„¼íŠ¸']}%")
        print(f"ì£¼ì˜ í¼ì„¼íŠ¸:     {result['1700ì£¼ì˜_í¼ì„¼íŠ¸']}%")
        
        print(f"\nì‚¬ìš© ì˜ˆì‹œ:")
        print(f"print(ensemble_value['í‰ê· ê°’'])          # {result['í‰ê· ê°’']}")
        print(f"print(ensemble_value['1200-1400ì–‘í˜¸_í¼ì„¼íŠ¸'])  # {result['1200-1400ì–‘í˜¸_í¼ì„¼íŠ¸']}%")
        print(f"print(ensemble_value['1600ì´ìƒê²½ê³ _í¼ì„¼íŠ¸'])   # {result['1600ì´ìƒê²½ê³ _í¼ì„¼íŠ¸']}%")
        print(f"print(ensemble_value['1700ì£¼ì˜_í¼ì„¼íŠ¸'])       # {result['1700ì£¼ì˜_í¼ì„¼íŠ¸']}%")

# ë‹¨ìˆœíˆ í‰ê· ê°’ë§Œ í•„ìš”í•  ë•Œ
def get_average_only():
    """í‰ê· ê°’ë§Œ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    result = ensemble_value()
    if result:
        return result['í‰ê· ê°’']
    return None

# êµ¬ê°„ë³„ í¼ì„¼íŠ¸ë§Œ í•„ìš”í•  ë•Œ  
def get_probabilities_only():
    """êµ¬ê°„ë³„ í™•ë¥ ë§Œ ê°€ì ¸ì˜¤ê¸°"""
    result = ensemble_value()
    if result:
        return {
            'ì–‘í˜¸': result['1200-1400ì–‘í˜¸_í¼ì„¼íŠ¸'],
            'ë³´í†µ': result['1400-1600ë³´í†µ_í¼ì„¼íŠ¸'], 
            'ê²½ê³ ': result['1600ì´ìƒê²½ê³ _í¼ì„¼íŠ¸'],
            'ì£¼ì˜': result['1700ì£¼ì˜_í¼ì„¼íŠ¸']
        }
    return None