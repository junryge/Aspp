"""
20250807_DATA.CSV ì•™ìƒë¸” ì˜ˆì¸¡ + êµ¬ê°„ë³„ í™•ë¥  ë¶„ì„
ê³¼ê±° 100ë¶„ ë°ì´í„°ë¡œ 10ë¶„ í›„ ì˜ˆì¸¡
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Input, LSTM, Dense, Dropout, BatchNormalization,
                                     GRU, Conv1D, MaxPooling1D, Bidirectional)
from tensorflow.keras.regularizers import l1_l2
from sklearn.preprocessing import RobustScaler
import joblib
import os
import warnings
warnings.filterwarnings('ignore')
tf.get_logger().setLevel('ERROR')

# scipy ì¶”ê°€ (í™•ë¥  ê³„ì‚°ìš©)
try:
    from scipy.stats import norm
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

class Predictor20250807:
    """20250807 ë°ì´í„° ì˜ˆì¸¡"""
   
    def __init__(self):
        # ëª¨ë¸ ê²½ë¡œ (ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)
        self.model_dir = './models'
        self.data_path = './data/20250807_DATA.csv'
       
        self.sequence_length = 100  # ê³¼ê±° 100ë¶„
        self.prediction_horizon = 10  # 10ë¶„ í›„ ì˜ˆì¸¡
        self.spike_threshold = 1400
       
        self.models = {}
        self.scaler = None
       
    def build_improved_lstm(self, input_shape):
        """LSTM ëª¨ë¸"""
        model = Sequential([
            Input(shape=input_shape),
            LSTM(128, return_sequences=True, kernel_regularizer=l1_l2(l1=0.005, l2=0.005)),
            Dropout(0.4),
            BatchNormalization(),
            LSTM(64, return_sequences=True),
            Dropout(0.4),
            LSTM(32, return_sequences=False),
            Dropout(0.3),
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(1)
        ])
        return model

    def build_improved_gru(self, input_shape):
        """GRU ëª¨ë¸"""
        model = Sequential([
            Input(shape=input_shape),
            GRU(128, return_sequences=True, kernel_regularizer=l1_l2(l1=0.005, l2=0.005)),
            Dropout(0.4),
            GRU(64, return_sequences=True),
            Dropout(0.4),
            GRU(32, return_sequences=False),
            Dropout(0.3),
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(1)
        ])
        return model

    def build_improved_cnn_lstm(self, input_shape):
        """CNN-LSTM ëª¨ë¸"""
        model = Sequential([
            Input(shape=input_shape),
            Conv1D(64, 3, activation='relu', padding='same'),
            BatchNormalization(),
            Conv1D(64, 3, activation='relu', padding='same'),
            MaxPooling1D(2),
            Dropout(0.3),
            LSTM(64, return_sequences=True),
            Dropout(0.4),
            LSTM(32, return_sequences=False),
            Dropout(0.3),
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(1)
        ])
        return model

    def build_improved_spike_detector(self, input_shape):
        """Spike Detector"""
        model = Sequential([
            Input(shape=input_shape),
            Conv1D(64, 3, activation='relu', padding='same'),
            BatchNormalization(),
            Conv1D(64, 3, activation='relu', padding='same'),
            MaxPooling1D(2),
            Dropout(0.3),
            Bidirectional(LSTM(64, return_sequences=True)),
            Dropout(0.4),
            Bidirectional(LSTM(32, return_sequences=False)),
            Dropout(0.3),
            Dense(64, activation='relu'),
            BatchNormalization(),
            Dropout(0.3),
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(1, activation='sigmoid')
        ])
        return model
   
    def load_models(self):
        """ëª¨ë¸ ë¡œë“œ"""
        input_shape = (self.sequence_length, 12)
       
        model_configs = {
            'lstm': (self.build_improved_lstm, 'lstm_final.keras'),
            'gru': (self.build_improved_gru, 'gru_final.keras'),
            'cnn_lstm': (self.build_improved_cnn_lstm, 'cnn_lstm_final.keras'),
            'spike_detector': (self.build_improved_spike_detector, 'spike_detector_final.keras')
        }
       
        for name, (build_func, filename) in model_configs.items():
            filepath = os.path.join(self.model_dir, filename)
           
            if os.path.exists(filepath):
                try:
                    model = build_func(input_shape)
                    model.load_weights(filepath)
                    model.compile(
                        optimizer='adam',
                        loss='mae' if name != 'spike_detector' else 'binary_crossentropy',
                        metrics=['mae'] if name != 'spike_detector' else ['accuracy']
                    )
                    self.models[name] = model
                except Exception as e:
                    pass
       
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        scaler_path = os.path.join(self.model_dir, './scaler/scaler.pkl')
        if os.path.exists(scaler_path):
            try:
                self.scaler = joblib.load(scaler_path)
            except:
                self.scaler = RobustScaler()
        else:
            self.scaler = RobustScaler()
       
        return len(self.models) > 0
   
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        df = pd.read_csv(self.data_path, encoding='utf-8')
        df['datetime'] = pd.to_datetime(df['CURRTIME'], format='%Y%m%d%H%M')
        return df
   
    def create_features(self, df):
        """íŠ¹ì§• ìƒì„±"""
        df = df.copy()
       
        # ì‹œê°„ íŠ¹ì§•
        df['hour'] = df['datetime'].dt.hour
        df['dayofweek'] = df['datetime'].dt.dayofweek
        df['is_weekend'] = (df['datetime'].dt.dayofweek >= 5).astype(int)
       
        # ì´ë™í‰ê· 
        df['MA_10'] = df['TOTALCNT'].rolling(10, min_periods=1).mean()
        df['MA_30'] = df['TOTALCNT'].rolling(30, min_periods=1).mean()
        df['MA_60'] = df['TOTALCNT'].rolling(60, min_periods=1).mean()
       
        # í‘œì¤€í¸ì°¨
        df['STD_10'] = df['TOTALCNT'].rolling(10, min_periods=1).std().fillna(0)
        df['STD_30'] = df['TOTALCNT'].rolling(30, min_periods=1).std().fillna(0)
       
        # ë³€í™”ìœ¨
        df['change_rate'] = df['TOTALCNT'].pct_change().fillna(0)
        df['change_rate_10'] = df['TOTALCNT'].pct_change(10).fillna(0)
       
        # íŠ¸ë Œë“œ
        df['trend'] = df['MA_10'] - df['MA_30']
       
        # NaN ì²˜ë¦¬
        df = df.fillna(method='ffill').fillna(0)
       
        return df
   
    def prepare_sequences(self, df):
        """100ë¶„ ì‹œí€€ìŠ¤ ìƒì„±"""
        # ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš° ì²˜ë¦¬
        if len(df) < self.sequence_length + self.prediction_horizon:
            self.sequence_length = min(50, len(df) - self.prediction_horizon - 1)
       
        # íŠ¹ì§• ì»¬ëŸ¼
        feature_cols = ['TOTALCNT', 'MA_10', 'MA_30', 'MA_60',
                       'STD_10', 'STD_30',
                       'change_rate', 'change_rate_10',
                       'hour', 'dayofweek', 'is_weekend', 'trend']
       
        # ìŠ¤ì¼€ì¼ë§
        scaled_data = self.scaler.fit_transform(df[feature_cols])
       
        X = []
        timestamps = []
       
        # ì‹œí€€ìŠ¤ ìƒì„±
        for i in range(self.sequence_length, len(df) - self.prediction_horizon + 1):
            # ê³¼ê±° ë°ì´í„°
            seq = scaled_data[i-self.sequence_length:i]
           
            # 100ë¶„ ë§ì¶”ê¸° ìœ„í•´ íŒ¨ë”© (í•„ìš”ì‹œ)
            if seq.shape[0] < 100:
                padding = np.zeros((100 - seq.shape[0], seq.shape[1]))
                seq = np.vstack([padding, seq])
           
            X.append(seq)
           
            # ì‹œê°„ ì •ë³´
            timestamps.append({
                'current_time': df['datetime'].iloc[i-1],
                'predict_time': df['datetime'].iloc[min(i+self.prediction_horizon-1, len(df)-1)],
                'current_value': df['TOTALCNT'].iloc[i-1]
            })
       
        X = np.array(X) if X else np.zeros((0, 100, len(feature_cols)))
       
        return X, timestamps
   
    def predict_ensemble(self, X, timestamps):
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        if len(X) == 0:
            return np.array([]), {}
       
        predictions = {}
       
        # ê° ëª¨ë¸ ì˜ˆì¸¡
        for name, model in self.models.items():
            pred = model.predict(X, batch_size=256, verbose=0)
            predictions[name] = pred.flatten()
       
        # ì•™ìƒë¸” ê³„ì‚°
        ensemble_pred = np.zeros(len(X))
       
        for i in range(len(X)):
            # Spike detector ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜
            spike_prob = predictions.get('spike_detector', [0.5])[i]
           
            if spike_prob > 0.7:  # ë†’ì€ ìŠ¤íŒŒì´í¬ í™•ë¥ 
                weights = {
                    'lstm': 0.15,
                    'gru': 0.20,
                    'cnn_lstm': 0.50,  # CNN-LSTM ê°•ì¡°
                    'spike_detector': 0.15
                }
                boost = 1.10  # 10% ìƒí–¥
            elif spike_prob > 0.4:  # ì¤‘ê°„ í™•ë¥ 
                weights = {
                    'lstm': 0.25,
                    'gru': 0.35,
                    'cnn_lstm': 0.30,
                    'spike_detector': 0.10
                }
                boost = 1.05  # 5% ìƒí–¥
            else:  # ì •ìƒ ë²”ìœ„
                weights = {
                    'lstm': 0.30,
                    'gru': 0.40,  # GRU ì¤‘ì‹¬
                    'cnn_lstm': 0.25,
                    'spike_detector': 0.05
                }
                boost = 1.0
           
            # ê°€ì¤‘ í‰ê· 
            weighted_sum = 0
            weight_total = 0
           
            for name, weight in weights.items():
                if name in predictions:
                    if name == 'spike_detector':
                        # spike_detectorëŠ” í™•ë¥ ê°’ì´ë¯€ë¡œ ìŠ¤ì¼€ì¼ ì¡°ì • ë¶ˆí•„ìš”
                        continue
                    else:
                        value = predictions[name][i]
                   
                    weighted_sum += value * weight
                    weight_total += weight
           
            ensemble_pred[i] = (weighted_sum / weight_total) * boost
       
        # ì—­ë³€í™˜
        dummy = np.zeros((len(ensemble_pred), 12))
        dummy[:, 0] = ensemble_pred
        ensemble_original = self.scaler.inverse_transform(dummy)[:, 0]
       
        return ensemble_original, predictions

def ensemble_value():
    """ë©”ì¸ ì‹¤í–‰ - ì›ë³¸ í•¨ìˆ˜"""
   
    predictor = Predictor20250807()
   
    # 1. ëª¨ë¸ ë¡œë“œ
    if not predictor.load_models():
        return None
   
    # 2. ë°ì´í„° ë¡œë“œ
    df = predictor.load_data()
   
    # 3. íŠ¹ì§• ìƒì„±
    df = predictor.create_features(df)
   
    # 4. ì‹œí€€ìŠ¤ ìƒì„± (100ë¶„)
    X, timestamps = predictor.prepare_sequences(df)
   
    if len(X) == 0:
        return None
   
    # 5. ì•™ìƒë¸” ì˜ˆì¸¡
    ensemble_pred, all_predictions = predictor.predict_ensemble(X, timestamps)
   
    if len(ensemble_pred) == 0:
        return None
   
    # ì•™ìƒë¸” ì˜ˆì¸¡ í†µê³„ë§Œ ì¶œë ¥ (ì •ìˆ˜ë¡œ ë°˜ì˜¬ë¦¼)
    ensemble_value = {
        'ìµœì†Œê°’': int(round(ensemble_pred.min())),
        'ìµœëŒ€ê°’': int(round(ensemble_pred.max())),
        'í‰ê· ê°’': int(round(ensemble_pred.mean())),
        'í‘œì¤€í¸ì°¨': int(round(ensemble_pred.std()))
    }
   
    print(ensemble_value)
    return ensemble_value

# ========================================
# ğŸ¯ êµ¬ê°„ë³„ í™•ë¥  ë¶„ì„ í•¨ìˆ˜ ì¶”ê°€!
# ========================================

def analyze_prediction_probabilities(ensemble_dict):
    """
    ensemble_value ë”•ì…”ë„ˆë¦¬ë¥¼ ë°›ì•„ì„œ êµ¬ê°„ë³„ í™•ë¥  ê³„ì‚°
    
    Args:
        ensemble_dict: {'ìµœì†Œê°’': 1356, 'ìµœëŒ€ê°’': 1849, 'í‰ê· ê°’': 1520, 'í‘œì¤€í¸ì°¨': 95}
    
    Returns:
        êµ¬ê°„ë³„ í™•ë¥  ë”•ì…”ë„ˆë¦¬
    """
    
    if not ensemble_dict:
        return None
        
    min_val = ensemble_dict.get('ìµœì†Œê°’', 1200)
    max_val = ensemble_dict.get('ìµœëŒ€ê°’', 1600) 
    avg_val = ensemble_dict.get('í‰ê· ê°’', 1400)
    std_val = ensemble_dict.get('í‘œì¤€í¸ì°¨', 50)
    
    # scipy ìˆì„ ë•Œì™€ ì—†ì„ ë•Œ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
    if SCIPY_AVAILABLE:
        # ì •ê·œë¶„í¬ ê°€ì •í•˜ì—¬ êµ¬ê°„ë³„ í™•ë¥  ê³„ì‚°
        mu = avg_val  # í‰ê· 
        sigma = max(std_val, 1)  # í‘œì¤€í¸ì°¨ (0 ë°©ì§€)
        
        # êµ¬ê°„ë³„ í™•ë¥  ê³„ì‚° (ëˆ„ì ë¶„í¬í•¨ìˆ˜ ì´ìš©)
        prob_below_1200 = norm.cdf(1200, mu, sigma) * 100
        prob_1200_1400 = (norm.cdf(1400, mu, sigma) - norm.cdf(1200, mu, sigma)) * 100
        prob_1400_1600 = (norm.cdf(1600, mu, sigma) - norm.cdf(1400, mu, sigma)) * 100  
        prob_1600_1700 = (norm.cdf(1700, mu, sigma) - norm.cdf(1600, mu, sigma)) * 100
        prob_above_1700 = (1 - norm.cdf(1700, mu, sigma)) * 100
    else:
        # scipy ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì¶”ì •
        if avg_val < 1300:
            prob_below_1200 = 20.0
            prob_1200_1400 = 60.0
            prob_1400_1600 = 15.0
            prob_1600_1700 = 4.0
            prob_above_1700 = 1.0
        elif avg_val < 1500:
            prob_below_1200 = 5.0
            prob_1200_1400 = 40.0
            prob_1400_1600 = 45.0
            prob_1600_1700 = 8.0
            prob_above_1700 = 2.0
        elif avg_val < 1650:
            prob_below_1200 = 1.0
            prob_1200_1400 = 20.0
            prob_1400_1600 = 55.0
            prob_1600_1700 = 20.0
            prob_above_1700 = 4.0
        else:
            prob_below_1200 = 0.0
            prob_1200_1400 = 10.0
            prob_1400_1600 = 30.0
            prob_1600_1700 = 45.0
            prob_above_1700 = 15.0
    
    # ì‹¤ì œ ë²”ìœ„ì— ë§ì¶° ì¡°ì •
    if min_val >= 1200:
        prob_below_1200 = 0.0
    if max_val <= 1700:
        prob_above_1700 = 0.0
        
    # ì´í•©ì´ 100%ê°€ ë˜ë„ë¡ ì •ê·œí™”
    total_prob = prob_below_1200 + prob_1200_1400 + prob_1400_1600 + prob_1600_1700 + prob_above_1700
    
    if total_prob > 0:
        prob_below_1200 = (prob_below_1200 / total_prob) * 100
        prob_1200_1400 = (prob_1200_1400 / total_prob) * 100
        prob_1400_1600 = (prob_1400_1600 / total_prob) * 100
        prob_1600_1700 = (prob_1600_1700 / total_prob) * 100
        prob_above_1700 = (prob_above_1700 / total_prob) * 100
    
    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    result = {
        'í‰ê· ê°’': avg_val,
        'ìµœì†Œê°’': min_val,
        'ìµœëŒ€ê°’': max_val,
        'í‘œì¤€í¸ì°¨': std_val,
        
        # êµ¬ê°„ë³„ í™•ë¥ 
        '1200ë¯¸ë§Œ_í¼ì„¼íŠ¸': round(prob_below_1200, 1),
        '1200-1400ì–‘í˜¸_í¼ì„¼íŠ¸': round(prob_1200_1400, 1),
        '1400-1600ë³´í†µ_í¼ì„¼íŠ¸': round(prob_1400_1600, 1), 
        '1600ì´ìƒê²½ê³ _í¼ì„¼íŠ¸': round(prob_1600_1700, 1),
        '1700ì£¼ì˜_í¼ì„¼íŠ¸': round(prob_above_1700, 1)
    }
    
    return result

def show_analysis(ensemble_dict):
    """êµ¬ê°„ë³„ í™•ë¥  ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
    
    analysis = analyze_prediction_probabilities(ensemble_dict)
    
    if not analysis:
        print("âŒ ë¶„ì„ ì‹¤íŒ¨")
        return None
    
    print("="*60)
    print("ğŸ¯ ì•™ìƒë¸” ì˜ˆì¸¡ êµ¬ê°„ë³„ í™•ë¥  ë¶„ì„")  
    print("="*60)
    print(f"ğŸ“Š ê¸°ë³¸ í†µê³„:")
    print(f"  í‰ê· ê°’: {analysis['í‰ê· ê°’']}")
    print(f"  ë²”ìœ„: {analysis['ìµœì†Œê°’']} ~ {analysis['ìµœëŒ€ê°’']}")
    print(f"  í‘œì¤€í¸ì°¨: {analysis['í‘œì¤€í¸ì°¨']}")
    print()
    
    print("ğŸ“‹ êµ¬ê°„ë³„ ì˜ˆì¸¡ í™•ë¥ :")
    print(f"  ğŸ”µ 1200ë¯¸ë§Œ      : {analysis['1200ë¯¸ë§Œ_í¼ì„¼íŠ¸']:5.1f}%")
    print(f"  ğŸŸ¢ 1200-1400ì–‘í˜¸ : {analysis['1200-1400ì–‘í˜¸_í¼ì„¼íŠ¸']:5.1f}%")  
    print(f"  ğŸŸ¡ 1400-1600ë³´í†µ : {analysis['1400-1600ë³´í†µ_í¼ì„¼íŠ¸']:5.1f}%")
    print(f"  ğŸŸ  1600ì´ìƒê²½ê³   : {analysis['1600ì´ìƒê²½ê³ _í¼ì„¼íŠ¸']:5.1f}%")
    print(f"  ğŸ”´ 1700ì£¼ì˜      : {analysis['1700ì£¼ì˜_í¼ì„¼íŠ¸']:5.1f}%")
    print("="*60)
    
    return analysis

# ========================================
# ğŸš€ ì‚¬ìš© ì˜ˆì‹œ
# ========================================

if __name__ == "__main__":
    print("ğŸ¯ V5 ì•™ìƒë¸” ëª¨ë¸ ì‹¤í–‰ + êµ¬ê°„ë³„ í™•ë¥  ë¶„ì„")
    print("="*60)
    
    # 1. ì›ë³¸ ensemble_value í•¨ìˆ˜ ì‹¤í–‰
    print("1ï¸âƒ£ ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤í–‰ ì¤‘...")
    ensemble_result = ensemble_value()
    
    if ensemble_result:
        print("\n2ï¸âƒ£ êµ¬ê°„ë³„ í™•ë¥  ë¶„ì„ ì¤‘...")
        
        # 2. êµ¬ê°„ë³„ í™•ë¥  ë¶„ì„
        analysis_result = show_analysis(ensemble_result)
        
        if analysis_result:
            print("\n3ï¸âƒ£ ê°œë³„ ê°’ ì¶”ì¶œ ì˜ˆì‹œ:")
            print("="*40)
            print(f"í‰ê· ê°’ë§Œ:        {analysis_result['í‰ê· ê°’']}")
            print(f"ì–‘í˜¸ í™•ë¥ :       {analysis_result['1200-1400ì–‘í˜¸_í¼ì„¼íŠ¸']}%")
            print(f"ê²½ê³  í™•ë¥ :       {analysis_result['1600ì´ìƒê²½ê³ _í¼ì„¼íŠ¸']}%")
            print(f"ì£¼ì˜ í™•ë¥ :       {analysis_result['1700ì£¼ì˜_í¼ì„¼íŠ¸']}%")
            
            print(f"\nğŸ“ ì‚¬ìš©ë²•:")
            print(f"# í‰ê· ê°’ ì¶œë ¥")
            print(f"print(analysis_result['í‰ê· ê°’'])  # {analysis_result['í‰ê· ê°’']}")
            print(f"# ì–‘í˜¸ í™•ë¥  ì¶œë ¥") 
            print(f"print(analysis_result['1200-1400ì–‘í˜¸_í¼ì„¼íŠ¸'])  # {analysis_result['1200-1400ì–‘í˜¸_í¼ì„¼íŠ¸']}%")
    else:
        print("âŒ ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤íŒ¨")

# ê°„ë‹¨ ì‚¬ìš© í•¨ìˆ˜ë“¤
def get_average_only():
    """í‰ê· ê°’ë§Œ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    result = ensemble_value()
    if result:
        return result['í‰ê· ê°’']
    return None

def get_full_analysis():
    """ì „ì²´ ë¶„ì„ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°"""
    ensemble_result = ensemble_value()
    if ensemble_result:
        return analyze_prediction_probabilities(ensemble_result)
    return None