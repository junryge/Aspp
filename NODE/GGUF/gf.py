# -*- coding: utf-8 -*-
"""
Korean Tech Document RAG System with GGUF - 2025
í•œê¸€ ê¸°ìˆ ë¬¸ì„œ ë° ì¿¼ë¦¬ ì²˜ë¦¬ì— ìµœì í™”ëœ RAG ì‹œìŠ¤í…œ
"""

import os
import sys
import json
import time
import threading
import queue
import hashlib
import re
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, List, Any, Generator, Tuple
import logging
from dataclasses import dataclass, field
from enum import Enum
import pickle
import sqlite3

# UI ë¼ì´ë¸ŒëŸ¬ë¦¬
import customtkinter as ctk
from tkinter import filedialog, messagebox, scrolledtext, ttk
import tkinter as tk

# GGUF ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False
    print("âš ï¸ llama-cpp-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜: pip install llama-cpp-python")

# ë¬¸ì„œ ì²˜ë¦¬ ë° RAG ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from docx import Document
    import numpy as np
    from sentence_transformers import SentenceTransformer
    import faiss
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    import chromadb
    from chromadb.config import Settings
    RAG_LIBS_AVAILABLE = True
except ImportError:
    RAG_LIBS_AVAILABLE = False
    print("âš ï¸ RAG ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜: pip install python-docx sentence-transformers faiss-cpu chromadb scikit-learn")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('gguf_rag_korean.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
class RAGPromptTemplate(Enum):
    """í•œê¸€ ê¸°ìˆ ë¬¸ì„œ RAG ì „ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
    
    KOREAN_TECH_RAG = """ë‹¹ì‹ ì€ ê¸°ìˆ  ë¬¸ì„œë¥¼ ì •í™•í•˜ê²Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ëŠ” AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì‹œìŠ¤í…œ ì§€ì¹¨]
{system_prompt}

[ì°¸ì¡° ë¬¸ì„œ]
{context}

[ì¤‘ìš” ì§€ì¹¨]
1. ìœ„ ì°¸ì¡° ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
3. SQL ì¿¼ë¦¬ë‚˜ ì½”ë“œê°€ ìˆë‹¤ë©´ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”.
4. ê¸°ìˆ  ìš©ì–´ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
5. ë‹µë³€ì˜ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.

[ëŒ€í™” ê¸°ë¡]
{chat_history}

ì‚¬ìš©ì: {user_message}
AI ì „ë¬¸ê°€:"""

    QUERY_FOCUSED_RAG = """ë‹¹ì‹ ì€ SQL ë° ê¸°ìˆ  ì¿¼ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì „ë¬¸ ë¶„ì•¼]
- SQL ì¿¼ë¦¬ ì‘ì„± ë° ìµœì í™”
- ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„
- ê¸°ìˆ  ë¬¸ì„œ í•´ì„

[ì°¸ì¡° ë¬¸ì„œ ë° ì˜ˆì œ]
{context}

[ì¿¼ë¦¬ ì‘ì„± ê·œì¹™]
1. ë¬¸ì„œì˜ ìŠ¤í‚¤ë§ˆì™€ ê·œì¹™ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”.
2. í•œê¸€ ì»¬ëŸ¼ëª…ì´ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
3. ì„±ëŠ¥ì„ ê³ ë ¤í•œ ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
4. ì£¼ì„ìœ¼ë¡œ ì„¤ëª…ì„ ì¶”ê°€í•˜ì„¸ìš”.

{chat_history}

ìš”ì²­ì‚¬í•­: {user_message}
ì‘ë‹µ:"""

@dataclass
class DocumentChunk:
    """ë¬¸ì„œ ì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[np.ndarray] = None
    chunk_id: str = ""
    
    def __post_init__(self):
        if not self.chunk_id:
            self.chunk_id = hashlib.md5(self.content.encode()).hexdigest()[:8]

@dataclass
class RAGConfig:
    """RAG ì„¤ì • ë°ì´í„° í´ë˜ìŠ¤"""
    chunk_size: int = 1000
    chunk_overlap: int = 200
    max_chunks_per_query: int = 5
    embedding_model_name: str = "jhgan/ko-sroberta-multitask"
    use_hybrid_search: bool = True
    min_relevance_score: float = 0.5
    
class KoreanTextSplitter:
    """í•œê¸€ ë¬¸ì„œ íŠ¹í™” í…ìŠ¤íŠ¸ ë¶„í• ê¸°"""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
    def split_text(self, text: str) -> List[str]:
        """í•œê¸€ ë¬¸ì„œë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
        # ë¬¸ë‹¨ ìš°ì„  ë¶„í• 
        paragraphs = text.split('\n\n')
        
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            # SQL ì¿¼ë¦¬ë‚˜ ì½”ë“œ ë¸”ë¡ì€ ë¶„í• í•˜ì§€ ì•ŠìŒ
            if self._is_code_block(para):
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                chunks.append(para.strip())
                continue
                
            # ì¼ë°˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            sentences = self._split_korean_sentences(para)
            
            for sentence in sentences:
                if len(current_chunk) + len(sentence) < self.chunk_size:
                    current_chunk += sentence + " "
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence + " "
                    
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        # ì˜¤ë²„ë© ì ìš©
        return self._apply_overlap(chunks)
        
    def _split_korean_sentences(self, text: str) -> List[str]:
        """í•œê¸€ ë¬¸ì¥ ë¶„í• """
        # í•œê¸€ ë¬¸ì¥ ì¢…ê²° íŒ¨í„´
        sentence_enders = r'[.!?ã€‚ï¼ï¼Ÿ][\s"]'
        sentences = re.split(sentence_enders, text)
        
        # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ì •ë¦¬
        return [s.strip() for s in sentences if s.strip()]
        
    def _is_code_block(self, text: str) -> bool:
        """ì½”ë“œ ë¸”ë¡ ê°ì§€"""
        code_indicators = [
            'SELECT', 'FROM', 'WHERE', 'CREATE TABLE',
            'INSERT INTO', 'UPDATE', 'DELETE',
            '```', 'def ', 'class ', 'function'
        ]
        return any(indicator in text.upper() for indicator in code_indicators)
        
    def _apply_overlap(self, chunks: List[str]) -> List[str]:
        """ì²­í¬ ê°„ ì˜¤ë²„ë© ì ìš©"""
        if not chunks or len(chunks) <= 1:
            return chunks
            
        overlapped_chunks = []
        for i, chunk in enumerate(chunks):
            if i > 0:
                # ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ ì¶”ê°€
                prev_chunk = chunks[i-1]
                overlap_text = prev_chunk[-self.chunk_overlap:] if len(prev_chunk) > self.chunk_overlap else prev_chunk
                chunk = overlap_text + " " + chunk
            overlapped_chunks.append(chunk)
            
        return overlapped_chunks

class DocumentProcessor:
    """ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”© ê´€ë¦¬"""
    
    def __init__(self, rag_config: RAGConfig):
        self.config = rag_config
        self.text_splitter = KoreanTextSplitter(
            chunk_size=rag_config.chunk_size,
            chunk_overlap=rag_config.chunk_overlap
        )
        self.embedding_model = None
        self.chunks_db = []
        self.embeddings = None
        self.index = None
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        
    def initialize_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.embedding_model = SentenceTransformer(self.config.embedding_model_name)
            logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.config.embedding_model_name}")
            return True
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
            
    def process_docx(self, file_path: str) -> List[DocumentChunk]:
        """DOCX íŒŒì¼ ì²˜ë¦¬"""
        try:
            doc = Document(file_path)
            full_text = ""
            metadata = {
                "source": os.path.basename(file_path),
                "type": "docx",
                "processed_at": datetime.now().isoformat()
            }
            
            # ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ
            for para in doc.paragraphs:
                if para.text.strip():
                    full_text += para.text + "\n\n"
                    
            # í…Œì´ë¸” ë‚´ìš© ì¶”ì¶œ
            for table in doc.tables:
                table_text = self._extract_table_text(table)
                if table_text:
                    full_text += "\n[í…Œì´ë¸”]\n" + table_text + "\n\n"
                    
            # í…ìŠ¤íŠ¸ ë¶„í• 
            chunks = self.text_splitter.split_text(full_text)
            
            # DocumentChunk ê°ì²´ ìƒì„±
            doc_chunks = []
            for i, chunk_text in enumerate(chunks):
                chunk = DocumentChunk(
                    content=chunk_text,
                    metadata={**metadata, "chunk_index": i}
                )
                doc_chunks.append(chunk)
                
            return doc_chunks
            
        except Exception as e:
            logger.error(f"DOCX ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            raise
            
    def _extract_table_text(self, table) -> str:
        """í…Œì´ë¸” ë‚´ìš© ì¶”ì¶œ"""
        table_text = ""
        for row in table.rows:
            row_text = " | ".join([cell.text.strip() for cell in row.cells])
            table_text += row_text + "\n"
        return table_text
        
    def create_embeddings(self, chunks: List[DocumentChunk]):
        """ì²­í¬ ì„ë² ë”© ìƒì„±"""
        if not self.embedding_model:
            raise RuntimeError("ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        texts = [chunk.content for chunk in chunks]
        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)
        
        for chunk, embedding in zip(chunks, embeddings):
            chunk.embedding = embedding
            
        # ì„ë² ë”© ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        self._update_index(chunks)
        
        # TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ ì—…ë°ì´íŠ¸ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš©)
        if self.config.use_hybrid_search:
            self._update_tfidf(chunks)
            
    def _update_index(self, new_chunks: List[DocumentChunk]):
        """FAISS ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸"""
        self.chunks_db.extend(new_chunks)
        
        all_embeddings = np.array([chunk.embedding for chunk in self.chunks_db])
        dimension = all_embeddings.shape[1]
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(all_embeddings.astype('float32'))
        
    def _update_tfidf(self, new_chunks: List[DocumentChunk]):
        """TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ ì—…ë°ì´íŠ¸"""
        all_texts = [chunk.content for chunk in self.chunks_db]
        
        self.tfidf_vectorizer = TfidfVectorizer(
            tokenizer=self._korean_tokenizer,
            max_features=5000,
            ngram_range=(1, 3)
        )
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(all_texts)
        
    def _korean_tokenizer(self, text):
        """ê°„ë‹¨í•œ í•œê¸€ í† í¬ë‚˜ì´ì €"""
        # ì‹¤ì œë¡œëŠ” KoNLPy ë“±ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ êµ¬í˜„
        return text.split()
        
    def search(self, query: str, k: int = 5) -> List[Tuple[DocumentChunk, float]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)"""
        if not self.index or not self.chunks_db:
            return []
            
        results = []
        
        # 1. ë²¡í„° ê²€ìƒ‰
        query_embedding = self.embedding_model.encode([query])
        distances, indices = self.index.search(
            query_embedding.astype('float32'), 
            min(k * 2, len(self.chunks_db))  # ë” ë§ì´ ê²€ìƒ‰ í›„ í•„í„°ë§
        )
        
        vector_scores = {}
        for idx, dist in zip(indices[0], distances[0]):
            if idx < len(self.chunks_db):
                # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (0~1)
                similarity = 1 / (1 + dist)
                vector_scores[idx] = similarity
                
        # 2. í‚¤ì›Œë“œ ê²€ìƒ‰ (TF-IDF)
        keyword_scores = {}
        if self.config.use_hybrid_search and self.tfidf_matrix is not None:
            query_tfidf = self.tfidf_vectorizer.transform([query])
            keyword_similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)[0]
            
            top_k_indices = np.argsort(keyword_similarities)[-k*2:][::-1]
            for idx in top_k_indices:
                if keyword_similarities[idx] > 0:
                    keyword_scores[idx] = keyword_similarities[idx]
                    
        # 3. ì ìˆ˜ ê²°í•©
        all_indices = set(vector_scores.keys()) | set(keyword_scores.keys())
        combined_scores = {}
        
        for idx in all_indices:
            vector_score = vector_scores.get(idx, 0)
            keyword_score = keyword_scores.get(idx, 0)
            
            # ê°€ì¤‘ í‰ê·  (ë²¡í„° 60%, í‚¤ì›Œë“œ 40%)
            combined_score = 0.6 * vector_score + 0.4 * keyword_score
            
            if combined_score >= self.config.min_relevance_score:
                combined_scores[idx] = combined_score
                
        # 4. ìƒìœ„ kê°œ ì„ íƒ
        sorted_indices = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:k]
        
        for idx, score in sorted_indices:
            results.append((self.chunks_db[idx], score))
            
        return results

class KoreanRAGChat(ctk.CTk):
    """í•œê¸€ ê¸°ìˆ ë¬¸ì„œ RAG ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜"""
    
    def __init__(self):
        super().__init__()
        
        # ê¸°ë³¸ ì„¤ì •
        self.title("Korean Tech RAG Chat - GGUF Integration")
        self.geometry("1400x900")
        
        # í…Œë§ˆ ì„¤ì •
        ctk.set_appearance_mode("dark")
        ctk.set_default_color_theme("blue")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.model_manager = None
        self.rag_config = RAGConfig()
        self.doc_processor = DocumentProcessor(self.rag_config)
        self.messages = []
        self.current_streaming = None
        
        # UI êµ¬ì„±
        self.setup_ui()
        
        # ì´ˆê¸° ë©”ì‹œì§€
        self.add_message("assistant", 
            "ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹\n\n"
            "í•œê¸€ ê¸°ìˆ ë¬¸ì„œ RAG ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n"
            "1. GGUF ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš” (SOLAR-10.7B ì¶”ì²œ)\n"
            "2. DOCX ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”\n"
            "3. ê¸°ìˆ  ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”!")
            
    def setup_ui(self):
        """UI êµ¬ì„±"""
        # ë©”ì¸ ê·¸ë¦¬ë“œ
        self.grid_columnconfigure(0, weight=0)  # ì¢Œì¸¡ ì‚¬ì´ë“œë°”
        self.grid_columnconfigure(1, weight=1)  # ì¤‘ì•™ ì±„íŒ…
        self.grid_columnconfigure(2, weight=0)  # ìš°ì¸¡ ë¬¸ì„œ íŒ¨ë„
        self.grid_rowconfigure(0, weight=1)
        
        # ì‚¬ì´ë“œë°”
        self.setup_sidebar()
        
        # ì±„íŒ… ì˜ì—­
        self.setup_chat_area()
        
        # ë¬¸ì„œ íŒ¨ë„
        self.setup_document_panel()
        
    def setup_sidebar(self):
        """ì‚¬ì´ë“œë°” êµ¬ì„±"""
        sidebar = ctk.CTkFrame(self, width=300, corner_radius=0)
        sidebar.grid(row=0, column=0, sticky="nsew")
        sidebar.grid_rowconfigure(10, weight=1)
        
        # íƒ€ì´í‹€
        title = ctk.CTkLabel(
            sidebar,
            text="Korean Tech RAG",
            font=("Arial", 24, "bold")
        )
        title.grid(row=0, column=0, padx=20, pady=(20, 10))
        
        # ëª¨ë¸ ì •ë³´
        self.model_info = ctk.CTkLabel(
            sidebar,
            text="ëª¨ë¸: ë¡œë“œë˜ì§€ ì•ŠìŒ",
            font=("Arial", 12),
            text_color="gray"
        )
        self.model_info.grid(row=1, column=0, padx=20, pady=(0, 20))
        
        # ëª¨ë¸ ê´€ë ¨ ë²„íŠ¼
        ctk.CTkButton(
            sidebar,
            text="GGUF ëª¨ë¸ ë¡œë“œ",
            command=self.load_model,
            height=40,
            fg_color="green",
            hover_color="darkgreen"
        ).grid(row=2, column=0, padx=20, pady=5, sticky="ew")
        
        # RAG ì„¤ì • ì„¹ì…˜
        ctk.CTkLabel(
            sidebar, 
            text="RAG ì„¤ì •",
            font=("Arial", 16, "bold")
        ).grid(row=3, column=0, padx=20, pady=(20, 10))
        
        # ì²­í¬ í¬ê¸°
        ctk.CTkLabel(sidebar, text="ì²­í¬ í¬ê¸°:").grid(row=4, column=0, padx=20, pady=5, sticky="w")
        self.chunk_size_var = ctk.IntVar(value=1000)
        ctk.CTkSlider(
            sidebar,
            from_=500,
            to=2000,
            number_of_steps=15,
            variable=self.chunk_size_var,
            command=self.update_chunk_size
        ).grid(row=5, column=0, padx=20, pady=5, sticky="ew")
        
        self.chunk_size_label = ctk.CTkLabel(sidebar, text="1000")
        self.chunk_size_label.grid(row=6, column=0, padx=20, pady=(0, 10))
        
        # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
        ctk.CTkLabel(sidebar, text="ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜:").grid(row=7, column=0, padx=20, pady=5, sticky="w")
        self.max_chunks_var = ctk.IntVar(value=5)
        ctk.CTkSlider(
            sidebar,
            from_=1,
            to=10,
            number_of_steps=9,
            variable=self.max_chunks_var,
            command=self.update_max_chunks
        ).grid(row=8, column=0, padx=20, pady=5, sticky="ew")
        
        self.max_chunks_label = ctk.CTkLabel(sidebar, text="5")
        self.max_chunks_label.grid(row=9, column=0, padx=20, pady=(0, 20))
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í† ê¸€
        self.hybrid_search_var = ctk.BooleanVar(value=True)
        ctk.CTkCheckBox(
            sidebar,
            text="í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš©",
            variable=self.hybrid_search_var,
            command=self.toggle_hybrid_search
        ).grid(row=10, column=0, padx=20, pady=10, sticky="w")
        
        # ëŒ€í™” ì´ˆê¸°í™”
        ctk.CTkButton(
            sidebar,
            text="ëŒ€í™” ì´ˆê¸°í™”",
            command=self.clear_chat,
            height=35,
            fg_color="red",
            hover_color="darkred"
        ).grid(row=11, column=0, padx=20, pady=(20, 20), sticky="ew")
        
    def setup_chat_area(self):
        """ì±„íŒ… ì˜ì—­ êµ¬ì„±"""
        chat_frame = ctk.CTkFrame(self)
        chat_frame.grid(row=0, column=1, sticky="nsew", padx=10, pady=10)
        chat_frame.grid_columnconfigure(0, weight=1)
        chat_frame.grid_rowconfigure(0, weight=1)
        
        # ì±„íŒ… ìŠ¤í¬ë¡¤ ì˜ì—­
        self.chat_scroll = ctk.CTkScrollableFrame(chat_frame)
        self.chat_scroll.grid(row=0, column=0, sticky="nsew", padx=10, pady=10)
        
        # ì…ë ¥ ì˜ì—­
        input_frame = ctk.CTkFrame(chat_frame)
        input_frame.grid(row=1, column=0, sticky="ew", padx=10, pady=(0, 10))
        input_frame.grid_columnconfigure(0, weight=1)
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ë°•ìŠ¤
        self.input_text = ctk.CTkTextbox(
            input_frame,
            height=100,
            wrap="word",
            font=("Arial", 14)
        )
        self.input_text.grid(row=0, column=0, sticky="ew", padx=(0, 10))
        self.input_text.bind("<Control-Return>", lambda e: self.send_message())
        
        # ë²„íŠ¼ í”„ë ˆì„
        btn_frame = ctk.CTkFrame(input_frame)
        btn_frame.grid(row=0, column=1, sticky="ns")
        
        self.send_btn = ctk.CTkButton(
            btn_frame,
            text="ì „ì†¡",
            command=self.send_message,
            width=100,
            height=40,
            state="disabled"
        )
        self.send_btn.pack(pady=(0, 5))
        
        self.stop_btn = ctk.CTkButton(
            btn_frame,
            text="ì¤‘ì§€",
            command=self.stop_generation,
            width=100,
            height=40,
            fg_color="orange",
            hover_color="darkorange",
            state="disabled"
        )
        self.stop_btn.pack()
        
    def setup_document_panel(self):
        """ë¬¸ì„œ íŒ¨ë„ êµ¬ì„±"""
        doc_panel = ctk.CTkFrame(self, width=350, corner_radius=0)
        doc_panel.grid(row=0, column=2, sticky="nsew")
        doc_panel.grid_rowconfigure(2, weight=1)
        
        # íƒ€ì´í‹€
        ctk.CTkLabel(
            doc_panel,
            text="ë¬¸ì„œ ê´€ë¦¬",
            font=("Arial", 20, "bold")
        ).grid(row=0, column=0, padx=20, pady=(20, 10))
        
        # ë¬¸ì„œ ì—…ë¡œë“œ ë²„íŠ¼
        ctk.CTkButton(
            doc_panel,
            text="DOCX ë¬¸ì„œ ì—…ë¡œë“œ",
            command=self.upload_documents,
            height=40,
            fg_color="blue",
            hover_color="darkblue"
        ).grid(row=1, column=0, padx=20, pady=10, sticky="ew")
        
        # ë¬¸ì„œ ëª©ë¡
        self.doc_listbox = ctk.CTkTextbox(
            doc_panel,
            height=400,
            font=("Arial", 12)
        )
        self.doc_listbox.grid(row=2, column=0, padx=20, pady=10, sticky="nsew")
        
        # ë¬¸ì„œ í†µê³„
        self.doc_stats = ctk.CTkLabel(
            doc_panel,
            text="ë¬¸ì„œ: 0ê°œ\nì²­í¬: 0ê°œ",
            font=("Arial", 12),
            justify="left"
        )
        self.doc_stats.grid(row=3, column=0, padx=20, pady=10, sticky="w")
        
        # ì„ë² ë”© ì´ˆê¸°í™” ë²„íŠ¼
        ctk.CTkButton(
            doc_panel,
            text="ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”",
            command=self.initialize_embeddings,
            height=35
        ).grid(row=4, column=0, padx=20, pady=10, sticky="ew")
        
        # ë¬¸ì„œ ì´ˆê¸°í™” ë²„íŠ¼
        ctk.CTkButton(
            doc_panel,
            text="ë¬¸ì„œ ì „ì²´ ì‚­ì œ",
            command=self.clear_documents,
            height=35,
            fg_color="red",
            hover_color="darkred"
        ).grid(row=5, column=0, padx=20, pady=(10, 20), sticky="ew")
        
    def add_message(self, role: str, content: str, streaming=False, metadata=None):
        """ë©”ì‹œì§€ ì¶”ê°€"""
        # ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆ
        msg_frame = ctk.CTkFrame(self.chat_scroll, corner_radius=10)
        
        if role == "user":
            msg_frame.configure(fg_color=("gray85", "gray25"))
            msg_frame.pack(anchor="e", padx=(100, 10), pady=5, fill="x")
        else:
            msg_frame.configure(fg_color=("gray90", "gray20"))
            msg_frame.pack(anchor="w", padx=(10, 100), pady=5, fill="x")
        
        # ì—­í•  ë¼ë²¨
        role_label = ctk.CTkLabel(
            msg_frame,
            text="You" if role == "user" else "AI",
            font=("Arial", 12, "bold"),
            text_color=("gray40", "gray60")
        )
        role_label.pack(anchor="w", padx=15, pady=(10, 0))
        
        # ë©”íƒ€ë°ì´í„° í‘œì‹œ (ì°¸ì¡° ë¬¸ì„œ ë“±)
        if metadata and "sources" in metadata:
            sources_text = "ğŸ“š ì°¸ì¡°: " + ", ".join(metadata["sources"])
            sources_label = ctk.CTkLabel(
                msg_frame,
                text=sources_text,
                font=("Arial", 10),
                text_color=("blue", "lightblue")
            )
            sources_label.pack(anchor="w", padx=15, pady=(5, 0))
        
        # ë©”ì‹œì§€ ë¼ë²¨
        msg_label = ctk.CTkLabel(
            msg_frame,
            text=content,
            font=("Arial", 14),
            wraplength=700,
            justify="left"
        )
        msg_label.pack(anchor="w", padx=15, pady=(5, 10))
        
        # ë©”ì‹œì§€ ì €ì¥
        if not streaming:
            self.messages.append({
                "role": role,
                "content": content,
                "timestamp": datetime.now(),
                "metadata": metadata
            })
        
        # ìŠ¤í¬ë¡¤ ë‹¤ìš´
        self.chat_scroll._parent_canvas.yview_moveto(1.0)
        
        return msg_label
        
    def load_model(self):
        """GGUF ëª¨ë¸ ë¡œë“œ"""
        filepath = filedialog.askopenfilename(
            title="GGUF ëª¨ë¸ ì„ íƒ (SOLAR-10.7B ê¶Œì¥)",
            filetypes=[("GGUF Files", "*.gguf"), ("All Files", "*.*")]
        )
        
        if not filepath:
            return
            
        # ë¡œë”© ë‹¤ì´ì–¼ë¡œê·¸
        self.show_loading("ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë¡œë“œ
        threading.Thread(
            target=self._load_model_thread,
            args=(filepath,),
            daemon=True
        ).start()
        
    def _load_model_thread(self, filepath):
        """ëª¨ë¸ ë¡œë“œ ìŠ¤ë ˆë“œ"""
        try:
            # GGUFModelManager import and initialization
            from llama_cpp import Llama
            
            # GPU ë ˆì´ì–´ ìë™ ê°ì§€
            n_gpu_layers = -1
            try:
                import torch
                if torch.cuda.is_available():
                    n_gpu_layers = 999
            except:
                n_gpu_layers = 0
            
            self.model_manager = Llama(
                model_path=filepath,
                n_ctx=32768,  # SOLARëŠ” 32K ì»¨í…ìŠ¤íŠ¸
                n_threads=8,
                n_gpu_layers=n_gpu_layers,
                verbose=False,
                use_mmap=True,
                use_mlock=False,
                n_batch=512,
                rope_scaling_type=1,
                mul_mat_q=True,
            )
            
            # UI ì—…ë°ì´íŠ¸
            self.after(0, self._on_model_loaded, filepath)
            
        except Exception as e:
            self.after(0, self._on_model_error, str(e))
            
    def _on_model_loaded(self, filepath):
        """ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"""
        self.hide_loading()
        model_name = os.path.basename(filepath)
        self.model_info.configure(text=f"ëª¨ë¸: {model_name}")
        self.send_btn.configure(state="normal")
        messagebox.showinfo("ì„±ê³µ", "ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    def _on_model_error(self, error):
        """ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜"""
        self.hide_loading()
        messagebox.showerror("ì˜¤ë¥˜", f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:\n{error}")
        
    def upload_documents(self):
        """ë¬¸ì„œ ì—…ë¡œë“œ"""
        filepaths = filedialog.askopenfilenames(
            title="DOCX ë¬¸ì„œ ì„ íƒ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)",
            filetypes=[("Word Documents", "*.docx"), ("All Files", "*.*")]
        )
        
        if not filepaths:
            return
            
        # ë¡œë”© í‘œì‹œ
        self.show_loading(f"{len(filepaths)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì²˜ë¦¬
        threading.Thread(
            target=self._process_documents_thread,
            args=(filepaths,),
            daemon=True
        ).start()
        
    def _process_documents_thread(self, filepaths):
        """ë¬¸ì„œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ"""
        try:
            processed_docs = []
            
            for filepath in filepaths:
                # DOCX ì²˜ë¦¬
                chunks = self.doc_processor.process_docx(filepath)
                processed_docs.append({
                    "path": filepath,
                    "name": os.path.basename(filepath),
                    "chunks": len(chunks)
                })
                
                # ì„ë² ë”© ìƒì„±
                if self.doc_processor.embedding_model:
                    self.doc_processor.create_embeddings(chunks)
                
            # UI ì—…ë°ì´íŠ¸
            self.after(0, self._on_documents_processed, processed_docs)
            
        except Exception as e:
            self.after(0, self._on_document_error, str(e))
            
    def _on_documents_processed(self, processed_docs):
        """ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ"""
        self.hide_loading()
        
        # ë¬¸ì„œ ëª©ë¡ ì—…ë°ì´íŠ¸
        self.doc_listbox.delete("1.0", "end")
        for doc in processed_docs:
            self.doc_listbox.insert("end", f"ğŸ“„ {doc['name']} ({doc['chunks']} chunks)\n")
            
        # í†µê³„ ì—…ë°ì´íŠ¸
        total_chunks = len(self.doc_processor.chunks_db)
        self.doc_stats.configure(
            text=f"ë¬¸ì„œ: {len(processed_docs)}ê°œ\nì²­í¬: {total_chunks}ê°œ"
        )
        
        messagebox.showinfo("ì„±ê³µ", f"{len(processed_docs)}ê°œ ë¬¸ì„œê°€ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    def _on_document_error(self, error):
        """ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜"""
        self.hide_loading()
        messagebox.showerror("ì˜¤ë¥˜", f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨:\n{error}")
        
    def initialize_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        self.show_loading("ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        threading.Thread(
            target=self._init_embeddings_thread,
            daemon=True
        ).start()
        
    def _init_embeddings_thread(self):
        """ì„ë² ë”© ì´ˆê¸°í™” ìŠ¤ë ˆë“œ"""
        try:
            success = self.doc_processor.initialize_embedding_model()
            self.after(0, self._on_embeddings_initialized, success)
        except Exception as e:
            self.after(0, self._on_embeddings_error, str(e))
            
    def _on_embeddings_initialized(self, success):
        """ì„ë² ë”© ì´ˆê¸°í™” ì™„ë£Œ"""
        self.hide_loading()
        if success:
            messagebox.showinfo("ì„±ê³µ", "ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            messagebox.showerror("ì˜¤ë¥˜", "ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
    def _on_embeddings_error(self, error):
        """ì„ë² ë”© ì´ˆê¸°í™” ì˜¤ë¥˜"""
        self.hide_loading()
        messagebox.showerror("ì˜¤ë¥˜", f"ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨:\n{error}")
        
    def send_message(self):
        """ë©”ì‹œì§€ ì „ì†¡"""
        content = self.input_text.get("1.0", "end").strip()
        if not content or not self.model_manager:
            return
            
        # ì…ë ¥ ì´ˆê¸°í™”
        self.input_text.delete("1.0", "end")
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        self.add_message("user", content)
        
        # UI ìƒíƒœ ë³€ê²½
        self.send_btn.configure(state="disabled")
        self.stop_btn.configure(state="normal")
        
        # RAG ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„±
        threading.Thread(
            target=self._generate_rag_response,
            args=(content,),
            daemon=True
        ).start()
        
    def _generate_rag_response(self, query: str):
        """RAG ì‘ë‹µ ìƒì„±"""
        try:
            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            relevant_chunks = []
            sources = []
            
            if self.doc_processor.chunks_db:
                search_results = self.doc_processor.search(
                    query, 
                    k=self.rag_config.max_chunks_per_query
                )
                
                for chunk, score in search_results:
                    relevant_chunks.append(chunk.content)
                    source = chunk.metadata.get("source", "Unknown")
                    if source not in sources:
                        sources.append(source)
                        
            # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
            context = "\n\n---\n\n".join(relevant_chunks) if relevant_chunks else "ì°¸ì¡°í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
            
            # ëŒ€í™” ê¸°ë¡ í¬ë§·íŒ…
            chat_history = ""
            for msg in self.messages[-6:]:  # ìµœê·¼ 6ê°œ ë©”ì‹œì§€
                if msg["role"] == "user":
                    chat_history += f"ì‚¬ìš©ì: {msg['content']}\n"
                elif msg["role"] == "assistant":
                    chat_history += f"AI: {msg['content']}\n"
                    
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ
            template = RAGPromptTemplate.KOREAN_TECH_RAG
            if "ì¿¼ë¦¬" in query.lower() or "sql" in query.lower():
                template = RAGPromptTemplate.QUERY_FOCUSED_RAG
                
            prompt = template.value.format(
                system_prompt="í•œê¸€ ê¸°ìˆ  ë¬¸ì„œë¥¼ ì •í™•í•˜ê²Œ ë¶„ì„í•˜ê³  ë‹µë³€í•©ë‹ˆë‹¤.",
                context=context,
                chat_history=chat_history,
                user_message=query
            )
            
            # 3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            full_response = ""
            msg_label = None
            
            stream = self.model_manager(
                prompt,
                max_tokens=2048,
                temperature=0.3,  # ì •í™•ë„ ìš°ì„ 
                top_p=0.9,
                top_k=40,
                repeat_penalty=1.1,
                stream=True,
                stop=["ì‚¬ìš©ì:", "Human:", "User:", "\n\n\n"]
            )
            
            for output in stream:
                token = output['choices'][0]['text']
                full_response += token
                
                # UI ì—…ë°ì´íŠ¸
                if msg_label is None:
                    metadata = {"sources": sources} if sources else None
                    self.after(0, lambda: setattr(self, '_temp_label', 
                        self.add_message("assistant", token, streaming=True, metadata=metadata)))
                    time.sleep(0.1)
                    msg_label = getattr(self, '_temp_label', None)
                else:
                    self.after(0, lambda t=full_response: msg_label.configure(text=t))
                    
            # ìµœì¢… ë©”ì‹œì§€ ì €ì¥
            self.messages.append({
                "role": "assistant",
                "content": full_response,
                "timestamp": datetime.now(),
                "metadata": {"sources": sources} if sources else None
            })
            
        except Exception as e:
            error_msg = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            self.after(0, lambda: self.add_message("assistant", error_msg))
            
        finally:
            # UI ìƒíƒœ ë³µì›
            self.after(0, self._reset_ui_state)
            
    def stop_generation(self):
        """ìƒì„± ì¤‘ì§€"""
        # êµ¬í˜„ í•„ìš”
        pass
        
    def _reset_ui_state(self):
        """UI ìƒíƒœ ì´ˆê¸°í™”"""
        self.send_btn.configure(state="normal")
        self.stop_btn.configure(state="disabled")
        
    def clear_chat(self):
        """ëŒ€í™” ì´ˆê¸°í™”"""
        if messagebox.askyesno("í™•ì¸", "ëŒ€í™”ë¥¼ ì´ˆê¸°í™”í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            self.messages.clear()
            for widget in self.chat_scroll.winfo_children():
                widget.destroy()
            self.add_message("assistant", "ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
    def clear_documents(self):
        """ë¬¸ì„œ ì´ˆê¸°í™”"""
        if messagebox.askyesno("í™•ì¸", "ëª¨ë“  ë¬¸ì„œë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            self.doc_processor.chunks_db.clear()
            self.doc_processor.index = None
            self.doc_processor.tfidf_matrix = None
            self.doc_listbox.delete("1.0", "end")
            self.doc_stats.configure(text="ë¬¸ì„œ: 0ê°œ\nì²­í¬: 0ê°œ")
            messagebox.showinfo("ì™„ë£Œ", "ëª¨ë“  ë¬¸ì„œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
    def update_chunk_size(self, value):
        """ì²­í¬ í¬ê¸° ì—…ë°ì´íŠ¸"""
        self.chunk_size_label.configure(text=str(int(value)))
        self.rag_config.chunk_size = int(value)
        self.doc_processor.text_splitter.chunk_size = int(value)
        
    def update_max_chunks(self, value):
        """ìµœëŒ€ ì²­í¬ ìˆ˜ ì—…ë°ì´íŠ¸"""
        self.max_chunks_label.configure(text=str(int(value)))
        self.rag_config.max_chunks_per_query = int(value)
        
    def toggle_hybrid_search(self):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í† ê¸€"""
        self.rag_config.use_hybrid_search = self.hybrid_search_var.get()
        
    def show_loading(self, message):
        """ë¡œë”© í‘œì‹œ"""
        self.loading_window = ctk.CTkToplevel(self)
        self.loading_window.title("ì²˜ë¦¬ ì¤‘")
        self.loading_window.geometry("300x150")
        self.loading_window.transient(self)
        self.loading_window.grab_set()
        
        # ì¤‘ì•™ ì •ë ¬
        self.loading_window.update_idletasks()
        x = (self.loading_window.winfo_screenwidth() // 2) - 150
        y = (self.loading_window.winfo_screenheight() // 2) - 75
        self.loading_window.geometry(f"+{x}+{y}")
        
        ctk.CTkLabel(
            self.loading_window,
            text=message,
            font=("Arial", 16)
        ).pack(pady=30)
        
        progress = ctk.CTkProgressBar(self.loading_window, mode="indeterminate")
        progress.pack(padx=40)
        progress.start()
        
    def hide_loading(self):
        """ë¡œë”© ìˆ¨ê¸°ê¸°"""
        if hasattr(self, 'loading_window'):
            self.loading_window.destroy()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    if not LLAMA_CPP_AVAILABLE:
        root = tk.Tk()
        root.withdraw()
        messagebox.showerror(
            "Error",
            "llama-cpp-python is not installed.\n\n"
            "Please install it using:\n"
            "pip install llama-cpp-python"
        )
        return
        
    if not RAG_LIBS_AVAILABLE:
        root = tk.Tk()
        root.withdraw()
        messagebox.showerror(
            "Error",
            "RAG libraries are not installed.\n\n"
            "Please install them using:\n"
            "pip install python-docx sentence-transformers faiss-cpu chromadb scikit-learn"
        )
        return
        
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    for dir_name in ["models", "documents", "embeddings", "logs"]:
        Path(dir_name).mkdir(exist_ok=True)
        
    # ì•± ì‹¤í–‰
    app = KoreanRAGChat()
    app.mainloop()

if __name__ == "__main__":
    main()