# -*- coding: utf-8 -*-
"""
Modern GGUF Chat Application - 2025
ìµœì‹  ê¸°ìˆ  íŠ¸ë Œë“œë¥¼ ë°˜ì˜í•œ GGUF ëŒ€í™”í˜• AI ì‹œìŠ¤í…œ
"""

import os
import sys
import json
import time
import threading
import queue
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, List, Any, Generator
import logging
from dataclasses import dataclass, field
from enum import Enum

# UI ë¼ì´ë¸ŒëŸ¬ë¦¬
import customtkinter as ctk
from tkinter import filedialog, messagebox, scrolledtext
import tkinter as tk

# GGUF ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False
    print("âš ï¸ llama-cpp-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜: pip install llama-cpp-python")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('gguf_chat.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ìµœì‹  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
class PromptTemplate(Enum):
    """2025ë…„ ìµœì‹  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
    
    CHATML = """<|im_start|>system
{system_prompt}<|im_end|>
{chat_history}<|im_start|>user
{user_message}<|im_end|>
<|im_start|>assistant
"""
    
    LLAMA3 = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_prompt}<|eot_id|>
{chat_history}<|start_header_id|>user<|end_header_id|>

{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
    
    ALPACA = """### System:
{system_prompt}

{chat_history}### Human:
{user_message}

### Assistant:
"""
    
    VICUNA = """A chat between a curious user and an artificial intelligence assistant.

{system_prompt}

{chat_history}USER: {user_message}
ASSISTANT: """

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ë°ì´í„° í´ë˜ìŠ¤"""
    model_path: str = ""
    context_size: int = 4096
    max_tokens: int = 2048
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 40
    repeat_penalty: float = 1.1
    n_threads: int = 4
    n_gpu_layers: int = -1  # -1ì€ ìë™ ê°ì§€
    seed: int = -1
    prompt_template: str = "CHATML"
    system_prompt: str = "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."
    
@dataclass
class ChatMessage:
    """ì±„íŒ… ë©”ì‹œì§€ ë°ì´í„° í´ë˜ìŠ¤"""
    role: str
    content: str
    timestamp: datetime = field(default_factory=datetime.now)
    tokens: int = 0

class StreamingResponse:
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, model, prompt, config: ModelConfig):
        self.model = model
        self.prompt = prompt
        self.config = config
        self.response_queue = queue.Queue()
        self.is_generating = True
        
    def generate(self) -> Generator[str, None, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""
        try:
            stream = self.model(
                self.prompt,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                top_k=self.config.top_k,
                repeat_penalty=self.config.repeat_penalty,
                stream=True,
                stop=["<|im_end|>", "<|eot_id|>", "</s>", "###", "\n\n\n"]
            )
            
            for output in stream:
                if not self.is_generating:
                    break
                    
                token = output['choices'][0]['text']
                yield token
                
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì˜¤ë¥˜: {str(e)}")
            yield f"\n[ì˜¤ë¥˜: {str(e)}]"
    
    def stop(self):
        """ìƒì„± ì¤‘ì§€"""
        self.is_generating = False

class GGUFModelManager:
    """GGUF ëª¨ë¸ ê´€ë¦¬ì - ìµœì‹  ê¸°ëŠ¥ í¬í•¨"""
    
    def __init__(self):
        self.model: Optional[Llama] = None
        self.config = ModelConfig()
        self.is_loaded = False
        
    def load_model(self, model_path: str, config: ModelConfig) -> bool:
        """ëª¨ë¸ ë¡œë“œ with ìµœì‹  ì„¤ì •"""
        try:
            # ê¸°ì¡´ ëª¨ë¸ ì •ë¦¬
            if self.model:
                del self.model
                self.model = None
                
            # GPU ìë™ ê°ì§€
            n_gpu_layers = config.n_gpu_layers
            if n_gpu_layers == -1:
                try:
                    # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
                    import torch
                    if torch.cuda.is_available():
                        n_gpu_layers = 999  # ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUë¡œ
                        logger.info(f"CUDA ê°ì§€ë¨: {torch.cuda.get_device_name(0)}")
                except:
                    n_gpu_layers = 0
                    
            # ëª¨ë¸ ë¡œë“œ
            self.model = Llama(
                model_path=model_path,
                n_ctx=config.context_size,
                n_threads=config.n_threads,
                n_gpu_layers=n_gpu_layers,
                seed=config.seed,
                verbose=False,
                use_mmap=True,  # ë©”ëª¨ë¦¬ ë§¤í•‘ ì‚¬ìš©
                use_mlock=False,  # ë©”ëª¨ë¦¬ ë½ ë¹„í™œì„±í™”
                n_batch=512,  # ë°°ì¹˜ í¬ê¸°
                rope_scaling_type=1,  # RoPE ìŠ¤ì¼€ì¼ë§
                mul_mat_q=True,  # ì–‘ìí™”ëœ í–‰ë ¬ ê³±ì…ˆ
            )
            
            self.config = config
            self.is_loaded = True
            logger.info(f"ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {os.path.basename(model_path)}")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise
            
    def build_prompt(self, messages: List[ChatMessage], template: PromptTemplate) -> str:
        """ìµœì‹  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¹Œë“œ"""
        chat_history = ""
        
        # í…œí”Œë¦¿ì— ë”°ë¥¸ ëŒ€í™” ê¸°ë¡ í¬ë§·íŒ…
        if template == PromptTemplate.CHATML:
            for msg in messages[:-1]:  # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì œì™¸
                if msg.role == "user":
                    chat_history += f"<|im_start|>user\n{msg.content}<|im_end|>\n"
                elif msg.role == "assistant":
                    chat_history += f"<|im_start|>assistant\n{msg.content}<|im_end|>\n"
                    
        elif template == PromptTemplate.LLAMA3:
            for msg in messages[:-1]:
                if msg.role == "user":
                    chat_history += f"<|start_header_id|>user<|end_header_id|>\n\n{msg.content}<|eot_id|>"
                elif msg.role == "assistant":
                    chat_history += f"<|start_header_id|>assistant<|end_header_id|>\n\n{msg.content}<|eot_id|>"
                    
        elif template == PromptTemplate.ALPACA:
            for msg in messages[:-1]:
                if msg.role == "user":
                    chat_history += f"### Human:\n{msg.content}\n\n"
                elif msg.role == "assistant":
                    chat_history += f"### Assistant:\n{msg.content}\n\n"
                    
        elif template == PromptTemplate.VICUNA:
            for msg in messages[:-1]:
                if msg.role == "user":
                    chat_history += f"USER: {msg.content}\n"
                elif msg.role == "assistant":
                    chat_history += f"ASSISTANT: {msg.content}\n"
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = template.value.format(
            system_prompt=self.config.system_prompt,
            chat_history=chat_history,
            user_message=messages[-1].content if messages else ""
        )
        
        return prompt
        
    def generate_streaming(self, messages: List[ChatMessage]) -> StreamingResponse:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
        if not self.is_loaded:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ
        template = PromptTemplate[self.config.prompt_template]
        prompt = self.build_prompt(messages, template)
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ê°ì²´ ìƒì„±
        return StreamingResponse(self.model, prompt, self.config)

class ModernGGUFChat(ctk.CTk):
    """í˜„ëŒ€ì ì¸ GGUF ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜"""
    
    def __init__(self):
        super().__init__()
        
        # ê¸°ë³¸ ì„¤ì •
        self.title("GGUF Chat AI - 2025 Edition")
        self.geometry("1200x800")
        
        # í…Œë§ˆ ì„¤ì •
        ctk.set_appearance_mode("dark")
        ctk.set_default_color_theme("blue")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.model_manager = GGUFModelManager()
        self.messages: List[ChatMessage] = []
        self.current_streaming: Optional[StreamingResponse] = None
        
        # UI êµ¬ì„±
        self.setup_ui()
        
        # ì´ˆê¸° ë©”ì‹œì§€
        self.add_message("assistant", "ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹ GGUF ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ëŒ€í™”ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”.")
        
    def setup_ui(self):
        """UI êµ¬ì„±"""
        # ë©”ì¸ ì»¨í…Œì´ë„ˆ
        self.grid_columnconfigure(0, weight=0)  # ì‚¬ì´ë“œë°”
        self.grid_columnconfigure(1, weight=1)  # ë©”ì¸ ì˜ì—­
        self.grid_rowconfigure(0, weight=1)
        
        # ì‚¬ì´ë“œë°”
        self.setup_sidebar()
        
        # ë©”ì¸ ì˜ì—­
        self.setup_main_area()
        
    def setup_sidebar(self):
        """ì‚¬ì´ë“œë°” êµ¬ì„±"""
        sidebar = ctk.CTkFrame(self, width=300, corner_radius=0)
        sidebar.grid(row=0, column=0, sticky="nsew")
        sidebar.grid_rowconfigure(10, weight=1)
        
        # íƒ€ì´í‹€
        title = ctk.CTkLabel(
            sidebar,
            text="GGUF Chat AI",
            font=("Arial", 24, "bold")
        )
        title.grid(row=0, column=0, padx=20, pady=(20, 10))
        
        # ëª¨ë¸ ì •ë³´
        self.model_info = ctk.CTkLabel(
            sidebar,
            text="ëª¨ë¸: ë¡œë“œë˜ì§€ ì•ŠìŒ",
            font=("Arial", 12),
            text_color="gray"
        )
        self.model_info.grid(row=1, column=0, padx=20, pady=(0, 20))
        
        # ë²„íŠ¼ë“¤
        ctk.CTkButton(
            sidebar,
            text="ëª¨ë¸ ë¡œë“œ",
            command=self.load_model,
            height=40
        ).grid(row=2, column=0, padx=20, pady=5, sticky="ew")
        
        ctk.CTkButton(
            sidebar,
            text="ì„¤ì •",
            command=self.show_settings,
            height=40
        ).grid(row=3, column=0, padx=20, pady=5, sticky="ew")
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ
        ctk.CTkLabel(sidebar, text="í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿:").grid(row=4, column=0, padx=20, pady=(20, 5), sticky="w")
        
        self.template_var = ctk.StringVar(value="CHATML")
        template_menu = ctk.CTkOptionMenu(
            sidebar,
            values=["CHATML", "LLAMA3", "ALPACA", "VICUNA"],
            variable=self.template_var,
            command=self.on_template_change
        )
        template_menu.grid(row=5, column=0, padx=20, pady=5, sticky="ew")
        
        # ì˜¨ë„ ìŠ¬ë¼ì´ë”
        ctk.CTkLabel(sidebar, text="Temperature:").grid(row=6, column=0, padx=20, pady=(20, 5), sticky="w")
        
        self.temp_slider = ctk.CTkSlider(
            sidebar,
            from_=0,
            to=1,
            number_of_steps=20,
            command=self.on_temp_change
        )
        self.temp_slider.set(0.7)
        self.temp_slider.grid(row=7, column=0, padx=20, pady=5, sticky="ew")
        
        self.temp_label = ctk.CTkLabel(sidebar, text="0.7")
        self.temp_label.grid(row=8, column=0, padx=20, pady=(0, 20))
        
        # ëŒ€í™” ê´€ë¦¬ ë²„íŠ¼ë“¤
        ctk.CTkButton(
            sidebar,
            text="ëŒ€í™” ì´ˆê¸°í™”",
            command=self.clear_chat,
            height=35,
            fg_color="red",
            hover_color="darkred"
        ).grid(row=9, column=0, padx=20, pady=5, sticky="ew")
        
    def setup_main_area(self):
        """ë©”ì¸ ì˜ì—­ êµ¬ì„±"""
        main_frame = ctk.CTkFrame(self, corner_radius=0)
        main_frame.grid(row=0, column=1, sticky="nsew", padx=(0, 0))
        main_frame.grid_columnconfigure(0, weight=1)
        main_frame.grid_rowconfigure(0, weight=1)
        
        # ì±„íŒ… ì˜ì—­
        self.chat_frame = ctk.CTkScrollableFrame(main_frame)
        self.chat_frame.grid(row=0, column=0, sticky="nsew", padx=20, pady=(20, 10))
        
        # ì…ë ¥ ì˜ì—­
        input_frame = ctk.CTkFrame(main_frame)
        input_frame.grid(row=1, column=0, sticky="ew", padx=20, pady=(0, 20))
        input_frame.grid_columnconfigure(0, weight=1)
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ë°•ìŠ¤
        self.input_text = ctk.CTkTextbox(
            input_frame,
            height=100,
            wrap="word",
            font=("Arial", 14)
        )
        self.input_text.grid(row=0, column=0, sticky="ew", padx=(0, 10))
        self.input_text.bind("<Control-Return>", lambda e: self.send_message())
        
        # ë²„íŠ¼ í”„ë ˆì„
        btn_frame = ctk.CTkFrame(input_frame)
        btn_frame.grid(row=0, column=1, sticky="ns")
        
        self.send_btn = ctk.CTkButton(
            btn_frame,
            text="ì „ì†¡",
            command=self.send_message,
            width=100,
            height=40,
            state="disabled"
        )
        self.send_btn.pack(pady=(0, 5))
        
        self.stop_btn = ctk.CTkButton(
            btn_frame,
            text="ì¤‘ì§€",
            command=self.stop_generation,
            width=100,
            height=40,
            fg_color="orange",
            hover_color="darkorange",
            state="disabled"
        )
        self.stop_btn.pack()
        
    def add_message(self, role: str, content: str, streaming=False):
        """ë©”ì‹œì§€ ì¶”ê°€"""
        # ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆ
        msg_frame = ctk.CTkFrame(self.chat_frame, corner_radius=10)
        
        if role == "user":
            msg_frame.configure(fg_color=("gray85", "gray25"))
            msg_frame.pack(anchor="e", padx=(100, 10), pady=5, fill="x")
        else:
            msg_frame.configure(fg_color=("gray90", "gray20"))
            msg_frame.pack(anchor="w", padx=(10, 100), pady=5, fill="x")
        
        # ì—­í•  ë¼ë²¨
        role_label = ctk.CTkLabel(
            msg_frame,
            text="You" if role == "user" else "AI",
            font=("Arial", 12, "bold"),
            text_color=("gray40", "gray60")
        )
        role_label.pack(anchor="w", padx=15, pady=(10, 0))
        
        # ë©”ì‹œì§€ ë¼ë²¨
        msg_label = ctk.CTkLabel(
            msg_frame,
            text=content,
            font=("Arial", 14),
            wraplength=600,
            justify="left"
        )
        msg_label.pack(anchor="w", padx=15, pady=(5, 10))
        
        # ë©”ì‹œì§€ ì €ì¥
        if not streaming:
            self.messages.append(ChatMessage(role=role, content=content))
        
        # ìŠ¤í¬ë¡¤ ë‹¤ìš´
        self.chat_frame._parent_canvas.yview_moveto(1.0)
        
        return msg_label
        
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        filepath = filedialog.askopenfilename(
            title="GGUF ëª¨ë¸ ì„ íƒ",
            filetypes=[("GGUF Files", "*.gguf"), ("All Files", "*.*")]
        )
        
        if not filepath:
            return
            
        # ë¡œë”© ë‹¤ì´ì–¼ë¡œê·¸
        self.show_loading("ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë¡œë“œ
        threading.Thread(
            target=self._load_model_thread,
            args=(filepath,),
            daemon=True
        ).start()
        
    def _load_model_thread(self, filepath):
        """ëª¨ë¸ ë¡œë“œ ìŠ¤ë ˆë“œ"""
        try:
            config = ModelConfig(
                model_path=filepath,
                prompt_template=self.template_var.get(),
                temperature=self.temp_slider.get()
            )
            
            self.model_manager.load_model(filepath, config)
            
            # UI ì—…ë°ì´íŠ¸
            self.after(0, self._on_model_loaded, filepath)
            
        except Exception as e:
            self.after(0, self._on_model_error, str(e))
            
    def _on_model_loaded(self, filepath):
        """ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"""
        self.hide_loading()
        model_name = os.path.basename(filepath)
        self.model_info.configure(text=f"ëª¨ë¸: {model_name}")
        self.send_btn.configure(state="normal")
        messagebox.showinfo("ì„±ê³µ", "ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    def _on_model_error(self, error):
        """ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜"""
        self.hide_loading()
        messagebox.showerror("ì˜¤ë¥˜", f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:\n{error}")
        
    def send_message(self):
        """ë©”ì‹œì§€ ì „ì†¡"""
        content = self.input_text.get("1.0", "end").strip()
        if not content or not self.model_manager.is_loaded:
            return
            
        # ì…ë ¥ ì´ˆê¸°í™”
        self.input_text.delete("1.0", "end")
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        self.add_message("user", content)
        
        # UI ìƒíƒœ ë³€ê²½
        self.send_btn.configure(state="disabled")
        self.stop_btn.configure(state="normal")
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        threading.Thread(
            target=self._generate_response,
            daemon=True
        ).start()
        
    def _generate_response(self):
        """ì‘ë‹µ ìƒì„±"""
        try:
            # AI ë©”ì‹œì§€ ë¼ë²¨ ìƒì„±
            msg_label = None
            full_response = ""
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘
            self.current_streaming = self.model_manager.generate_streaming(self.messages)
            
            for token in self.current_streaming.generate():
                full_response += token
                
                # UI ì—…ë°ì´íŠ¸
                if msg_label is None:
                    self.after(0, lambda: setattr(self, '_temp_label', 
                        self.add_message("assistant", token, streaming=True)))
                    time.sleep(0.1)  # UI ìƒì„± ëŒ€ê¸°
                    msg_label = getattr(self, '_temp_label', None)
                else:
                    self.after(0, lambda t=full_response: msg_label.configure(text=t))
                    
            # ìµœì¢… ë©”ì‹œì§€ ì €ì¥
            self.messages.append(ChatMessage(role="assistant", content=full_response))
            
        except Exception as e:
            error_msg = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            self.after(0, lambda: self.add_message("assistant", error_msg))
            
        finally:
            # UI ìƒíƒœ ë³µì›
            self.after(0, self._reset_ui_state)
            
    def stop_generation(self):
        """ìƒì„± ì¤‘ì§€"""
        if self.current_streaming:
            self.current_streaming.stop()
            
    def _reset_ui_state(self):
        """UI ìƒíƒœ ì´ˆê¸°í™”"""
        self.send_btn.configure(state="normal")
        self.stop_btn.configure(state="disabled")
        self.current_streaming = None
        
    def clear_chat(self):
        """ëŒ€í™” ì´ˆê¸°í™”"""
        if messagebox.askyesno("í™•ì¸", "ëŒ€í™”ë¥¼ ì´ˆê¸°í™”í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            self.messages.clear()
            for widget in self.chat_frame.winfo_children():
                widget.destroy()
            self.add_message("assistant", "ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”!")
            
    def show_settings(self):
        """ì„¤ì • ì°½"""
        settings_window = ctk.CTkToplevel(self)
        settings_window.title("ì„¤ì •")
        settings_window.geometry("600x700")
        settings_window.transient(self)
        settings_window.grab_set()
        
        # ì„¤ì • í•­ëª©ë“¤
        settings = [
            ("Context Size", "context_size", 128, 32768, 4096),
            ("Max Tokens", "max_tokens", 128, 4096, 2048),
            ("Top K", "top_k", 1, 100, 40),
            ("Repeat Penalty", "repeat_penalty", 0.5, 2.0, 1.1),
            ("Threads", "n_threads", 1, 32, 4),
            ("GPU Layers", "n_gpu_layers", -1, 100, -1),
        ]
        
        row = 0
        self.setting_vars = {}
        
        for label, key, min_val, max_val, default in settings:
            ctk.CTkLabel(settings_window, text=f"{label}:").grid(
                row=row, column=0, padx=20, pady=10, sticky="w"
            )
            
            if isinstance(min_val, float):
                var = ctk.DoubleVar(value=getattr(self.model_manager.config, key, default))
            else:
                var = ctk.IntVar(value=getattr(self.model_manager.config, key, default))
                
            self.setting_vars[key] = var
            
            if key in ["context_size", "max_tokens", "n_threads", "n_gpu_layers", "top_k"]:
                spinbox = ctk.CTkEntry(settings_window, textvariable=var, width=150)
                spinbox.grid(row=row, column=1, padx=20, pady=10)
            else:
                slider = ctk.CTkSlider(
                    settings_window,
                    from_=min_val,
                    to=max_val,
                    variable=var,
                    width=200
                )
                slider.grid(row=row, column=1, padx=20, pady=10)
                
                value_label = ctk.CTkLabel(settings_window, text=f"{var.get():.2f}")
                value_label.grid(row=row, column=2, padx=10, pady=10)
                
                slider.configure(command=lambda v, l=value_label, var=var: l.configure(text=f"{var.get():.2f}"))
                
            row += 1
            
        # System Prompt
        ctk.CTkLabel(settings_window, text="System Prompt:").grid(
            row=row, column=0, columnspan=3, padx=20, pady=(20, 5), sticky="w"
        )
        row += 1
        
        self.system_prompt_text = ctk.CTkTextbox(settings_window, height=150, width=550)
        self.system_prompt_text.grid(row=row, column=0, columnspan=3, padx=20, pady=5)
        self.system_prompt_text.insert("1.0", self.model_manager.config.system_prompt)
        row += 1
        
        # ë²„íŠ¼ë“¤
        btn_frame = ctk.CTkFrame(settings_window)
        btn_frame.grid(row=row, column=0, columnspan=3, pady=20)
        
        ctk.CTkButton(
            btn_frame,
            text="ì €ì¥",
            command=lambda: self.save_settings(settings_window)
        ).pack(side="left", padx=10)
        
        ctk.CTkButton(
            btn_frame,
            text="ì·¨ì†Œ",
            command=settings_window.destroy
        ).pack(side="left")
        
    def save_settings(self, window):
        """ì„¤ì • ì €ì¥"""
        try:
            # ì„¤ì • ì—…ë°ì´íŠ¸
            for key, var in self.setting_vars.items():
                setattr(self.model_manager.config, key, var.get())
                
            self.model_manager.config.system_prompt = self.system_prompt_text.get("1.0", "end").strip()
            self.model_manager.config.prompt_template = self.template_var.get()
            self.model_manager.config.temperature = self.temp_slider.get()
            
            messagebox.showinfo("ì„±ê³µ", "ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            window.destroy()
            
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"ì„¤ì • ì €ì¥ ì‹¤íŒ¨:\n{str(e)}")
            
    def on_template_change(self, value):
        """í…œí”Œë¦¿ ë³€ê²½"""
        if self.model_manager.is_loaded:
            self.model_manager.config.prompt_template = value
            
    def on_temp_change(self, value):
        """ì˜¨ë„ ë³€ê²½"""
        self.temp_label.configure(text=f"{value:.2f}")
        if self.model_manager.is_loaded:
            self.model_manager.config.temperature = value
            
    def show_loading(self, message):
        """ë¡œë”© í‘œì‹œ"""
        self.loading_window = ctk.CTkToplevel(self)
        self.loading_window.title("ë¡œë”©")
        self.loading_window.geometry("300x150")
        self.loading_window.transient(self)
        self.loading_window.grab_set()
        
        # ì¤‘ì•™ ì •ë ¬
        self.loading_window.update_idletasks()
        x = (self.loading_window.winfo_screenwidth() // 2) - 150
        y = (self.loading_window.winfo_screenheight() // 2) - 75
        self.loading_window.geometry(f"+{x}+{y}")
        
        ctk.CTkLabel(
            self.loading_window,
            text=message,
            font=("Arial", 16)
        ).pack(pady=30)
        
        progress = ctk.CTkProgressBar(self.loading_window, mode="indeterminate")
        progress.pack(padx=40)
        progress.start()
        
    def hide_loading(self):
        """ë¡œë”© ìˆ¨ê¸°ê¸°"""
        if hasattr(self, 'loading_window'):
            self.loading_window.destroy()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    if not LLAMA_CPP_AVAILABLE:
        root = tk.Tk()
        root.withdraw()
        messagebox.showerror(
            "Error",
            "llama-cpp-python is not installed.\n\n"
            "Please install it using:\n"
            "pip install llama-cpp-python"
        )
        return
        
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    for dir_name in ["models", "logs", "exports"]:
        Path(dir_name).mkdir(exist_ok=True)
        
    # ì•± ì‹¤í–‰
    app = ModernGGUFChat()
    app.mainloop()

if __name__ == "__main__":
    main()