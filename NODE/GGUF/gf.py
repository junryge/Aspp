# -*- coding: utf-8 -*-
"""
Korean Tech Document RAG System with GGUF - Complete Offline Version
íì‡„ë§ í™˜ê²½ì„ ìœ„í•œ ì™„ì „í•œ ì˜¤í”„ë¼ì¸ í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ
"""

import os
import sys
import json
import time
import threading
import queue
import hashlib
import re
import pickle
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, List, Any, Generator, Tuple
import logging
from dataclasses import dataclass, field
from enum import Enum
from collections import Counter, defaultdict
import math

# UI ë¼ì´ë¸ŒëŸ¬ë¦¬
import customtkinter as ctk
from tkinter import filedialog, messagebox, scrolledtext, ttk
import tkinter as tk

# GGUF ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False
    print("âš ï¸ llama-cpp-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜: pip install llama-cpp-python")

# ë¬¸ì„œ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from docx import Document
    import faiss
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    RAG_LIBS_AVAILABLE = True
except ImportError:
    RAG_LIBS_AVAILABLE = False
    print("âš ï¸ RAG ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜: pip install python-docx faiss-cpu scikit-learn")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('gguf_rag_offline.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
class RAGPromptTemplate(Enum):
    """í•œê¸€ ê¸°ìˆ ë¬¸ì„œ RAG ì „ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
    
    KOREAN_TECH_RAG = """ë‹¹ì‹ ì€ ê¸°ìˆ  ë¬¸ì„œë¥¼ ì •í™•í•˜ê²Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ëŠ” AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì‹œìŠ¤í…œ ì§€ì¹¨]
{system_prompt}

[ì°¸ì¡° ë¬¸ì„œ]
{context}

[ì¤‘ìš” ì§€ì¹¨]
1. ìœ„ ì°¸ì¡° ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
3. SQL ì¿¼ë¦¬ë‚˜ ì½”ë“œê°€ ìˆë‹¤ë©´ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”.
4. ê¸°ìˆ  ìš©ì–´ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
5. ë‹µë³€ì˜ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.

[ëŒ€í™” ê¸°ë¡]
{chat_history}

ì‚¬ìš©ì: {user_message}
AI ì „ë¬¸ê°€:"""

    QUERY_FOCUSED_RAG = """ë‹¹ì‹ ì€ SQL ë° ê¸°ìˆ  ì¿¼ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì „ë¬¸ ë¶„ì•¼]
- SQL ì¿¼ë¦¬ ì‘ì„± ë° ìµœì í™”
- ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„
- ê¸°ìˆ  ë¬¸ì„œ í•´ì„

[ì°¸ì¡° ë¬¸ì„œ ë° ì˜ˆì œ]
{context}

[ì¿¼ë¦¬ ì‘ì„± ê·œì¹™]
1. ë¬¸ì„œì˜ ìŠ¤í‚¤ë§ˆì™€ ê·œì¹™ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”.
2. í•œê¸€ ì»¬ëŸ¼ëª…ì´ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
3. ì„±ëŠ¥ì„ ê³ ë ¤í•œ ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
4. ì£¼ì„ìœ¼ë¡œ ì„¤ëª…ì„ ì¶”ê°€í•˜ì„¸ìš”.

{chat_history}

ìš”ì²­ì‚¬í•­: {user_message}
ì‘ë‹µ:"""

@dataclass
class DocumentChunk:
    """ë¬¸ì„œ ì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[np.ndarray] = None
    chunk_id: str = ""
    
    def __post_init__(self):
        if not self.chunk_id:
            self.chunk_id = hashlib.md5(self.content.encode()).hexdigest()[:8]

@dataclass
class RAGConfig:
    """RAG ì„¤ì • ë°ì´í„° í´ë˜ìŠ¤"""
    chunk_size: int = 1000
    chunk_overlap: int = 200
    max_chunks_per_query: int = 5
    embedding_model_name: str = "offline"  # ì˜¤í”„ë¼ì¸ ëª¨ë“œ
    use_hybrid_search: bool = True
    min_relevance_score: float = 0.3
    embedding_dimension: int = 768

class KoreanOfflineEmbedding:
    """íì‡„ë§ìš© ì˜¤í”„ë¼ì¸ í•œêµ­ì–´ ì„ë² ë”© ì‹œìŠ¤í…œ"""
    
    def __init__(self, dimension: int = 768):
        self.dimension = dimension
        self.vocab = {}
        self.idf_scores = {}
        self.word_vectors = None
        self.char_vocab = {}
        self.subword_vocab = {}
        self.document_count = 0
        self.trained = False
        
    def build_from_documents(self, documents: List[str]):
        """ë¬¸ì„œ ì§‘í•©ìœ¼ë¡œë¶€í„° ì„ë² ë”© ëª¨ë¸ êµ¬ì¶•"""
        logger.info("ì˜¤í”„ë¼ì¸ ì„ë² ë”© ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        
        self.document_count = len(documents)
        
        # 1. ì–´íœ˜ êµ¬ì¶•
        self._build_vocabulary(documents)
        
        # 2. IDF ê³„ì‚°
        self._calculate_idf(documents)
        
        # 3. ë‹¨ì–´ ë²¡í„° ì´ˆê¸°í™”
        self._initialize_word_vectors()
        
        # 4. ê°„ë‹¨í•œ Word2Vec í•™ìŠµ (ì„ íƒì )
        if len(documents) > 10:
            self._train_simple_word2vec(documents)
        
        self.trained = True
        logger.info(f"âœ“ ì„ë² ë”© ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! ì–´íœ˜ í¬ê¸°: {len(self.vocab)}")
        
    def _build_vocabulary(self, documents: List[str]):
        """ì–´íœ˜ êµ¬ì¶•"""
        word_freq = Counter()
        char_freq = Counter()
        
        for doc in documents:
            tokens = self._tokenize_korean(doc)
            word_freq.update(tokens)
            
            for token in tokens:
                char_freq.update(token)
        
        # ë¹ˆë„ 2 ì´ìƒì¸ ë‹¨ì–´ë§Œ ì‚¬ìš©
        self.vocab = {word: idx for idx, (word, freq) in 
                     enumerate(word_freq.most_common(50000)) if freq >= 2}
        
        # ë¬¸ì ì–´íœ˜
        self.char_vocab = {char: idx for idx, (char, _) in 
                          enumerate(char_freq.most_common(5000))}
        
        logger.info(f"ì–´íœ˜ í¬ê¸°: {len(self.vocab)}, ë¬¸ì ì–´íœ˜: {len(self.char_vocab)}")
        
    def _tokenize_korean(self, text: str) -> List[str]:
        """í•œêµ­ì–´ í† í¬ë‚˜ì´ì € (í˜•íƒœì†Œ ë¶„ì„ê¸° ì—†ì´)"""
        text = text.lower().strip()
        
        # ê¸°ë³¸ í† í°í™”
        tokens = re.findall(r'[ê°€-í£]+|[a-zA-Z]+|[0-9]+', text)
        
        # ê°„ë‹¨í•œ ì¡°ì‚¬ ë¶„ë¦¬
        korean_tokens = []
        josa_patterns = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 
                        'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ì˜', 'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€']
        
        for token in tokens:
            if re.match(r'^[ê°€-í£]+$', token) and len(token) > 2:
                for josa in josa_patterns:
                    if token.endswith(josa) and len(token) > len(josa) + 1:
                        word = token[:-len(josa)]
                        korean_tokens.extend([word, josa])
                        break
                else:
                    korean_tokens.append(token)
            else:
                korean_tokens.append(token)
                
        return korean_tokens
        
    def _calculate_idf(self, documents: List[str]):
        """IDF ê³„ì‚°"""
        doc_count = len(documents)
        word_doc_freq = defaultdict(int)
        
        for doc in documents:
            tokens = set(self._tokenize_korean(doc))
            for token in tokens:
                if token in self.vocab:
                    word_doc_freq[token] += 1
        
        for word, doc_freq in word_doc_freq.items():
            self.idf_scores[word] = math.log(doc_count / (doc_freq + 1))
            
    def _initialize_word_vectors(self):
        """ë‹¨ì–´ ë²¡í„° ì´ˆê¸°í™”"""
        vocab_size = len(self.vocab)
        
        # Xavier ì´ˆê¸°í™”
        scale = np.sqrt(2.0 / self.dimension)
        self.word_vectors = np.random.normal(0, scale, (vocab_size, self.dimension))
        
        # ë¬¸ì ê¸°ë°˜ ì´ˆê¸°í™” ë³´ê°•
        for word, idx in self.vocab.items():
            char_vector = self._get_char_vector(word)
            self.word_vectors[idx] = 0.7 * self.word_vectors[idx] + 0.3 * char_vector
            
    def _get_char_vector(self, word: str) -> np.ndarray:
        """ë¬¸ì ê¸°ë°˜ ë²¡í„° ìƒì„±"""
        vector = np.zeros(self.dimension)
        
        for i, char in enumerate(word):
            if char in self.char_vocab:
                # ë¬¸ìì™€ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¡°í•©í•œ í•´ì‹±
                seed = f"{char}_{i}_{word}"
                hash_val = int(hashlib.md5(seed.encode()).hexdigest(), 16)
                
                # ì—¬ëŸ¬ ìœ„ì¹˜ì— ë¶„ì‚°
                for j in range(3):
                    idx = (hash_val + j * 1000) % self.dimension
                    vector[idx] += 1.0 / (i + 1)  # ìœ„ì¹˜ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
        
        # ì •ê·œí™”
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector /= norm
            
        return vector
        
    def _train_simple_word2vec(self, documents: List[str], epochs: int = 2):
        """ê°„ë‹¨í•œ Word2Vec ìŠ¤íƒ€ì¼ í•™ìŠµ"""
        logger.info("Word2Vec í•™ìŠµ ì¤‘...")
        
        learning_rate = 0.025
        window_size = 5
        
        for epoch in range(epochs):
            logger.info(f"  Epoch {epoch + 1}/{epochs}")
            
            for doc_idx, doc in enumerate(documents):
                if doc_idx % 100 == 0:
                    logger.info(f"    ë¬¸ì„œ {doc_idx}/{len(documents)}")
                    
                tokens = self._tokenize_korean(doc)
                token_indices = [self.vocab.get(token, -1) for token in tokens]
                token_indices = [idx for idx in token_indices if idx != -1]
                
                for i, center_idx in enumerate(token_indices):
                    context_indices = []
                    
                    for j in range(max(0, i - window_size), min(len(token_indices), i + window_size + 1)):
                        if i != j:
                            context_indices.append(token_indices[j])
                    
                    if context_indices:
                        # ê°„ë‹¨í•œ ì—…ë°ì´íŠ¸
                        center_vec = self.word_vectors[center_idx]
                        
                        for context_idx in context_indices:
                            context_vec = self.word_vectors[context_idx]
                            
                            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                            similarity = np.dot(center_vec, context_vec)
                            
                            # ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸
                            gradient = learning_rate * (1 - similarity) * context_vec
                            self.word_vectors[center_idx] += gradient
                            self.word_vectors[context_idx] += gradient * 0.5
            
            # í•™ìŠµë¥  ê°ì†Œ
            learning_rate *= 0.9
            
            # ì •ê·œí™”
            norms = np.linalg.norm(self.word_vectors, axis=1, keepdims=True)
            self.word_vectors /= (norms + 1e-8)
        
        logger.info("âœ“ Word2Vec í•™ìŠµ ì™„ë£Œ")
        
    def encode(self, texts: Union[str, List[str]], show_progress_bar: bool = False) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        if isinstance(texts, str):
            texts = [texts]
            
        embeddings = []
        
        for text in texts:
            # 1. TF-IDF ë²¡í„°
            tfidf_vector = self._get_tfidf_vector(text)
            
            # 2. í‰ê·  ë‹¨ì–´ ë²¡í„°
            avg_word_vector = self._get_avg_word_vector(text)
            
            # 3. ë¬¸ì ê¸°ë°˜ ë²¡í„°
            char_vector = self._get_char_vector(text[:100])  # ì²˜ìŒ 100ìë§Œ
            
            # ê²°í•© (ê°€ì¤‘ í‰ê· )
            combined_vector = (
                0.4 * tfidf_vector + 
                0.4 * avg_word_vector + 
                0.2 * char_vector
            )
            
            # ìµœì¢… ì •ê·œí™”
            norm = np.linalg.norm(combined_vector)
            if norm > 0:
                combined_vector /= norm
                
            embeddings.append(combined_vector)
            
        return np.array(embeddings)
        
    def _get_tfidf_vector(self, text: str) -> np.ndarray:
        """TF-IDF ë²¡í„° ìƒì„±"""
        vector = np.zeros(self.dimension)
        tokens = self._tokenize_korean(text)
        
        if not tokens:
            return vector
            
        # TF ê³„ì‚°
        token_counts = Counter(tokens)
        total_tokens = len(tokens)
        
        # TF-IDF ë²¡í„° ìƒì„±
        for token, count in token_counts.items():
            if token in self.vocab:
                tf = count / total_tokens
                idf = self.idf_scores.get(token, 0)
                tfidf = tf * idf
                
                # ì—¬ëŸ¬ ì°¨ì›ì— ë¶„ì‚°
                base_idx = self.vocab[token]
                indices = [
                    base_idx % self.dimension,
                    (base_idx * 7) % self.dimension,
                    (base_idx * 13) % self.dimension
                ]
                
                for idx in indices:
                    vector[idx] += tfidf
                    
        return vector
        
    def _get_avg_word_vector(self, text: str) -> np.ndarray:
        """í‰ê·  ë‹¨ì–´ ë²¡í„° ìƒì„±"""
        if self.word_vectors is None:
            return np.zeros(self.dimension)
            
        tokens = self._tokenize_korean(text)
        vectors = []
        
        for token in tokens:
            if token in self.vocab:
                idx = self.vocab[token]
                vectors.append(self.word_vectors[idx])
                
        if vectors:
            return np.mean(vectors, axis=0)
        else:
            return np.zeros(self.dimension)
            
    def save_model(self, path: str):
        """ëª¨ë¸ ì €ì¥"""
        model_data = {
            'vocab': self.vocab,
            'idf_scores': self.idf_scores,
            'word_vectors': self.word_vectors,
            'char_vocab': self.char_vocab,
            'dimension': self.dimension,
            'document_count': self.document_count
        }
        
        with open(path, 'wb') as f:
            pickle.dump(model_data, f)
            
        logger.info(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")
        
    def load_model(self, path: str):
        """ëª¨ë¸ ë¡œë“œ"""
        with open(path, 'rb') as f:
            model_data = pickle.load(f)
            
        self.vocab = model_data['vocab']
        self.idf_scores = model_data['idf_scores']
        self.word_vectors = model_data['word_vectors']
        self.char_vocab = model_data['char_vocab']
        self.dimension = model_data['dimension']
        self.document_count = model_data['document_count']
        self.trained = True
        
        logger.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")

class KoreanTextSplitter:
    """í•œê¸€ ë¬¸ì„œ íŠ¹í™” í…ìŠ¤íŠ¸ ë¶„í• ê¸°"""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
    def split_text(self, text: str) -> List[str]:
        """í•œê¸€ ë¬¸ì„œë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
        # ë¬¸ë‹¨ ìš°ì„  ë¶„í• 
        paragraphs = text.split('\n\n')
        
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            # SQL ì¿¼ë¦¬ë‚˜ ì½”ë“œ ë¸”ë¡ì€ ë¶„í• í•˜ì§€ ì•ŠìŒ
            if self._is_code_block(para):
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                chunks.append(para.strip())
                continue
                
            # ì¼ë°˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            sentences = self._split_korean_sentences(para)
            
            for sentence in sentences:
                if len(current_chunk) + len(sentence) < self.chunk_size:
                    current_chunk += sentence + " "
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence + " "
                    
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        # ì˜¤ë²„ë© ì ìš©
        return self._apply_overlap(chunks)
        
    def _split_korean_sentences(self, text: str) -> List[str]:
        """í•œê¸€ ë¬¸ì¥ ë¶„í• """
        # í•œê¸€ ë¬¸ì¥ ì¢…ê²° íŒ¨í„´
        sentence_enders = r'[.!?ã€‚ï¼ï¼Ÿ][\s"]'
        sentences = re.split(sentence_enders, text)
        
        # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ì •ë¦¬
        return [s.strip() for s in sentences if s.strip()]
        
    def _is_code_block(self, text: str) -> bool:
        """ì½”ë“œ ë¸”ë¡ ê°ì§€"""
        code_indicators = [
            'SELECT', 'FROM', 'WHERE', 'CREATE TABLE',
            'INSERT INTO', 'UPDATE', 'DELETE',
            '```', 'def ', 'class ', 'function'
        ]
        return any(indicator in text.upper() for indicator in code_indicators)
        
    def _apply_overlap(self, chunks: List[str]) -> List[str]:
        """ì²­í¬ ê°„ ì˜¤ë²„ë© ì ìš©"""
        if not chunks or len(chunks) <= 1:
            return chunks
            
        overlapped_chunks = []
        for i, chunk in enumerate(chunks):
            if i > 0:
                # ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ ì¶”ê°€
                prev_chunk = chunks[i-1]
                overlap_text = prev_chunk[-self.chunk_overlap:] if len(prev_chunk) > self.chunk_overlap else prev_chunk
                chunk = overlap_text + " " + chunk
            overlapped_chunks.append(chunk)
            
        return overlapped_chunks

class DocumentProcessor:
    """ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”© ê´€ë¦¬"""
    
    def __init__(self, rag_config: RAGConfig):
        self.config = rag_config
        self.text_splitter = KoreanTextSplitter(
            chunk_size=rag_config.chunk_size,
            chunk_overlap=rag_config.chunk_overlap
        )
        self.embedding_model = KoreanOfflineEmbedding(rag_config.embedding_dimension)
        self.chunks_db = []
        self.embeddings = None
        self.index = None
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        
    def initialize_embedding_model(self, model_path: Optional[str] = None) -> bool:
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if model_path and os.path.exists(model_path):
                # ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
                self.embedding_model.load_model(model_path)
                logger.info(f"ì €ì¥ëœ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {model_path}")
            else:
                # ìƒˆë¡œìš´ ëª¨ë¸ì€ ë¬¸ì„œ ì¶”ê°€ ì‹œ í•™ìŠµ
                logger.info("ìƒˆë¡œìš´ ì˜¤í”„ë¼ì¸ ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„")
                
            return True
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False
            
    def process_docx(self, file_path: str) -> List[DocumentChunk]:
        """DOCX íŒŒì¼ ì²˜ë¦¬"""
        try:
            doc = Document(file_path)
            full_text = ""
            metadata = {
                "source": os.path.basename(file_path),
                "type": "docx",
                "processed_at": datetime.now().isoformat()
            }
            
            # ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ
            for para in doc.paragraphs:
                if para.text.strip():
                    full_text += para.text + "\n\n"
                    
            # í…Œì´ë¸” ë‚´ìš© ì¶”ì¶œ
            for table in doc.tables:
                table_text = self._extract_table_text(table)
                if table_text:
                    full_text += "\n[í…Œì´ë¸”]\n" + table_text + "\n\n"
                    
            # í…ìŠ¤íŠ¸ ë¶„í• 
            chunks = self.text_splitter.split_text(full_text)
            
            # DocumentChunk ê°ì²´ ìƒì„±
            doc_chunks = []
            for i, chunk_text in enumerate(chunks):
                chunk = DocumentChunk(
                    content=chunk_text,
                    metadata={**metadata, "chunk_index": i}
                )
                doc_chunks.append(chunk)
                
            return doc_chunks
            
        except Exception as e:
            logger.error(f"DOCX ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            raise
            
    def _extract_table_text(self, table) -> str:
        """í…Œì´ë¸” ë‚´ìš© ì¶”ì¶œ"""
        table_text = ""
        for row in table.rows:
            row_text = " | ".join([cell.text.strip() for cell in row.cells])
            table_text += row_text + "\n"
        return table_text
        
    def create_embeddings(self, chunks: List[DocumentChunk]):
        """ì²­í¬ ì„ë² ë”© ìƒì„±"""
        # ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ë‹¤ë©´ í˜„ì¬ ë¬¸ì„œë¡œ í•™ìŠµ
        if not self.embedding_model.trained:
            all_texts = [chunk.content for chunk in self.chunks_db] + [chunk.content for chunk in chunks]
            self.embedding_model.build_from_documents(all_texts)
            
        texts = [chunk.content for chunk in chunks]
        embeddings = self.embedding_model.encode(texts)
        
        for chunk, embedding in zip(chunks, embeddings):
            chunk.embedding = embedding
            
        # ì„ë² ë”© ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        self._update_index(chunks)
        
        # TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ ì—…ë°ì´íŠ¸ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš©)
        if self.config.use_hybrid_search:
            self._update_tfidf(chunks)
            
    def _update_index(self, new_chunks: List[DocumentChunk]):
        """FAISS ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸"""
        self.chunks_db.extend(new_chunks)
        
        all_embeddings = np.array([chunk.embedding for chunk in self.chunks_db])
        dimension = all_embeddings.shape[1]
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(all_embeddings.astype('float32'))
        
    def _update_tfidf(self, new_chunks: List[DocumentChunk]):
        """TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ ì—…ë°ì´íŠ¸"""
        all_texts = [chunk.content for chunk in self.chunks_db]
        
        self.tfidf_vectorizer = TfidfVectorizer(
            tokenizer=self._korean_tokenizer,
            max_features=5000,
            ngram_range=(1, 3)
        )
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(all_texts)
        
    def _korean_tokenizer(self, text):
        """ê°„ë‹¨í•œ í•œê¸€ í† í¬ë‚˜ì´ì €"""
        return self.embedding_model._tokenize_korean(text)
        
    def search(self, query: str, k: int = 5) -> List[Tuple[DocumentChunk, float]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)"""
        if not self.index or not self.chunks_db:
            return []
            
        results = []
        
        # 1. ë²¡í„° ê²€ìƒ‰
        query_embedding = self.embedding_model.encode([query])[0]
        distances, indices = self.index.search(
            query_embedding.reshape(1, -1).astype('float32'), 
            min(k * 2, len(self.chunks_db))
        )
        
        vector_scores = {}
        for idx, dist in zip(indices[0], distances[0]):
            if idx < len(self.chunks_db):
                # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                similarity = 1 / (1 + dist)
                vector_scores[idx] = similarity
                
        # 2. í‚¤ì›Œë“œ ê²€ìƒ‰ (TF-IDF)
        keyword_scores = {}
        if self.config.use_hybrid_search and self.tfidf_matrix is not None:
            query_tfidf = self.tfidf_vectorizer.transform([query])
            keyword_similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)[0]
            
            top_k_indices = np.argsort(keyword_similarities)[-k*2:][::-1]
            for idx in top_k_indices:
                if keyword_similarities[idx] > 0:
                    keyword_scores[idx] = keyword_similarities[idx]
                    
        # 3. ì ìˆ˜ ê²°í•©
        all_indices = set(vector_scores.keys()) | set(keyword_scores.keys())
        combined_scores = {}
        
        for idx in all_indices:
            vector_score = vector_scores.get(idx, 0)
            keyword_score = keyword_scores.get(idx, 0)
            
            # ê°€ì¤‘ í‰ê· 
            combined_score = 0.6 * vector_score + 0.4 * keyword_score
            
            if combined_score >= self.config.min_relevance_score:
                combined_scores[idx] = combined_score
                
        # 4. ìƒìœ„ kê°œ ì„ íƒ
        sorted_indices = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:k]
        
        for idx, score in sorted_indices:
            results.append((self.chunks_db[idx], score))
            
        return results
        
    def save_embedding_model(self, path: str):
        """ì„ë² ë”© ëª¨ë¸ ì €ì¥"""
        if self.embedding_model.trained:
            self.embedding_model.save_model(path)

class KoreanRAGChat(ctk.CTk):
    """í•œê¸€ ê¸°ìˆ ë¬¸ì„œ RAG ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜"""
    
    def __init__(self):
        super().__init__()
        
        # ê¸°ë³¸ ì„¤ì •
        self.title("Korean Tech RAG Chat - Offline Edition")
        self.geometry("1400x900")
        
        # í…Œë§ˆ ì„¤ì •
        ctk.set_appearance_mode("dark")
        ctk.set_default_color_theme("blue")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.model_manager = None
        self.rag_config = RAGConfig()
        self.doc_processor = DocumentProcessor(self.rag_config)
        self.messages = []
        self.current_streaming = None
        
        # UI êµ¬ì„±
        self.setup_ui()
        
        # ì´ˆê¸° ë©”ì‹œì§€
        self.add_message("assistant", 
            "ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹\n\n"
            "ì˜¤í”„ë¼ì¸ í•œê¸€ ê¸°ìˆ ë¬¸ì„œ RAG ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n"
            "1. GGUF ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš” (SOLAR-10.7B ì¶”ì²œ)\n"
            "2. DOCX ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”\n"
            "3. ê¸°ìˆ  ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”!\n\n"
            "ğŸ’¡ íì‡„ë§ í™˜ê²½ì—ì„œë„ ì™„ë²½í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤!")
            
    def setup_ui(self):
        """UI êµ¬ì„±"""
        # ë©”ì¸ ê·¸ë¦¬ë“œ
        self.grid_columnconfigure(0, weight=0)  # ì¢Œì¸¡ ì‚¬ì´ë“œë°”
        self.grid_columnconfigure(1, weight=1)  # ì¤‘ì•™ ì±„íŒ…
        self.grid_columnconfigure(2, weight=0)  # ìš°ì¸¡ ë¬¸ì„œ íŒ¨ë„
        self.grid_rowconfigure(0, weight=1)
        
        # ì‚¬ì´ë“œë°”
        self.setup_sidebar()
        
        # ì±„íŒ… ì˜ì—­
        self.setup_chat_area()
        
        # ë¬¸ì„œ íŒ¨ë„
        self.setup_document_panel()
        
    def setup_sidebar(self):
        """ì‚¬ì´ë“œë°” êµ¬ì„±"""
        sidebar = ctk.CTkFrame(self, width=300, corner_radius=0)
        sidebar.grid(row=0, column=0, sticky="nsew")
        sidebar.grid_rowconfigure(10, weight=1)
        
        # íƒ€ì´í‹€
        title = ctk.CTkLabel(
            sidebar,
            text="Korean Tech RAG",
            font=("Arial", 24, "bold")
        )
        title.grid(row=0, column=0, padx=20, pady=(20, 10))
        
        # ì˜¤í”„ë¼ì¸ ëª¨ë“œ í‘œì‹œ
        offline_label = ctk.CTkLabel(
            sidebar,
            text="ğŸ”’ ì˜¤í”„ë¼ì¸ ëª¨ë“œ",
            font=("Arial", 14, "bold"),
            text_color="lightgreen"
        )
        offline_label.grid(row=1, column=0, padx=20, pady=(0, 10))
        
        # ëª¨ë¸ ì •ë³´
        self.model_info = ctk.CTkLabel(
            sidebar,
            text="ëª¨ë¸: ë¡œë“œë˜ì§€ ì•ŠìŒ",
            font=("Arial", 12),
            text_color="gray"
        )
        self.model_info.grid(row=2, column=0, padx=20, pady=(0, 20))
        
        # ëª¨ë¸ ê´€ë ¨ ë²„íŠ¼
        ctk.CTkButton(
            sidebar,
            text="GGUF ëª¨ë¸ ë¡œë“œ",
            command=self.load_model,
            height=40,
            fg_color="green",
            hover_color="darkgreen"
        ).grid(row=3, column=0, padx=20, pady=5, sticky="ew")
        
        # RAG ì„¤ì • ì„¹ì…˜
        ctk.CTkLabel(
            sidebar, 
            text="RAG ì„¤ì •",
            font=("Arial", 16, "bold")
        ).grid(row=4, column=0, padx=20, pady=(20, 10))
        
        # ì²­í¬ í¬ê¸°
        ctk.CTkLabel(sidebar, text="ì²­í¬ í¬ê¸°:").grid(row=5, column=0, padx=20, pady=5, sticky="w")
        self.chunk_size_var = ctk.IntVar(value=1000)
        ctk.CTkSlider(
            sidebar,
            from_=500,
            to=2000,
            number_of_steps=15,
            variable=self.chunk_size_var,
            command=self.update_chunk_size
        ).grid(row=6, column=0, padx=20, pady=5, sticky="ew")
        
        self.chunk_size_label = ctk.CTkLabel(sidebar, text="1000")
        self.chunk_size_label.grid(row=7, column=0, padx=20, pady=(0, 10))
        
        # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
        ctk.CTkLabel(sidebar, text="ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜:").grid(row=8, column=0, padx=20, pady=5, sticky="w")
        self.max_chunks_var = ctk.IntVar(value=5)
        ctk.CTkSlider(
            sidebar,
            from_=1,
            to=10,
            number_of_steps=9,
            variable=self.max_chunks_var,
            command=self.update_max_chunks
        ).grid(row=9, column=0, padx=20, pady=5, sticky="ew")
        
        self.max_chunks_label = ctk.CTkLabel(sidebar, text="5")
        self.max_chunks_label.grid(row=10, column=0, padx=20, pady=(0, 20))
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í† ê¸€
        self.hybrid_search_var = ctk.BooleanVar(value=True)
        ctk.CTkCheckBox(
            sidebar,
            text="í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš©",
            variable=self.hybrid_search_var,
            command=self.toggle_hybrid_search
        ).grid(row=11, column=0, padx=20, pady=10, sticky="w")
        
        # ì„ë² ë”© ëª¨ë¸ ì €ì¥/ë¡œë“œ
        ctk.CTkButton(
            sidebar,
            text="ì„ë² ë”© ëª¨ë¸ ì €ì¥",
            command=self.save_embedding_model,
            height=35
        ).grid(row=12, column=0, padx=20, pady=5, sticky="ew")
        
        ctk.CTkButton(
            sidebar,
            text="ì„ë² ë”© ëª¨ë¸ ë¡œë“œ",
            command=self.load_embedding_model,
            height=35
        ).grid(row=13, column=0, padx=20, pady=5, sticky="ew")
        
        # ëŒ€í™” ì´ˆê¸°í™”
        ctk.CTkButton(
            sidebar,
            text="ëŒ€í™” ì´ˆê¸°í™”",
            command=self.clear_chat,
            height=35,
            fg_color="red",
            hover_color="darkred"
        ).grid(row=14, column=0, padx=20, pady=(20, 20), sticky="ew")
        
    def setup_chat_area(self):
        """ì±„íŒ… ì˜ì—­ êµ¬ì„±"""
        chat_frame = ctk.CTkFrame(self)
        chat_frame.grid(row=0, column=1, sticky="nsew", padx=10, pady=10)
        chat_frame.grid_columnconfigure(0, weight=1)
        chat_frame.grid_rowconfigure(0, weight=1)
        
        # ì±„íŒ… ìŠ¤í¬ë¡¤ ì˜ì—­
        self.chat_scroll = ctk.CTkScrollableFrame(chat_frame)
        self.chat_scroll.grid(row=0, column=0, sticky="nsew", padx=10, pady=10)
        
        # ì…ë ¥ ì˜ì—­
        input_frame = ctk.CTkFrame(chat_frame)
        input_frame.grid(row=1, column=0, sticky="ew", padx=10, pady=(0, 10))
        input_frame.grid_columnconfigure(0, weight=1)
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ë°•ìŠ¤
        self.input_text = ctk.CTkTextbox(
            input_frame,
            height=100,
            wrap="word",
            font=("Arial", 14)
        )
        self.input_text.grid(row=0, column=0, sticky="ew", padx=(0, 10))
        self.input_text.bind("<Control-Return>", lambda e: self.send_message())
        
        # ë²„íŠ¼ í”„ë ˆì„
        btn_frame = ctk.CTkFrame(input_frame)
        btn_frame.grid(row=0, column=1, sticky="ns")
        
        self.send_btn = ctk.CTkButton(
            btn_frame,
            text="ì „ì†¡",
            command=self.send_message,
            width=100,
            height=40,
            state="disabled"
        )
        self.send_btn.pack(pady=(0, 5))
        
        self.stop_btn = ctk.CTkButton(
            btn_frame,
            text="ì¤‘ì§€",
            command=self.stop_generation,
            width=100,
            height=40,
            fg_color="orange",
            hover_color="darkorange",
            state="disabled"
        )
        self.stop_btn.pack()
        
    def setup_document_panel(self):
        """ë¬¸ì„œ íŒ¨ë„ êµ¬ì„±"""
        doc_panel = ctk.CTkFrame(self, width=350, corner_radius=0)
        doc_panel.grid(row=0, column=2, sticky="nsew")
        doc_panel.grid_rowconfigure(2, weight=1)
        
        # íƒ€ì´í‹€
        ctk.CTkLabel(
            doc_panel,
            text="ë¬¸ì„œ ê´€ë¦¬",
            font=("Arial", 20, "bold")
        ).grid(row=0, column=0, padx=20, pady=(20, 10))
        
        # ë¬¸ì„œ ì—…ë¡œë“œ ë²„íŠ¼
        ctk.CTkButton(
            doc_panel,
            text="DOCX ë¬¸ì„œ ì—…ë¡œë“œ",
            command=self.upload_documents,
            height=40,
            fg_color="blue",
            hover_color="darkblue"
        ).grid(row=1, column=0, padx=20, pady=10, sticky="ew")
        
        # ë¬¸ì„œ ëª©ë¡
        self.doc_listbox = ctk.CTkTextbox(
            doc_panel,
            height=400,
            font=("Arial", 12)
        )
        self.doc_listbox.grid(row=2, column=0, padx=20, pady=10, sticky="nsew")
        
        # ë¬¸ì„œ í†µê³„
        self.doc_stats = ctk.CTkLabel(
            doc_panel,
            text="ë¬¸ì„œ: 0ê°œ\nì²­í¬: 0ê°œ\nì„ë² ë”©: âŒ",
            font=("Arial", 12),
            justify="left"
        )
        self.doc_stats.grid(row=3, column=0, padx=20, pady=10, sticky="w")
        
        # ë¬¸ì„œ ì´ˆê¸°í™” ë²„íŠ¼
        ctk.CTkButton(
            doc_panel,
            text="ë¬¸ì„œ ì „ì²´ ì‚­ì œ",
            command=self.clear_documents,
            height=35,
            fg_color="red",
            hover_color="darkred"
        ).grid(row=4, column=0, padx=20, pady=(10, 20), sticky="ew")
        
    def add_message(self, role: str, content: str, streaming=False, metadata=None):
        """ë©”ì‹œì§€ ì¶”ê°€"""
        # ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆ
        msg_frame = ctk.CTkFrame(self.chat_scroll, corner_radius=10)
        
        if role == "user":
            msg_frame.configure(fg_color=("gray85", "gray25"))
            msg_frame.pack(anchor="e", padx=(100, 10), pady=5, fill="x")
        else:
            msg_frame.configure(fg_color=("gray90", "gray20"))
            msg_frame.pack(anchor="w", padx=(10, 100), pady=5, fill="x")
        
        # ì—­í•  ë¼ë²¨
        role_label = ctk.CTkLabel(
            msg_frame,
            text="You" if role == "user" else "AI",
            font=("Arial", 12, "bold"),
            text_color=("gray40", "gray60")
        )
        role_label.pack(anchor="w", padx=15, pady=(10, 0))
        
        # ë©”íƒ€ë°ì´í„° í‘œì‹œ (ì°¸ì¡° ë¬¸ì„œ ë“±)
        if metadata and "sources" in metadata:
            sources_text = "ğŸ“š ì°¸ì¡°: " + ", ".join(metadata["sources"])
            sources_label = ctk.CTkLabel(
                msg_frame,
                text=sources_text,
                font=("Arial", 10),
                text_color=("blue", "lightblue")
            )
            sources_label.pack(anchor="w", padx=15, pady=(5, 0))
        
        # ë©”ì‹œì§€ ë¼ë²¨
        msg_label = ctk.CTkLabel(
            msg_frame,
            text=content,
            font=("Arial", 14),
            wraplength=700,
            justify="left"
        )
        msg_label.pack(anchor="w", padx=15, pady=(5, 10))
        
        # ë©”ì‹œì§€ ì €ì¥
        if not streaming:
            self.messages.append({
                "role": role,
                "content": content,
                "timestamp": datetime.now(),
                "metadata": metadata
            })
        
        # ìŠ¤í¬ë¡¤ ë‹¤ìš´
        self.chat_scroll._parent_canvas.yview_moveto(1.0)
        
        return msg_label
        
    def load_model(self):
        """GGUF ëª¨ë¸ ë¡œë“œ"""
        filepath = filedialog.askopenfilename(
            title="GGUF ëª¨ë¸ ì„ íƒ (SOLAR-10.7B ê¶Œì¥)",
            filetypes=[("GGUF Files", "*.gguf"), ("All Files", "*.*")]
        )
        
        if not filepath:
            return
            
        # ë¡œë”© ë‹¤ì´ì–¼ë¡œê·¸
        self.show_loading("ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë¡œë“œ
        threading.Thread(
            target=self._load_model_thread,
            args=(filepath,),
            daemon=True
        ).start()
        
    def _load_model_thread(self, filepath):
        """ëª¨ë¸ ë¡œë“œ ìŠ¤ë ˆë“œ"""
        try:
            # GPU ë ˆì´ì–´ ìë™ ê°ì§€
            n_gpu_layers = -1
            try:
                import torch
                if torch.cuda.is_available():
                    n_gpu_layers = 999
            except:
                n_gpu_layers = 0
            
            self.model_manager = Llama(
                model_path=filepath,
                n_ctx=32768,  # SOLARëŠ” 32K ì»¨í…ìŠ¤íŠ¸
                n_threads=8,
                n_gpu_layers=n_gpu_layers,
                verbose=False,
                use_mmap=True,
                use_mlock=False,
                n_batch=512,
                rope_scaling_type=1,
                mul_mat_q=True,
            )
            
            # UI ì—…ë°ì´íŠ¸
            self.after(0, self._on_model_loaded, filepath)
            
        except Exception as e:
            self.after(0, self._on_model_error, str(e))
            
    def _on_model_loaded(self, filepath):
        """ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"""
        self.hide_loading()
        model_name = os.path.basename(filepath)
        self.model_info.configure(text=f"ëª¨ë¸: {model_name}")
        self.send_btn.configure(state="normal")
        messagebox.showinfo("ì„±ê³µ", "ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    def _on_model_error(self, error):
        """ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜"""
        self.hide_loading()
        messagebox.showerror("ì˜¤ë¥˜", f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:\n{error}")
        
    def upload_documents(self):
        """ë¬¸ì„œ ì—…ë¡œë“œ"""
        filepaths = filedialog.askopenfilenames(
            title="DOCX ë¬¸ì„œ ì„ íƒ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)",
            filetypes=[("Word Documents", "*.docx"), ("All Files", "*.*")]
        )
        
        if not filepaths:
            return
            
        # ë¡œë”© í‘œì‹œ
        self.show_loading(f"{len(filepaths)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì²˜ë¦¬
        threading.Thread(
            target=self._process_documents_thread,
            args=(filepaths,),
            daemon=True
        ).start()
        
    def _process_documents_thread(self, filepaths):
        """ë¬¸ì„œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ"""
        try:
            processed_docs = []
            
            for filepath in filepaths:
                # DOCX ì²˜ë¦¬
                chunks = self.doc_processor.process_docx(filepath)
                processed_docs.append({
                    "path": filepath,
                    "name": os.path.basename(filepath),
                    "chunks": len(chunks)
                })
                
                # ì„ë² ë”© ìƒì„±
                self.doc_processor.create_embeddings(chunks)
                
            # UI ì—…ë°ì´íŠ¸
            self.after(0, self._on_documents_processed, processed_docs)
            
        except Exception as e:
            self.after(0, self._on_document_error, str(e))
            
    def _on_documents_processed(self, processed_docs):
        """ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ"""
        self.hide_loading()
        
        # ë¬¸ì„œ ëª©ë¡ ì—…ë°ì´íŠ¸
        current_text = self.doc_listbox.get("1.0", "end").strip()
        for doc in processed_docs:
            new_text = f"ğŸ“„ {doc['name']} ({doc['chunks']} chunks)\n"
            if current_text:
                current_text += "\n" + new_text
            else:
                current_text = new_text
                
        self.doc_listbox.delete("1.0", "end")
        self.doc_listbox.insert("1.0", current_text)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        total_chunks = len(self.doc_processor.chunks_db)
        embedding_status = "âœ…" if self.doc_processor.embedding_model.trained else "âŒ"
        self.doc_stats.configure(
            text=f"ë¬¸ì„œ: {len(processed_docs)}ê°œ\nì²­í¬: {total_chunks}ê°œ\nì„ë² ë”©: {embedding_status}"
        )
        
        messagebox.showinfo("ì„±ê³µ", f"{len(processed_docs)}ê°œ ë¬¸ì„œê°€ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    def _on_document_error(self, error):
        """ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜"""
        self.hide_loading()
        messagebox.showerror("ì˜¤ë¥˜", f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨:\n{error}")
        
    def send_message(self):
        """ë©”ì‹œì§€ ì „ì†¡"""
        content = self.input_text.get("1.0", "end").strip()
        if not content or not self.model_manager:
            return
            
        # ì…ë ¥ ì´ˆê¸°í™”
        self.input_text.delete("1.0", "end")
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        self.add_message("user", content)
        
        # UI ìƒíƒœ ë³€ê²½
        self.send_btn.configure(state="disabled")
        self.stop_btn.configure(state="normal")
        
        # RAG ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„±
        threading.Thread(
            target=self._generate_rag_response,
            args=(content,),
            daemon=True
        ).start()
        
    def _generate_rag_response(self, query: str):
        """RAG ì‘ë‹µ ìƒì„±"""
        try:
            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            relevant_chunks = []
            sources = []
            
            if self.doc_processor.chunks_db:
                search_results = self.doc_processor.search(
                    query, 
                    k=self.rag_config.max_chunks_per_query
                )
                
                for chunk, score in search_results:
                    relevant_chunks.append(f"[ê´€ë ¨ë„: {score:.2f}]\n{chunk.content}")
                    source = chunk.metadata.get("source", "Unknown")
                    if source not in sources:
                        sources.append(source)
                        
            # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
            context = "\n\n---\n\n".join(relevant_chunks) if relevant_chunks else "ì°¸ì¡°í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
            
            # ëŒ€í™” ê¸°ë¡ í¬ë§·íŒ…
            chat_history = ""
            for msg in self.messages[-6:]:  # ìµœê·¼ 6ê°œ ë©”ì‹œì§€
                if msg["role"] == "user":
                    chat_history += f"ì‚¬ìš©ì: {msg['content']}\n"
                elif msg["role"] == "assistant":
                    chat_history += f"AI: {msg['content']}\n"
                    
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ
            template = RAGPromptTemplate.KOREAN_TECH_RAG
            if "ì¿¼ë¦¬" in query.lower() or "sql" in query.lower():
                template = RAGPromptTemplate.QUERY_FOCUSED_RAG
                
            prompt = template.value.format(
                system_prompt="í•œê¸€ ê¸°ìˆ  ë¬¸ì„œë¥¼ ì •í™•í•˜ê²Œ ë¶„ì„í•˜ê³  ë‹µë³€í•©ë‹ˆë‹¤.",
                context=context,
                chat_history=chat_history,
                user_message=query
            )
            
            # 3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            full_response = ""
            msg_label = None
            
            stream = self.model_manager(
                prompt,
                max_tokens=2048,
                temperature=0.3,  # ì •í™•ë„ ìš°ì„ 
                top_p=0.9,
                top_k=40,
                repeat_penalty=1.1,
                stream=True,
                stop=["ì‚¬ìš©ì:", "Human:", "User:", "\n\n\n"]
            )
            
            for output in stream:
                token = output['choices'][0]['text']
                full_response += token
                
                # UI ì—…ë°ì´íŠ¸
                if msg_label is None:
                    metadata = {"sources": sources} if sources else None
                    self.after(0, lambda: setattr(self, '_temp_label', 
                        self.add_message("assistant", token, streaming=True, metadata=metadata)))
                    time.sleep(0.1)
                    msg_label = getattr(self, '_temp_label', None)
                else:
                    self.after(0, lambda t=full_response: msg_label.configure(text=t))
                    
            # ìµœì¢… ë©”ì‹œì§€ ì €ì¥
            self.messages.append({
                "role": "assistant",
                "content": full_response,
                "timestamp": datetime.now(),
                "metadata": {"sources": sources} if sources else None
            })
            
        except Exception as e:
            error_msg = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            self.after(0, lambda: self.add_message("assistant", error_msg))
            
        finally:
            # UI ìƒíƒœ ë³µì›
            self.after(0, self._reset_ui_state)
            
    def stop_generation(self):
        """ìƒì„± ì¤‘ì§€"""
        # êµ¬í˜„ í•„ìš”
        pass
        
    def _reset_ui_state(self):
        """UI ìƒíƒœ ì´ˆê¸°í™”"""
        self.send_btn.configure(state="normal")
        self.stop_btn.configure(state="disabled")
        
    def clear_chat(self):
        """ëŒ€í™” ì´ˆê¸°í™”"""
        if messagebox.askyesno("í™•ì¸", "ëŒ€í™”ë¥¼ ì´ˆê¸°í™”í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            self.messages.clear()
            for widget in self.chat_scroll.winfo_children():
                widget.destroy()
            self.add_message("assistant", "ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
    def clear_documents(self):
        """ë¬¸ì„œ ì´ˆê¸°í™”"""
        if messagebox.askyesno("í™•ì¸", "ëª¨ë“  ë¬¸ì„œë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            self.doc_processor.chunks_db.clear()
            self.doc_processor.index = None
            self.doc_processor.tfidf_matrix = None
            self.doc_processor.embedding_model = KoreanOfflineEmbedding(self.rag_config.embedding_dimension)
            self.doc_listbox.delete("1.0", "end")
            self.doc_stats.configure(text="ë¬¸ì„œ: 0ê°œ\nì²­í¬: 0ê°œ\nì„ë² ë”©: âŒ")
            messagebox.showinfo("ì™„ë£Œ", "ëª¨ë“  ë¬¸ì„œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
    def save_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì €ì¥"""
        if not self.doc_processor.embedding_model.trained:
            messagebox.showwarning("ê²½ê³ ", "í•™ìŠµëœ ì„ë² ë”© ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        filepath = filedialog.asksaveasfilename(
            title="ì„ë² ë”© ëª¨ë¸ ì €ì¥",
            defaultextension=".pkl",
            filetypes=[("Pickle Files", "*.pkl"), ("All Files", "*.*")]
        )
        
        if filepath:
            self.doc_processor.save_embedding_model(filepath)
            messagebox.showinfo("ì„±ê³µ", "ì„ë² ë”© ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            
    def load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        filepath = filedialog.askopenfilename(
            title="ì„ë² ë”© ëª¨ë¸ ë¡œë“œ",
            filetypes=[("Pickle Files", "*.pkl"), ("All Files", "*.*")]
        )
        
        if filepath:
            success = self.doc_processor.initialize_embedding_model(filepath)
            if success:
                messagebox.showinfo("ì„±ê³µ", "ì„ë² ë”© ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                self.doc_stats.configure(
                    text=self.doc_stats.cget("text").replace("ì„ë² ë”©: âŒ", "ì„ë² ë”©: âœ…")
                )
            else:
                messagebox.showerror("ì˜¤ë¥˜", "ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                
    def update_chunk_size(self, value):
        """ì²­í¬ í¬ê¸° ì—…ë°ì´íŠ¸"""
        self.chunk_size_label.configure(text=str(int(value)))
        self.rag_config.chunk_size = int(value)
        self.doc_processor.text_splitter.chunk_size = int(value)
        
    def update_max_chunks(self, value):
        """ìµœëŒ€ ì²­í¬ ìˆ˜ ì—…ë°ì´íŠ¸"""
        self.max_chunks_label.configure(text=str(int(value)))
        self.rag_config.max_chunks_per_query = int(value)
        
    def toggle_hybrid_search(self):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í† ê¸€"""
        self.rag_config.use_hybrid_search = self.hybrid_search_var.get()
        
    def show_loading(self, message):
        """ë¡œë”© í‘œì‹œ"""
        self.loading_window = ctk.CTkToplevel(self)
        self.loading_window.title("ì²˜ë¦¬ ì¤‘")
        self.loading_window.geometry("300x150")
        self.loading_window.transient(self)
        self.loading_window.grab_set()
        
        # ì¤‘ì•™ ì •ë ¬
        self.loading_window.update_idletasks()
        x = (self.loading_window.winfo_screenwidth() // 2) - 150
        y = (self.loading_window.winfo_screenheight() // 2) - 75
        self.loading_window.geometry(f"+{x}+{y}")
        
        ctk.CTkLabel(
            self.loading_window,
            text=message,
            font=("Arial", 16)
        ).pack(pady=30)
        
        progress = ctk.CTkProgressBar(self.loading_window, mode="indeterminate")
        progress.pack(padx=40)
        progress.start()
        
    def hide_loading(self):
        """ë¡œë”© ìˆ¨ê¸°ê¸°"""
        if hasattr(self, 'loading_window'):
            self.loading_window.destroy()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    if not LLAMA_CPP_AVAILABLE:
        root = tk.Tk()
        root.withdraw()
        messagebox.showerror(
            "Error",
            "llama-cpp-python is not installed.\n\n"
            "Please install it using:\n"
            "pip install llama-cpp-python"
        )
        return
        
    if not RAG_LIBS_AVAILABLE:
        root = tk.Tk()
        root.withdraw()
        messagebox.showerror(
            "Error",
            "RAG libraries are not installed.\n\n"
            "Please install them using:\n"
            "pip install python-docx faiss-cpu scikit-learn numpy"
        )
        return
        
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    for dir_name in ["models", "documents", "embeddings", "logs"]:
        Path(dir_name).mkdir(exist_ok=True)
        
    # ì•± ì‹¤í–‰
    app = KoreanRAGChat()
    app.mainloop()

if __name__ == "__main__":
    main()