# test_phi3_fixed.py
"""
Phi-3.1 ëª¨ë¸ í…ŒìŠ¤íŠ¸ - integer divide by zero ì˜¤ë¥˜ í•´ê²°
"""

import os
import sys
import warnings

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore")

# ===== FILENO íŒ¨ì¹˜ =====
if not hasattr(sys.stdout, 'fileno'):
    sys.stdout.fileno = lambda: 1
if not hasattr(sys.stderr, 'fileno'):
    sys.stderr.fileno = lambda: 2

# í™˜ê²½ ë³€ìˆ˜
os.environ['LLAMA_CPP_VERBOSE'] = '0'

def test_minimal():
    """ìµœì†Œí•œì˜ í…ŒìŠ¤íŠ¸"""
    
    MODEL_PATH = r"D:/LLM_MODEL/GGUF/Phi-3.1-mini-4k-instruct-IQ2_M.gguf"
    
    print("="*60)
    print("Phi-3.1 ëª¨ë¸ ìµœì†Œ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # 1. íŒŒì¼ í™•ì¸
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ íŒŒì¼ ì—†ìŒ: {MODEL_PATH}")
        return None
    
    file_size = os.path.getsize(MODEL_PATH) / (1024**3)
    print(f"âœ… íŒŒì¼ í¬ê¸°: {file_size:.2f} GB")
    
    # 2. llama-cpp-python ë²„ì „ í™•ì¸
    try:
        import llama_cpp
        print(f"âœ… llama-cpp-python ë²„ì „: {llama_cpp.__version__}")
        
        if llama_cpp.__version__ == "0.2.20":
            print("âš ï¸ 0.2.20 ë²„ì „ì—ì„œ IQ2_M ì§€ì› ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("ë‹¤ë¥¸ ë²„ì „ì„ ì‹œë„í•´ë³´ì„¸ìš”:")
            print("  pip install llama-cpp-python==0.2.32")
            print("  ë˜ëŠ”")
            print("  pip install llama-cpp-python==0.2.11")
    except:
        print("âŒ llama-cpp-python ì„¤ì¹˜ í•„ìš”")
        return None
    
    # 3. ì—¬ëŸ¬ ì„¤ì •ìœ¼ë¡œ ì‹œë„
    from llama_cpp import Llama
    
    configs = [
        # ì„¤ì • 1: ìµœì†Œ ì„¤ì •
        {
            "n_ctx": 512,
            "n_threads": 1,
            "n_gpu_layers": 0,
            "verbose": False,
            "use_mlock": False,
            "use_mmap": False
        },
        # ì„¤ì • 2: mmap í™œì„±í™”
        {
            "n_ctx": 512,
            "n_threads": 2,
            "n_gpu_layers": 0,
            "verbose": False,
            "use_mlock": False,
            "use_mmap": True
        },
        # ì„¤ì • 3: ê¸°ë³¸ê°’ë§Œ
        {
            "n_ctx": 512,
            "verbose": False
        }
    ]
    
    for i, config in enumerate(configs, 1):
        print(f"\nì‹œë„ {i}: {config}")
        try:
            llm = Llama(
                model_path=MODEL_PATH,
                **config
            )
            print(f"âœ… ì„¤ì • {i} ì„±ê³µ!")
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            result = llm("Hello", max_tokens=10, temperature=0.1)
            print(f"í…ŒìŠ¤íŠ¸ ì¶œë ¥: {result['choices'][0]['text'][:50]}")
            
            return llm
            
        except Exception as e:
            print(f"âŒ ì„¤ì • {i} ì‹¤íŒ¨: {e}")
            continue
    
    return None

def test_different_model():
    """ë‹¤ë¥¸ quantization í…ŒìŠ¤íŠ¸"""
    
    base_path = r"D:/LLM_MODEL/GGUF/"
    
    # ê°€ëŠ¥í•œ ë‹¤ë¥¸ íŒŒì¼ë“¤
    possible_files = [
        "Phi-3.1-mini-4k-instruct-Q4_K_M.gguf",
        "Phi-3.1-mini-4k-instruct-Q5_K_M.gguf",
        "Phi-3.1-mini-4k-instruct-Q8_0.gguf",
        "Phi-3.1-mini-4k-instruct.gguf"
    ]
    
    print("\në‹¤ë¥¸ Phi-3 ëª¨ë¸ ì°¾ê¸°:")
    for filename in possible_files:
        full_path = os.path.join(base_path, filename)
        if os.path.exists(full_path):
            print(f"âœ… ë°œê²¬: {filename}")
            print(f"   ì´ íŒŒì¼ë¡œ ì‹œë„í•´ë³´ì„¸ìš”: {full_path}")
    
    # ì „ì²´ í´ë” ìŠ¤ìº”
    print("\nGGUF í´ë”ì˜ ëª¨ë“  .gguf íŒŒì¼:")
    if os.path.exists(base_path):
        for file in os.listdir(base_path):
            if file.endswith('.gguf'):
                size = os.path.getsize(os.path.join(base_path, file)) / (1024**3)
                print(f"  - {file} ({size:.2f} GB)")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸ” ë¬¸ì œ ì§„ë‹¨ ì‹œì‘\n")
    
    # 1. Python ë° ì‹œìŠ¤í…œ ì •ë³´
    import platform
    print(f"ì‹œìŠ¤í…œ: {platform.system()} {platform.release()}")
    print(f"Python: {platform.python_version()}")
    print(f"ì•„í‚¤í…ì²˜: {platform.machine()}")
    
    # 2. ìµœì†Œ í…ŒìŠ¤íŠ¸
    llm = test_minimal()
    
    if llm:
        print("\n" + "="*60)
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        print("="*60)
        
        # ëŒ€í™” í…ŒìŠ¤íŠ¸
        while True:
            user_input = input("\ní…ŒìŠ¤íŠ¸ ì…ë ¥ (quit ì¢…ë£Œ): ").strip()
            if user_input.lower() == 'quit':
                break
            
            try:
                # Phi-3 í˜•ì‹
                prompt = f"<|user|>\n{user_input}<|end|>\n<|assistant|>"
                result = llm(prompt, max_tokens=100, stop=["<|end|>"])
                print(f"ì‘ë‹µ: {result['choices'][0]['text']}")
            except Exception as e:
                print(f"ì˜¤ë¥˜: {e}")
    else:
        print("\n" + "="*60)
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        print("="*60)
        
        print("\nğŸ“ í•´ê²° ë°©ë²•:")
        print("\n1. llama-cpp-python ë²„ì „ ë³€ê²½:")
        print("   pip uninstall llama-cpp-python -y")
        print("   pip install llama-cpp-python==0.2.32 --no-cache-dir")
        print("   ë˜ëŠ”")
        print("   pip install llama-cpp-python==0.2.11 --no-cache-dir")
        
        print("\n2. ë‹¤ë¥¸ quantization ì‚¬ìš©:")
        test_different_model()
        
        print("\n3. ìµœì‹  ë²„ì „ ì‹œë„:")
        print("   pip install llama-cpp-python --upgrade --no-cache-dir")
        
        print("\n4. CPU ì „ìš© ë¹Œë“œ:")
        print("   pip install llama-cpp-python --force-reinstall --no-cache-dir")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()