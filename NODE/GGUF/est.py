# test_phi3.py
"""
Phi-3.1-mini-4k-instruct ëª¨ë¸ ì „ìš© í…ŒìŠ¤íŠ¸
fileno ì˜¤ë¥˜ ì™„ë²½ í•´ê²° ë²„ì „
"""

import os
import sys
import warnings

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore")

# ===== FILENO íŒ¨ì¹˜ (ê°€ì¥ ì¤‘ìš”!) =====
class FakeStream:
    def __init__(self, original_stream):
        self.original = original_stream
        self.buffer = self
        
    def write(self, s):
        if self.original:
            try:
                return self.original.write(s)
            except:
                pass
        return len(s) if s else 0
    
    def flush(self):
        if self.original:
            try:
                self.original.flush()
            except:
                pass
    
    def fileno(self):
        return 1 if 'out' in str(self.original) else 2
    
    def isatty(self):
        return False
    
    def readable(self):
        return False
    
    def writable(self):
        return True
    
    def seekable(self):
        return False

# stdout/stderr êµì²´
original_stdout = sys.stdout
original_stderr = sys.stderr
sys.stdout = FakeStream(original_stdout)
sys.stderr = FakeStream(original_stderr)

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['LLAMA_CPP_VERBOSE'] = '0'
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

# ===== ëª¨ë¸ í…ŒìŠ¤íŠ¸ =====
def test_phi3_model():
    """Phi-3.1 ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    
    # ëª¨ë¸ ê²½ë¡œ
    MODEL_PATH = r"D:/LLM_MODEL/GGUF/Phi-3.1-mini-4k-instruct-IQ2_M.gguf"
    
    print("="*60)
    print("Phi-3.1-mini-4k-instruct ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # íŒŒì¼ í™•ì¸
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_PATH}")
        return False
    
    file_size = os.path.getsize(MODEL_PATH) / (1024**3)
    print(f"âœ… ëª¨ë¸ íŒŒì¼ ë°œê²¬: {os.path.basename(MODEL_PATH)}")
    print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size:.2f} GB")
    
    # GGUF í™•ì¸
    with open(MODEL_PATH, 'rb') as f:
        header = f.read(4)
        if header == b'GGUF':
            print("âœ… GGUF í˜•ì‹ í™•ì¸")
        else:
            print(f"âŒ GGUF í˜•ì‹ì´ ì•„ë‹˜: {header}")
            return False
    
    # llama_cpp ì„í¬íŠ¸
    print("\nâ³ llama_cpp ëª¨ë“ˆ ë¡œë“œ ì¤‘...")
    try:
        from llama_cpp import Llama
        import llama_cpp
        print(f"âœ… llama-cpp-python ë²„ì „: {llama_cpp.__version__}")
    except ImportError as e:
        print(f"âŒ llama_cpp ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        print("\nì„¤ì¹˜ ëª…ë ¹ì–´:")
        print("pip install llama-cpp-python==0.2.20")
        return False
    
    # ëª¨ë¸ ë¡œë“œ
    print("\nâ³ Phi-3.1 ëª¨ë¸ ë¡œë“œ ì¤‘... (1-2ë¶„ ì†Œìš”)")
    try:
        llm = Llama(
            model_path=MODEL_PATH,
            n_ctx=4096,         # Phi-3.1ì€ 4k ì»¨í…ìŠ¤íŠ¸
            n_threads=4,        # CPU ìŠ¤ë ˆë“œ
            n_gpu_layers=0,     # GPU ë¹„í™œì„±í™” (CPU ëª¨ë“œ)
            seed=42,
            verbose=False,
            use_mlock=False,
            use_mmap=True,      # ë©”ëª¨ë¦¬ ë§µ ì‚¬ìš© (ë” ë¹ ë¦„)
            n_batch=512,        # ë°°ì¹˜ í¬ê¸°
            f16_kv=False,
            logits_all=False,
            vocab_only=False,
            embedding=False,
            rope_scaling_type=-1,  # RoPE ìŠ¤ì¼€ì¼ë§ ë¹„í™œì„±í™”
            rope_freq_base=0,
            rope_freq_scale=0,
            numa=False          # NUMA ë¹„í™œì„±í™”
        )
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        print("\nìƒì„¸ ì˜¤ë¥˜:")
        traceback.print_exc()
        return False
    
    # í…ŒìŠ¤íŠ¸ ëŒ€í™”
    print("\n" + "="*60)
    print("ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # Phi-3 í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸
    test_prompts = [
        "Hello! How are you?",
        "What is 2+2?",
        "Tell me a short joke."
    ]
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\ní…ŒìŠ¤íŠ¸ {i}:")
        print(f"Q: {prompt}")
        
        # Phi-3 instruction í˜•ì‹
        formatted_prompt = f"<|user|>\n{prompt}<|end|>\n<|assistant|>"
        
        try:
            response = llm(
                formatted_prompt,
                max_tokens=50,
                temperature=0.7,
                top_p=0.95,
                stop=["<|end|>", "<|user|>"],
                echo=False
            )
            
            answer = response['choices'][0]['text'].strip()
            print(f"A: {answer}")
            
        except Exception as e:
            print(f"âŒ ìƒì„± ì˜¤ë¥˜: {e}")
    
    print("\n" + "="*60)
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*60)
    
    return True, llm

def interactive_mode(llm):
    """ëŒ€í™” ëª¨ë“œ"""
    print("\nğŸ’¬ ëŒ€í™” ëª¨ë“œ ì‹œì‘ (ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit')")
    print("-"*60)
    
    conversation_history = []
    
    while True:
        user_input = input("\nYou: ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            print("ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if not user_input:
            continue
        
        # Phi-3 í”„ë¡¬í”„íŠ¸ í˜•ì‹
        if len(conversation_history) > 0:
            # ì´ì „ ëŒ€í™” í¬í•¨
            context = ""
            for h in conversation_history[-4:]:  # ìµœê·¼ 4ê°œ ëŒ€í™”
                context += h + "\n"
            prompt = f"{context}<|user|>\n{user_input}<|end|>\n<|assistant|>"
        else:
            prompt = f"<|user|>\n{user_input}<|end|>\n<|assistant|>"
        
        print("Phi-3: ", end="", flush=True)
        
        try:
            response = llm(
                prompt,
                max_tokens=200,
                temperature=0.7,
                top_p=0.95,
                stop=["<|end|>", "<|user|>", "\n\n"],
                stream=False
            )
            
            answer = response['choices'][0]['text'].strip()
            print(answer)
            
            # ëŒ€í™” ê¸°ë¡
            conversation_history.append(f"<|user|>\n{user_input}<|end|>")
            conversation_history.append(f"<|assistant|>\n{answer}<|end|>")
            
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\nğŸš€ Phi-3.1-mini-4k-instruct ëª¨ë¸ í…ŒìŠ¤í„°")
    print("ëª¨ë¸: D:/LLM_MODEL/GGUF/Phi-3.1-mini-4k-instruct-IQ2_M.gguf")
    
    # ì‹œìŠ¤í…œ ì •ë³´
    import platform
    print(f"\nì‹œìŠ¤í…œ: {platform.system()} {platform.release()}")
    print(f"Python: {platform.python_version()}")
    
    # ëª¨ë¸ í…ŒìŠ¤íŠ¸
    success, llm = test_phi3_model()
    
    if success:
        print("\n" + "ğŸ‰"*20)
        print("ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!")
        print("ğŸ‰"*20)
        
        # ëŒ€í™” ëª¨ë“œ ì œì•ˆ
        answer = input("\nëŒ€í™” ëª¨ë“œë¡œ ì§„ì…í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
        if answer == 'y':
            interactive_mode(llm)
    else:
        print("\nëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("\ní•´ê²° ë°©ë²•:")
        print("1. llama-cpp-python ë²„ì „ ë³€ê²½:")
        print("   pip install llama-cpp-python==0.2.20 --force-reinstall")
        print("2. ë‹¤ë¥¸ Phi-3 quantization ì‹œë„ (Q4_K_M ë“±)")
        print("3. Python 3.10ìœ¼ë¡œ ì‹œë„")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # stdout ë³µì›
        sys.stdout = original_stdout
        sys.stderr = original_stderr