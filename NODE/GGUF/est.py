# final_test.py
"""
ìµœì¢… í…ŒìŠ¤íŠ¸ - llama-cpp-python 0.3.16ìš©
ì´ê²ƒë„ ì•ˆ ë˜ë©´ ë‹¤ë¥¸ ë°©ë²• ìˆìŠµë‹ˆë‹¤!
"""

import os
import sys

# ===== 1. ìµœê°• FILENO íŒ¨ì¹˜ =====
# stdout/stderrë¥¼ ì™„ì „íˆ ëŒ€ì²´
import io

class DummyFile:
    def write(self, x): return len(x) if x else 0
    def flush(self): pass
    def fileno(self): return 1
    def isatty(self): return False
    def readable(self): return False
    def writable(self): return True
    def seekable(self): return False
    def close(self): pass
    def __enter__(self): return self
    def __exit__(self, *args): pass

# ë°±ì—…
original_stdout = sys.stdout
original_stderr = sys.stderr

# ì™„ì „ ëŒ€ì²´
sys.stdout = DummyFile()
sys.stderr = DummyFile()

# print í•¨ìˆ˜ ì¬ì •ì˜
def safe_print(*args, **kwargs):
    """ì•ˆì „í•œ print"""
    message = ' '.join(str(arg) for arg in args)
    original_stdout.write(message + '\n')
    original_stdout.flush()

# ===== 2. ëª¨ë¸ í…ŒìŠ¤íŠ¸ =====
def test_model():
    MODEL_PATH = r"D:/LLM_MODEL/GGUF/Phi-3.1-mini-4k-instruct-IQ2_M.gguf"
    
    safe_print("="*60)
    safe_print("ğŸš€ llama-cpp-python 0.3.16 í…ŒìŠ¤íŠ¸")
    safe_print("="*60)
    
    # íŒŒì¼ í™•ì¸
    if not os.path.exists(MODEL_PATH):
        safe_print(f"âŒ íŒŒì¼ ì—†ìŒ: {MODEL_PATH}")
        return False
    
    file_size = os.path.getsize(MODEL_PATH) / (1024**3)
    safe_print(f"âœ… íŒŒì¼ í¬ê¸°: {file_size:.2f} GB")
    
    # llama_cpp ì„í¬íŠ¸
    try:
        from llama_cpp import Llama
        import llama_cpp
        safe_print(f"âœ… llama-cpp-python ë²„ì „: {llama_cpp.__version__}")
    except ImportError as e:
        safe_print(f"âŒ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        return False
    
    # ì—¬ëŸ¬ ì„¤ì • ì‹œë„
    configs = [
        # ì„¤ì • 1: 0.3.x ë²„ì „ ê¸°ë³¸
        {
            "n_ctx": 2048,
            "n_batch": 512,
            "verbose": False,
        },
        # ì„¤ì • 2: ìµœì†Œ ì„¤ì •
        {
            "n_ctx": 512,
            "n_batch": 128,
            "n_threads": 4,
            "verbose": False,
            "use_mlock": False,
        },
        # ì„¤ì • 3: í˜¸í™˜ì„± ëª¨ë“œ
        {
            "n_ctx": 1024,
            "verbose": False,
            "n_gpu_layers": 0,
            "seed": 1337,
            "f16_kv": True,
            "logits_all": False,
            "vocab_only": False,
            "use_mmap": True,
            "use_mlock": False,
        }
    ]
    
    for i, config in enumerate(configs, 1):
        safe_print(f"\nì‹œë„ {i}/{len(configs)}...")
        try:
            llm = Llama(
                model_path=MODEL_PATH,
                **config
            )
            safe_print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ! (ì„¤ì • {i})")
            
            # í…ŒìŠ¤íŠ¸
            safe_print("\ní…ŒìŠ¤íŠ¸ ìƒì„± ì¤‘...")
            result = llm(
                "Hello, how are you?",
                max_tokens=20,
                temperature=0.7,
                echo=False
            )
            
            response = result['choices'][0]['text']
            safe_print(f"ì‘ë‹µ: {response[:100]}")
            
            return llm
            
        except Exception as e:
            safe_print(f"âŒ ì„¤ì • {i} ì‹¤íŒ¨: {str(e)[:100]}")
    
    return None

def alternative_solution():
    """ëŒ€ì²´ ì†”ë£¨ì…˜ ì œì•ˆ"""
    safe_print("\n" + "="*60)
    safe_print("ğŸ’¡ ëŒ€ì²´ ì†”ë£¨ì…˜")
    safe_print("="*60)
    
    safe_print("""
1. ğŸ”§ Ollama ì‚¬ìš© (ê°€ì¥ ì‰¬ì›€):
   - https://ollama.ai ë‹¤ìš´ë¡œë“œ
   - ollama pull phi3
   - ollama run phi3
   
2. ğŸ¤— Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬:
   pip install transformers torch
   
3. ğŸ“¦ LM Studio (GUI):
   - https://lmstudio.ai ë‹¤ìš´ë¡œë“œ
   - GGUF íŒŒì¼ ë“œë˜ê·¸ ì•¤ ë“œë¡­
   
4. ğŸ–¥ï¸ koboldcpp (GUI + API):
   - https://github.com/LostRuins/koboldcpp/releases
   - exe íŒŒì¼ ì‹¤í–‰ í›„ ëª¨ë¸ ë¡œë“œ
   
5. ğŸ ctransformers (ëŒ€ì²´ ë¼ì´ë¸ŒëŸ¬ë¦¬):
   pip install ctransformers
   
   from ctransformers import AutoModelForCausalLM
   model = AutoModelForCausalLM.from_pretrained(
       'D:/LLM_MODEL/GGUF/Phi-3.1-mini-4k-instruct-IQ2_M.gguf',
       model_type='phi3'
   )
""")

def main():
    safe_print("\nğŸ”¬ ìµœì¢… í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    # ë²„ì „ ì •ë³´
    import platform
    safe_print(f"Python: {platform.python_version()}")
    safe_print(f"OS: {platform.system()}")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    llm = test_model()
    
    if llm:
        safe_print("\n" + "ğŸ‰"*20)
        safe_print("ì„±ê³µ! ëª¨ë¸ì´ ì‘ë™í•©ë‹ˆë‹¤!")
        safe_print("ğŸ‰"*20)
        
        # ê°„ë‹¨í•œ ëŒ€í™”
        safe_print("\nê°„ë‹¨í•œ ëŒ€í™” í…ŒìŠ¤íŠ¸:")
        prompts = [
            "What is 2+2?",
            "Tell me a joke",
            "Hello!"
        ]
        
        for prompt in prompts:
            safe_print(f"\nQ: {prompt}")
            try:
                result = llm(f"User: {prompt}\nAssistant:", 
                           max_tokens=50,
                           stop=["User:", "\n\n"])
                safe_print(f"A: {result['choices'][0]['text'].strip()}")
            except:
                pass
    else:
        safe_print("\nğŸ˜” ì—¬ì „íˆ ë¬¸ì œê°€ ìˆë„¤ìš”...")
        alternative_solution()
        
        safe_print("\nğŸ¯ ì¦‰ì‹œ ì‹œë„í•  ìˆ˜ ìˆëŠ” ê²ƒ:")
        safe_print("1. Koboldcpp ë‹¤ìš´ë¡œë“œ (ê°€ì¥ ì‰¬ì›€)")
        safe_print("2. LM Studio ì„¤ì¹˜ (GUI, ì‚¬ìš© í¸í•¨)")
        safe_print("3. Ollama ì„¤ì¹˜ (ëª…ë ¹ì–´ ê°„ë‹¨)")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        safe_print(f"\nì˜¤ë¥˜: {e}")
    finally:
        # stdout ë³µì›
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        print("\ní”„ë¡œê·¸ë¨ ì¢…ë£Œ")