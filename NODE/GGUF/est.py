# console_test.py
"""
GGUF ëª¨ë¸ ì½˜ì†” í…ŒìŠ¤íŠ¸ - fileno ì˜¤ë¥˜ ì™„ë²½ í•´ê²° ë²„ì „
GUI ì—†ì´ ìˆœìˆ˜ ì½˜ì†”ì—ì„œ ì‹¤í–‰
"""

import os
import sys
import warnings

# ===== 1ë‹¨ê³„: ëª¨ë“  ê²½ê³  ë¬´ì‹œ =====
warnings.filterwarnings("ignore")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# ===== 2ë‹¨ê³„: ì™„ë²½í•œ fileno íŒ¨ì¹˜ =====
import io

# stdout ë°±ì—…
original_stdout = sys.stdout
original_stderr = sys.stderr

class FakeStdout:
    """ê°€ì§œ stdout - fileno í¬í•¨"""
    def __init__(self, original):
        self.original = original
        self.buffer = self
    
    def write(self, s):
        if self.original:
            return self.original.write(s)
        return len(s)
    
    def flush(self):
        if self.original:
            self.original.flush()
    
    def fileno(self):
        return 1
    
    def isatty(self):
        return False
    
    def readable(self):
        return False
    
    def writable(self):
        return True
    
    def seekable(self):
        return False

class FakeStderr:
    """ê°€ì§œ stderr - fileno í¬í•¨"""
    def __init__(self, original):
        self.original = original
        self.buffer = self
    
    def write(self, s):
        if self.original:
            return self.original.write(s)
        return len(s)
    
    def flush(self):
        if self.original:
            self.original.flush()
    
    def fileno(self):
        return 2
    
    def isatty(self):
        return False
    
    def readable(self):
        return False
    
    def writable(self):
        return True
    
    def seekable(self):
        return False

# stdout/stderr êµì²´
sys.stdout = FakeStdout(original_stdout)
sys.stderr = FakeStderr(original_stderr)

# ===== 3ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì • =====
os.environ['LLAMA_CPP_VERBOSE'] = '0'
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # GPU ë¹„í™œì„±í™”
os.environ['LLAMA_CPP_LOG_DISABLE'] = '1'

# ===== 4ë‹¨ê³„: llama_cpp import =====
print("=" * 60)
print("GGUF ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
print("=" * 60)

try:
    from llama_cpp import Llama
    print("âœ… llama_cpp ëª¨ë“ˆ ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ llama_cpp ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    print("\nì„¤ì¹˜ ëª…ë ¹ì–´:")
    print("pip install llama-cpp-python==0.2.20")
    sys.exit(1)

# ===== 5ë‹¨ê³„: ëª¨ë¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ =====
def test_model(model_path):
    """ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸"""
    
    # íŒŒì¼ í™•ì¸
    if not os.path.exists(model_path):
        print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
        return False
    
    # íŒŒì¼ ì •ë³´
    file_size = os.path.getsize(model_path) / (1024**3)  # GB
    print(f"\nğŸ“ ëª¨ë¸ íŒŒì¼: {os.path.basename(model_path)}")
    print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size:.2f} GB")
    
    # GGUF í—¤ë” í™•ì¸
    with open(model_path, 'rb') as f:
        header = f.read(4)
        if header != b'GGUF':
            print(f"âŒ GGUF íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤. í—¤ë”: {header}")
            return False
        print("âœ… GGUF í˜•ì‹ í™•ì¸")
    
    # ëª¨ë¸ ë¡œë“œ
    print("\nâ³ ëª¨ë¸ ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    try:
        # ìµœì†Œ ì„¤ì •ìœ¼ë¡œ ë¡œë“œ
        llm = Llama(
            model_path=model_path,
            n_ctx=512,          # ì‘ì€ ì»¨í…ìŠ¤íŠ¸
            n_threads=2,        # ì ì€ ìŠ¤ë ˆë“œ
            n_gpu_layers=0,     # GPU ë¹„í™œì„±í™”
            seed=42,            # ê³ ì • ì‹œë“œ
            verbose=False,      # ì¶œë ¥ ë¹„í™œì„±í™”
            use_mlock=False,    # mlock ë¹„í™œì„±í™”
            use_mmap=False,     # mmap ë¹„í™œì„±í™”
            n_batch=8,          # ì‘ì€ ë°°ì¹˜
            f16_kv=False,       # f16 ë¹„í™œì„±í™”
            logits_all=False,   # logits ë¹„í™œì„±í™”
            vocab_only=False,   # vocabë§Œ ë¡œë“œ ì•ˆí•¨
            embedding=False     # ì„ë² ë”© ëª¨ë“œ ë¹„í™œì„±í™”
        )
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        print("\nğŸ§ª ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
        prompt = "Hello"
        
        result = llm(
            prompt,
            max_tokens=10,
            temperature=0.1,
            top_p=0.95,
            echo=False,
            stop=[]
        )
        
        response = result['choices'][0]['text']
        print(f"ì…ë ¥: {prompt}")
        print(f"ì¶œë ¥: {response}")
        print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´
        import traceback
        print("\nìƒì„¸ ì˜¤ë¥˜:")
        print("-" * 40)
        traceback.print_exc()
        print("-" * 40)
        
        return False

# ===== 6ë‹¨ê³„: ë©”ì¸ ì‹¤í–‰ =====
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ì‹œìŠ¤í…œ ì •ë³´
    import platform
    print("\nğŸ“‹ ì‹œìŠ¤í…œ ì •ë³´:")
    print(f"  OS: {platform.system()} {platform.release()}")
    print(f"  Python: {platform.python_version()}")
    print(f"  ì•„í‚¤í…ì²˜: {platform.machine()}")
    
    # llama-cpp-python ë²„ì „
    try:
        import llama_cpp
        print(f"  llama-cpp-python: {llama_cpp.__version__}")
    except:
        pass
    
    print("\n" + "=" * 60)
    
    # ëª¨ë¸ ê²½ë¡œ ì…ë ¥
    while True:
        print("\nGGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”")
        print("(ì „ì²´ ê²½ë¡œ ì…ë ¥, ì˜ˆ: C:\\models\\llama-2-7b.gguf)")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥")
        
        model_path = input("\nê²½ë¡œ: ").strip()
        
        if model_path.lower() == 'quit':
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        # ë”°ì˜´í‘œ ì œê±°
        model_path = model_path.strip('"').strip("'")
        
        if not model_path:
            continue
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        success = test_model(model_path)
        
        if success:
            print("\n" + "ğŸ‰" * 20)
            print("ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
            print("ì´ì œ GUI ë²„ì „ë„ ì‘ë™í•  ê²ƒì…ë‹ˆë‹¤.")
            print("ğŸ‰" * 20)
            
            # ëŒ€í™” í…ŒìŠ¤íŠ¸
            answer = input("\nëŒ€í™”ë¥¼ ê³„ì† í…ŒìŠ¤íŠ¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            if answer.lower() == 'y':
                interactive_chat(model_path)
        else:
            print("\në‹¤ë¥¸ ëª¨ë¸ë¡œ ì‹œë„í•´ë³´ì„¸ìš”.")

def interactive_chat(model_path):
    """ê°„ë‹¨í•œ ëŒ€í™” í…ŒìŠ¤íŠ¸"""
    print("\nğŸ’¬ ëŒ€í™” ëª¨ë“œ (ì¢…ë£Œ: 'quit')")
    print("-" * 40)
    
    # ëª¨ë¸ ë¡œë“œ
    llm = Llama(
        model_path=model_path,
        n_ctx=2048,
        n_threads=4,
        n_gpu_layers=0,
        verbose=False,
        use_mlock=False,
        use_mmap=False
    )
    
    conversation = []
    
    while True:
        user_input = input("\nYou: ").strip()
        if user_input.lower() == 'quit':
            break
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = ""
        for msg in conversation[-4:]:  # ìµœê·¼ 4ê°œ ë©”ì‹œì§€
            prompt += msg + "\n"
        prompt += f"User: {user_input}\nAssistant:"
        
        # ì‘ë‹µ ìƒì„±
        print("Assistant: ", end="", flush=True)
        result = llm(
            prompt,
            max_tokens=200,
            temperature=0.7,
            stop=["User:", "\n\n"]
        )
        
        response = result['choices'][0]['text'].strip()
        print(response)
        
        # ëŒ€í™” ê¸°ë¡
        conversation.append(f"User: {user_input}")
        conversation.append(f"Assistant: {response}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # stdout ë³µêµ¬
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        print("\nì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")