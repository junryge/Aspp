# -*- coding: utf-8 -*-
"""
íì‡„ë§ GGUF + CSV ê²€ìƒ‰ ì‹œìŠ¤í…œ (ëŒ€ìš©ëŸ‰ ë°ì´í„° ê³ ì† ì²˜ë¦¬ ë²„ì „)
79ë§Œê°œ ë°ì´í„° ìµœì í™”
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
import logging
import time
from tqdm import tqdm

# LangChain
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.schema import Document

# GGUF
from llama_cpp import Llama

# UI
import customtkinter as ctk
from tkinter import messagebox, filedialog, ttk
import tkinter as tk

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FastCSVSearch:
    """ëŒ€ìš©ëŸ‰ CSV ê³ ì† ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # ê²½ë¡œ ì„¤ì •
        self.embedding_model_path = "./offline_models/all-MiniLM-L6-v2"
        self.csv_dir = "./output_by_date"
        self.vector_db_path = "./vector_db/faiss_index"
        
        # ëª¨ë¸
        self.embeddings = None
        self.vector_store = None
        self.llm = None
        
        # ë°ì´í„°
        self.all_data = pd.DataFrame()
        self.summary_data = {}  # ìš”ì•½ ë°ì´í„° ìºì‹œ
        
    def load_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.embedding_model_path,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'batch_size': 32}  # ë°°ì¹˜ ì²˜ë¦¬
        )
        logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
    def load_all_csv_data(self):
        """ëª¨ë“  CSV íŒŒì¼ì„ ë¹ ë¥´ê²Œ ë¡œë“œ"""
        csv_files = sorted(Path(self.csv_dir).glob("*.csv"))
        logger.info(f"ğŸ“ CSV íŒŒì¼ {len(csv_files)}ê°œ ë°œê²¬")
        
        all_dfs = []
        for csv_file in tqdm(csv_files, desc="CSV ë¡œë”©"):
            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
            df = pd.read_csv(csv_file, usecols=[
                'CURRTIME', 'TOTALCNT', 
                'M14AM10ASUM', 'M14AM14BSUM', 'M14AM16SUM'
            ])
            df['FILE_DATE'] = csv_file.stem
            all_dfs.append(df)
        
        self.all_data = pd.concat(all_dfs, ignore_index=True)
        logger.info(f"âœ… ì´ {len(self.all_data):,}ê°œ í–‰ ë¡œë“œ ì™„ë£Œ")
        
    def create_summary_statistics(self):
        """ê³ ì† ìš”ì•½ í†µê³„ ìƒì„± (ë²¡í„°í™” ì—†ì´)"""
        logger.info("ğŸ“Š ìš”ì•½ í†µê³„ ìƒì„± ì¤‘...")
        
        for date in self.all_data['FILE_DATE'].unique():
            daily_data = self.all_data[self.all_data['FILE_DATE'] == date]
            
            # ì¼ë³„ í†µê³„ ì €ì¥
            self.summary_data[date] = {
                'count': len(daily_data),
                'totalcnt_mean': daily_data['TOTALCNT'].mean(),
                'totalcnt_max': daily_data['TOTALCNT'].max(),
                'totalcnt_min': daily_data['TOTALCNT'].min(),
                'peak_time': daily_data.loc[daily_data['TOTALCNT'].idxmax(), 'CURRTIME'],
                'data': daily_data  # ì›ë³¸ ë°ì´í„° ì°¸ì¡°
            }
        
        logger.info(f"âœ… {len(self.summary_data)}ê°œ ë‚ ì§œ ìš”ì•½ ì™„ë£Œ")
    
    def create_search_documents_fast(self) -> List[Document]:
        """ì´ˆê³ ì† ë¬¸ì„œ ìƒì„± (ìš”ì•½ë§Œ ë²¡í„°í™”)"""
        documents = []
        
        logger.info("ğŸš€ ê³ ì† ë¬¸ì„œ ìƒì„± ì‹œì‘")
        
        # 1. ì¼ë³„ ìš”ì•½ë§Œ ë²¡í„°í™” (79ë§Œê°œ â†’ ì•½ 300ê°œ)
        for date, stats in tqdm(self.summary_data.items(), desc="ë¬¸ì„œ ìƒì„±"):
            # ì¼ë³„ ìš”ì•½
            daily_text = f"""
ë‚ ì§œ: {date}
ë°ì´í„° ìˆ˜: {stats['count']:,}ê°œ
TOTALCNT í‰ê· : {stats['totalcnt_mean']:.0f}
TOTALCNT ìµœëŒ€: {stats['totalcnt_max']} (ì‹œê°„: {stats['peak_time']})
TOTALCNT ìµœì†Œ: {stats['totalcnt_min']}
"""
            documents.append(Document(
                page_content=daily_text,
                metadata={"date": date, "type": "daily"}
            ))
            
            # ì‹œê°„ëŒ€ë³„ ìš”ì•½ (ì„ íƒì )
            if stats['count'] > 100:  # ë°ì´í„°ê°€ ë§ì€ ë‚ ë§Œ
                hourly = stats['data'].copy()
                hourly['hour'] = hourly['CURRTIME'].astype(str).str[8:10]
                
                for hour in hourly['hour'].unique():
                    hour_data = hourly[hourly['hour'] == hour]
                    if len(hour_data) > 5:  # 5ê°œ ì´ìƒ ë°ì´í„°ê°€ ìˆëŠ” ì‹œê°„ë§Œ
                        hour_text = f"""
ë‚ ì§œ: {date}
ì‹œê°„ëŒ€: {hour}ì‹œ
TOTALCNT í‰ê· : {hour_data['TOTALCNT'].mean():.0f}
TOTALCNT ìµœëŒ€: {hour_data['TOTALCNT'].max()}
ë°ì´í„° ìˆ˜: {len(hour_data)}ê°œ
"""
                        documents.append(Document(
                            page_content=hour_text,
                            metadata={"date": date, "hour": hour, "type": "hourly"}
                        ))
        
        logger.info(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ìƒì„± ì™„ë£Œ (ì›ë³¸ ëŒ€ë¹„ {len(documents)/len(self.all_data)*100:.2f}%)")
        return documents
    
    def build_vector_db_fast(self):
        """ê³ ì† ë²¡í„° DB êµ¬ì¶•"""
        start_time = time.time()
        
        # 1. CSV ë¡œë“œ
        self.load_all_csv_data()
        
        # 2. ìš”ì•½ í†µê³„ ìƒì„±
        self.create_summary_statistics()
        
        # 3. ë¬¸ì„œ ìƒì„± (ìš”ì•½ë§Œ)
        documents = self.create_search_documents_fast()
        
        # 4. FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        logger.info("ğŸ”¨ ë²¡í„° DB êµ¬ì¶• ì¤‘...")
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë¹ ë¥´ê²Œ
        batch_size = 100
        if len(documents) > batch_size:
            # ì²« ë°°ì¹˜ë¡œ ì´ˆê¸°í™”
            self.vector_store = FAISS.from_documents(
                documents[:batch_size], 
                self.embeddings
            )
            # ë‚˜ë¨¸ì§€ ë°°ì¹˜ ì¶”ê°€
            for i in tqdm(range(batch_size, len(documents), batch_size), desc="ë²¡í„°í™”"):
                batch = documents[i:i+batch_size]
                self.vector_store.add_documents(batch)
        else:
            self.vector_store = FAISS.from_documents(documents, self.embeddings)
        
        # 5. ì €ì¥
        Path("./vector_db").mkdir(exist_ok=True)
        self.vector_store.save_local(self.vector_db_path)
        
        elapsed = time.time() - start_time
        logger.info(f"âœ… ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ ({elapsed:.1f}ì´ˆ ì†Œìš”)")
        
    def search_direct_data(self, query: str) -> str:
        """ì§ì ‘ ë°ì´í„° ê²€ìƒ‰ (ë²¡í„° ê²€ìƒ‰ + ì‹¤ì œ ë°ì´í„°)"""
        # ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë‚ ì§œ/ì‹œê°„ ì°¾ê¸°
        if self.vector_store:
            docs = self.vector_store.similarity_search(query, k=3)
            relevant_dates = [doc.metadata.get('date') for doc in docs]
            
            # í•´ë‹¹ ë‚ ì§œì˜ ì‹¤ì œ ë°ì´í„°ì—ì„œ ê²€ìƒ‰
            results = []
            for date in relevant_dates:
                if date in self.summary_data:
                    data = self.summary_data[date]['data']
                    
                    # ì¿¼ë¦¬ì— ë”°ë¥¸ í•„í„°ë§
                    if "ìµœëŒ€" in query or "max" in query.lower():
                        max_row = data.loc[data['TOTALCNT'].idxmax()]
                        results.append(f"[{date}] ìµœëŒ€ê°’: {max_row['TOTALCNT']} (ì‹œê°„: {max_row['CURRTIME']})")
                    
                    elif "1450" in query or "1500" in query:
                        threshold = 1450 if "1450" in query else 1500
                        filtered = data[data['TOTALCNT'] >= threshold]
                        if not filtered.empty:
                            results.append(f"[{date}] {threshold} ì´ìƒ: {len(filtered)}ê°œ ì‹œê°„ëŒ€")
                            results.append(f"  ì‹œê°„: {filtered['CURRTIME'].head(5).tolist()}")
                    
                    elif "í‰ê· " in query:
                        avg = data['TOTALCNT'].mean()
                        results.append(f"[{date}] í‰ê· : {avg:.0f}")
            
            return "\n".join(results) if results else "ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        return "ë²¡í„° DBê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    def load_vector_db(self):
        """ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ"""
        if Path(self.vector_db_path).exists():
            self.vector_store = FAISS.load_local(
                self.vector_db_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            logger.info("âœ… ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ ì™„ë£Œ")
            
            # ìš”ì•½ ë°ì´í„°ë„ ë¡œë“œ (ìˆìœ¼ë©´)
            summary_path = Path("./vector_db/summary_cache.pkl")
            if summary_path.exists():
                import pickle
                with open(summary_path, 'rb') as f:
                    self.summary_data = pickle.load(f)
                logger.info("âœ… ìš”ì•½ ìºì‹œ ë¡œë“œ ì™„ë£Œ")
            else:
                # ìš”ì•½ ë°ì´í„° ì¬ìƒì„±
                self.load_all_csv_data()
                self.create_summary_statistics()
                # ìºì‹œ ì €ì¥
                with open(summary_path, 'wb') as f:
                    pickle.dump(self.summary_data, f)
            
            return True
        return False
        
    def load_gguf_model(self, model_path: str):
        """GGUF ëª¨ë¸ ë¡œë“œ"""
        logger.info(f"ğŸ¤– GGUF ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
        
        self.llm = Llama(
            model_path=model_path,
            n_ctx=8192,
            n_gpu_layers=35,  # Q6_Kìš©
            n_threads=8,
            verbose=False
        )
        
        logger.info("âœ… GGUF ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

class FastUI(ctk.CTk):
    """ê³ ì† ì²˜ë¦¬ UI"""
    
    def __init__(self):
        super().__init__()
        
        self.title("ëŒ€ìš©ëŸ‰ CSV ê³ ì† ê²€ìƒ‰ ì‹œìŠ¤í…œ")
        self.geometry("1000x700")
        
        ctk.set_appearance_mode("dark")
        
        # ê²€ìƒ‰ ì‹œìŠ¤í…œ
        self.search_system = FastCSVSearch()
        
        # UI êµ¬ì„±
        self.setup_ui()
        
    def setup_ui(self):
        """UI êµ¬ì„±"""
        # ìƒë‹¨ ì •ë³´
        info_frame = ctk.CTkFrame(self)
        info_frame.pack(fill="x", padx=10, pady=10)
        
        self.info_label = ctk.CTkLabel(
            info_frame,
            text="ğŸ’¡ 79ë§Œê°œ ë°ì´í„°ë¥¼ ìš”ì•½í•˜ì—¬ ë¹ ë¥´ê²Œ ê²€ìƒ‰í•©ë‹ˆë‹¤",
            font=("Arial", 14)
        )
        self.info_label.pack()
        
        # ì´ˆê¸°í™” ë²„íŠ¼ë“¤
        btn_frame = ctk.CTkFrame(self)
        btn_frame.pack(fill="x", padx=10, pady=10)
        
        ctk.CTkButton(
            btn_frame,
            text="1. ì„ë² ë”© ë¡œë“œ",
            command=self.load_embeddings,
            width=150
        ).pack(side="left", padx=5)
        
        ctk.CTkButton(
            btn_frame,
            text="2. ê³ ì† DB êµ¬ì¶•",
            command=self.build_vector_db,
            width=150,
            fg_color="green"
        ).pack(side="left", padx=5)
        
        ctk.CTkButton(
            btn_frame,
            text="3. GGUF ë¡œë“œ (ì„ íƒ)",
            command=self.load_gguf,
            width=150
        ).pack(side="left", padx=5)
        
        self.status_label = ctk.CTkLabel(btn_frame, text="â³ ì¤€ë¹„ ì•ˆë¨")
        self.status_label.pack(side="left", padx=20)
        
        # ì§„í–‰ë¥  í‘œì‹œ
        self.progress = ttk.Progressbar(
            btn_frame,
            mode='indeterminate',
            length=200
        )
        self.progress.pack(side="left", padx=10)
        
        # ê²€ìƒ‰ ì…ë ¥
        search_frame = ctk.CTkFrame(self)
        search_frame.pack(fill="x", padx=10, pady=10)
        
        self.search_input = ctk.CTkEntry(
            search_frame,
            placeholder_text="ê²€ìƒ‰ì–´ ì…ë ¥ (ì˜ˆ: TOTALCNT 1450 ì´ìƒ, 15ì‹œ ë°ì´í„°)",
            font=("Arial", 14)
        )
        self.search_input.pack(side="left", fill="x", expand=True, padx=(0,10))
        
        ctk.CTkButton(
            search_frame,
            text="ë¹ ë¥¸ ê²€ìƒ‰",
            command=self.quick_search,
            width=100,
            fg_color="orange"
        ).pack(side="left", padx=5)
        
        ctk.CTkButton(
            search_frame,
            text="AI ê²€ìƒ‰",
            command=self.ai_search,
            width=100
        ).pack(side="left")
        
        # ê²€ìƒ‰ ê²°ê³¼
        result_frame = ctk.CTkFrame(self)
        result_frame.pack(fill="both", expand=True, padx=10, pady=10)
        
        ctk.CTkLabel(result_frame, text="ğŸ” ê²€ìƒ‰ ê²°ê³¼:").pack(anchor="w", padx=10, pady=5)
        
        self.result_text = ctk.CTkTextbox(
            result_frame,
            font=("Consolas", 12)
        )
        self.result_text.pack(fill="both", expand=True, padx=10, pady=5)
        
        # ì˜ˆì œ ë²„íŠ¼ë“¤
        example_frame = ctk.CTkFrame(self)
        example_frame.pack(fill="x", padx=10, pady=10)
        
        examples = [
            "TOTALCNT 1450 ì´ìƒ",
            "ìµœëŒ€ê°’ ì‹œê°„ëŒ€",
            "8ì›” 7ì¼ í‰ê· ",
            "15ì‹œ ë°ì´í„°"
        ]
        
        for ex in examples:
            ctk.CTkButton(
                example_frame,
                text=ex,
                command=lambda e=ex: self.search_input.insert(0, e),
                width=120,
                height=30
            ).pack(side="left", padx=5)
            
    def load_embeddings(self):
        """ì„ë² ë”© ë¡œë“œ"""
        try:
            self.progress.start()
            self.search_system.load_embeddings()
            self.status_label.configure(text="âœ… ì„ë² ë”© ì¤€ë¹„")
            messagebox.showinfo("ì„±ê³µ", "ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", str(e))
        finally:
            self.progress.stop()
            
    def build_vector_db(self):
        """ê³ ì† ë²¡í„°DB êµ¬ì¶•"""
        try:
            if not self.search_system.embeddings:
                messagebox.showwarning("ê²½ê³ ", "ë¨¼ì € ì„ë² ë”©ì„ ë¡œë“œí•˜ì„¸ìš”")
                return
            
            # ê¸°ì¡´ DB í™•ì¸
            if self.search_system.load_vector_db():
                response = messagebox.askyesnocancel(
                    "í™•ì¸", 
                    "ê¸°ì¡´ DBê°€ ìˆìŠµë‹ˆë‹¤.\n"
                    "Yes: ê¸°ì¡´ DB ì‚¬ìš©\n"
                    "No: ìƒˆë¡œ êµ¬ì¶• (10-30ë¶„ ì†Œìš”)\n"
                    "Cancel: ì·¨ì†Œ"
                )
                if response is True:  # Yes - ê¸°ì¡´ ì‚¬ìš©
                    self.status_label.configure(text="âœ… ê²€ìƒ‰ ì¤€ë¹„ ì™„ë£Œ")
                    return
                elif response is False:  # No - ì¬êµ¬ì¶•
                    self.progress.start()
                    self.info_label.configure(text="â³ DB êµ¬ì¶• ì¤‘... (10-30ë¶„ ì†Œìš”)")
                    self.update()
                    
                    self.search_system.build_vector_db_fast()
                    
                    self.info_label.configure(text="âœ… ê³ ì† DB êµ¬ì¶• ì™„ë£Œ!")
                    messagebox.showinfo("ì„±ê³µ", "ë²¡í„°DB ì¬êµ¬ì¶• ì™„ë£Œ")
            else:
                # ìƒˆë¡œ êµ¬ì¶•
                self.progress.start()
                self.info_label.configure(text="â³ DB êµ¬ì¶• ì¤‘... (ì²« ì‹¤í–‰ 10-30ë¶„ ì†Œìš”)")
                self.update()
                
                self.search_system.build_vector_db_fast()
                
                self.info_label.configure(text="âœ… ê³ ì† DB êµ¬ì¶• ì™„ë£Œ!")
                messagebox.showinfo("ì„±ê³µ", "ë²¡í„°DB êµ¬ì¶• ì™„ë£Œ")
                
            self.status_label.configure(text="âœ… ê²€ìƒ‰ ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", str(e))
        finally:
            self.progress.stop()
            
    def load_gguf(self):
        """GGUF ëª¨ë¸ ë¡œë“œ (ì„ íƒì‚¬í•­)"""
        filepath = filedialog.askopenfilename(
            title="GGUF ëª¨ë¸ ì„ íƒ",
            filetypes=[("GGUF Files", "*.gguf")]
        )
        
        if filepath:
            try:
                self.progress.start()
                self.search_system.load_gguf_model(filepath)
                self.status_label.configure(text="âœ… ì „ì²´ ì‹œìŠ¤í…œ ì¤€ë¹„")
                messagebox.showinfo("ì„±ê³µ", "GGUF ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                messagebox.showerror("ì˜¤ë¥˜", str(e))
            finally:
                self.progress.stop()
                
    def quick_search(self):
        """ë¹ ë¥¸ ê²€ìƒ‰ (ë²¡í„° + ì§ì ‘ ë°ì´í„°)"""
        query = self.search_input.get()
        if not query:
            return
            
        try:
            self.result_text.delete("1.0", "end")
            self.result_text.insert("1.0", f"ğŸ” ë¹ ë¥¸ ê²€ìƒ‰: {query}\n")
            self.result_text.insert("end", "="*50 + "\n\n")
            
            # ì§ì ‘ ë°ì´í„° ê²€ìƒ‰
            result = self.search_system.search_direct_data(query)
            self.result_text.insert("end", result)
            
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", str(e))
            
    def ai_search(self):
        """AI ê²€ìƒ‰ (LLM ì‚¬ìš©)"""
        if not self.search_system.llm:
            messagebox.showwarning("ê²½ê³ ", "GGUF ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”")
            return
            
        # ì¼ë°˜ ê²€ìƒ‰ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
        self.quick_search()

if __name__ == "__main__":
    app = FastUI()
    app.mainloop()