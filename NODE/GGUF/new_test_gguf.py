# -*- coding: utf-8 -*-
"""
GGUF + LangChain CSV ê²€ìƒ‰ ì‹œìŠ¤í…œ (ê°œì„ íŒ)
ë²¡í„° DB ì €ì¥/ë¡œë“œ ìµœì í™” ë° ê²€ìƒ‰ ì„±ëŠ¥ ê°œì„ 
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
import logging
import time
from tqdm import tqdm
import pickle

# LangChain
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.prompts import PromptTemplate

# GGUF
from llama_cpp import Llama

# UI
import customtkinter as ctk
from tkinter import messagebox, filedialog, ttk
import tkinter as tk
import threading

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('csv_search.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class GGUFLangChainLLM(LLM):
    """LangChainìš© GGUF LLM Wrapper"""
    
    model: Llama
    
    @property
    def _llm_type(self) -> str:
        return "gguf"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """LLM í˜¸ì¶œ"""
        response = self.model(
            prompt,
            max_tokens=kwargs.get("max_tokens", 512),
            temperature=kwargs.get("temperature", 0.7),
            stop=stop or ["</s>", "\n\n"],
            echo=False
        )
        return response['choices'][0]['text']

class CSVSearchSystem:
    """CSV ê²€ìƒ‰ ì‹œìŠ¤í…œ - LangChain í†µí•©"""
    
    def __init__(self):
        # ê²½ë¡œ ì„¤ì •
        self.embedding_model_path = "./models/embeddings/BAAI/bge-m3"
        self.csv_dir = "./csv_data"
        self.vector_db_dir = "./vector_db"
        self.metadata_path = "./vector_db/metadata.pkl"
        
        # ëª¨ë¸
        self.embeddings = None
        self.vector_store = None
        self.llm = None
        self.qa_chain = None
        
        # ë°ì´í„°
        self.all_data = pd.DataFrame()
        self.metadata = {}
        
        # ì„¤ì •
        self.chunk_size = 500
        self.chunk_overlap = 50
        
    def load_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        logger.info("ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        # ì˜¤í”„ë¼ì¸ ëª¨ë¸ ê²½ë¡œ í™•ì¸
        if not Path(self.embedding_model_path).exists():
            # ì˜¨ë¼ì¸ ëª¨ë¸ ì‚¬ìš©
            model_name = "BAAI/bge-m3"
            logger.info(f"ë¡œì»¬ ëª¨ë¸ì´ ì—†ì–´ ì˜¨ë¼ì¸ ëª¨ë¸ ì‚¬ìš©: {model_name}")
        else:
            model_name = self.embedding_model_path
            
        self.embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
    def load_csv_data(self):
        """CSV ë°ì´í„° ë¡œë“œ"""
        csv_files = list(Path(self.csv_dir).glob("*.csv"))
        if not csv_files:
            # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë””ë ‰í† ë¦¬
            csv_files = list(Path(".").glob("*.csv"))
            
        logger.info(f"ğŸ“ {len(csv_files)}ê°œ CSV íŒŒì¼ ë°œê²¬")
        
        all_dfs = []
        for csv_file in tqdm(csv_files, desc="CSV ë¡œë”©"):
            try:
                df = pd.read_csv(csv_file, encoding='utf-8')
                df['source_file'] = csv_file.name
                df['file_date'] = csv_file.stem
                all_dfs.append(df)
            except Exception as e:
                logger.error(f"CSV ë¡œë“œ ì˜¤ë¥˜ ({csv_file}): {e}")
                
        if all_dfs:
            self.all_data = pd.concat(all_dfs, ignore_index=True)
            logger.info(f"âœ… ì´ {len(self.all_data):,}ê°œ í–‰ ë¡œë“œ ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ ë¡œë“œëœ CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            
    def create_documents(self) -> List[Document]:
        """ë¬¸ì„œ ìƒì„± - ê°œì„ ëœ ë²„ì „"""
        documents = []
        
        if self.all_data.empty:
            return documents
            
        logger.info("ğŸ“„ ë¬¸ì„œ ìƒì„± ì‹œì‘...")
        
        # 1. ì „ì²´ ìš”ì•½ ë¬¸ì„œ
        overall_summary = f"""
ì „ì²´ ë°ì´í„° ìš”ì•½:
- ì´ ë°ì´í„° ìˆ˜: {len(self.all_data):,}ê°œ
- ë‚ ì§œ ë²”ìœ„: {self.all_data['file_date'].min()} ~ {self.all_data['file_date'].max()}
- ì»¬ëŸ¼: {', '.join(self.all_data.columns.tolist())}
"""
        documents.append(Document(
            page_content=overall_summary,
            metadata={"type": "overall_summary", "doc_id": "summary_0"}
        ))
        
        # 2. ë‚ ì§œë³„ ìš”ì•½
        for date in self.all_data['file_date'].unique():
            daily_data = self.all_data[self.all_data['file_date'] == date]
            
            # TOTALCNT ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°
            if 'TOTALCNT' in daily_data.columns:
                daily_text = f"""
ë‚ ì§œ: {date}
ë°ì´í„° ìˆ˜: {len(daily_data):,}ê°œ
TOTALCNT í†µê³„:
- í‰ê· : {daily_data['TOTALCNT'].mean():.0f}
- ìµœëŒ€: {daily_data['TOTALCNT'].max()}
- ìµœì†Œ: {daily_data['TOTALCNT'].min()}
- í‘œì¤€í¸ì°¨: {daily_data['TOTALCNT'].std():.0f}
"""
                # ìµœëŒ€ê°’ ì‹œê°„ ì •ë³´ ì¶”ê°€
                if 'CURRTIME' in daily_data.columns:
                    max_time = daily_data.loc[daily_data['TOTALCNT'].idxmax(), 'CURRTIME']
                    daily_text += f"- ìµœëŒ€ê°’ ì‹œê°„: {max_time}\n"
                    
            else:
                # ì¼ë°˜ì ì¸ ìš”ì•½
                daily_text = f"""
ë‚ ì§œ: {date}
ë°ì´í„° ìˆ˜: {len(daily_data):,}ê°œ
ì»¬ëŸ¼ ì •ë³´: {', '.join(daily_data.columns.tolist())}
"""
            
            documents.append(Document(
                page_content=daily_text,
                metadata={
                    "type": "daily_summary", 
                    "date": date,
                    "doc_id": f"daily_{date}"
                }
            ))
            
        # 3. ì‹œê°„ëŒ€ë³„ ìƒì„¸ ë°ì´í„° (ìƒ˜í”Œë§)
        if 'CURRTIME' in self.all_data.columns and 'TOTALCNT' in self.all_data.columns:
            # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”
            for date in self.all_data['file_date'].unique()[:5]:  # ìµœê·¼ 5ì¼ë§Œ
                daily_data = self.all_data[self.all_data['file_date'] == date]
                
                # ì‹œê°„ëŒ€ë³„ ì§‘ê³„
                daily_data['hour'] = daily_data['CURRTIME'].astype(str).str[8:10]
                
                for hour in daily_data['hour'].unique():
                    hour_data = daily_data[daily_data['hour'] == hour]
                    if len(hour_data) > 3:  # ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš°ë§Œ
                        hour_text = f"""
ë‚ ì§œ: {date} {hour}ì‹œ
ë°ì´í„° ìˆ˜: {len(hour_data)}ê°œ
TOTALCNT í‰ê· : {hour_data['TOTALCNT'].mean():.0f}
TOTALCNT ìµœëŒ€: {hour_data['TOTALCNT'].max()}
TOTALCNT ìµœì†Œ: {hour_data['TOTALCNT'].min()}
"""
                        # ë‹¤ë¥¸ ì»¬ëŸ¼ë“¤ì˜ í‰ê· ê°’ë„ ì¶”ê°€
                        numeric_cols = hour_data.select_dtypes(include=[np.number]).columns
                        for col in numeric_cols[:5]:  # ìµœëŒ€ 5ê°œ ì»¬ëŸ¼ë§Œ
                            if col not in ['TOTALCNT', 'hour']:
                                hour_text += f"{col} í‰ê· : {hour_data[col].mean():.0f}\n"
                                
                        documents.append(Document(
                            page_content=hour_text,
                            metadata={
                                "type": "hourly_detail",
                                "date": date,
                                "hour": hour,
                                "doc_id": f"hour_{date}_{hour}"
                            }
                        ))
        
        logger.info(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ìƒì„± ì™„ë£Œ")
        return documents
        
    def build_vector_db(self):
        """ë²¡í„° DB êµ¬ì¶• - ê°œì„ ëœ ë²„ì „"""
        logger.info("ğŸ”¨ ë²¡í„° DB êµ¬ì¶• ì‹œì‘...")
        
        # CSV ë°ì´í„° ë¡œë“œ
        self.load_csv_data()
        
        if self.all_data.empty:
            raise ValueError("CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            
        # ë¬¸ì„œ ìƒì„±
        documents = self.create_documents()
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
        split_docs = text_splitter.split_documents(documents)
        
        logger.info(f"ğŸ“„ {len(documents)}ê°œ ë¬¸ì„œë¥¼ {len(split_docs)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        
        # FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        self.vector_store = FAISS.from_documents(
            split_docs,
            self.embeddings
        )
        
        # ë²¡í„° DB ì €ì¥
        Path(self.vector_db_dir).mkdir(exist_ok=True)
        self.vector_store.save_local(self.vector_db_dir)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'total_documents': len(documents),
            'total_chunks': len(split_docs),
            'chunk_size': self.chunk_size,
            'data_shape': self.all_data.shape,
            'columns': self.all_data.columns.tolist(),
            'date_range': {
                'min': str(self.all_data['file_date'].min()),
                'max': str(self.all_data['file_date'].max())
            },
            'build_time': datetime.now().isoformat()
        }
        
        with open(self.metadata_path, 'wb') as f:
            pickle.dump(metadata, f)
            
        # ì›ë³¸ ë°ì´í„°ë„ ì €ì¥ (ë¹ ë¥¸ ê²€ìƒ‰ìš©)
        self.all_data.to_pickle(Path(self.vector_db_dir) / "raw_data.pkl")
        
        logger.info(f"âœ… ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ (ì €ì¥ ìœ„ì¹˜: {self.vector_db_dir})")
        
    def load_vector_db(self) -> bool:
        """ë²¡í„° DB ë¡œë“œ - ê°œì„ ëœ ë²„ì „"""
        try:
            if not Path(self.vector_db_dir).exists():
                logger.warning("ë²¡í„° DB ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
                
            logger.info("ğŸ“‚ ë²¡í„° DB ë¡œë“œ ì¤‘...")
            
            # ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
            self.vector_store = FAISS.load_local(
                self.vector_db_dir,
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            if Path(self.metadata_path).exists():
                with open(self.metadata_path, 'rb') as f:
                    self.metadata = pickle.load(f)
                logger.info(f"ğŸ“Š ë©”íƒ€ë°ì´í„°: {self.metadata}")
                
            # ì›ë³¸ ë°ì´í„° ë¡œë“œ
            raw_data_path = Path(self.vector_db_dir) / "raw_data.pkl"
            if raw_data_path.exists():
                self.all_data = pd.read_pickle(raw_data_path)
                logger.info(f"âœ… ì›ë³¸ ë°ì´í„° ë¡œë“œ: {self.all_data.shape}")
                
            logger.info("âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
            
    def load_gguf_model(self, model_path: str):
        """GGUF ëª¨ë¸ ë¡œë“œ ë° QA ì²´ì¸ ì„¤ì •"""
        logger.info(f"ğŸ¤– GGUF ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
        
        # GGUF ëª¨ë¸ ë¡œë“œ
        llama_model = Llama(
            model_path=model_path,
            n_ctx=4096,
            n_gpu_layers=-1,  # GPU ìë™ ê°ì§€
            n_threads=8,
            verbose=False
        )
        
        # LangChain LLM ë˜í¼
        self.llm = GGUFLangChainLLM(model=llama_model)
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        prompt_template = """ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€: ë°ì´í„°ë¥¼ ë¶„ì„í•œ ê²°ê³¼, """

        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # QA ì²´ì¸ ìƒì„±
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vector_store.as_retriever(
                search_kwargs={"k": 5}  # ìƒìœ„ 5ê°œ ë¬¸ì„œ ê²€ìƒ‰
            ),
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )
        
        logger.info("âœ… GGUF ëª¨ë¸ ë° QA ì²´ì¸ ì„¤ì • ì™„ë£Œ")
        
    def search(self, query: str, use_llm: bool = True) -> Dict[str, Any]:
        """í†µí•© ê²€ìƒ‰ í•¨ìˆ˜"""
        results = {
            'query': query,
            'vector_results': [],
            'direct_results': '',
            'llm_answer': '',
            'source_docs': []
        }
        
        try:
            # 1. ë²¡í„° ê²€ìƒ‰
            if self.vector_store:
                docs = self.vector_store.similarity_search_with_score(query, k=5)
                results['vector_results'] = [
                    {
                        'content': doc.page_content,
                        'metadata': doc.metadata,
                        'score': float(score)
                    }
                    for doc, score in docs
                ]
                
            # 2. ì§ì ‘ ë°ì´í„° ê²€ìƒ‰ (pandas)
            if not self.all_data.empty:
                direct_result = self._direct_search(query)
                results['direct_results'] = direct_result
                
            # 3. LLM ê¸°ë°˜ ë‹µë³€
            if use_llm and self.qa_chain:
                qa_result = self.qa_chain({"query": query})
                results['llm_answer'] = qa_result['result']
                results['source_docs'] = [
                    {
                        'content': doc.page_content,
                        'metadata': doc.metadata
                    }
                    for doc in qa_result.get('source_documents', [])
                ]
                
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            results['error'] = str(e)
            
        return results
        
    def _direct_search(self, query: str) -> str:
        """ì§ì ‘ ë°ì´í„° ê²€ìƒ‰"""
        results = []
        query_lower = query.lower()
        
        # TOTALCNT ê´€ë ¨ ê²€ìƒ‰
        if 'TOTALCNT' in self.all_data.columns:
            if "ìµœëŒ€" in query or "max" in query_lower:
                max_row = self.all_data.loc[self.all_data['TOTALCNT'].idxmax()]
                results.append(f"ì „ì²´ ìµœëŒ€ê°’: TOTALCNT={max_row['TOTALCNT']}")
                if 'CURRTIME' in max_row:
                    results.append(f"ë°œìƒ ì‹œê°„: {max_row['CURRTIME']}")
                if 'file_date' in max_row:
                    results.append(f"ë‚ ì§œ: {max_row['file_date']}")
                    
            elif "1450" in query or "1500" in query:
                threshold = 1450 if "1450" in query else 1500
                filtered = self.all_data[self.all_data['TOTALCNT'] >= threshold]
                if not filtered.empty:
                    results.append(f"\nTOTALCNT {threshold} ì´ìƒ:")
                    results.append(f"- ì´ {len(filtered)}ê°œ ë°ì´í„°")
                    
                    # ë‚ ì§œë³„ ì§‘ê³„
                    if 'file_date' in filtered.columns:
                        date_counts = filtered['file_date'].value_counts()
                        results.append("\në‚ ì§œë³„ ë¶„í¬:")
                        for date, count in date_counts.head(5).items():
                            results.append(f"  - {date}: {count}ê°œ")
                            
            elif "í‰ê· " in query:
                avg_total = self.all_data['TOTALCNT'].mean()
                results.append(f"ì „ì²´ í‰ê· : {avg_total:.0f}")
                
                # ë‚ ì§œë³„ í‰ê· 
                if 'file_date' in self.all_data.columns:
                    date_avg = self.all_data.groupby('file_date')['TOTALCNT'].mean()
                    results.append("\në‚ ì§œë³„ í‰ê· :")
                    for date, avg in date_avg.head(5).items():
                        results.append(f"  - {date}: {avg:.0f}")
                        
        # íŠ¹ì • ë‚ ì§œ ê²€ìƒ‰
        for col in ['file_date', 'CURRTIME']:
            if col in self.all_data.columns:
                for date_part in query.split():
                    if date_part.replace('-', '').isdigit() and len(date_part) >= 6:
                        mask = self.all_data[col].astype(str).str.contains(date_part)
                        filtered = self.all_data[mask]
                        if not filtered.empty:
                            results.append(f"\n{date_part} ê´€ë ¨ ë°ì´í„°: {len(filtered)}ê°œ")
                            break
                            
        return "\n".join(results) if results else "ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

class ModernSearchUI(ctk.CTk):
    """í˜„ëŒ€ì ì¸ ê²€ìƒ‰ UI"""
    
    def __init__(self):
        super().__init__()
        
        self.title("GGUF + LangChain CSV ê²€ìƒ‰ ì‹œìŠ¤í…œ")
        self.geometry("1200x800")
        
        # í…Œë§ˆ ì„¤ì •
        ctk.set_appearance_mode("dark")
        ctk.set_default_color_theme("blue")
        
        # ê²€ìƒ‰ ì‹œìŠ¤í…œ
        self.search_system = CSVSearchSystem()
        
        # UI êµ¬ì„±
        self.setup_ui()
        
    def setup_ui(self):
        """UI êµ¬ì„±"""
        # ë©”ì¸ ë ˆì´ì•„ì›ƒ
        self.grid_columnconfigure(1, weight=1)
        self.grid_rowconfigure(0, weight=1)
        
        # ì¢Œì¸¡ ì‚¬ì´ë“œë°”
        self.setup_sidebar()
        
        # ìš°ì¸¡ ë©”ì¸ ì˜ì—­
        self.setup_main_area()
        
    def setup_sidebar(self):
        """ì‚¬ì´ë“œë°” êµ¬ì„±"""
        sidebar = ctk.CTkFrame(self, width=250)
        sidebar.grid(row=0, column=0, sticky="nsew", padx=(10, 5), pady=10)
        sidebar.grid_propagate(False)
        
        # íƒ€ì´í‹€
        title = ctk.CTkLabel(
            sidebar,
            text="ğŸ” CSV Search AI",
            font=("Arial", 24, "bold")
        )
        title.pack(pady=20)
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        self.status_frame = ctk.CTkFrame(sidebar)
        self.status_frame.pack(fill="x", padx=10, pady=10)
        
        self.embedding_status = ctk.CTkLabel(
            self.status_frame,
            text="âŒ ì„ë² ë”©: ë¯¸ë¡œë“œ",
            font=("Arial", 12)
        )
        self.embedding_status.pack(anchor="w", padx=10, pady=2)
        
        self.vector_status = ctk.CTkLabel(
            self.status_frame,
            text="âŒ ë²¡í„°DB: ë¯¸ë¡œë“œ",
            font=("Arial", 12)
        )
        self.vector_status.pack(anchor="w", padx=10, pady=2)
        
        self.llm_status = ctk.CTkLabel(
            self.status_frame,
            text="âŒ LLM: ë¯¸ë¡œë“œ",
            font=("Arial", 12)
        )
        self.llm_status.pack(anchor="w", padx=10, pady=2)
        
        # ë²„íŠ¼ë“¤
        ctk.CTkLabel(sidebar, text="ì´ˆê¸° ì„¤ì •", font=("Arial", 14, "bold")).pack(pady=(20, 10))
        
        self.init_btn = ctk.CTkButton(
            sidebar,
            text="1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ",
            command=self.load_embeddings,
            height=35
        )
        self.init_btn.pack(fill="x", padx=10, pady=5)
        
        self.vector_btn = ctk.CTkButton(
            sidebar,
            text="2. ë²¡í„° DB ë¡œë“œ/êµ¬ì¶•",
            command=self.setup_vector_db,
            height=35
        )
        self.vector_btn.pack(fill="x", padx=10, pady=5)
        
        self.llm_btn = ctk.CTkButton(
            sidebar,
            text="3. GGUF ëª¨ë¸ ë¡œë“œ",
            command=self.load_gguf,
            height=35
        )
        self.llm_btn.pack(fill="x", padx=10, pady=5)
        
        # ê²€ìƒ‰ ì˜µì…˜
        ctk.CTkLabel(sidebar, text="ê²€ìƒ‰ ì˜µì…˜", font=("Arial", 14, "bold")).pack(pady=(20, 10))
        
        self.use_llm_var = ctk.BooleanVar(value=True)
        self.use_llm_check = ctk.CTkCheckBox(
            sidebar,
            text="LLM ë‹µë³€ ì‚¬ìš©",
            variable=self.use_llm_var
        )
        self.use_llm_check.pack(anchor="w", padx=10, pady=5)
        
        # ì˜ˆì œ ì¿¼ë¦¬
        ctk.CTkLabel(sidebar, text="ì˜ˆì œ ê²€ìƒ‰ì–´", font=("Arial", 14, "bold")).pack(pady=(20, 10))
        
        examples = [
            "TOTALCNT ìµœëŒ€ê°’ì€?",
            "1450 ì´ìƒì¸ ë°ì´í„°",
            "8ì›” 7ì¼ 15ì‹œ ë°ì´í„°",
            "ì „ì²´ í‰ê·  í†µê³„"
        ]
        
        for ex in examples:
            ctk.CTkButton(
                sidebar,
                text=ex,
                command=lambda e=ex: self.search_box.insert("end", e),
                height=30,
                fg_color="gray40"
            ).pack(fill="x", padx=10, pady=2)
            
    def setup_main_area(self):
        """ë©”ì¸ ì˜ì—­ êµ¬ì„±"""
        main_frame = ctk.CTkFrame(self)
        main_frame.grid(row=0, column=1, sticky="nsew", padx=(5, 10), pady=10)
        main_frame.grid_columnconfigure(0, weight=1)
        main_frame.grid_rowconfigure(1, weight=1)
        
        # ê²€ìƒ‰ ì˜ì—­
        search_frame = ctk.CTkFrame(main_frame)
        search_frame.grid(row=0, column=0, sticky="ew", padx=10, pady=(10, 5))
        search_frame.grid_columnconfigure(0, weight=1)
        
        self.search_box = ctk.CTkEntry(
            search_frame,
            placeholder_text="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
            font=("Arial", 16),
            height=40
        )
        self.search_box.grid(row=0, column=0, sticky="ew", padx=(0, 10))
        self.search_box.bind("<Return>", lambda e: self.search())
        
        self.search_btn = ctk.CTkButton(
            search_frame,
            text="ê²€ìƒ‰",
            command=self.search,
            width=100,
            height=40,
            font=("Arial", 16)
        )
        self.search_btn.grid(row=0, column=1)
        
        # ê²°ê³¼ í‘œì‹œ ì˜ì—­ (íƒ­)
        self.tab_view = ctk.CTkTabview(main_frame)
        self.tab_view.grid(row=1, column=0, sticky="nsew", padx=10, pady=(5, 10))
        
        # íƒ­ ìƒì„±
        self.tab_view.add("ì¢…í•© ê²°ê³¼")
        self.tab_view.add("ë²¡í„° ê²€ìƒ‰")
        self.tab_view.add("ì§ì ‘ ê²€ìƒ‰")
        self.tab_view.add("ì†ŒìŠ¤ ë¬¸ì„œ")
        
        # ê° íƒ­ì— í…ìŠ¤íŠ¸ ìœ„ì ¯
        self.result_text = ctk.CTkTextbox(
            self.tab_view.tab("ì¢…í•© ê²°ê³¼"),
            font=("Consolas", 12),
            wrap="word"
        )
        self.result_text.pack(fill="both", expand=True)
        
        self.vector_text = ctk.CTkTextbox(
            self.tab_view.tab("ë²¡í„° ê²€ìƒ‰"),
            font=("Consolas", 12),
            wrap="word"
        )
        self.vector_text.pack(fill="both", expand=True)
        
        self.direct_text = ctk.CTkTextbox(
            self.tab_view.tab("ì§ì ‘ ê²€ìƒ‰"),
            font=("Consolas", 12),
            wrap="word"
        )
        self.direct_text.pack(fill="both", expand=True)
        
        self.source_text = ctk.CTkTextbox(
            self.tab_view.tab("ì†ŒìŠ¤ ë¬¸ì„œ"),
            font=("Consolas", 12),
            wrap="word"
        )
        self.source_text.pack(fill="both", expand=True)
        
        # ì§„í–‰ë¥  í‘œì‹œ
        self.progress = ttk.Progressbar(
            main_frame,
            mode='indeterminate',
            style="TProgressbar"
        )
        
    def load_embeddings(self):
        """ì„ë² ë”© ë¡œë“œ"""
        self.show_progress()
        
        def task():
            try:
                self.search_system.load_embeddings()
                self.after(0, lambda: self.embedding_status.configure(text="âœ… ì„ë² ë”©: ë¡œë“œë¨"))
                self.after(0, lambda: messagebox.showinfo("ì„±ê³µ", "ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"))
            except Exception as e:
                self.after(0, lambda: messagebox.showerror("ì˜¤ë¥˜", f"ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨:\n{str(e)}"))
            finally:
                self.after(0, self.hide_progress)
                
        threading.Thread(target=task, daemon=True).start()
        
    def setup_vector_db(self):
        """ë²¡í„° DB ì„¤ì •"""
        if not self.search_system.embeddings:
            messagebox.showwarning("ê²½ê³ ", "ë¨¼ì € ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš”")
            return
            
        # ê¸°ì¡´ DB í™•ì¸
        if self.search_system.load_vector_db():
            result = messagebox.askyesnocancel(
                "ë²¡í„° DB",
                "ê¸°ì¡´ ë²¡í„° DBê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                "Yes: ê¸°ì¡´ DB ì‚¬ìš©\n"
                "No: ìƒˆë¡œ êµ¬ì¶•\n"
                "Cancel: ì·¨ì†Œ"
            )
            
            if result is True:  # Yes - ê¸°ì¡´ ì‚¬ìš©
                self.vector_status.configure(text="âœ… ë²¡í„°DB: ë¡œë“œë¨")
                messagebox.showinfo("ì„±ê³µ", "ê¸°ì¡´ ë²¡í„° DBë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤")
                return
            elif result is None:  # Cancel
                return
                
        # ìƒˆë¡œ êµ¬ì¶•
        self.show_progress()
        
        def task():
            try:
                self.search_system.build_vector_db()
                self.after(0, lambda: self.vector_status.configure(text="âœ… ë²¡í„°DB: êµ¬ì¶•ë¨"))
                self.after(0, lambda: messagebox.showinfo("ì„±ê³µ", "ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ"))
            except Exception as e:
                self.after(0, lambda: messagebox.showerror("ì˜¤ë¥˜", f"ë²¡í„° DB êµ¬ì¶• ì‹¤íŒ¨:\n{str(e)}"))
            finally:
                self.after(0, self.hide_progress)
                
        threading.Thread(target=task, daemon=True).start()
        
    def load_gguf(self):
        """GGUF ëª¨ë¸ ë¡œë“œ"""
        if not self.search_system.vector_store:
            messagebox.showwarning("ê²½ê³ ", "ë¨¼ì € ë²¡í„° DBë¥¼ ì„¤ì •í•˜ì„¸ìš”")
            return
            
        filepath = filedialog.askopenfilename(
            title="GGUF ëª¨ë¸ ì„ íƒ",
            filetypes=[("GGUF Files", "*.gguf"), ("All Files", "*.*")]
        )
        
        if not filepath:
            return
            
        self.show_progress()
        
        def task():
            try:
                self.search_system.load_gguf_model(filepath)
                self.after(0, lambda: self.llm_status.configure(text="âœ… LLM: ë¡œë“œë¨"))
                self.after(0, lambda: messagebox.showinfo("ì„±ê³µ", "GGUF ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"))
            except Exception as e:
                self.after(0, lambda: messagebox.showerror("ì˜¤ë¥˜", f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:\n{str(e)}"))
            finally:
                self.after(0, self.hide_progress)
                
        threading.Thread(target=task, daemon=True).start()
        
    def search(self):
        """ê²€ìƒ‰ ì‹¤í–‰"""
        query = self.search_box.get().strip()
        if not query:
            return
            
        if not self.search_system.vector_store:
            messagebox.showwarning("ê²½ê³ ", "ë¨¼ì € ë²¡í„° DBë¥¼ ì„¤ì •í•˜ì„¸ìš”")
            return
            
        self.show_progress()
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
        for text_widget in [self.result_text, self.vector_text, self.direct_text, self.source_text]:
            text_widget.delete("1.0", "end")
            
        def task():
            try:
                # ê²€ìƒ‰ ì‹¤í–‰
                results = self.search_system.search(
                    query, 
                    use_llm=self.use_llm_var.get()
                )
                
                # UI ì—…ë°ì´íŠ¸ëŠ” ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ
                self.after(0, lambda: self.display_results(results))
                
            except Exception as e:
                self.after(0, lambda: messagebox.showerror("ê²€ìƒ‰ ì˜¤ë¥˜", str(e)))
            finally:
                self.after(0, self.hide_progress)
                
        threading.Thread(target=task, daemon=True).start()
        
    def display_results(self, results: Dict[str, Any]):
        """ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ"""
        # ì¢…í•© ê²°ê³¼
        summary = f"ğŸ” ê²€ìƒ‰ì–´: {results['query']}\n"
        summary += "="*50 + "\n\n"
        
        if results.get('llm_answer'):
            summary += "ğŸ¤– AI ë‹µë³€:\n"
            summary += results['llm_answer'] + "\n\n"
            
        if results.get('direct_results'):
            summary += "ğŸ“Š ì§ì ‘ ê²€ìƒ‰ ê²°ê³¼:\n"
            summary += results['direct_results'] + "\n"
            
        self.result_text.insert("1.0", summary)
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼
        if results.get('vector_results'):
            vector_text = "ğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼:\n\n"
            for i, doc in enumerate(results['vector_results'], 1):
                vector_text += f"[{i}] ìœ ì‚¬ë„: {1-doc['score']:.3f}\n"
                vector_text += f"ë‚´ìš©: {doc['content']}\n"
                vector_text += f"ë©”íƒ€ë°ì´í„°: {doc['metadata']}\n"
                vector_text += "-"*40 + "\n\n"
            self.vector_text.insert("1.0", vector_text)
            
        # ì§ì ‘ ê²€ìƒ‰ ê²°ê³¼
        if results.get('direct_results'):
            self.direct_text.insert("1.0", results['direct_results'])
            
        # ì†ŒìŠ¤ ë¬¸ì„œ
        if results.get('source_docs'):
            source_text = "ğŸ“„ ì°¸ì¡°ëœ ì†ŒìŠ¤ ë¬¸ì„œ:\n\n"
            for i, doc in enumerate(results['source_docs'], 1):
                source_text += f"[ë¬¸ì„œ {i}]\n"
                source_text += doc['content'] + "\n"
                source_text += f"ì¶œì²˜: {doc['metadata']}\n"
                source_text += "="*50 + "\n\n"
            self.source_text.insert("1.0", source_text)
            
        # ì²« ë²ˆì§¸ íƒ­ìœ¼ë¡œ ì „í™˜
        self.tab_view.set("ì¢…í•© ê²°ê³¼")
        
    def show_progress(self):
        """ì§„í–‰ë¥  í‘œì‹œ"""
        self.progress.grid(row=2, column=0, sticky="ew", padx=10, pady=5)
        self.progress.start()
        
    def hide_progress(self):
        """ì§„í–‰ë¥  ìˆ¨ê¸°ê¸°"""
        self.progress.stop()
        self.progress.grid_remove()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    for dir_name in ["csv_data", "vector_db", "models"]:
        Path(dir_name).mkdir(exist_ok=True)
        
    # UI ì‹¤í–‰
    app = ModernSearchUI()
    app.mainloop()

if __name__ == "__main__":
    main()