# -*- coding: utf-8 -*-
"""
í™˜ê°ë¥ 
  â†‘
  |     ğŸ“7B (ë§ìŒ)
  |    /
  |   /
  |  ğŸ“13B
  | /
  |ğŸ“30B (ìµœì )
  |        \
  |         ğŸ“70B (ë‹¤ì‹œ ì¦ê°€)
  |              \
  |               ğŸ“175B
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ëª¨ë¸ í¬ê¸°
ë„¤! CSV ë°ì´í„°ì— ìˆëŠ” ì •ë³´ë¼ë©´ ë‹¤ì–‘í•œ ì§ˆë¬¸ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
ê°€ëŠ¥í•œ ì§ˆë¬¸ ì˜ˆì‹œ:
ğŸ“Š ê¸°ë³¸ í†µê³„ ì§ˆë¬¸

"ì „ì²´ ë°ì´í„°ëŠ” ëª‡ ê°œì•¼?"
"ì´ í–‰ ìˆ˜ëŠ”?"
"ë°ì´í„°ê°€ ëª‡ ê°œ ìˆì–´?"
"ì»¬ëŸ¼ì€ ë­ê°€ ìˆì–´?"
"ì–´ë–¤ ì»¬ëŸ¼ë“¤ì´ ìˆë‚˜?"

ğŸ“ˆ TOTALCNT ê´€ë ¨ ì§ˆë¬¸ (TOTALCNT ì»¬ëŸ¼ì´ ìˆë‹¤ë©´)

"TOTALCNT í‰ê· ì€?"
"TOTALCNT ìµœëŒ€ê°’ì€?"
"TOTALCNT ìµœì†Œê°’ì€?"
"TOTALCNTê°€ 1400 ì´ìƒì¸ ë°ì´í„°ëŠ” ëª‡ ê°œ?"
"TOTALCNTê°€ 1500 ì´ìƒì¸ ê±´ ëª‡ ê°œì•¼?"
"TOTALCNT ì¤‘ê°„ê°’ì€?"

ğŸ” ë‹¤ë¥¸ ì»¬ëŸ¼ ì§ˆë¬¸ (M14AM14B, M14AM10A ë“±ì´ ìˆë‹¤ë©´)

"M14AM14B í‰ê· ì€?"
"M14AM10A ìµœëŒ€ê°’ì€?"
"M14AM14Bê°€ 300 ì´ìƒì¸ ë°ì´í„°ëŠ”?"

ğŸ“… ì‹œê°„ ê´€ë ¨ (CURRTIME ì»¬ëŸ¼ì´ ìˆë‹¤ë©´)

"ê°€ì¥ ìµœê·¼ ë°ì´í„°ëŠ” ì–¸ì œì•¼?"
"ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„°ëŠ”?"
"ë°ì´í„° ê¸°ê°„ì€?"

ğŸ¯ íŠ¹ì • ë°ì´í„° ì¡°íšŒ

"ì¸ë±ìŠ¤ 10ë²ˆ ë°ì´í„° ì •ë³´ ì•Œë ¤ì¤˜"
"ì²« ë²ˆì§¸ ë°ì´í„°ëŠ”?"

ì£¼ì˜ì‚¬í•­:

CSVì— ì‹¤ì œë¡œ ìˆëŠ” ì»¬ëŸ¼ëª…ê³¼ ë°ì´í„°ë§Œ ì§ˆë¬¸ ê°€ëŠ¥
ë„ˆë¬´ ë³µì¡í•œ ë¶„ì„(ìƒê´€ê´€ê³„, íšŒê·€ë¶„ì„ ë“±)ì€ RAGë¡œëŠ” í•œê³„ê°€ ìˆìŒ
ë¬¸ì„œë¡œ ì €ì¥ëœ ì •ë³´ë§Œ ê²€ìƒ‰ ê°€ëŠ¥

ì‹¤ì œ CSV íŒŒì¼ì˜ ì»¬ëŸ¼ëª…ì„ í™•ì¸í•˜ê³  ê·¸ì— ë§ê²Œ ì§ˆë¬¸í•˜ì‹œë©´ ë©ë‹ˆë‹¤!
ìˆ˜ì •ëœ GGUF CSV RAG ì‹œìŠ¤í…œ
ë‹µë³€ ì •í™•ë„ ê°œì„  ë²„ì „
"""

import os
import glob
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

def main():
    print("=" * 60)
    print("ğŸ“Š ì˜¤í”„ë¼ì¸ GGUF CSV RAG ì‹œìŠ¤í…œ (ê°œì„  ë²„ì „)")
    print("=" * 60)
   
    # 1. ì„ë² ë”© ëª¨ë¸ ê²½ë¡œ
    local_model_path = "./offline_models/all-MiniLM-L6-v2"
   
    if not os.path.exists(local_model_path):
        print(f"âŒ ì„ë² ë”© ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {local_model_path}")
        print("\nì˜¨ë¼ì¸ ëª¨ë¸ ì‚¬ìš©ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        local_model_path = "sentence-transformers/all-MiniLM-L6-v2"
   
    # 2. CSV íŒŒì¼ ë¡œë“œ
    folder_path = "./output_by_date"
    print(f"\nğŸ“ CSV í´ë”: {folder_path}")
   
    csv_files = glob.glob(os.path.join(folder_path, "*.csv"))
    if not csv_files:
        print("âŒ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return
   
    print(f"âœ… ë°œê²¬ëœ íŒŒì¼: {len(csv_files)}ê°œ")
   
    # ë°ì´í„° ë¡œë“œ
    all_data = []
    for i, file in enumerate(csv_files[:30]):
        try:
            df = pd.read_csv(file)
            all_data.append(df)
            print(f"  - {os.path.basename(file)}: {len(df)}í–‰")
        except Exception as e:
            print(f"  - ì˜¤ë¥˜: {file} - {e}")
   
    if not all_data:
        print("âŒ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
        
    df = pd.concat(all_data, ignore_index=True)
    print(f"\nâœ… ì´ ë¡œë“œëœ ë°ì´í„°: {len(df)}í–‰")
    print(f"âœ… ì»¬ëŸ¼: {list(df.columns)}")
   
    # 3. ë¬¸ì„œ ìƒì„± (ì¤‘ìš”: ë” ëª…í™•í•œ ë¬¸ì„œ ìƒì„±)
    print("\nğŸ“„ ë¬¸ì„œ ìƒì„± ì¤‘...")
    documents = []
   
    # === í•µì‹¬ ê°œì„ : ì •í™•í•œ í†µê³„ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë¬¸ì„œí™” ===
    
    # ì „ì²´ ìš”ì•½ ë¬¸ì„œ
    summary_text = f"""
ë°ì´í„° ìš”ì•½ ì •ë³´:
- ì „ì²´ ë°ì´í„° ê°œìˆ˜: {len(df)}ê°œ
- ì „ì²´ í–‰ ìˆ˜: {len(df)}í–‰
- ì»¬ëŸ¼ ëª©ë¡: {', '.join(df.columns)}
"""
    documents.append(Document(page_content=summary_text))
   
    # TOTALCNT í†µê³„ (ìˆëŠ” ê²½ìš°)
    if 'TOTALCNT' in df.columns:
        # ì •í™•í•œ í†µê³„ ê³„ì‚°
        avg_val = df['TOTALCNT'].mean()
        max_val = df['TOTALCNT'].max()
        min_val = df['TOTALCNT'].min()
        count_1400 = (df['TOTALCNT'] >= 1400).sum()
        count_1500 = (df['TOTALCNT'] >= 1500).sum()
        
        stats_text = f"""
TOTALCNT í†µê³„ ì •ë³´:
- TOTALCNT í‰ê· ê°’: {avg_val:.0f}
- TOTALCNT ìµœëŒ€ê°’: {max_val}
- TOTALCNT ìµœì†Œê°’: {min_val}
- TOTALCNTê°€ 1400 ì´ìƒì¸ ë°ì´í„°: {count_1400}ê°œ
- TOTALCNTê°€ 1500 ì´ìƒì¸ ë°ì´í„°: {count_1500}ê°œ
"""
        documents.append(Document(page_content=stats_text))
   
    # ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€ (ìƒìœ„ 100ê°œ)
    for idx, row in df.head(100).iterrows():
        row_text = f"ë°ì´í„° ì¸ë±ìŠ¤ {idx}: "
        for col in df.columns:
            if pd.notna(row[col]):
                row_text += f"{col}={row[col]}, "
        documents.append(Document(page_content=row_text[:500]))  # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
   
    print(f"âœ… ìƒì„±ëœ ë¬¸ì„œ: {len(documents)}ê°œ")
   
    # 4. í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100,  # ì˜¤ë²„ë© ì¦ê°€
        separators=["\n\n", "\n", " ", ""]
    )
    texts = text_splitter.split_documents(documents)
    print(f"âœ… ë¶„í• ëœ í…ìŠ¤íŠ¸: {len(texts)}ê°œ")
   
    # 5. ì„ë² ë”© ìƒì„±
    print("\nğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=local_model_path,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
   
    # 6. ë²¡í„° ìŠ¤í† ì–´
    print("ğŸ”„ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
    vectorstore = FAISS.from_documents(texts, embeddings)
    print("âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
    
    # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ (ì¬ì‚¬ìš© ê°€ëŠ¥)
    vectorstore.save_local("./faiss_index")
    print("âœ… ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ")
   
    # 7. GGUF ëª¨ë¸ ë¡œë“œ
    print("\nğŸ¤– GGUF ëª¨ë¸ ë¡œë”© ì¤‘...")
    model_path = "./KoSOLAR-10.7B-v0.2.Q3_K_M.gguf"
   
    if not os.path.exists(model_path):
        print(f"âŒ GGUF ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return
   
    # ì½œë°± ë§¤ë‹ˆì € (ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥)
    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
   
    llm = LlamaCpp(
        model_path=model_path,
        n_ctx=2048,
        max_tokens=256,
        temperature=0.1,  # ë‚®ì€ ì˜¨ë„ë¡œ ì •í™•ë„ í–¥ìƒ
        top_p=0.9,
        n_threads=8,
        callback_manager=callback_manager,
        verbose=False
    )
    print("âœ… GGUF ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
   
    # 8. ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt_template = """ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•˜ì„¸ìš”.

ì •ë³´:
{context}

ì§ˆë¬¸: {question}
ë‹µë³€:"""

    PROMPT = PromptTemplate(
        template=prompt_template, 
        input_variables=["context", "question"]
    )
   
    # 9. QA ì²´ì¸ ìƒì„±
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(
            search_kwargs={"k": 5}  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
        ),
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True  # ì†ŒìŠ¤ ë¬¸ì„œë„ ë°˜í™˜
    )
   
    print("\n" + "=" * 60)
    print("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    print("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: quit/exit)")
    print("=" * 60)
    
    while True:
        question = input("\nğŸ’¬ ì§ˆë¬¸: ")
        if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
           
        try:
            print("\nğŸ¤” ê²€ìƒ‰ ì¤‘...")
            result = qa_chain({"query": question})
            print("\nğŸ¤– ë‹µë³€:")
            print(result['result'])
            

                    
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()