# -*- coding: utf-8 -*-
"""
ìˆ˜ì •ëœ GGUF CSV RAG ì‹œìŠ¤í…œ
ë‹µë³€ ì •í™•ë„ ê°œì„  ë²„ì „
"""

import os
import glob
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

def main():
    print("=" * 60)
    print("ğŸ“Š ì˜¤í”„ë¼ì¸ GGUF CSV RAG ì‹œìŠ¤í…œ (ê°œì„  ë²„ì „)")
    print("=" * 60)
   
    # 1. ì„ë² ë”© ëª¨ë¸ ê²½ë¡œ
    local_model_path = "./offline_models/all-MiniLM-L6-v2"
   
    if not os.path.exists(local_model_path):
        print(f"âŒ ì„ë² ë”© ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {local_model_path}")
        print("\nì˜¨ë¼ì¸ ëª¨ë¸ ì‚¬ìš©ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        local_model_path = "sentence-transformers/all-MiniLM-L6-v2"
   
    # 2. CSV íŒŒì¼ ë¡œë“œ
    folder_path = "./output_by_date"
    print(f"\nğŸ“ CSV í´ë”: {folder_path}")
   
    csv_files = glob.glob(os.path.join(folder_path, "*.csv"))
    if not csv_files:
        print("âŒ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return
   
    print(f"âœ… ë°œê²¬ëœ íŒŒì¼: {len(csv_files)}ê°œ")
   
    # ë°ì´í„° ë¡œë“œ
    all_data = []
    for i, file in enumerate(csv_files[:30]):
        try:
            df = pd.read_csv(file)
            all_data.append(df)
            print(f"  - {os.path.basename(file)}: {len(df)}í–‰")
        except Exception as e:
            print(f"  - ì˜¤ë¥˜: {file} - {e}")
   
    if not all_data:
        print("âŒ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
        
    df = pd.concat(all_data, ignore_index=True)
    print(f"\nâœ… ì´ ë¡œë“œëœ ë°ì´í„°: {len(df)}í–‰")
    print(f"âœ… ì»¬ëŸ¼: {list(df.columns)}")
   
    # 3. ë¬¸ì„œ ìƒì„± (ì¤‘ìš”: ë” ëª…í™•í•œ ë¬¸ì„œ ìƒì„±)
    print("\nğŸ“„ ë¬¸ì„œ ìƒì„± ì¤‘...")
    documents = []
   
    # === í•µì‹¬ ê°œì„ : ì •í™•í•œ í†µê³„ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë¬¸ì„œí™” ===
    
    # ì „ì²´ ìš”ì•½ ë¬¸ì„œ
    summary_text = f"""
ë°ì´í„° ìš”ì•½ ì •ë³´:
- ì „ì²´ ë°ì´í„° ê°œìˆ˜: {len(df)}ê°œ
- ì „ì²´ í–‰ ìˆ˜: {len(df)}í–‰
- ì»¬ëŸ¼ ëª©ë¡: {', '.join(df.columns)}
"""
    documents.append(Document(page_content=summary_text))
   
    # TOTALCNT í†µê³„ (ìˆëŠ” ê²½ìš°)
    if 'TOTALCNT' in df.columns:
        # ì •í™•í•œ í†µê³„ ê³„ì‚°
        avg_val = df['TOTALCNT'].mean()
        max_val = df['TOTALCNT'].max()
        min_val = df['TOTALCNT'].min()
        count_1400 = (df['TOTALCNT'] >= 1400).sum()
        count_1500 = (df['TOTALCNT'] >= 1500).sum()
        
        stats_text = f"""
TOTALCNT í†µê³„:
- TOTALCNT í‰ê· : {avg_val:.0f}
- TOTALCNT ìµœëŒ€ê°’: {max_val}
- TOTALCNT ìµœì†Œê°’: {min_val}
- 1400 ì´ìƒì¸ ë°ì´í„° ê°œìˆ˜: {count_1400}ê°œ
- 1500 ì´ìƒì¸ ë°ì´í„° ê°œìˆ˜: {count_1500}ê°œ
"""
        documents.append(Document(page_content=stats_text))
        
        # ì§ˆë¬¸ ë‹µë³€ìš© ëª…ì‹œì  ë¬¸ì„œ
        qa_text = f"""
ì§ˆë¬¸: ì „ì²´ ë°ì´í„°ëŠ” ëª‡ ê°œì•¼?
ë‹µë³€: {len(df)}ê°œ

ì§ˆë¬¸: TOTALCNT í‰ê· ì€?
ë‹µë³€: {avg_val:.0f}

ì§ˆë¬¸: 1500 ì´ìƒì¸ ë°ì´í„°ëŠ” ëª‡ ê°œ?
ë‹µë³€: {count_1500}ê°œ

ì§ˆë¬¸: 1400 ì´ìƒì¸ ë°ì´í„°ëŠ” ëª‡ ê°œ?
ë‹µë³€: {count_1400}ê°œ
"""
        documents.append(Document(page_content=qa_text))
   
    # ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€ (ìƒìœ„ 100ê°œ)
    for idx, row in df.head(100).iterrows():
        row_text = f"ë°ì´í„° ì¸ë±ìŠ¤ {idx}: "
        for col in df.columns:
            if pd.notna(row[col]):
                row_text += f"{col}={row[col]}, "
        documents.append(Document(page_content=row_text[:500]))  # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
   
    print(f"âœ… ìƒì„±ëœ ë¬¸ì„œ: {len(documents)}ê°œ")
   
    # 4. í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100,  # ì˜¤ë²„ë© ì¦ê°€
        separators=["\n\n", "\n", " ", ""]
    )
    texts = text_splitter.split_documents(documents)
    print(f"âœ… ë¶„í• ëœ í…ìŠ¤íŠ¸: {len(texts)}ê°œ")
   
    # 5. ì„ë² ë”© ìƒì„±
    print("\nğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=local_model_path,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
   
    # 6. ë²¡í„° ìŠ¤í† ì–´
    print("ğŸ”„ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
    vectorstore = FAISS.from_documents(texts, embeddings)
    print("âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
    
    # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ (ì¬ì‚¬ìš© ê°€ëŠ¥)
    vectorstore.save_local("./faiss_index")
    print("âœ… ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ")
   
    # 7. GGUF ëª¨ë¸ ë¡œë“œ
    print("\nğŸ¤– GGUF ëª¨ë¸ ë¡œë”© ì¤‘...")
    model_path = "./KoSOLAR-10.7B-v0.2.Q3_K_M.gguf"
   
    if not os.path.exists(model_path):
        print(f"âŒ GGUF ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return
   
    # ì½œë°± ë§¤ë‹ˆì € (ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥)
    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
   
    llm = LlamaCpp(
        model_path=model_path,
        n_ctx=2048,
        max_tokens=256,
        temperature=0.1,  # ë‚®ì€ ì˜¨ë„ë¡œ ì •í™•ë„ í–¥ìƒ
        top_p=0.9,
        n_threads=8,
        callback_manager=callback_manager,
        verbose=False
    )
    print("âœ… GGUF ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
   
    # 8. ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt_template = """ì•„ë˜ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë§Œì•½ ë¬¸ë§¥ì— ë‹µì´ ì—†ë‹¤ë©´ "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.

ë¬¸ë§¥:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€: """

    PROMPT = PromptTemplate(
        template=prompt_template, 
        input_variables=["context", "question"]
    )
   
    # 9. QA ì²´ì¸ ìƒì„±
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(
            search_kwargs={"k": 5}  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
        ),
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True  # ì†ŒìŠ¤ ë¬¸ì„œë„ ë°˜í™˜
    )
   
    print("\n" + "=" * 60)
    print("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    print("\nğŸ“Š ë°ì´í„° ì •ë³´:")
    print(f"  - ì´ ë°ì´í„°: {len(df)}ê°œ")
    if 'TOTALCNT' in df.columns:
        print(f"  - TOTALCNT í‰ê· : {df['TOTALCNT'].mean():.0f}")
        print(f"  - 1500 ì´ìƒ: {(df['TOTALCNT'] >= 1500).sum()}ê°œ")
    print("\nğŸ’¡ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸:")
    print("  - ì „ì²´ ë°ì´í„°ëŠ” ëª‡ ê°œì•¼?")
    print("  - TOTALCNT í‰ê· ì€?")
    print("  - 1500 ì´ìƒì¸ ë°ì´í„°ëŠ” ëª‡ ê°œ?")
    print("=" * 60)
   
    # 10. ìë™ í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰:")
    test_questions = [
        "ì „ì²´ ë°ì´í„°ëŠ” ëª‡ ê°œì•¼?",
        "TOTALCNT í‰ê· ì€?",
        "1500 ì´ìƒì¸ ë°ì´í„°ëŠ” ëª‡ ê°œ?"
    ]
    
    for q in test_questions:
        print(f"\nâ“ {q}")
        try:
            result = qa_chain({"query": q})
            print(f"âœ… {result['result']}")
            # ê²€ìƒ‰ëœ ë¬¸ì„œ í™•ì¸
            if result.get('source_documents'):
                print(f"   (ê²€ìƒ‰ëœ ë¬¸ì„œ {len(result['source_documents'])}ê°œ)")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
    
    # 11. ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ
    print("\n" + "=" * 60)
    print("ğŸ’¬ ëŒ€í™” ëª¨ë“œ (ì¢…ë£Œ: quit/exit)")
    print("=" * 60)
    
    while True:
        question = input("\nğŸ’¬ ì§ˆë¬¸: ")
        if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
           
        try:
            print("\nğŸ¤” ê²€ìƒ‰ ì¤‘...")
            result = qa_chain({"query": question})
            print("\nğŸ¤– ë‹µë³€:")
            print(result['result'])
            
            # ë””ë²„ê·¸ ì •ë³´ (ì„ íƒì )
            show_debug = input("\nğŸ“‹ ê²€ìƒ‰ëœ ë¬¸ì„œ ë³´ê¸°? (y/n): ")
            if show_debug.lower() == 'y':
                print("\nğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ:")
                for i, doc in enumerate(result.get('source_documents', [])[:3]):
                    print(f"\n[ë¬¸ì„œ {i+1}]")
                    print(doc.page_content[:200])
                    
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()