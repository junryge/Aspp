"""
ì œì¡°ê³µì • ë³‘ëª© ì˜ˆì¸¡ ì‹œìŠ¤í…œ
Python 3.11.4 í˜¸í™˜
MCS ë¡œê·¸ ë°ì´í„° ê¸°ë°˜ ë³‘ëª© ì˜ˆì¸¡
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import random
from typing import Dict, List, Tuple, Optional
import json
import pickle
from pathlib import Path

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import xgboost as xgb

# ë²¡í„° ì €ì¥ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# ì‹œê³„ì—´ ë¶„ì„
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
import warnings
warnings.filterwarnings('ignore')

# ===== 1. MCS ë¡œê·¸ ìƒ˜í”Œ ë°ì´í„° ìƒì„± =====
def generate_mcs_sample_data(n_days=30, interval_minutes=5):
    """
    MCS ë¡œê·¸ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    - 30ì¼ê°„ì˜ ë°ì´í„°
    - 5ë¶„ ê°„ê²©ìœ¼ë¡œ ìˆ˜ì§‘
    - 3ê°œì˜ ìƒì‚° ë¼ì¸ (LINE_A, LINE_B, LINE_C)
    """
    
    print("MCS ë¡œê·¸ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
    
    # ì‹œê°„ ë²”ìœ„ ì„¤ì •
    start_date = datetime.now() - timedelta(days=n_days)
    end_date = datetime.now()
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    timestamps = pd.date_range(start=start_date, end=end_date, freq=f'{interval_minutes}min')
    
    # ë¼ì¸ ì •ë³´
    lines = ['LINE_A', 'LINE_B', 'LINE_C']
    
    data = []
    
    for timestamp in timestamps:
        hour = timestamp.hour
        day_of_week = timestamp.dayofweek
        
        for line_id in lines:
            # ê¸°ë³¸ ì •ìƒ ê°’
            normal_throughput = 100  # ê°œ/ì‹œê°„
            normal_cycle_time = 5    # ë¶„
            normal_wait_time = 10    # ë¶„
            normal_utilization = 95  # %
            
            # ì‹œê°„ëŒ€ë³„ ë³‘ëª© ë°œìƒ íŒ¨í„´
            # ì˜¤ì „ 9-11ì‹œ, ì˜¤í›„ 2-4ì‹œì— ë³‘ëª© ê°€ëŠ¥ì„± ë†’ìŒ
            if (9 <= hour <= 11) or (14 <= hour <= 16):
                bottleneck_prob = 0.35
            # ì ì‹¬ì‹œê°„
            elif 12 <= hour <= 13:
                bottleneck_prob = 0.15
            # ì•¼ê°„
            elif hour >= 22 or hour <= 6:
                bottleneck_prob = 0.05
            else:
                bottleneck_prob = 0.1
            
            # ì£¼ë§ì€ ë³‘ëª© ê°€ëŠ¥ì„± ë‚®ìŒ
            if day_of_week in [5, 6]:  # í† , ì¼
                bottleneck_prob *= 0.5
            
            # íŠ¹ì • ë¼ì¸ì˜ ê³ ì¥ íŒ¨í„´ ì‹œë®¬ë ˆì´ì…˜
            if line_id == 'LINE_B' and hour in [10, 15]:
                bottleneck_prob += 0.2
            
            # ë³‘ëª© ìƒí™© ê²°ì •
            is_bottleneck = random.random() < bottleneck_prob
            
            if is_bottleneck:
                # ë³‘ëª© ìƒí™© ë°ì´í„°
                throughput = normal_throughput * random.uniform(0.5, 0.75)
                cycle_time = normal_cycle_time * random.uniform(1.2, 1.8)
                wait_time = normal_wait_time * random.uniform(1.5, 2.5)
                utilization = normal_utilization * random.uniform(0.6, 0.8)
                error_count = random.randint(2, 8)
                downtime_minutes = random.uniform(5, 30)
            else:
                # ì •ìƒ ìƒí™© ë°ì´í„° (ì•½ê°„ì˜ ë³€ë™ í¬í•¨)
                throughput = normal_throughput * random.uniform(0.95, 1.05)
                cycle_time = normal_cycle_time * random.uniform(0.95, 1.05)
                wait_time = normal_wait_time * random.uniform(0.9, 1.1)
                utilization = normal_utilization * random.uniform(0.95, 1.0)
                error_count = random.randint(0, 2)
                downtime_minutes = random.uniform(0, 5)
            
            # ë°ì´í„° ì €ì¥
            data.append({
                'timestamp': timestamp,
                'line_id': line_id,
                'throughput': round(throughput, 2),
                'cycle_time': round(cycle_time, 2),
                'wait_time': round(wait_time, 2),
                'utilization': round(utilization, 2),
                'error_count': error_count,
                'downtime_minutes': round(downtime_minutes, 2),
                'is_bottleneck': int(is_bottleneck),
                'shift': 'day' if 8 <= hour <= 16 else 'evening' if 16 < hour <= 24 else 'night'
            })
    
    df = pd.DataFrame(data)
    print(f"ìƒì„±ëœ ë°ì´í„°: {len(df)} í–‰")
    print(f"ë³‘ëª© ë¹„ìœ¨: {df['is_bottleneck'].mean():.2%}")
    
    return df

# ===== 2. ë³‘ëª© ìœ„í—˜ë„ ì§€ìˆ˜(BRI) ê³„ì‚° =====
def calculate_bri(row, weights=None):
    """
    ë³‘ëª© ìœ„í—˜ë„ ì§€ìˆ˜(Bottleneck Risk Index) ê³„ì‚°
    BRI = Î±1*U + Î±2*W + Î±3*C + Î±4*T
    """
    if weights is None:
        weights = {
            'utilization': 0.3,
            'wait_time': 0.25,
            'cycle_time': 0.25,
            'throughput': 0.2
        }
    
    # ê° ì§€ìˆ˜ ê³„ì‚° (ì •ê·œí™”)
    U = (100 - row['utilization']) / 100  # ê°€ë™ë¥  ì§€ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ìœ„í—˜)
    W = row['wait_time'] / 10  # ëŒ€ê¸°ì‹œê°„ ì§€ìˆ˜ (10ë¶„ ê¸°ì¤€)
    C = row['cycle_time'] / 5  # ì‚¬ì´í´íƒ€ì„ ì§€ìˆ˜ (5ë¶„ ê¸°ì¤€)
    T = 100 / max(row['throughput'], 1)  # ì²˜ë¦¬ëŸ‰ ì§€ìˆ˜ (ì—­ìˆ˜)
    
    # BRI ê³„ì‚°
    bri = (weights['utilization'] * U + 
           weights['wait_time'] * W + 
           weights['cycle_time'] * C + 
           weights['throughput'] * T)
    
    return bri

# ===== 3. ë°ì´í„° ì „ì²˜ë¦¬ =====
class DataPreprocessor:
    """ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        
    def preprocess(self, df):
        """ë°ì´í„° ì „ì²˜ë¦¬ ìˆ˜í–‰"""
        print("\në°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
        
        # 1. ì´ìƒì¹˜ ì œê±° (IQR ë°©ë²•)
        df_clean = self.remove_outliers(df)
        
        # 2. ì‹œê°„ íŠ¹ì§• ì¶”ê°€
        df_clean = self.add_time_features(df_clean)
        
        # 3. BRI ê³„ì‚°
        df_clean['bri'] = df_clean.apply(calculate_bri, axis=1)
        
        # 4. ê²½ë¡œë³„ ì§‘ê³„ íŠ¹ì§• ì¶”ê°€
        df_clean = self.add_path_features(df_clean)
        
        print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df_clean)} í–‰")
        
        return df_clean
    
    def remove_outliers(self, df, columns=['throughput', 'cycle_time', 'wait_time']):
        """IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ ì œê±°"""
        df_clean = df.copy()
        
        for col in columns:
            Q1 = df_clean[col].quantile(0.25)
            Q3 = df_clean[col].quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # ì´ìƒì¹˜ ì œê±°
            df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]
        
        print(f"ì´ìƒì¹˜ ì œê±°: {len(df) - len(df_clean)} í–‰ ì œê±°ë¨")
        
        return df_clean
    
    def add_time_features(self, df):
        """ì‹œê°„ ê´€ë ¨ íŠ¹ì§• ì¶”ê°€"""
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['is_weekend'] = (df['day_of_week'].isin([5, 6])).astype(int)
        df['month'] = df['timestamp'].dt.month
        df['day'] = df['timestamp'].dt.day
        
        # ì‹œê°„ëŒ€ë³„ êµ¬ë¶„
        df['time_period'] = pd.cut(df['hour'], 
                                   bins=[0, 6, 12, 18, 24], 
                                   labels=['dawn', 'morning', 'afternoon', 'night'])
        
        return df
    
    def add_path_features(self, df):
        """ê²½ë¡œë³„ ì§‘ê³„ íŠ¹ì§• ì¶”ê°€"""
        # ë¼ì¸ë³„ 1ì‹œê°„ ì´ë™í‰ê· 
        for col in ['throughput', 'wait_time', 'utilization']:
            df[f'{col}_ma_1h'] = df.groupby('line_id')[col].transform(
                lambda x: x.rolling('1H', on=df['timestamp']).mean()
            )
        
        # ë¼ì¸ë³„ ëˆ„ì  ì—ëŸ¬ ìˆ˜
        df['cumulative_errors'] = df.groupby('line_id')['error_count'].cumsum()
        
        return df

# ===== 4. XGBoost ëª¨ë¸ (1ì°¨ í•„í„°) =====
class XGBoostBottleneckDetector:
    """XGBoost ê¸°ë°˜ ì‹¤ì‹œê°„ ë³‘ëª© ê°ì§€"""
    
    def __init__(self):
        self.model = None
        self.feature_cols = None
        self.scaler = StandardScaler()
        
    def prepare_features(self, df):
        """ëª¨ë¸ í•™ìŠµìš© íŠ¹ì§• ì¤€ë¹„"""
        feature_cols = [
            'throughput', 'cycle_time', 'wait_time', 'utilization',
            'error_count', 'downtime_minutes', 'bri',
            'hour', 'day_of_week', 'is_weekend',
            'throughput_ma_1h', 'wait_time_ma_1h', 'utilization_ma_1h',
            'cumulative_errors'
        ]
        
        # NaN ê°’ ì²˜ë¦¬
        for col in feature_cols:
            if col in df.columns:
                df[col] = df[col].fillna(df[col].mean())
        
        self.feature_cols = [col for col in feature_cols if col in df.columns]
        
        return df[self.feature_cols]
    
    def train(self, X_train, y_train):
        """ëª¨ë¸ í•™ìŠµ"""
        print("\nXGBoost ëª¨ë¸ í•™ìŠµ ì¤‘...")
        
        # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        X_train_scaled = self.scaler.fit_transform(X_train)
        
        # XGBoost íŒŒë¼ë¯¸í„°
        params = {
            'objective': 'binary:logistic',
            'max_depth': 6,
            'learning_rate': 0.1,
            'n_estimators': 100,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'use_label_encoder': False,
            'eval_metric': 'logloss'
        }
        
        # ëª¨ë¸ í•™ìŠµ
        self.model = xgb.XGBClassifier(**params)
        self.model.fit(X_train_scaled, y_train)
        
        print("XGBoost í•™ìŠµ ì™„ë£Œ!")
        
    def predict(self, X):
        """ì˜ˆì¸¡ ìˆ˜í–‰ (0.01ì´ˆ ì´ë‚´)"""
        X_scaled = self.scaler.transform(X)
        
        # ì˜ˆì¸¡ í™•ë¥ 
        proba = self.model.predict_proba(X_scaled)[:, 1]
        
        # ìœ„í—˜ë„ ë ˆë²¨ ë¶„ë¥˜
        risk_levels = []
        for p in proba:
            if p < 0.3:
                risk_levels.append('ì •ìƒ')
            elif p < 0.5:
                risk_levels.append('ì£¼ì˜')
            elif p < 0.7:
                risk_levels.append('ê²½ê³ ')
            else:
                risk_levels.append('ìœ„í—˜')
        
        return proba, risk_levels
    
    def get_feature_importance(self):
        """íŠ¹ì§• ì¤‘ìš”ë„ ë°˜í™˜"""
        importance = pd.DataFrame({
            'feature': self.feature_cols,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        return importance

# ===== 5. GRU ëª¨ë¸ (2ì°¨ ì •ë°€ë¶„ì„) - ê°„ë‹¨í•œ êµ¬í˜„ =====
class SimpleGRUPredictor:
    """ê°„ë‹¨í•œ GRU ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” TensorFlow/PyTorch ì‚¬ìš©)"""
    
    def __init__(self, sequence_length=24):
        self.sequence_length = sequence_length  # 24ì‹œê°„ ë°ì´í„° ì‚¬ìš©
        self.patterns = {}  # íŒ¨í„´ ì €ì¥
        
    def learn_patterns(self, df):
        """ê³¼ê±° íŒ¨í„´ í•™ìŠµ"""
        print("\nGRU íŒ¨í„´ í•™ìŠµ ì¤‘...")
        
        # ë¼ì¸ë³„ë¡œ ë³‘ëª© íŒ¨í„´ í•™ìŠµ
        for line_id in df['line_id'].unique():
            line_data = df[df['line_id'] == line_id].sort_values('timestamp')
            
            # ë³‘ëª© ë°œìƒ ì „ 24ì‹œê°„ íŒ¨í„´ ìˆ˜ì§‘
            bottleneck_indices = line_data[line_data['is_bottleneck'] == 1].index
            
            patterns = []
            for idx in bottleneck_indices:
                if idx >= self.sequence_length:
                    pattern = line_data.loc[idx-self.sequence_length:idx-1, 
                                           ['throughput', 'wait_time', 'utilization']].values
                    patterns.append(pattern)
            
            if patterns:
                self.patterns[line_id] = np.array(patterns).mean(axis=0)
        
        print("GRU íŒ¨í„´ í•™ìŠµ ì™„ë£Œ!")
    
    def predict_bottleneck_time(self, current_data, line_id):
        """ë³‘ëª© ë°œìƒ ì‹œê°„ ì˜ˆì¸¡"""
        if line_id not in self.patterns:
            return None, 0.5
        
        # í˜„ì¬ íŒ¨í„´ê³¼ í•™ìŠµëœ íŒ¨í„´ ë¹„êµ (ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚°)
        learned_pattern = self.patterns[line_id]
        
        if len(current_data) < self.sequence_length:
            return None, 0.5
        
        current_pattern = current_data[-self.sequence_length:][['throughput', 'wait_time', 'utilization']].values
        
        # íŒ¨í„´ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = 1 - np.mean(np.abs(current_pattern - learned_pattern))
        
        # ì˜ˆì¸¡ ì‹œê°„ (ìœ ì‚¬ë„ê°€ ë†’ì„ìˆ˜ë¡ ë¹¨ë¦¬ ë°œìƒ)
        if similarity > 0.8:
            predicted_hours = random.uniform(1, 3)
        elif similarity > 0.6:
            predicted_hours = random.uniform(3, 6)
        else:
            predicted_hours = None
        
        return predicted_hours, similarity

# ===== 6. RAG ì‹œìŠ¤í…œ (ê³¼ê±° ì‚¬ë¡€ ê²€ìƒ‰) =====
class RAGSystem:
    """ê³¼ê±° ë³‘ëª© ì‚¬ë¡€ ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.case_database = []
        self.vectorizer = TfidfVectorizer(max_features=100)
        self.vectors = None
        
    def build_case_database(self, df):
        """ë³‘ëª© ì‚¬ë¡€ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"""
        print("\nRAG ì‚¬ë¡€ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        bottleneck_cases = df[df['is_bottleneck'] == 1]
        
        for _, row in bottleneck_cases.iterrows():
            # ì‚¬ë¡€ ì„¤ëª… ìƒì„±
            case_description = f"""
            ë¼ì¸: {row['line_id']}
            ì‹œê°„: {row['timestamp']}
            ì²˜ë¦¬ëŸ‰: {row['throughput']:.1f}
            ëŒ€ê¸°ì‹œê°„: {row['wait_time']:.1f}ë¶„
            ê°€ë™ë¥ : {row['utilization']:.1f}%
            ì—ëŸ¬ìˆ˜: {row['error_count']}
            """
            
            # í•´ê²° ë°©ì•ˆ ì‹œë®¬ë ˆì´ì…˜
            if row['error_count'] > 5:
                solution = "ì„¤ë¹„ ì ê²€ ë° ì¬ì‹œì‘ í•„ìš”"
                root_cause = "ì„¤ë¹„ ì˜¤ë¥˜ ëˆ„ì "
            elif row['wait_time'] > 20:
                solution = "ë²„í¼ ìš©ëŸ‰ ì¦ì„¤ ë˜ëŠ” ë¼ì¸ ë°¸ëŸ°ì‹±"
                root_cause = "ê³µì •ê°„ ë¶ˆê· í˜•"
            elif row['utilization'] < 70:
                solution = "ì˜ˆë°© ì •ë¹„ ì‹¤ì‹œ"
                root_cause = "ì„¤ë¹„ ë…¸í›„í™”"
            else:
                solution = "ìƒì‚° ì¼ì • ì¡°ì •"
                root_cause = "ê³¼ë¶€í•˜"
            
            self.case_database.append({
                'description': case_description,
                'solution': solution,
                'root_cause': root_cause,
                'line_id': row['line_id'],
                'bri': calculate_bri(row)
            })
        
        # ë²¡í„°í™”
        descriptions = [case['description'] for case in self.case_database]
        self.vectors = self.vectorizer.fit_transform(descriptions)
        
        print(f"RAG ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self.case_database)}ê°œ ì‚¬ë¡€")
    
    def search_similar_cases(self, current_situation, top_k=5):
        """ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰"""
        # í˜„ì¬ ìƒí™© ì„¤ëª…
        query = f"""
        ì²˜ë¦¬ëŸ‰: {current_situation['throughput']:.1f}
        ëŒ€ê¸°ì‹œê°„: {current_situation['wait_time']:.1f}ë¶„
        ê°€ë™ë¥ : {current_situation['utilization']:.1f}%
        ì—ëŸ¬ìˆ˜: {current_situation['error_count']}
        """
        
        # ë²¡í„°í™” ë° ìœ ì‚¬ë„ ê³„ì‚°
        query_vector = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.vectors)[0]
        
        # ìƒìœ„ kê°œ ì‚¬ë¡€ ì„ íƒ
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        similar_cases = []
        for idx in top_indices:
            if similarities[idx] > 0.1:  # ìµœì†Œ ìœ ì‚¬ë„
                case = self.case_database[idx].copy()
                case['similarity'] = similarities[idx]
                similar_cases.append(case)
        
        return similar_cases

# ===== 7. í†µí•© ì˜ˆì¸¡ ì‹œìŠ¤í…œ =====
class BottleneckPredictionSystem:
    """ë³‘ëª© ì˜ˆì¸¡ í†µí•© ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.xgboost_model = XGBoostBottleneckDetector()
        self.gru_model = SimpleGRUPredictor()
        self.rag_system = RAGSystem()
        self.preprocessor = DataPreprocessor()
        
    def train(self, df):
        """ì „ì²´ ì‹œìŠ¤í…œ í•™ìŠµ"""
        print("\n=== ë³‘ëª© ì˜ˆì¸¡ ì‹œìŠ¤í…œ í•™ìŠµ ì‹œì‘ ===")
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        df_processed = self.preprocessor.preprocess(df)
        
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
        train_df = df_processed[df_processed['timestamp'] < df_processed['timestamp'].max() - timedelta(days=7)]
        test_df = df_processed[df_processed['timestamp'] >= df_processed['timestamp'].max() - timedelta(days=7)]
        
        # XGBoost í•™ìŠµ
        X_train = self.xgboost_model.prepare_features(train_df)
        y_train = train_df['is_bottleneck']
        self.xgboost_model.train(X_train, y_train)
        
        # GRU íŒ¨í„´ í•™ìŠµ
        self.gru_model.learn_patterns(train_df)
        
        # RAG ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        self.rag_system.build_case_database(train_df)
        
        print("\n=== ì‹œìŠ¤í…œ í•™ìŠµ ì™„ë£Œ ===")
        
        return test_df
    
    def predict(self, current_data):
        """í†µí•© ì˜ˆì¸¡ ìˆ˜í–‰"""
        # 1ì°¨ í•„í„°: XGBoost
        X = self.xgboost_model.prepare_features(current_data)
        xgb_proba, risk_levels = self.xgboost_model.predict(X)
        
        results = []
        
        for idx, (_, row) in enumerate(current_data.iterrows()):
            result = {
                'timestamp': row['timestamp'],
                'line_id': row['line_id'],
                'xgboost_probability': xgb_proba[idx],
                'risk_level': risk_levels[idx],
                'bri': row['bri']
            }
            
            # 2ì°¨ ë¶„ì„: ìœ„í—˜ë„ 70% ì´ìƒì¼ ë•Œë§Œ
            if xgb_proba[idx] > 0.7:
                # GRU ì˜ˆì¸¡
                line_data = current_data[current_data['line_id'] == row['line_id']]
                predicted_hours, pattern_similarity = self.gru_model.predict_bottleneck_time(
                    line_data, row['line_id']
                )
                
                result['gru_predicted_hours'] = predicted_hours
                result['pattern_similarity'] = pattern_similarity
                
                # RAG ê²€ìƒ‰
                similar_cases = self.rag_system.search_similar_cases(row, top_k=3)
                result['similar_cases'] = similar_cases
                
                # ìµœì¢… íŒë‹¨ (PHI-4 ì‹œë®¬ë ˆì´ì…˜)
                result['final_diagnosis'] = self._phi4_analysis(result, row)
            
            results.append(result)
        
        return pd.DataFrame(results)
    
    def _phi4_analysis(self, prediction_result, current_data):
        """PHI-4 LLM ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜"""
        diagnosis = {
            'bottleneck_probability': 0,
            'root_cause': '',
            'immediate_action': '',
            'long_term_solution': ''
        }
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì¢…í•©
        xgb_weight = 0.4
        gru_weight = 0.3
        rag_weight = 0.3
        
        # XGBoost í™•ë¥ 
        xgb_prob = prediction_result['xgboost_probability']
        
        # GRU í™•ë¥  (íŒ¨í„´ ìœ ì‚¬ë„ ê¸°ë°˜)
        gru_prob = prediction_result.get('pattern_similarity', 0.5)
        
        # RAG í™•ë¥  (ìœ ì‚¬ ì‚¬ë¡€ ê¸°ë°˜)
        if prediction_result.get('similar_cases'):
            rag_prob = np.mean([case['similarity'] for case in prediction_result['similar_cases'][:3]])
        else:
            rag_prob = 0.5
        
        # ìµœì¢… í™•ë¥  ê³„ì‚°
        final_prob = (xgb_weight * xgb_prob + 
                     gru_weight * gru_prob + 
                     rag_weight * rag_prob)
        
        diagnosis['bottleneck_probability'] = final_prob
        
        # ê·¼ë³¸ ì›ì¸ ë¶„ì„
        if current_data['error_count'] > 5:
            diagnosis['root_cause'] = "ì„¤ë¹„ ì˜¤ë¥˜ ëˆ„ì ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜"
            diagnosis['immediate_action'] = "í•´ë‹¹ ë¼ì¸ ê¸´ê¸‰ ì ê²€ ë° ì—ëŸ¬ ë¡œê·¸ í™•ì¸"
            diagnosis['long_term_solution'] = "ì˜ˆë°© ì •ë¹„ ì£¼ê¸° ë‹¨ì¶• ë° ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ê°•í™”"
        elif current_data['wait_time'] > 20:
            diagnosis['root_cause'] = "ê³µì •ê°„ ì²˜ë¦¬ ì†ë„ ë¶ˆê· í˜•"
            diagnosis['immediate_action'] = "ëŒ€ì²´ ë¼ì¸ìœ¼ë¡œ ìƒì‚° ë¶„ì‚°"
            diagnosis['long_term_solution'] = "ë¼ì¸ ë°¸ëŸ°ì‹± ì¬ì„¤ê³„ ë° ë²„í¼ ìš©ëŸ‰ ì¦ëŒ€"
        elif current_data['utilization'] < 70:
            diagnosis['root_cause'] = "ì„¤ë¹„ ë…¸í›„í™”ë¡œ ì¸í•œ íš¨ìœ¨ ì €í•˜"
            diagnosis['immediate_action'] = "ê°€ë™ ì†ë„ ì¡°ì • ë° ë¶€ë¶„ ì •ë¹„"
            diagnosis['long_term_solution'] = "í•µì‹¬ ë¶€í’ˆ êµì²´ ê³„íš ìˆ˜ë¦½"
        else:
            diagnosis['root_cause'] = "ì¼ì‹œì  ê³¼ë¶€í•˜"
            diagnosis['immediate_action'] = "ìƒì‚° ì¼ì • ì¡°ì •"
            diagnosis['long_term_solution'] = "ìˆ˜ìš” ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ"
        
        return diagnosis

# ===== 8. ì‹¤í–‰ ì˜ˆì œ =====
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=== ì œì¡°ê³µì • ë³‘ëª© ì˜ˆì¸¡ ì‹œìŠ¤í…œ ===")
    print("Python 3.11.4 í˜¸í™˜ ë²„ì „")
    print("="*40)
    
    # 1. ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    df = generate_mcs_sample_data(n_days=30, interval_minutes=5)
    
    # 2. ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    print("\në°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
    print(df.head())
    print(f"\në°ì´í„° shape: {df.shape}")
    
    # 3. ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° í•™ìŠµ
    system = BottleneckPredictionSystem()
    test_df = system.train(df)
    
    # 4. ì˜ˆì¸¡ ìˆ˜í–‰
    print("\n=== ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘ ===")
    
    # ìµœê·¼ 24ì‹œê°„ ë°ì´í„°ë¡œ ì˜ˆì¸¡
    recent_data = test_df.tail(24*12)  # 24ì‹œê°„ * (60ë¶„/5ë¶„)
    predictions = system.predict(recent_data)
    
    # 5. ê²°ê³¼ ì¶œë ¥
    print("\n=== ì˜ˆì¸¡ ê²°ê³¼ ===")
    
    # ìœ„í—˜ë„ê°€ ë†’ì€ ì¼€ì´ìŠ¤ë§Œ ì¶œë ¥
    high_risk = predictions[predictions['xgboost_probability'] > 0.7]
    
    if not high_risk.empty:
        print(f"\nâš ï¸  ê³ ìœ„í—˜ ë³‘ëª© ê°ì§€: {len(high_risk)}ê±´")
        
        for _, pred in high_risk.iterrows():
            print(f"\në¼ì¸: {pred['line_id']}")
            print(f"ì‹œê°„: {pred['timestamp']}")
            print(f"ìœ„í—˜ë„: {pred['risk_level']} ({pred['xgboost_probability']:.1%})")
            print(f"BRI ì§€ìˆ˜: {pred['bri']:.2f}")
            
            if 'final_diagnosis' in pred and pred['final_diagnosis']:
                diag = pred['final_diagnosis']
                print(f"\nğŸ“Š ìµœì¢… ì§„ë‹¨:")
                print(f"  - ë³‘ëª© í™•ë¥ : {diag['bottleneck_probability']:.1%}")
                print(f"  - ê·¼ë³¸ ì›ì¸: {diag['root_cause']}")
                print(f"  - ì¦‰ì‹œ ì¡°ì¹˜: {diag['immediate_action']}")
                print(f"  - ì¥ê¸° í•´ê²°ì±…: {diag['long_term_solution']}")
    else:
        print("\nâœ… í˜„ì¬ ëª¨ë“  ë¼ì¸ ì •ìƒ ìš´ì˜ ì¤‘")
    
    # 6. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
    print("\n=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")
    
    # XGBoost íŠ¹ì§• ì¤‘ìš”ë„
    importance = system.xgboost_model.get_feature_importance()
    print("\nXGBoost ì£¼ìš” íŠ¹ì§• (ìƒìœ„ 5ê°œ):")
    print(importance.head())
    
    # ì „ì²´ ì •í™•ë„
    X_test = system.xgboost_model.prepare_features(test_df)
    y_test = test_df['is_bottleneck']
    
    X_test_scaled = system.xgboost_model.scaler.transform(X_test)
    y_pred = system.xgboost_model.model.predict(X_test_scaled)
    
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    
    print(f"\nëª¨ë¸ ì„±ëŠ¥:")
    print(f"  - ì •í™•ë„: {accuracy:.1%}")
    print(f"  - ì •ë°€ë„: {precision:.1%}")
    print(f"  - ì¬í˜„ìœ¨: {recall:.1%}")
    print(f"  - F1 ì ìˆ˜: {f1:.3f}")
    
    # 7. ì‹œìŠ¤í…œ ìƒíƒœ ìš”ì•½
    print("\n=== ì‹œìŠ¤í…œ ìƒíƒœ ìš”ì•½ ===")
    print(f"ì´ ì²˜ë¦¬ ë°ì´í„°: {len(df):,} í–‰")
    print(f"í•™ìŠµ ë°ì´í„°: {len(df) - len(test_df):,} í–‰")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,} í–‰")
    print(f"RAG ì‚¬ë¡€ DB: {len(system.rag_system.case_database)} ê±´")
    print("\nì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! ì‹¤ì‹œê°„ ì˜ˆì¸¡ ê°€ëŠ¥ ìƒíƒœì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()