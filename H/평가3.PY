"""
í•˜ì´ë¸Œë¦¬ë“œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ (í¼ì„¼íŠ¸ ê¸°ë°˜)
=======================================================
í•™ìŠµì´ ì™„ë£Œëœ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸(LSTM, RNN, GRU, Bi-LSTM)ì˜ 
ì„±ëŠ¥ì„ ë‹¤ì–‘í•œ í¼ì„¼íŠ¸ ì§€í‘œë¡œ í‰ê°€í•©ë‹ˆë‹¤.

ì£¼ìš” í‰ê°€ ì§€í‘œ:
1. MAPE (Mean Absolute Percentage Error) - í‰ê·  ì ˆëŒ€ í¼ì„¼íŠ¸ ì˜¤ì°¨
2. ì˜ˆì¸¡ ì •í™•ë„ - ì¼ì • ì˜¤ì°¨ ë²”ìœ„ ë‚´ ì˜ˆì¸¡ ë¹„ìœ¨
3. RÂ² Score - ëª¨ë¸ ì„¤ëª…ë ¥
4. ë°©í–¥ì„± ì •í™•ë„ - ì¦ê° ì˜ˆì¸¡ ì •í™•ë„
5. ë³‘ëª© ì˜ˆì¸¡ ì •í™•ë„ - ë³‘ëª© êµ¬ê°„ ì˜ˆì¸¡ ì„±ê³µë¥ 

ê°œë°œì¼: 2024ë…„
ë²„ì „: 1.0
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import load_model
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
import os
import platform
from datetime import datetime, timedelta
import joblib
import json
import warnings
import traceback

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')

# ===================================
# í•œê¸€ í°íŠ¸ ì„¤ì •
# ===================================
def set_korean_font():
    """ìš´ì˜ì²´ì œë³„ í•œê¸€ í°íŠ¸ ìë™ ì„¤ì •"""
    system = platform.system()
    
    if system == 'Windows':
        # ìœˆë„ìš°
        font_paths = [
            'C:/Windows/Fonts/malgun.ttf',  # ë§‘ì€ ê³ ë”•
            'C:/Windows/Fonts/ngulim.ttf',  # ë‚˜ëˆ”ê³ ë”•
            'C:/Windows/Fonts/NanumGothic.ttf'
        ]
        font_family = 'Malgun Gothic'
    elif system == 'Darwin':
        # ë§¥OS
        font_paths = [
            '/System/Library/Fonts/Supplemental/AppleGothic.ttf',
            '/Library/Fonts/NanumGothic.ttf'
        ]
        font_family = 'AppleGothic'
    else:
        # ë¦¬ëˆ…ìŠ¤
        font_paths = [
            '/usr/share/fonts/truetype/nanum/NanumGothic.ttf',
            '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf'
        ]
        font_family = 'NanumGothic'
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ í°íŠ¸ ì°¾ê¸°
    font_set = False
    for font_path in font_paths:
        if os.path.exists(font_path):
            try:
                font_prop = fm.FontProperties(fname=font_path)
                plt.rcParams['font.family'] = font_prop.get_name()
                font_set = True
                print(f"âœ“ í•œê¸€ í°íŠ¸ ì„¤ì •: {font_prop.get_name()}")
                break
            except:
                continue
    
    # í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ ì„¤ì •
    if not font_set:
        try:
            plt.rcParams['font.family'] = font_family
            print(f"âœ“ í•œê¸€ í°íŠ¸ ì„¤ì •: {font_family}")
        except:
            print("âš  í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ë¬¸ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")
            # ì˜ë¬¸ ë¼ë²¨ ì‚¬ìš©ì„ ìœ„í•œ í”Œë˜ê·¸
            return False
    
    # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
    plt.rcParams['axes.unicode_minus'] = False
    return True

# í•œê¸€ í°íŠ¸ ì„¤ì • ì‹¤í–‰
USE_KOREAN = set_korean_font()

# CPU ëª¨ë“œ ì„¤ì •
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
tf.config.set_visible_devices([], 'GPU')

# ëœë¤ ì‹œë“œ ê³ ì •
RANDOM_SEED = 2079936
tf.random.set_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

class ModelPercentEvaluator:
    """ëª¨ë¸ ì„±ëŠ¥ì„ í¼ì„¼íŠ¸ë¡œ í‰ê°€í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.models = {}
        self.scaler = None
        self.config = None
        
    def load_models_and_config(self):
        """í•™ìŠµëœ ëª¨ë¸ê³¼ ì„¤ì • ë¡œë“œ"""
        print("="*70)
        print("í•™ìŠµëœ ëª¨ë¸ ë¡œë”© ì¤‘..." if USE_KOREAN else "Loading trained models...")
        print("="*70)
        
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ ë¡œë“œ
        model_names = ['lstm', 'gru', 'rnn', 'bi_lstm']
        for model_name in model_names:
            try:
                # ì²« ë²ˆì§¸ ê²½ë¡œ ì‹œë„
                model_path = f'model/{model_name}_final_hybrid.keras'
                if os.path.exists(model_path):
                    self.models[model_name] = load_model(model_path, compile=False)
                    print(f"âœ“ {model_name.upper()} {'ëª¨ë¸ ë¡œë“œ ì™„ë£Œ' if USE_KOREAN else 'model loaded'}")
                else:
                    # ëŒ€ì²´ ê²½ë¡œ ì‹œë„
                    alt_paths = [
                        f'model/BM_s30f10_0731_2079936.keras',
                        f'model/Model_s30f10_0731_2079936.keras',
                        f'model/{model_name}_best.keras'
                    ]
                    for alt_path in alt_paths:
                        if os.path.exists(alt_path):
                            self.models[model_name] = load_model(alt_path, compile=False)
                            print(f"âœ“ {model_name.upper()} {'ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ëŒ€ì²´ ê²½ë¡œ)' if USE_KOREAN else 'model loaded (alt path)'}")
                            break
            except Exception as e:
                print(f"âš  {model_name.upper()} {'ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨' if USE_KOREAN else 'model load failed'}: {str(e)}")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        try:
            scaler_paths = [
                'scaler/standard_scaler_hybrid.pkl',
                'scaler/StdScaler_s30f10_0731_2079936.save'
            ]
            for scaler_path in scaler_paths:
                if os.path.exists(scaler_path):
                    self.scaler = joblib.load(scaler_path)
                    print(f"âœ“ {'ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ' if USE_KOREAN else 'Scaler loaded'}")
                    break
        except Exception as e:
            print(f"âš  {'ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨' if USE_KOREAN else 'Scaler load failed'}: {str(e)}")
        
        # ì„¤ì • ë¡œë“œ
        try:
            config_path = 'results/training_config.json'
            if os.path.exists(config_path):
                with open(config_path, 'r') as f:
                    self.config = json.load(f)
            else:
                self.config = {
                    'seq_length': 30,
                    'future_minutes': 10,
                    'bottleneck_threshold': 2000
                }
            print(f"âœ“ {'ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ' if USE_KOREAN else 'Config file loaded'}")
        except:
            self.config = {'seq_length': 30, 'future_minutes': 10, 'bottleneck_threshold': 2000}
            
    def calculate_mape(self, y_true, y_pred):
        """MAPE (Mean Absolute Percentage Error) ê³„ì‚°"""
        # 0ê°’ ì œì™¸ (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
        mask = y_true != 0
        if not np.any(mask):
            return 0
        
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        return mape
    
    def calculate_smape(self, y_true, y_pred):
        """Symmetric MAPE ê³„ì‚° (0ê°’ ë¬¸ì œ í•´ê²°)"""
        denominator = (np.abs(y_true) + np.abs(y_pred)) / 2
        mask = denominator != 0
        if not np.any(mask):
            return 0
            
        smape = np.mean(np.abs(y_true[mask] - y_pred[mask]) / denominator[mask]) * 100
        return smape
    
    def calculate_accuracy_within_threshold(self, y_true, y_pred, threshold_percent=5):
        """íŠ¹ì • ì˜¤ì°¨ ë²”ìœ„ ë‚´ ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚°"""
        threshold = np.mean(y_true) * (threshold_percent / 100)
        within_threshold = np.abs(y_true - y_pred) <= threshold
        accuracy = np.mean(within_threshold) * 100
        return accuracy
    
    def calculate_directional_accuracy(self, y_true, y_pred, y_previous=None):
        """ë°©í–¥ì„± ì •í™•ë„ (ì¦ê°€/ê°ì†Œ ì˜ˆì¸¡ ì •í™•ë„)"""
        if y_previous is None:
            # ì´ì „ ê°’ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ê°’ì„ ì œì™¸í•˜ê³  ê³„ì‚°
            true_direction = np.diff(y_true) > 0
            pred_direction = np.diff(y_pred) > 0
        else:
            true_direction = (y_true - y_previous) > 0
            pred_direction = (y_pred - y_previous) > 0
        
        directional_accuracy = np.mean(true_direction == pred_direction) * 100
        return directional_accuracy
    
    def calculate_bottleneck_accuracy(self, y_true, y_pred, threshold):
        """ë³‘ëª© êµ¬ê°„ ì˜ˆì¸¡ ì •í™•ë„"""
        true_bottleneck = y_true > threshold
        pred_bottleneck = y_pred > threshold
        
        # True Positive, True Negative, False Positive, False Negative
        tp = np.sum((true_bottleneck == True) & (pred_bottleneck == True))
        tn = np.sum((true_bottleneck == False) & (pred_bottleneck == False))
        fp = np.sum((true_bottleneck == False) & (pred_bottleneck == True))
        fn = np.sum((true_bottleneck == True) & (pred_bottleneck == False))
        
        # ì •í™•ë„
        accuracy = (tp + tn) / (tp + tn + fp + fn) * 100 if (tp + tn + fp + fn) > 0 else 0
        
        # ì •ë°€ë„ (Precision)
        precision = tp / (tp + fp) * 100 if (tp + fp) > 0 else 0
        
        # ì¬í˜„ìœ¨ (Recall)
        recall = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0
        
        # F1 Score
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        }
    
    def comprehensive_evaluation(self, y_true, y_pred, model_name="Model"):
        """ì¢…í•©ì ì¸ í¼ì„¼íŠ¸ ê¸°ë°˜ í‰ê°€"""
        print(f"\n{'='*70}")
        print(f"{model_name} {'ì„±ëŠ¥ í‰ê°€ (í¼ì„¼íŠ¸ ê¸°ì¤€)' if USE_KOREAN else 'Performance Evaluation (Percentage)'}")
        print(f"{'='*70}")
        
        # 1. MAPEì™€ SMAPE
        mape = self.calculate_mape(y_true, y_pred)
        smape = self.calculate_smape(y_true, y_pred)
        
        # 2. ì •í™•ë„ (ë‹¤ì–‘í•œ ì˜¤ì°¨ ë²”ìœ„)
        accuracy_5 = self.calculate_accuracy_within_threshold(y_true, y_pred, 5)
        accuracy_10 = self.calculate_accuracy_within_threshold(y_true, y_pred, 10)
        accuracy_15 = self.calculate_accuracy_within_threshold(y_true, y_pred, 15)
        
        # 3. RÂ² Score (í¼ì„¼íŠ¸ë¡œ ë³€í™˜)
        r2 = r2_score(y_true, y_pred) * 100
        
        # 4. ë°©í–¥ì„± ì •í™•ë„
        directional_acc = self.calculate_directional_accuracy(y_true, y_pred)
        
        # 5. ë³‘ëª© ì˜ˆì¸¡ ì •í™•ë„
        threshold = self.config.get('bottleneck_threshold', 2000)
        bottleneck_metrics = self.calculate_bottleneck_accuracy(y_true, y_pred, threshold)
        
        # 6. ì „ì²´ ì„±ëŠ¥ ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
        overall_score = (
            (100 - min(mape, 100)) * 0.2 +  # MAPEëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            accuracy_10 * 0.3 +               # 10% ì˜¤ì°¨ ë‚´ ì •í™•ë„
            r2 * 0.2 +                        # RÂ² ì ìˆ˜
            directional_acc * 0.15 +          # ë°©í–¥ì„± ì •í™•ë„
            bottleneck_metrics['accuracy'] * 0.15  # ë³‘ëª© ì˜ˆì¸¡ ì •í™•ë„
        )
        
        # ê²°ê³¼ ì¶œë ¥
        if USE_KOREAN:
            print("\nğŸ“Š ì˜ˆì¸¡ ì˜¤ì°¨ ì§€í‘œ:")
            print(f"  â€¢ MAPE (í‰ê·  ì ˆëŒ€ í¼ì„¼íŠ¸ ì˜¤ì°¨): {mape:.2f}%")
            print(f"  â€¢ SMAPE (ëŒ€ì¹­ í‰ê·  ì ˆëŒ€ í¼ì„¼íŠ¸ ì˜¤ì°¨): {smape:.2f}%")
            
            print("\nğŸ¯ ì˜ˆì¸¡ ì •í™•ë„:")
            print(f"  â€¢ 5% ì˜¤ì°¨ ë²”ìœ„ ë‚´ ì •í™•ë„: {accuracy_5:.1f}%")
            print(f"  â€¢ 10% ì˜¤ì°¨ ë²”ìœ„ ë‚´ ì •í™•ë„: {accuracy_10:.1f}%")
            print(f"  â€¢ 15% ì˜¤ì°¨ ë²”ìœ„ ë‚´ ì •í™•ë„: {accuracy_15:.1f}%")
            
            print("\nğŸ“ˆ ëª¨ë¸ ì„¤ëª…ë ¥:")
            print(f"  â€¢ RÂ² Score: {r2:.1f}%")
            print(f"  â€¢ ë°©í–¥ì„± ì˜ˆì¸¡ ì •í™•ë„: {directional_acc:.1f}%")
            
            print("\nğŸš¨ ë³‘ëª© êµ¬ê°„ ì˜ˆì¸¡ ì„±ëŠ¥:")
            print(f"  â€¢ ë³‘ëª© ì˜ˆì¸¡ ì •í™•ë„: {bottleneck_metrics['accuracy']:.1f}%")
            print(f"  â€¢ ë³‘ëª© ì˜ˆì¸¡ ì •ë°€ë„: {bottleneck_metrics['precision']:.1f}%")
            print(f"  â€¢ ë³‘ëª© ì˜ˆì¸¡ ì¬í˜„ìœ¨: {bottleneck_metrics['recall']:.1f}%")
            print(f"  â€¢ F1 Score: {bottleneck_metrics['f1_score']:.1f}%")
            
            print("\nâ­ ì¢…í•© ì„±ëŠ¥ ì ìˆ˜: {:.1f}%".format(overall_score))
        else:
            print("\nğŸ“Š Prediction Error Metrics:")
            print(f"  â€¢ MAPE (Mean Absolute Percentage Error): {mape:.2f}%")
            print(f"  â€¢ SMAPE (Symmetric MAPE): {smape:.2f}%")
            
            print("\nğŸ¯ Prediction Accuracy:")
            print(f"  â€¢ Accuracy within 5% error: {accuracy_5:.1f}%")
            print(f"  â€¢ Accuracy within 10% error: {accuracy_10:.1f}%")
            print(f"  â€¢ Accuracy within 15% error: {accuracy_15:.1f}%")
            
            print("\nğŸ“ˆ Model Performance:")
            print(f"  â€¢ RÂ² Score: {r2:.1f}%")
            print(f"  â€¢ Directional Accuracy: {directional_acc:.1f}%")
            
            print("\nğŸš¨ Bottleneck Prediction Performance:")
            print(f"  â€¢ Bottleneck Accuracy: {bottleneck_metrics['accuracy']:.1f}%")
            print(f"  â€¢ Bottleneck Precision: {bottleneck_metrics['precision']:.1f}%")
            print(f"  â€¢ Bottleneck Recall: {bottleneck_metrics['recall']:.1f}%")
            print(f"  â€¢ F1 Score: {bottleneck_metrics['f1_score']:.1f}%")
            
            print("\nâ­ Overall Performance Score: {:.1f}%".format(overall_score))
        
        # ì„±ëŠ¥ ë“±ê¸‰ íŒì •
        if overall_score >= 90:
            grade = "A+ (íƒì›”í•¨)" if USE_KOREAN else "A+ (Excellent)"
        elif overall_score >= 85:
            grade = "A (ìš°ìˆ˜í•¨)" if USE_KOREAN else "A (Outstanding)"
        elif overall_score >= 80:
            grade = "B+ (ë§¤ìš° ì¢‹ìŒ)" if USE_KOREAN else "B+ (Very Good)"
        elif overall_score >= 75:
            grade = "B (ì¢‹ìŒ)" if USE_KOREAN else "B (Good)"
        elif overall_score >= 70:
            grade = "C+ (ì–‘í˜¸)" if USE_KOREAN else "C+ (Satisfactory)"
        elif overall_score >= 65:
            grade = "C (ë³´í†µ)" if USE_KOREAN else "C (Average)"
        else:
            grade = "D (ê°œì„  í•„ìš”)" if USE_KOREAN else "D (Needs Improvement)"
        
        print(f"ğŸ“Š {'ì„±ëŠ¥ ë“±ê¸‰' if USE_KOREAN else 'Performance Grade'}: {grade}")
        
        return {
            'mape': mape,
            'smape': smape,
            'accuracy_5': accuracy_5,
            'accuracy_10': accuracy_10,
            'accuracy_15': accuracy_15,
            'r2_score': r2,
            'directional_accuracy': directional_acc,
            'bottleneck_metrics': bottleneck_metrics,
            'overall_score': overall_score,
            'grade': grade
        }
    
    def plot_evaluation_results(self, evaluation_results):
        """í‰ê°€ ê²°ê³¼ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        
        # í•œê¸€/ì˜ë¬¸ ë¼ë²¨ ì„¤ì •
        if USE_KOREAN:
            labels = {
                'overall_score': 'ëª¨ë¸ë³„ ì¢…í•© ì„±ëŠ¥ ì ìˆ˜',
                'mape': 'MAPE (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)',
                'accuracy': '10% ì˜¤ì°¨ ë²”ìœ„ ë‚´ ì •í™•ë„',
                'r2': 'RÂ² Score (ëª¨ë¸ ì„¤ëª…ë ¥)',
                'bottleneck': 'ë³‘ëª© êµ¬ê°„ ì˜ˆì¸¡ ì •í™•ë„',
                'best_model': 'ìµœê³  ì„±ëŠ¥ ëª¨ë¸',
                'categories': ['MAPE\n(ì—­)', '10% ì •í™•ë„', 'RÂ² Score', 'ë°©í–¥ì„±\nì •í™•ë„', 'ë³‘ëª© ì˜ˆì¸¡'],
                'good_line': 'ìš°ìˆ˜ ê¸°ì¤€ì„ ',
                'overall_score_y': 'ì¢…í•© ì ìˆ˜ (%)',
                'accuracy_y': 'ì •í™•ë„ (%)'
            }
        else:
            labels = {
                'overall_score': 'Overall Performance Score by Model',
                'mape': 'MAPE (Lower is Better)',
                'accuracy': 'Accuracy within 10% Error Range',
                'r2': 'RÂ² Score (Model Explanatory Power)',
                'bottleneck': 'Bottleneck Prediction Accuracy',
                'best_model': 'Best Performance Model',
                'categories': ['MAPE\n(inv)', '10% Acc', 'RÂ² Score', 'Direction\nAcc', 'Bottleneck'],
                'good_line': 'Excellence Baseline',
                'overall_score_y': 'Overall Score (%)',
                'accuracy_y': 'Accuracy (%)'
            }
        
        # 1. ëª¨ë¸ë³„ ì¢…í•© ì ìˆ˜
        ax = axes[0, 0]
        models = list(evaluation_results.keys())
        scores = [evaluation_results[m]['overall_score'] for m in models]
        colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E']
        bars = ax.bar(models, scores, color=colors[:len(models)])
        ax.set_title(labels['overall_score'], fontsize=14, fontweight='bold')
        ax.set_ylabel(labels['overall_score_y'], fontsize=12)
        ax.set_ylim(0, 100)
        ax.axhline(y=80, color='r', linestyle='--', alpha=0.5, label=labels['good_line'])
        
        # ë§‰ëŒ€ ìœ„ì— ì ìˆ˜ í‘œì‹œ
        for bar, score in zip(bars, scores):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                   f'{score:.1f}%', ha='center', va='bottom', fontsize=10)
        
        # 2. MAPE ë¹„êµ
        ax = axes[0, 1]
        mapes = [evaluation_results[m]['mape'] for m in models]
        bars = ax.bar(models, mapes, color='coral')
        ax.set_title(labels['mape'], fontsize=14, fontweight='bold')
        ax.set_ylabel('MAPE (%)', fontsize=12)
        
        for bar, mape in zip(bars, mapes):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                   f'{mape:.1f}%', ha='center', va='bottom', fontsize=10)
        
        # 3. ì •í™•ë„ ë¹„êµ (10% ì˜¤ì°¨ ë²”ìœ„)
        ax = axes[0, 2]
        accuracies = [evaluation_results[m]['accuracy_10'] for m in models]
        bars = ax.bar(models, accuracies, color='lightgreen')
        ax.set_title(labels['accuracy'], fontsize=14, fontweight='bold')
        ax.set_ylabel(labels['accuracy_y'], fontsize=12)
        ax.set_ylim(0, 100)
        
        for bar, acc in zip(bars, accuracies):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                   f'{acc:.1f}%', ha='center', va='bottom', fontsize=10)
        
        # 4. RÂ² Score ë¹„êµ
        ax = axes[1, 0]
        r2_scores = [evaluation_results[m]['r2_score'] for m in models]
        bars = ax.bar(models, r2_scores, color='skyblue')
        ax.set_title(labels['r2'], fontsize=14, fontweight='bold')
        ax.set_ylabel('RÂ² (%)', fontsize=12)
        ax.set_ylim(0, 100)
        
        for bar, r2 in zip(bars, r2_scores):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                   f'{r2:.1f}%', ha='center', va='bottom', fontsize=10)
        
        # 5. ë³‘ëª© ì˜ˆì¸¡ ì •í™•ë„
        ax = axes[1, 1]
        bottleneck_accs = [evaluation_results[m]['bottleneck_metrics']['accuracy'] for m in models]
        bars = ax.bar(models, bottleneck_accs, color='salmon')
        ax.set_title(labels['bottleneck'], fontsize=14, fontweight='bold')
        ax.set_ylabel(labels['accuracy_y'], fontsize=12)
        ax.set_ylim(0, 100)
        
        for bar, acc in zip(bars, bottleneck_accs):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                   f'{acc:.1f}%', ha='center', va='bottom', fontsize=10)
        
        # 6. ë ˆì´ë” ì°¨íŠ¸ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸)
        ax = axes[1, 2]
        best_model = max(evaluation_results.items(), key=lambda x: x[1]['overall_score'])
        best_model_name, best_model_data = best_model
        
        categories = labels['categories']
        values = [
            100 - min(best_model_data['mape'], 100),  # MAPEëŠ” ì—­ìœ¼ë¡œ
            best_model_data['accuracy_10'],
            best_model_data['r2_score'],
            best_model_data['directional_accuracy'],
            best_model_data['bottleneck_metrics']['accuracy']
        ]
        
        # ë ˆì´ë” ì°¨íŠ¸ë¥¼ ìœ„í•œ ê°ë„
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        values += values[:1]  # ë‹«íŒ ë„í˜•ì„ ìœ„í•´
        angles += angles[:1]
        
        ax = plt.subplot(2, 3, 6, projection='polar')
        ax.plot(angles, values, 'o-', linewidth=2, label=best_model_name.upper())
        ax.fill(angles, values, alpha=0.25)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories)
        ax.set_ylim(0, 100)
        ax.set_title(f"{labels['best_model']}: {best_model_name.upper()}", 
                    fontsize=14, fontweight='bold', pad=20)
        ax.grid(True)
        
        plt.tight_layout()
        
        # ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        plt.savefig(f'model_evaluation_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def evaluate_all_models(self, test_data_path=None):
        """ëª¨ë“  ëª¨ë¸ í‰ê°€ ì‹¤í–‰"""
        # ë°ì´í„° ê²½ë¡œ ìë™ íƒìƒ‰
        if test_data_path is None:
            possible_paths = [
                'data/0730to31.csv',
                'data/Final_Predicted_Data_0731_BM.csv',
                'data/20240201_TO_202507281705.csv'
            ]
            for path in possible_paths:
                if os.path.exists(path):
                    test_data_path = path
                    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ: {test_data_path}")
                    break
            
            if test_data_path is None:
                print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("   ë‹¤ìŒ ê²½ë¡œ ì¤‘ í•˜ë‚˜ì— ë°ì´í„°ë¥¼ ë°°ì¹˜í•˜ì„¸ìš”:")
                for path in possible_paths:
                    print(f"   - {path}")
                return None
        # ëª¨ë¸ ë¡œë“œ
        self.load_models_and_config()
        
        if not self.models:
            print("âŒ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì„ ë¨¼ì € ì™„ë£Œí•˜ì„¸ìš”.")
            return None
        
        print(f"\në¡œë“œëœ ëª¨ë¸ ìˆ˜: {len(self.models)}ê°œ")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        print("\ní…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        X_test, y_test = self.prepare_test_data(test_data_path)
        
        if X_test is None or y_test is None:
            print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨")
            return None
        
        # ê° ëª¨ë¸ í‰ê°€
        evaluation_results = {}
        
        for model_name, model in self.models.items():
            print(f"\n{model_name.upper()} ëª¨ë¸ í‰ê°€ ì¤‘...")
            
            # ì˜ˆì¸¡
            y_pred_scaled = model.predict(X_test, verbose=0).flatten()
            
            # ì—­ìŠ¤ì¼€ì¼ë§
            y_pred = self.inverse_scale(y_pred_scaled)
            y_true = self.inverse_scale(y_test)
            
            # í‰ê°€
            results = self.comprehensive_evaluation(y_true, y_pred, model_name.upper())
            evaluation_results[model_name] = results
        
        # ì•™ìƒë¸” ëª¨ë¸ í‰ê°€
        if len(self.models) > 1:
            print("\nì•™ìƒë¸” ëª¨ë¸ í‰ê°€ ì¤‘...")
            ensemble_pred = self.ensemble_predict(X_test)
            ensemble_pred = self.inverse_scale(ensemble_pred)
            
            results = self.comprehensive_evaluation(y_true, ensemble_pred, "ENSEMBLE")
            evaluation_results['ensemble'] = results
        
        # ì‹œê°í™”
        self.plot_evaluation_results(evaluation_results)
        
        # ê²°ê³¼ ì €ì¥
        self.save_evaluation_results(evaluation_results)
        
        # ìµœì¢… ìš”ì•½
        print("\n" + "="*70)
        print("í‰ê°€ ì™„ë£Œ - ìµœì¢… ìš”ì•½")
        print("="*70)
        
        best_model = max(evaluation_results.items(), key=lambda x: x[1]['overall_score'])
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model[0].upper()}")
        print(f"   ì¢…í•© ì ìˆ˜: {best_model[1]['overall_score']:.1f}%")
        print(f"   ì„±ëŠ¥ ë“±ê¸‰: {best_model[1]['grade']}")
        
        return evaluation_results
    
    def prepare_test_data(self, data_path):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„"""
        try:
            # ë°ì´í„° ë¡œë“œ
            data = pd.read_csv(data_path)
            
            # ì‹œê°„ ì»¬ëŸ¼ ë³€í™˜
            data['CURRTIME'] = pd.to_datetime(data['CURRTIME'], format='%Y%m%d%H%M')
            data['TIME'] = pd.to_datetime(data['TIME'], format='%Y%m%d%H%M')
            
            # SUM ì»¬ëŸ¼ ì œê±°
            columns_to_drop = [col for col in data.columns if 'SUM' in col]
            data = data.drop(columns=columns_to_drop)
            
            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
            data = data[['CURRTIME', 'TOTALCNT', 'TIME']]
            data.set_index('CURRTIME', inplace=True)
            
            # FUTURE ì»¬ëŸ¼ ìƒì„±
            data['FUTURE'] = pd.NA
            future_minutes = self.config.get('future_minutes', 10)
            
            for i in data.index:
                future_time = i + pd.Timedelta(minutes=future_minutes)
                if (future_time <= data.index.max()) & (future_time in data.index):
                    data.loc[i, 'FUTURE'] = data.loc[future_time, 'TOTALCNT']
            
            data.dropna(subset=['FUTURE'], inplace=True)
            
            # ===== íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì¶”ê°€ (í•™ìŠµ ì‹œì™€ ë™ì¼í•˜ê²Œ) =====
            print(f"{'íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ìˆ˜í–‰ ì¤‘...' if USE_KOREAN else 'Performing feature engineering...'}")
            
            # ì‹œê°„ íŠ¹ì§•
            data['hour'] = data.index.hour
            data['dayofweek'] = data.index.dayofweek
            data['is_weekend'] = (data.index.dayofweek >= 5).astype(int)
            
            # ì´ë™ í‰ê·  íŠ¹ì§•
            data['MA_5'] = data['TOTALCNT'].rolling(window=5, min_periods=1).mean()
            data['MA_10'] = data['TOTALCNT'].rolling(window=10, min_periods=1).mean()
            data['MA_30'] = data['TOTALCNT'].rolling(window=30, min_periods=1).mean()
            
            # í‘œì¤€í¸ì°¨ íŠ¹ì§•
            data['STD_5'] = data['TOTALCNT'].rolling(window=5, min_periods=1).std()
            data['STD_10'] = data['TOTALCNT'].rolling(window=10, min_periods=1).std()
            
            # ë³€í™”ìœ¨ íŠ¹ì§•
            data['change_rate'] = data['TOTALCNT'].pct_change()
            data['change_rate_5'] = data['TOTALCNT'].pct_change(5)
            
            # ê²°ì¸¡ê°’ ì²˜ë¦¬
            data = data.ffill().fillna(0)
            
            # ìŠ¤ì¼€ì¼ëŸ¬ê°€ ê¸°ëŒ€í•˜ëŠ” ì»¬ëŸ¼ í™•ì¸
            expected_features = self.scaler.feature_names_in_ if hasattr(self.scaler, 'feature_names_in_') else None
            
            if expected_features is not None:
                print(f"{'ìŠ¤ì¼€ì¼ëŸ¬ê°€ ê¸°ëŒ€í•˜ëŠ” íŠ¹ì§•' if USE_KOREAN else 'Expected features by scaler'}: {list(expected_features)}")
                scale_columns = list(expected_features)
            else:
                # ê¸°ë³¸ ìŠ¤ì¼€ì¼ë§ ì»¬ëŸ¼ (í•™ìŠµ ì‹œì™€ ë™ì¼í•˜ê²Œ)
                scale_columns = ['TOTALCNT', 'FUTURE', 'MA_5', 'MA_10', 'MA_30', 'STD_5', 'STD_10']
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
            scale_columns = [col for col in scale_columns if col in data.columns]
            print(f"{'ìŠ¤ì¼€ì¼ë§í•  ì»¬ëŸ¼' if USE_KOREAN else 'Columns to scale'}: {scale_columns}")
            
            # ìŠ¤ì¼€ì¼ë§
            scaled_data = self.scaler.transform(data[scale_columns])
            scaled_df = pd.DataFrame(scaled_data, columns=[f'scaled_{col}' for col in scale_columns], index=data.index)
            
            # ì›ë³¸ ë°ì´í„°ì™€ ë³‘í•©
            data = pd.merge(data, scaled_df, left_index=True, right_index=True, how='left')
            
            # ì—°ì†ì„± í™•ì¸í•˜ì—¬ ë°ì´í„° ë¶„í• 
            time_diff = data.index.to_series().diff()
            split_points = time_diff > pd.Timedelta(minutes=1)
            segment_ids = split_points.cumsum()
            
            # ì…ë ¥ íŠ¹ì§• ì„ íƒ (scaled_FUTURE ì œì™¸)
            input_features = [col for col in data.columns if col.startswith('scaled_') and col != 'scaled_FUTURE']
            print(f"{'ì…ë ¥ íŠ¹ì§•' if USE_KOREAN else 'Input features'}: {input_features}")
            
            # ì‹œí€€ìŠ¤ ìƒì„±
            seq_length = self.config.get('seq_length', 30)
            X, y = [], []
            
            for segment_id in segment_ids.unique():
                segment = data[segment_ids == segment_id]
                
                if len(segment) > seq_length:
                    X_data = segment[input_features].values
                    y_data = segment['scaled_FUTURE'].values if 'scaled_FUTURE' in segment.columns else segment['FUTURE'].values
                    
                    for i in range(len(segment) - seq_length):
                        X.append(X_data[i:i+seq_length])
                        y.append(y_data[i+seq_length])
            
            X = np.array(X)
            y = np.array(y)
            
            print(f"{'í…ŒìŠ¤íŠ¸ ë°ì´í„° shape' if USE_KOREAN else 'Test data shape'}: X={X.shape}, y={y.shape}")
            
            return X, y
            
        except Exception as e:
            print(f"{'í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨' if USE_KOREAN else 'Test data preparation failed'}: {str(e)}")
            traceback.print_exc()
            return None, None
    
    def inverse_scale(self, scaled_data):
        """ì—­ìŠ¤ì¼€ì¼ë§"""
        try:
            n_features = self.scaler.n_features_in_
            dummy = np.zeros((len(scaled_data), n_features))
            
            # FUTUREê°€ ëª‡ ë²ˆì§¸ ì»¬ëŸ¼ì¸ì§€ ì°¾ê¸°
            if hasattr(self.scaler, 'feature_names_in_'):
                feature_names = list(self.scaler.feature_names_in_)
                if 'FUTURE' in feature_names:
                    future_idx = feature_names.index('FUTURE')
                elif 'TOTALCNT' in feature_names:
                    # FUTUREê°€ ì—†ìœ¼ë©´ TOTALCNT ìœ„ì¹˜ ì‚¬ìš©
                    future_idx = feature_names.index('TOTALCNT')
                else:
                    future_idx = 0
            else:
                # ê¸°ë³¸ì ìœ¼ë¡œ ì²« ë²ˆì§¸ ì»¬ëŸ¼ ì‚¬ìš©
                future_idx = 0
            
            dummy[:, future_idx] = scaled_data
            return self.scaler.inverse_transform(dummy)[:, future_idx]
            
        except Exception as e:
            print(f"{'ì—­ìŠ¤ì¼€ì¼ë§ ì‹¤íŒ¨' if USE_KOREAN else 'Inverse scaling failed'}: {str(e)}")
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°ì´í„° ë°˜í™˜
            return scaled_data
    
    def ensemble_predict(self, X_test):
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        predictions = []
        weights = self.config.get('model_weights', {
            'lstm': 0.3,
            'gru': 0.25,
            'rnn': 0.15,
            'bi_lstm': 0.3
        })
        
        total_weight = 0
        for model_name, model in self.models.items():
            pred = model.predict(X_test, verbose=0).flatten()
            weight = weights.get(model_name, 1/len(self.models))
            predictions.append(pred * weight)
            total_weight += weight
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        if total_weight > 0:
            ensemble_pred = np.sum(predictions, axis=0) / total_weight
        else:
            ensemble_pred = np.mean(predictions, axis=0)
            
        return ensemble_pred
    
    def save_evaluation_results(self, results):
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        rows = []
        for model_name, metrics in results.items():
            row = {
                'Model': model_name.upper(),
                'Overall Score (%)': metrics['overall_score'],
                'Grade': metrics['grade'],
                'MAPE (%)': metrics['mape'],
                'SMAPE (%)': metrics['smape'],
                'Accuracy 5% (%)': metrics['accuracy_5'],
                'Accuracy 10% (%)': metrics['accuracy_10'],
                'Accuracy 15% (%)': metrics['accuracy_15'],
                'RÂ² Score (%)': metrics['r2_score'],
                'Directional Accuracy (%)': metrics['directional_accuracy'],
                'Bottleneck Accuracy (%)': metrics['bottleneck_metrics']['accuracy'],
                'Bottleneck Precision (%)': metrics['bottleneck_metrics']['precision'],
                'Bottleneck Recall (%)': metrics['bottleneck_metrics']['recall'],
                'Bottleneck F1 (%)': metrics['bottleneck_metrics']['f1_score']
            }
            rows.append(row)
        
        df = pd.DataFrame(rows)
        df = df.sort_values('Overall Score (%)', ascending=False)
        
        # CSVë¡œ ì €ì¥
        csv_path = f'model_evaluation_results_{timestamp}.csv'
        df.to_csv(csv_path, index=False)
        print(f"\nğŸ“ {'í‰ê°€ ê²°ê³¼ ì €ì¥' if USE_KOREAN else 'Evaluation results saved'}: {csv_path}")
        
        # ìš”ì•½ í…ìŠ¤íŠ¸ ì €ì¥
        txt_path = f'model_evaluation_summary_{timestamp}.txt'
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write("="*70 + "\n")
            if USE_KOREAN:
                f.write("í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼\n")
                f.write(f"í‰ê°€ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            else:
                f.write("Hybrid Model Performance Evaluation Results\n")
                f.write(f"Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*70 + "\n\n")
            
            for model_name, metrics in sorted(results.items(), 
                                             key=lambda x: x[1]['overall_score'], 
                                             reverse=True):
                f.write(f"\n{model_name.upper()} {'ëª¨ë¸' if USE_KOREAN else 'Model'}\n")
                f.write("-"*40 + "\n")
                if USE_KOREAN:
                    f.write(f"ì¢…í•© ì ìˆ˜: {metrics['overall_score']:.1f}%\n")
                    f.write(f"ì„±ëŠ¥ ë“±ê¸‰: {metrics['grade']}\n")
                    f.write(f"MAPE: {metrics['mape']:.2f}%\n")
                    f.write(f"10% ì˜¤ì°¨ ë‚´ ì •í™•ë„: {metrics['accuracy_10']:.1f}%\n")
                    f.write(f"RÂ² Score: {metrics['r2_score']:.1f}%\n")
                    f.write(f"ë³‘ëª© ì˜ˆì¸¡ ì •í™•ë„: {metrics['bottleneck_metrics']['accuracy']:.1f}%\n")
                else:
                    f.write(f"Overall Score: {metrics['overall_score']:.1f}%\n")
                    f.write(f"Performance Grade: {metrics['grade']}\n")
                    f.write(f"MAPE: {metrics['mape']:.2f}%\n")
                    f.write(f"Accuracy within 10% error: {metrics['accuracy_10']:.1f}%\n")
                    f.write(f"RÂ² Score: {metrics['r2_score']:.1f}%\n")
                    f.write(f"Bottleneck Prediction Accuracy: {metrics['bottleneck_metrics']['accuracy']:.1f}%\n")
        
        print(f"ğŸ“ {'í‰ê°€ ìš”ì•½ ì €ì¥' if USE_KOREAN else 'Evaluation summary saved'}: {txt_path}")

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main(test_data_path=None):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        test_data_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ (Noneì´ë©´ ìë™ íƒìƒ‰)
    """
    print("\n" + "="*70)
    if USE_KOREAN:
        print("í•˜ì´ë¸Œë¦¬ë“œ ë”¥ëŸ¬ë‹ ëª¨ë¸ í¼ì„¼íŠ¸ í‰ê°€ ì‹œìŠ¤í…œ")
    else:
        print("Hybrid Deep Learning Model Percentage Evaluation System")
    print("="*70)
    
    evaluator = ModelPercentEvaluator()
    
    # í‰ê°€ ì‹¤í–‰
    results = evaluator.evaluate_all_models(test_data_path)
    
    if results:
        if USE_KOREAN:
            print("\nâœ… ëª¨ë“  í‰ê°€ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("   - í‰ê°€ ê²°ê³¼ CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ")
            print("   - í‰ê°€ ìš”ì•½ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ ì™„ë£Œ")
            print("   - ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ")
        else:
            print("\nâœ… All evaluations completed successfully!")
            print("   - Evaluation results CSV saved")
            print("   - Evaluation summary text saved")
            print("   - Visualization images saved")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê°•ì¡°
        best_model = max(results.items(), key=lambda x: x[1]['overall_score'])
        if USE_KOREAN:
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model[0].upper()}")
            print(f"   ì¢…í•© ì ìˆ˜: {best_model[1]['overall_score']:.1f}%")
            print(f"   ì„±ëŠ¥ ë“±ê¸‰: {best_model[1]['grade']}")
        else:
            print(f"\nğŸ† Best Performance Model: {best_model[0].upper()}")
            print(f"   Overall Score: {best_model[1]['overall_score']:.1f}%")
            print(f"   Performance Grade: {best_model[1]['grade']}")
        
        return results
    else:
        if USE_KOREAN:
            print("\nâŒ í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
            print("   1. í•™ìŠµì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
            print("   2. model/ í´ë”ì— .keras íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
            print("   3. scaler/ í´ë”ì— ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
            print("   4. í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ì§€ í™•ì¸í•˜ì„¸ìš”")
        else:
            print("\nâŒ Error occurred during evaluation.")
            print("\nğŸ’¡ Solutions:")
            print("   1. Check if training is completed")
            print("   2. Check if .keras files exist in model/ folder")
            print("   3. Check if scaler file exists in scaler/ folder")
            print("   4. Check if test data file has correct format")
        return None

if __name__ == "__main__":
    import sys
    
    # ëª…ë ¹ì¤„ ì¸ìë¡œ ë°ì´í„° ê²½ë¡œ ë°›ê¸°
    if len(sys.argv) > 1:
        test_path = sys.argv[1]
        print(f"ì‚¬ìš©ì ì§€ì • í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_path}")
        main(test_path)
    else:
        main()