"""
ì œì¡°ê³µì • ë³‘ëª© ì˜ˆì¸¡ ì‹œìŠ¤í…œ (LLM ì œì™¸ ë²„ì „)
Python 3.11.4 í˜¸í™˜
MCS ë¡œê·¸ + ì§„ë™ì„¼ì„œ ë°ì´í„° ê¸°ë°˜ ë³‘ëª© ì˜ˆì¸¡
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import random
from typing import Dict, List, Tuple, Optional
import json
import pickle
from pathlib import Path

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import xgboost as xgb

# ë²¡í„° ì €ì¥ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# ì‹œê³„ì—´ ë¶„ì„
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose

# LSTM ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ê°„ë‹¨í•œ êµ¬í˜„
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

import warnings
warnings.filterwarnings('ignore')

# ===== 1. MCS ë¡œê·¸ + ì§„ë™ì„¼ì„œ ìƒ˜í”Œ ë°ì´í„° ìƒì„± =====
def generate_sample_data(n_days=30, interval_minutes=5):
    """
    MCS ë¡œê·¸ + ì§„ë™ì„¼ì„œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    - 30ì¼ê°„ì˜ ë°ì´í„°
    - 5ë¶„ ê°„ê²©ìœ¼ë¡œ ìˆ˜ì§‘
    - 3ê°œì˜ ìƒì‚° ë¼ì¸ (LINE_A, LINE_B, LINE_C)
    - ì§„ë™ì„¼ì„œ ë°ì´í„° ì¶”ê°€
    """
    
    print("MCS ë¡œê·¸ + ì§„ë™ì„¼ì„œ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
    
    # ì‹œê°„ ë²”ìœ„ ì„¤ì •
    start_date = datetime.now() - timedelta(days=n_days)
    end_date = datetime.now()
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    timestamps = pd.date_range(start=start_date, end=end_date, freq=f'{interval_minutes}min')
    
    # ë¼ì¸ ì •ë³´
    lines = ['LINE_A', 'LINE_B', 'LINE_C']
    
    data = []
    
    for timestamp in timestamps:
        hour = timestamp.hour
        day_of_week = timestamp.dayofweek
        
        for line_id in lines:
            # ê¸°ë³¸ ì •ìƒ ê°’
            normal_throughput = 100  # ê°œ/ì‹œê°„
            normal_cycle_time = 5    # ë¶„
            normal_wait_time = 10    # ë¶„
            normal_utilization = 95  # %
            normal_vibration = 2.5   # mm/s (ì§„ë™ ì†ë„)
            normal_temperature = 65  # Â°C
            
            # ì‹œê°„ëŒ€ë³„ ë³‘ëª© ë°œìƒ íŒ¨í„´
            if (9 <= hour <= 11) or (14 <= hour <= 16):
                bottleneck_prob = 0.35
            elif 12 <= hour <= 13:
                bottleneck_prob = 0.15
            elif hour >= 22 or hour <= 6:
                bottleneck_prob = 0.05
            else:
                bottleneck_prob = 0.1
            
            # ì£¼ë§ì€ ë³‘ëª© ê°€ëŠ¥ì„± ë‚®ìŒ
            if day_of_week in [5, 6]:
                bottleneck_prob *= 0.5
            
            # íŠ¹ì • ë¼ì¸ì˜ ê³ ì¥ íŒ¨í„´ ì‹œë®¬ë ˆì´ì…˜
            if line_id == 'LINE_B' and hour in [10, 15]:
                bottleneck_prob += 0.2
            
            # ë³‘ëª© ìƒí™© ê²°ì •
            is_bottleneck = random.random() < bottleneck_prob
            
            if is_bottleneck:
                # ë³‘ëª© ìƒí™© ë°ì´í„°
                throughput = normal_throughput * random.uniform(0.5, 0.75)
                cycle_time = normal_cycle_time * random.uniform(1.2, 1.8)
                wait_time = normal_wait_time * random.uniform(1.5, 2.5)
                utilization = normal_utilization * random.uniform(0.6, 0.8)
                error_count = random.randint(2, 8)
                downtime_minutes = random.uniform(5, 30)
                
                # ì§„ë™ì„¼ì„œ ë°ì´í„° (ë³‘ëª©ì‹œ ì¦ê°€)
                vibration_x = normal_vibration * random.uniform(1.5, 3.0)
                vibration_y = normal_vibration * random.uniform(1.5, 3.0)
                vibration_z = normal_vibration * random.uniform(1.5, 3.0)
                temperature = normal_temperature * random.uniform(1.1, 1.3)
                noise_level = random.uniform(85, 95)  # dB
            else:
                # ì •ìƒ ìƒí™© ë°ì´í„°
                throughput = normal_throughput * random.uniform(0.95, 1.05)
                cycle_time = normal_cycle_time * random.uniform(0.95, 1.05)
                wait_time = normal_wait_time * random.uniform(0.9, 1.1)
                utilization = normal_utilization * random.uniform(0.95, 1.0)
                error_count = random.randint(0, 2)
                downtime_minutes = random.uniform(0, 5)
                
                # ì§„ë™ì„¼ì„œ ë°ì´í„° (ì •ìƒ)
                vibration_x = normal_vibration * random.uniform(0.8, 1.2)
                vibration_y = normal_vibration * random.uniform(0.8, 1.2)
                vibration_z = normal_vibration * random.uniform(0.8, 1.2)
                temperature = normal_temperature * random.uniform(0.95, 1.05)
                noise_level = random.uniform(70, 80)  # dB
            
            # ë°ì´í„° ì €ì¥
            data.append({
                'timestamp': timestamp,
                'line_id': line_id,
                'throughput': round(throughput, 2),
                'cycle_time': round(cycle_time, 2),
                'wait_time': round(wait_time, 2),
                'utilization': round(utilization, 2),
                'error_count': error_count,
                'downtime_minutes': round(downtime_minutes, 2),
                'vibration_x': round(vibration_x, 3),
                'vibration_y': round(vibration_y, 3),
                'vibration_z': round(vibration_z, 3),
                'vibration_rms': round(np.sqrt(vibration_x**2 + vibration_y**2 + vibration_z**2), 3),
                'temperature': round(temperature, 1),
                'noise_level': round(noise_level, 1),
                'is_bottleneck': int(is_bottleneck),
                'shift': 'day' if 8 <= hour <= 16 else 'evening' if 16 < hour <= 24 else 'night'
            })
    
    df = pd.DataFrame(data)
    print(f"ìƒì„±ëœ ë°ì´í„°: {len(df)} í–‰")
    print(f"ë³‘ëª© ë¹„ìœ¨: {df['is_bottleneck'].mean():.2%}")
    
    return df

# ===== 2. ë³‘ëª© ìœ„í—˜ë„ ì§€ìˆ˜(BRI) ê³„ì‚° (ì§„ë™ì„¼ì„œ í¬í•¨) =====
def calculate_bri(row, weights=None):
    """
    ë³‘ëª© ìœ„í—˜ë„ ì§€ìˆ˜(Bottleneck Risk Index) ê³„ì‚°
    ì§„ë™ì„¼ì„œ ë°ì´í„° í¬í•¨
    """
    if weights is None:
        weights = {
            'utilization': 0.25,
            'wait_time': 0.20,
            'cycle_time': 0.20,
            'throughput': 0.15,
            'vibration': 0.15,
            'temperature': 0.05
        }
    
    # ê° ì§€ìˆ˜ ê³„ì‚° (ì •ê·œí™”)
    U = (100 - row['utilization']) / 100
    W = row['wait_time'] / 10
    C = row['cycle_time'] / 5
    T = 100 / max(row['throughput'], 1)
    V = row['vibration_rms'] / 10  # 10mm/s ê¸°ì¤€
    Temp = (row['temperature'] - 50) / 50  # 50-100Â°C ë²”ìœ„
    
    # BRI ê³„ì‚°
    bri = (weights['utilization'] * U + 
           weights['wait_time'] * W + 
           weights['cycle_time'] * C + 
           weights['throughput'] * T +
           weights['vibration'] * V +
           weights['temperature'] * Temp)
    
    return bri

# ===== 3. ë°ì´í„° ì „ì²˜ë¦¬ (ì§„ë™ì„¼ì„œ í¬í•¨) =====
class DataPreprocessor:
    """ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        
    def preprocess(self, df):
        """ë°ì´í„° ì „ì²˜ë¦¬ ìˆ˜í–‰"""
        print("\në°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
        
        # 1. ì´ìƒì¹˜ ì œê±° (IQR ë°©ë²•)
        df_clean = self.remove_outliers(df)
        
        # 2. ì‹œê°„ íŠ¹ì§• ì¶”ê°€
        df_clean = self.add_time_features(df_clean)
        
        # 3. BRI ê³„ì‚°
        df_clean['bri'] = df_clean.apply(calculate_bri, axis=1)
        
        # 4. ê²½ë¡œë³„ ì§‘ê³„ íŠ¹ì§• ì¶”ê°€
        df_clean = self.add_path_features(df_clean)
        
        # 5. ì§„ë™ì„¼ì„œ íŠ¹ì§• ì¶”ê°€
        df_clean = self.add_vibration_features(df_clean)
        
        print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df_clean)} í–‰")
        
        return df_clean
    
    def remove_outliers(self, df, columns=['throughput', 'cycle_time', 'wait_time', 'vibration_rms']):
        """IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ ì œê±°"""
        df_clean = df.copy()
        
        for col in columns:
            Q1 = df_clean[col].quantile(0.25)
            Q3 = df_clean[col].quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # ì´ìƒì¹˜ ì œê±°
            df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]
        
        print(f"ì´ìƒì¹˜ ì œê±°: {len(df) - len(df_clean)} í–‰ ì œê±°ë¨")
        
        return df_clean
    
    def add_time_features(self, df):
        """ì‹œê°„ ê´€ë ¨ íŠ¹ì§• ì¶”ê°€"""
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['is_weekend'] = (df['day_of_week'].isin([5, 6])).astype(int)
        df['month'] = df['timestamp'].dt.month
        df['day'] = df['timestamp'].dt.day
        
        # ì‹œê°„ëŒ€ë³„ êµ¬ë¶„
        df['time_period'] = pd.cut(df['hour'], 
                                   bins=[0, 6, 12, 18, 24], 
                                   labels=['dawn', 'morning', 'afternoon', 'night'])
        
        return df
    
    def add_path_features(self, df):
        """ê²½ë¡œë³„ ì§‘ê³„ íŠ¹ì§• ì¶”ê°€"""
        # ë¼ì¸ë³„ 1ì‹œê°„ ì´ë™í‰ê· 
        for col in ['throughput', 'wait_time', 'utilization', 'vibration_rms', 'temperature']:
            df[f'{col}_ma_1h'] = df.groupby('line_id')[col].transform(
                lambda x: x.rolling('1H', on=df['timestamp']).mean()
            )
        
        # ë¼ì¸ë³„ ëˆ„ì  ì—ëŸ¬ ìˆ˜
        df['cumulative_errors'] = df.groupby('line_id')['error_count'].cumsum()
        
        return df
    
    def add_vibration_features(self, df):
        """ì§„ë™ì„¼ì„œ íŠ¹ì§• ì¶”ê°€"""
        # ì§„ë™ ë³€í™”ìœ¨
        df['vibration_change_rate'] = df.groupby('line_id')['vibration_rms'].pct_change()
        
        # ì§„ë™ í‘œì¤€í¸ì°¨ (1ì‹œê°„ ìœˆë„ìš°)
        df['vibration_std_1h'] = df.groupby('line_id')['vibration_rms'].transform(
            lambda x: x.rolling('1H', on=df['timestamp']).std()
        )
        
        # ì˜¨ë„ ë³€í™”ìœ¨
        df['temp_change_rate'] = df.groupby('line_id')['temperature'].pct_change()
        
        return df

# ===== 4. ì‹œê³„ì—´ ëª¨ë¸ êµ¬í˜„ =====
class TimeSeriesModels:
    """ì‹œê³„ì—´ ëª¨ë¸ (ARIMA, LSTM ì‹œë®¬ë ˆì´ì…˜, RNN ì‹œë®¬ë ˆì´ì…˜)"""
    
    def __init__(self):
        self.arima_models = {}
        self.lstm_simulator = RandomForestRegressor(n_estimators=100, random_state=42)
        self.rnn_simulator = LinearRegression()
        
    def train_arima(self, df, target_col='bri'):
        """ARIMA ëª¨ë¸ í•™ìŠµ"""
        print("\nARIMA ëª¨ë¸ í•™ìŠµ ì¤‘...")
        
        for line_id in df['line_id'].unique():
            line_data = df[df['line_id'] == line_id].set_index('timestamp')[target_col]
            
            # ì •ìƒì„± ê²€ì •
            adf_result = adfuller(line_data)
            print(f"{line_id} ADF í†µê³„ëŸ‰: {adf_result[0]:.4f}, p-value: {adf_result[1]:.4f}")
            
            # ARIMA ëª¨ë¸ í•™ìŠµ
            try:
                model = ARIMA(line_data, order=(2, 1, 2))
                fitted_model = model.fit()
                self.arima_models[line_id] = fitted_model
                print(f"{line_id} ARIMA ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            except Exception as e:
                print(f"{line_id} ARIMA í•™ìŠµ ì‹¤íŒ¨: {e}")
    
    def train_lstm_simulator(self, X_train, y_train):
        """LSTM ì‹œë®¬ë ˆì´í„° í•™ìŠµ (RandomForestë¡œ ëŒ€ì²´)"""
        print("\nLSTM ì‹œë®¬ë ˆì´í„° í•™ìŠµ ì¤‘...")
        self.lstm_simulator.fit(X_train, y_train)
        print("LSTM ì‹œë®¬ë ˆì´í„° í•™ìŠµ ì™„ë£Œ!")
    
    def train_rnn_simulator(self, X_train, y_train):
        """RNN ì‹œë®¬ë ˆì´í„° í•™ìŠµ (Linear Regressionìœ¼ë¡œ ëŒ€ì²´)"""
        print("\nRNN ì‹œë®¬ë ˆì´í„° í•™ìŠµ ì¤‘...")
        self.rnn_simulator.fit(X_train, y_train)
        print("RNN ì‹œë®¬ë ˆì´í„° í•™ìŠµ ì™„ë£Œ!")
    
    def predict_arima(self, line_id, steps=24):
        """ARIMA ì˜ˆì¸¡ (24ì‹œê°„)"""
        if line_id not in self.arima_models:
            return None
        
        forecast = self.arima_models[line_id].forecast(steps=steps)
        return forecast
    
    def create_sequences(self, data, sequence_length=24):
        """ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜"""
        sequences = []
        targets = []
        
        for i in range(len(data) - sequence_length):
            seq = data[i:i+sequence_length]
            target = data[i+sequence_length]
            sequences.append(seq)
            targets.append(target)
        
        return np.array(sequences), np.array(targets)

# ===== 5. XGBoost ëª¨ë¸ (1ì°¨ í•„í„°) =====
class XGBoostBottleneckDetector:
    """XGBoost ê¸°ë°˜ ì‹¤ì‹œê°„ ë³‘ëª© ê°ì§€"""
    
    def __init__(self):
        self.model = None
        self.feature_cols = None
        self.scaler = StandardScaler()
        
    def prepare_features(self, df):
        """ëª¨ë¸ í•™ìŠµìš© íŠ¹ì§• ì¤€ë¹„"""
        feature_cols = [
            'throughput', 'cycle_time', 'wait_time', 'utilization',
            'error_count', 'downtime_minutes', 'bri',
            'hour', 'day_of_week', 'is_weekend',
            'throughput_ma_1h', 'wait_time_ma_1h', 'utilization_ma_1h',
            'cumulative_errors',
            'vibration_rms', 'temperature', 'noise_level',
            'vibration_x', 'vibration_y', 'vibration_z',
            'vibration_rms_ma_1h', 'temperature_ma_1h',
            'vibration_change_rate', 'vibration_std_1h', 'temp_change_rate'
        ]
        
        # NaN ê°’ ì²˜ë¦¬
        for col in feature_cols:
            if col in df.columns:
                df[col] = df[col].fillna(df[col].mean())
        
        self.feature_cols = [col for col in feature_cols if col in df.columns]
        
        return df[self.feature_cols]
    
    def train(self, X_train, y_train):
        """ëª¨ë¸ í•™ìŠµ"""
        print("\nXGBoost ëª¨ë¸ í•™ìŠµ ì¤‘...")
        
        # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        X_train_scaled = self.scaler.fit_transform(X_train)
        
        # XGBoost íŒŒë¼ë¯¸í„°
        params = {
            'objective': 'binary:logistic',
            'max_depth': 8,
            'learning_rate': 0.1,
            'n_estimators': 150,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'use_label_encoder': False,
            'eval_metric': 'logloss'
        }
        
        # ëª¨ë¸ í•™ìŠµ
        self.model = xgb.XGBClassifier(**params)
        self.model.fit(X_train_scaled, y_train)
        
        print("XGBoost í•™ìŠµ ì™„ë£Œ!")
        
    def predict(self, X):
        """ì˜ˆì¸¡ ìˆ˜í–‰ (0.01ì´ˆ ì´ë‚´)"""
        X_scaled = self.scaler.transform(X)
        
        # ì˜ˆì¸¡ í™•ë¥ 
        proba = self.model.predict_proba(X_scaled)[:, 1]
        
        # ìœ„í—˜ë„ ë ˆë²¨ ë¶„ë¥˜
        risk_levels = []
        for p in proba:
            if p < 0.3:
                risk_levels.append('ì •ìƒ')
            elif p < 0.5:
                risk_levels.append('ì£¼ì˜')
            elif p < 0.7:
                risk_levels.append('ê²½ê³ ')
            else:
                risk_levels.append('ìœ„í—˜')
        
        return proba, risk_levels
    
    def get_feature_importance(self):
        """íŠ¹ì§• ì¤‘ìš”ë„ ë°˜í™˜"""
        importance = pd.DataFrame({
            'feature': self.feature_cols,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        return importance

# ===== 6. RAG ì‹œìŠ¤í…œ (ê³¼ê±° ì‚¬ë¡€ ê²€ìƒ‰) =====
class RAGSystem:
    """ê³¼ê±° ë³‘ëª© ì‚¬ë¡€ ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.case_database = []
        self.vectorizer = TfidfVectorizer(max_features=100)
        self.vectors = None
        
    def build_case_database(self, df):
        """ë³‘ëª© ì‚¬ë¡€ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"""
        print("\nRAG ì‚¬ë¡€ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        bottleneck_cases = df[df['is_bottleneck'] == 1]
        
        for _, row in bottleneck_cases.iterrows():
            # ì‚¬ë¡€ ì„¤ëª… ìƒì„±
            case_description = f"""
            ë¼ì¸: {row['line_id']}
            ì‹œê°„: {row['timestamp']}
            ì²˜ë¦¬ëŸ‰: {row['throughput']:.1f}
            ëŒ€ê¸°ì‹œê°„: {row['wait_time']:.1f}ë¶„
            ê°€ë™ë¥ : {row['utilization']:.1f}%
            ì—ëŸ¬ìˆ˜: {row['error_count']}
            ì§„ë™: {row['vibration_rms']:.2f}mm/s
            ì˜¨ë„: {row['temperature']:.1f}Â°C
            """
            
            # í•´ê²° ë°©ì•ˆ ì‹œë®¬ë ˆì´ì…˜
            if row['vibration_rms'] > 7.5:
                solution = "ë² ì–´ë§ êµì²´ ë° ì¶• ì •ë ¬ ì ê²€"
                root_cause = "ê³¼ë„í•œ ì§„ë™ - ê¸°ê³„ì  ë§ˆëª¨"
            elif row['temperature'] > 80:
                solution = "ëƒ‰ê° ì‹œìŠ¤í…œ ì ê²€ ë° ìœ¤í™œìœ  êµì²´"
                root_cause = "ê³¼ì—´ - ëƒ‰ê° ì‹œìŠ¤í…œ ì´ìƒ"
            elif row['error_count'] > 5:
                solution = "ì„¤ë¹„ ì ê²€ ë° ì¬ì‹œì‘ í•„ìš”"
                root_cause = "ì„¤ë¹„ ì˜¤ë¥˜ ëˆ„ì "
            elif row['wait_time'] > 20:
                solution = "ë²„í¼ ìš©ëŸ‰ ì¦ì„¤ ë˜ëŠ” ë¼ì¸ ë°¸ëŸ°ì‹±"
                root_cause = "ê³µì •ê°„ ë¶ˆê· í˜•"
            elif row['utilization'] < 70:
                solution = "ì˜ˆë°© ì •ë¹„ ì‹¤ì‹œ"
                root_cause = "ì„¤ë¹„ ë…¸í›„í™”"
            else:
                solution = "ìƒì‚° ì¼ì • ì¡°ì •"
                root_cause = "ê³¼ë¶€í•˜"
            
            self.case_database.append({
                'description': case_description,
                'solution': solution,
                'root_cause': root_cause,
                'line_id': row['line_id'],
                'bri': row['bri'],
                'vibration': row['vibration_rms'],
                'temperature': row['temperature']
            })
        
        # ë²¡í„°í™”
        descriptions = [case['description'] for case in self.case_database]
        self.vectors = self.vectorizer.fit_transform(descriptions)
        
        print(f"RAG ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self.case_database)}ê°œ ì‚¬ë¡€")
    
    def search_similar_cases(self, current_situation, top_k=5):
        """ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰"""
        # í˜„ì¬ ìƒí™© ì„¤ëª…
        query = f"""
        ì²˜ë¦¬ëŸ‰: {current_situation['throughput']:.1f}
        ëŒ€ê¸°ì‹œê°„: {current_situation['wait_time']:.1f}ë¶„
        ê°€ë™ë¥ : {current_situation['utilization']:.1f}%
        ì—ëŸ¬ìˆ˜: {current_situation['error_count']}
        ì§„ë™: {current_situation['vibration_rms']:.2f}mm/s
        ì˜¨ë„: {current_situation['temperature']:.1f}Â°C
        """
        
        # ë²¡í„°í™” ë° ìœ ì‚¬ë„ ê³„ì‚°
        query_vector = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.vectors)[0]
        
        # ìƒìœ„ kê°œ ì‚¬ë¡€ ì„ íƒ
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        similar_cases = []
        for idx in top_indices:
            if similarities[idx] > 0.1:  # ìµœì†Œ ìœ ì‚¬ë„
                case = self.case_database[idx].copy()
                case['similarity'] = similarities[idx]
                similar_cases.append(case)
        
        return similar_cases

# ===== 7. ê·œì¹™ ê¸°ë°˜ ì¢…í•© íŒë‹¨ ì‹œìŠ¤í…œ (LLM ëŒ€ì²´) =====
class RuleBasedDiagnostics:
    """ê·œì¹™ ê¸°ë°˜ ë³‘ëª© ì§„ë‹¨ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.diagnosis_rules = self._create_diagnosis_rules()
    
    def _create_diagnosis_rules(self):
        """ì§„ë‹¨ ê·œì¹™ ì •ì˜"""
        return {
            'high_vibration': {
                'condition': lambda d: d['vibration_rms'] > 7.5,
                'root_cause': 'ê³¼ë„í•œ ì§„ë™ - ë² ì–´ë§ ë§ˆëª¨ ë˜ëŠ” ì¶• ì •ë ¬ ë¶ˆëŸ‰',
                'immediate_action': 'ì§„ë™ ì¸¡ì • ë° ë² ì–´ë§ ìƒíƒœ ì ê²€',
                'long_term_solution': 'ë² ì–´ë§ êµì²´ ë° ì •ê¸°ì ì¸ ì¶• ì •ë ¬ ì ê²€ í”„ë¡œê·¸ë¨ ë„ì…',
                'priority': 1
            },
            'high_temperature': {
                'condition': lambda d: d['temperature'] > 80,
                'root_cause': 'ê³¼ì—´ - ëƒ‰ê° ì‹œìŠ¤í…œ ì´ìƒ ë˜ëŠ” ê³¼ë¶€í•˜',
                'immediate_action': 'ë¶€í•˜ ê°ì†Œ ë° ëƒ‰ê° ì‹œìŠ¤í…œ ì ê²€',
                'long_term_solution': 'ëƒ‰ê° ì‹œìŠ¤í…œ ì—…ê·¸ë ˆì´ë“œ ë° ì˜¨ë„ ëª¨ë‹ˆí„°ë§ ê°•í™”',
                'priority': 1
            },
            'high_error_count': {
                'condition': lambda d: d['error_count'] > 5,
                'root_cause': 'ì œì–´ ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë˜ëŠ” ì„¼ì„œ ì´ìƒ',
                'immediate_action': 'ì‹œìŠ¤í…œ ì¬ì‹œì‘ ë° ì˜¤ë¥˜ ë¡œê·¸ ë¶„ì„',
                'long_term_solution': 'ì œì–´ ì‹œìŠ¤í…œ íŒì›¨ì–´ ì—…ë°ì´íŠ¸ ë° ì„¼ì„œ êµì²´',
                'priority': 2
            },
            'high_wait_time': {
                'condition': lambda d: d['wait_time'] > 20,
                'root_cause': 'ê³µì •ê°„ ì²˜ë¦¬ ì†ë„ ë¶ˆê· í˜•',
                'immediate_action': 'ë³‘ëª© ê³µì • í™•ì¸ ë° ë²„í¼ ì¡°ì •',
                'long_term_solution': 'ë¼ì¸ ë°¸ëŸ°ì‹± ì¬ì„¤ê³„ ë° ë²„í¼ ìš©ëŸ‰ ìµœì í™”',
                'priority': 3
            },
            'low_utilization': {
                'condition': lambda d: d['utilization'] < 70,
                'root_cause': 'ì„¤ë¹„ íš¨ìœ¨ ì €í•˜ - ìœ ì§€ë³´ìˆ˜ í•„ìš”',
                'immediate_action': 'ì„¤ë¹„ ìƒíƒœ ì ê²€ ë° ê°„ë‹¨í•œ ì •ë¹„',
                'long_term_solution': 'ì˜ˆë°© ì •ë¹„ ê³„íš ìˆ˜ë¦½ ë° ì„¤ë¹„ í˜„ëŒ€í™”',
                'priority': 3
            },
            'complex_issue': {
                'condition': lambda d: d['bri'] > 0.7,
                'root_cause': 'ë³µí•©ì  ë¬¸ì œ - ë‹¤ê°ë„ ë¶„ì„ í•„ìš”',
                'immediate_action': 'ì „ì²´ ë¼ì¸ ê¸´ê¸‰ ì ê²€',
                'long_term_solution': 'í†µí•© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶• ë° AI ê¸°ë°˜ ì˜ˆì¸¡ ì •ë¹„',
                'priority': 1
            }
        }
    
    def diagnose(self, current_data, predictions):
        """ê·œì¹™ ê¸°ë°˜ ì§„ë‹¨"""
        diagnosis = {
            'bottleneck_probability': 0,
            'root_causes': [],
            'immediate_actions': [],
            'long_term_solutions': [],
            'risk_score': 0
        }
        
        # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ ì¢…í•©
        xgb_prob = predictions.get('xgboost_probability', 0.5)
        arima_trend = predictions.get('arima_trend', 0)
        pattern_similarity = predictions.get('pattern_similarity', 0.5)
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… í™•ë¥  ê³„ì‚°
        weights = {'xgb': 0.5, 'arima': 0.2, 'pattern': 0.3}
        final_prob = (weights['xgb'] * xgb_prob + 
                     weights['arima'] * arima_trend + 
                     weights['pattern'] * pattern_similarity)
        
        diagnosis['bottleneck_probability'] = final_prob
        
        # ê·œì¹™ ê¸°ë°˜ ì§„ë‹¨ ìˆ˜í–‰
        triggered_rules = []
        for rule_name, rule in self.diagnosis_rules.items():
            if rule['condition'](current_data):
                triggered_rules.append({
                    'name': rule_name,
                    'priority': rule['priority'],
                    'root_cause': rule['root_cause'],
                    'immediate_action': rule['immediate_action'],
                    'long_term_solution': rule['long_term_solution']
                })
        
        # ìš°ì„ ìˆœìœ„ë³„ë¡œ ì •ë ¬
        triggered_rules.sort(key=lambda x: x['priority'])
        
        # ì§„ë‹¨ ê²°ê³¼ ì •ë¦¬
        if triggered_rules:
            for rule in triggered_rules[:3]:  # ìƒìœ„ 3ê°œë§Œ
                diagnosis['root_causes'].append(rule['root_cause'])
                diagnosis['immediate_actions'].append(rule['immediate_action'])
                diagnosis['long_term_solutions'].append(rule['long_term_solution'])
            
            # ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°
            diagnosis['risk_score'] = min(len(triggered_rules) * 0.25 + final_prob, 1.0)
        else:
            diagnosis['root_causes'] = ['ì •ìƒ ìš´ì˜ ìƒíƒœ']
            diagnosis['immediate_actions'] = ['ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§']
            diagnosis['long_term_solutions'] = ['í˜„ì¬ ìš´ì˜ ë°©ì‹ ìœ ì§€']
            diagnosis['risk_score'] = final_prob
        
        return diagnosis

# ===== 8. í†µí•© ì˜ˆì¸¡ ì‹œìŠ¤í…œ =====
class BottleneckPredictionSystem:
    """ë³‘ëª© ì˜ˆì¸¡ í†µí•© ì‹œìŠ¤í…œ (LLM ì—†ìŒ)"""
    
    def __init__(self):
        self.xgboost_model = XGBoostBottleneckDetector()
        self.time_series_models = TimeSeriesModels()
        self.rag_system = RAGSystem()
        self.preprocessor = DataPreprocessor()
        self.diagnostics = RuleBasedDiagnostics()
        
    def train(self, df):
        """ì „ì²´ ì‹œìŠ¤í…œ í•™ìŠµ"""
        print("\n=== ë³‘ëª© ì˜ˆì¸¡ ì‹œìŠ¤í…œ í•™ìŠµ ì‹œì‘ ===")
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        df_processed = self.preprocessor.preprocess(df)
        
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
        train_df = df_processed[df_processed['timestamp'] < df_processed['timestamp'].max() - timedelta(days=7)]
        test_df = df_processed[df_processed['timestamp'] >= df_processed['timestamp'].max() - timedelta(days=7)]
        
        # 1. XGBoost í•™ìŠµ
        X_train = self.xgboost_model.prepare_features(train_df)
        y_train = train_df['is_bottleneck']
        self.xgboost_model.train(X_train, y_train)
        
        # 2. ì‹œê³„ì—´ ëª¨ë¸ í•™ìŠµ
        # ARIMA
        self.time_series_models.train_arima(train_df, target_col='bri')
        
        # LSTM/RNN ì‹œë®¬ë ˆì´í„° í•™ìŠµ
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
        for line_id in train_df['line_id'].unique():
            line_data = train_df[train_df['line_id'] == line_id].sort_values('timestamp')
            if len(line_data) > 48:  # ìµœì†Œ 48ê°œ ë°ì´í„° í•„ìš”
                sequences, targets = self.time_series_models.create_sequences(
                    line_data['bri'].values, sequence_length=24
                )
                if len(sequences) > 0:
                    # LSTM ì‹œë®¬ë ˆì´í„°
                    self.time_series_models.train_lstm_simulator(
                        sequences.reshape(sequences.shape[0], -1), targets
                    )
                    # RNN ì‹œë®¬ë ˆì´í„°
                    self.time_series_models.train_rnn_simulator(
                        sequences.reshape(sequences.shape[0], -1), targets
                    )
                    break  # ì²« ë²ˆì§¸ ë¼ì¸ë§Œ í•™ìŠµ (ë°ëª¨ìš©)
        
        # 3. RAG ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        self.rag_system.build_case_database(train_df)
        
        print("\n=== ì‹œìŠ¤í…œ í•™ìŠµ ì™„ë£Œ ===")
        
        return test_df
    
    def predict(self, current_data):
        """í†µí•© ì˜ˆì¸¡ ìˆ˜í–‰"""
        # 1ì°¨ í•„í„°: XGBoost
        X = self.xgboost_model.prepare_features(current_data)
        xgb_proba, risk_levels = self.xgboost_model.predict(X)
        
        results = []
        
        for idx, (_, row) in enumerate(current_data.iterrows()):
            result = {
                'timestamp': row['timestamp'],
                'line_id': row['line_id'],
                'xgboost_probability': xgb_proba[idx],
                'risk_level': risk_levels[idx],
                'bri': row['bri']
            }
            
            # 2ì°¨ ë¶„ì„: ìœ„í—˜ë„ 50% ì´ìƒì¼ ë•Œ
            if xgb_proba[idx] > 0.5:
                # ARIMA ì˜ˆì¸¡
                try:
                    arima_forecast = self.time_series_models.predict_arima(row['line_id'], steps=24)
                    if arima_forecast is not None:
                        result['arima_trend'] = float(arima_forecast.mean() > row['bri'])
                    else:
                        result['arima_trend'] = 0.5
                except:
                    result['arima_trend'] = 0.5
                
                # RAG ê²€ìƒ‰
                similar_cases = self.rag_system.search_similar_cases(row, top_k=3)
                result['similar_cases'] = similar_cases
                
                # íŒ¨í„´ ìœ ì‚¬ë„ ê³„ì‚°
                if similar_cases:
                    result['pattern_similarity'] = np.mean([case['similarity'] for case in similar_cases[:3]])
                else:
                    result['pattern_similarity'] = 0.5
                
                # ê·œì¹™ ê¸°ë°˜ ì§„ë‹¨
                predictions_dict = {
                    'xgboost_probability': xgb_proba[idx],
                    'arima_trend': result.get('arima_trend', 0.5),
                    'pattern_similarity': result.get('pattern_similarity', 0.5)
                }
                
                result['diagnosis'] = self.diagnostics.diagnose(row, predictions_dict)
            
            results.append(result)
        
        return pd.DataFrame(results)

# ===== 9. ì‹¤í–‰ ì˜ˆì œ =====
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=== ì œì¡°ê³µì • ë³‘ëª© ì˜ˆì¸¡ ì‹œìŠ¤í…œ (LLM ì œì™¸) ===")
    print("Python 3.11.4 í˜¸í™˜ ë²„ì „")
    print("="*50)
    
    # 1. ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    df = generate_sample_data(n_days=30, interval_minutes=5)
    
    # 2. ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    print("\në°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
    print(df.head())
    print(f"\në°ì´í„° shape: {df.shape}")
    print(f"\nì»¬ëŸ¼ ëª©ë¡: {df.columns.tolist()}")
    
    # 3. ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° í•™ìŠµ
    system = BottleneckPredictionSystem()
    test_df = system.train(df)
    
    # 4. ì˜ˆì¸¡ ìˆ˜í–‰
    print("\n=== ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘ ===")
    
    # ìµœê·¼ 24ì‹œê°„ ë°ì´í„°ë¡œ ì˜ˆì¸¡
    recent_data = test_df.tail(24*12)  # 24ì‹œê°„ * (60ë¶„/5ë¶„)
    predictions = system.predict(recent_data)
    
    # 5. ê²°ê³¼ ì¶œë ¥
    print("\n=== ì˜ˆì¸¡ ê²°ê³¼ ===")
    
    # ìœ„í—˜ë„ê°€ ë†’ì€ ì¼€ì´ìŠ¤ë§Œ ì¶œë ¥
    high_risk = predictions[predictions['xgboost_probability'] > 0.5]
    
    if not high_risk.empty:
        print(f"\nâš ï¸  ë³‘ëª© ìœ„í—˜ ê°ì§€: {len(high_risk)}ê±´")
        
        for _, pred in high_risk.iterrows():
            print(f"\n{'='*50}")
            print(f"ë¼ì¸: {pred['line_id']}")
            print(f"ì‹œê°„: {pred['timestamp']}")
            print(f"ìœ„í—˜ë„: {pred['risk_level']} ({pred['xgboost_probability']:.1%})")
            print(f"BRI ì§€ìˆ˜: {pred['bri']:.2f}")
            
            if 'diagnosis' in pred and pred['diagnosis']:
                diag = pred['diagnosis']
                print(f"\nğŸ“Š ì¢…í•© ì§„ë‹¨:")
                print(f"  - ë³‘ëª© í™•ë¥ : {diag['bottleneck_probability']:.1%}")
                print(f"  - ìœ„í—˜ë„ ì ìˆ˜: {diag['risk_score']:.2f}")
                
                print(f"\nğŸ” ê·¼ë³¸ ì›ì¸:")
                for i, cause in enumerate(diag['root_causes'], 1):
                    print(f"  {i}. {cause}")
                
                print(f"\nâš¡ ì¦‰ì‹œ ì¡°ì¹˜:")
                for i, action in enumerate(diag['immediate_actions'], 1):
                    print(f"  {i}. {action}")
                
                print(f"\nğŸ› ï¸ ì¥ê¸° í•´ê²°ì±…:")
                for i, solution in enumerate(diag['long_term_solutions'], 1):
                    print(f"  {i}. {solution}")
                
                if 'similar_cases' in pred and pred['similar_cases']:
                    print(f"\nğŸ“š ìœ ì‚¬ ì‚¬ë¡€ (ìƒìœ„ 3ê°œ):")
                    for i, case in enumerate(pred['similar_cases'][:3], 1):
                        print(f"  {i}. ìœ ì‚¬ë„: {case['similarity']:.1%}")
                        print(f"     - ì›ì¸: {case['root_cause']}")
                        print(f"     - í•´ê²°: {case['solution']}")
    else:
        print("\nâœ… í˜„ì¬ ëª¨ë“  ë¼ì¸ ì •ìƒ ìš´ì˜ ì¤‘")
    
    # 6. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
    print("\n\n=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")
    
    # XGBoost íŠ¹ì§• ì¤‘ìš”ë„
    importance = system.xgboost_model.get_feature_importance()
    print("\nXGBoost ì£¼ìš” íŠ¹ì§• (ìƒìœ„ 10ê°œ):")
    print(importance.head(10))
    
    # ì „ì²´ ì •í™•ë„
    X_test = system.xgboost_model.prepare_features(test_df)
    y_test = test_df['is_bottleneck']
    
    X_test_scaled = system.xgboost_model.scaler.transform(X_test)
    y_pred = system.xgboost_model.model.predict(X_test_scaled)
    
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    
    print(f"\nëª¨ë¸ ì„±ëŠ¥:")
    print(f"  - ì •í™•ë„: {accuracy:.1%}")
    print(f"  - ì •ë°€ë„: {precision:.1%}")
    print(f"  - ì¬í˜„ìœ¨: {recall:.1%}")
    print(f"  - F1 ì ìˆ˜: {f1:.3f}")
    
    # 7. ì‹œìŠ¤í…œ ìƒíƒœ ìš”ì•½
    print("\n=== ì‹œìŠ¤í…œ ìƒíƒœ ìš”ì•½ ===")
    print(f"ì´ ì²˜ë¦¬ ë°ì´í„°: {len(df):,} í–‰")
    print(f"í•™ìŠµ ë°ì´í„°: {len(df) - len(test_df):,} í–‰")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,} í–‰")
    print(f"RAG ì‚¬ë¡€ DB: {len(system.rag_system.case_database)} ê±´")
    print(f"ARIMA ëª¨ë¸: {len(system.time_series_models.arima_models)} ê°œ")
    print("\nâœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! ì‹¤ì‹œê°„ ì˜ˆì¸¡ ê°€ëŠ¥ ìƒíƒœì…ë‹ˆë‹¤.")
    print("ğŸ“Œ ì°¸ê³ : LLM ì—†ì´ ê·œì¹™ ê¸°ë°˜ ì§„ë‹¨ ì‹œìŠ¤í…œìœ¼ë¡œ ì‘ë™ ì¤‘")

if __name__ == "__main__":
    main()